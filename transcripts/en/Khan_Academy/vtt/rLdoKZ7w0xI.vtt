WEBVTT
Kind: captions
Language: en

00:00:00.370 --> 00:00:03.470
What I want to do in this video
is build up some tools

00:00:03.470 --> 00:00:07.720
in our tool kit for dealing with
sums and differences of

00:00:07.720 --> 00:00:08.740
random variables.

00:00:08.740 --> 00:00:13.650
So let's say that we have two
random variables, x and y, and

00:00:13.650 --> 00:00:15.660
they are completely
independent.

00:00:15.660 --> 00:00:20.280
They are independent
random variables.

00:00:24.770 --> 00:00:26.090
And I'm just going to
go over a little bit

00:00:26.090 --> 00:00:27.440
of a notation here.

00:00:27.440 --> 00:00:30.730
If we wanted to know the
expected, or if we talked

00:00:30.730 --> 00:00:34.960
about the expected value of this
random variable x, that

00:00:34.960 --> 00:00:39.670
is the same thing as the
mean value of this

00:00:39.670 --> 00:00:41.130
random variable x.

00:00:41.130 --> 00:00:48.540
If we talk about the expected
the value of y, that is the

00:00:48.540 --> 00:00:53.080
same thing as the mean of y.

00:00:53.080 --> 00:01:00.350
If we talk about the variance of
the random variable x, that

00:01:00.350 --> 00:01:05.670
is it the same thing as the
expected value of the squared

00:01:05.670 --> 00:01:11.210
distances between our random
variable x and its mean.

00:01:11.210 --> 00:01:13.270
And that right there squared.

00:01:13.270 --> 00:01:19.790
So the expected value of these
squared differences, and that

00:01:19.790 --> 00:01:24.120
you could also use the notation
sigma squared for the

00:01:24.120 --> 00:01:25.560
random variable x.

00:01:25.560 --> 00:01:27.670
This is just a review of things
we already know, but I

00:01:27.670 --> 00:01:30.920
just want to reintroduce it
because I'll use this to build

00:01:30.920 --> 00:01:32.770
up some of our tools.

00:01:32.770 --> 00:01:35.390
So you do the same thing with
this with random variable y.

00:01:35.390 --> 00:01:40.090
The variance of random variable
y is the expected

00:01:40.090 --> 00:01:44.030
value of the squared difference
between our random

00:01:44.030 --> 00:01:50.960
variable y and the
mean of y, or the

00:01:50.960 --> 00:01:53.790
expected value of y, squared.

00:01:53.790 --> 00:01:59.000
And that's the same thing
as sigma squared of y.

00:01:59.000 --> 00:02:01.570
There is the variance of y.

00:02:01.570 --> 00:02:05.180
Now you may or may not already
know these properties of

00:02:05.180 --> 00:02:07.360
expected values and variances,
but I will

00:02:07.360 --> 00:02:08.330
reintroduce them to you.

00:02:08.330 --> 00:02:10.500
And I won't go into some
rigorous proof-- actually, I

00:02:10.500 --> 00:02:12.890
think they're fairly
easy to digest.

00:02:12.890 --> 00:02:17.080
So one is is that if I have some
third random variable,

00:02:17.080 --> 00:02:19.100
let's say I have some third
random variable that is

00:02:19.100 --> 00:02:23.070
defined as being the random
variable x plus the random

00:02:23.070 --> 00:02:23.620
variable y.

00:02:23.620 --> 00:02:25.760
Let me stay with my
colors just so

00:02:25.760 --> 00:02:27.210
everything becomes clear.

00:02:27.210 --> 00:02:33.260
The random variable x plus
the random variable y.

00:02:33.260 --> 00:02:36.400
What is the expected value
of z going to be?

00:02:36.400 --> 00:02:40.720
The expected the value of z is
going to be equal to the

00:02:40.720 --> 00:02:44.090
expected value of x plus y.

00:02:44.090 --> 00:02:46.190
And this is a property of
expected values-- I'm not

00:02:46.190 --> 00:02:48.760
going to prove it rigorously
right here-- but the expected

00:02:48.760 --> 00:02:54.610
value of x plus the expected
value of y, or another way to

00:02:54.610 --> 00:02:58.940
think about this is that the
mean of z is going to be the

00:02:58.940 --> 00:03:04.340
mean of x plus the mean of y.

00:03:04.340 --> 00:03:08.220
Or another way to view it is if
I wanted to take, let's say

00:03:08.220 --> 00:03:10.240
I have some other
random variable.

00:03:10.240 --> 00:03:11.370
I'm running out of
letters here.

00:03:11.370 --> 00:03:14.760
Let's say I have the random
variable a, and I define

00:03:14.760 --> 00:03:18.790
random variable a
to be x minus y.

00:03:18.790 --> 00:03:21.280
So what's its expected
value going to be?

00:03:21.280 --> 00:03:24.830
The expected value of a is
going to be equal to the

00:03:24.830 --> 00:03:29.750
expected value of x minus y,
which is equal to-- you could

00:03:29.750 --> 00:03:32.920
either view it as the expected
value of x plus the expected

00:03:32.920 --> 00:03:38.970
value of negative y, or the
expected value of x minus the

00:03:38.970 --> 00:03:44.310
expected value of y, which is
the same thing as the mean of

00:03:44.310 --> 00:03:47.680
x minus the mean of y.

00:03:47.680 --> 00:03:52.520
So this is what the mean of
our random variable a

00:03:52.520 --> 00:03:54.180
would be equal to.

00:03:54.180 --> 00:03:56.700
And all of this is review and
I'm going to use this when we

00:03:56.700 --> 00:03:59.140
start talking about the
distributions that are sums

00:03:59.140 --> 00:04:01.580
and differences of other
distributions.

00:04:01.580 --> 00:04:05.610
Now let's think about what the
variance of random variable z

00:04:05.610 --> 00:04:09.260
is and what the variance of
random variable a is.

00:04:09.260 --> 00:04:18.149
So the variance of z-- and just
to kind of always focus

00:04:18.149 --> 00:04:20.200
back on the intuition,
it makes sense.

00:04:20.200 --> 00:04:24.880
If x is completely independent
of y and if I have some random

00:04:24.880 --> 00:04:27.780
variable that is the sum of
the two, then the expected

00:04:27.780 --> 00:04:32.440
value of that variable, of that
new variable, is going to

00:04:32.440 --> 00:04:35.530
be the sum of the expected
values of the other two

00:04:35.530 --> 00:04:37.440
because they are unrelated.

00:04:37.440 --> 00:04:41.690
If my expected value here is 5
and my expected value here is

00:04:41.690 --> 00:04:46.370
7, completely reasonable that my
expected value here is 12,

00:04:46.370 --> 00:04:48.500
assuming that they're completely
independent.

00:04:48.500 --> 00:04:54.780
Now if we have a situation, so
what is the variance of my

00:04:54.780 --> 00:04:56.070
random variable z?

00:04:56.070 --> 00:04:58.390
And once again, I'm not going do
a rigorous proof here, this

00:04:58.390 --> 00:05:01.040
is really just a property
of variances.

00:05:01.040 --> 00:05:03.840
But I'm going to use this to
establish what the variance of

00:05:03.840 --> 00:05:06.450
our random variable a is.

00:05:06.450 --> 00:05:11.870
So if this squared distance on
average is some variance, and

00:05:11.870 --> 00:05:14.150
this one is completely
independent, it's squared

00:05:14.150 --> 00:05:17.410
distance on average is some
distance, then the variance of

00:05:17.410 --> 00:05:21.360
their sum is actually going to
be the sum of their variances.

00:05:21.360 --> 00:05:29.520
So this is going to be equal
to the variance of random

00:05:29.520 --> 00:05:33.723
variable x plus the variance
of random variable y.

00:05:36.740 --> 00:05:42.870
Or another way of thinking about
it is that the variance

00:05:42.870 --> 00:05:52.800
of z, which is the same thing
as the variance of x plus y,

00:05:52.800 --> 00:06:02.570
is equal to the variance of x
plus the variance of random

00:06:02.570 --> 00:06:04.190
variable y.

00:06:04.190 --> 00:06:05.170
Hopefully that make
some sense.

00:06:05.170 --> 00:06:06.780
I'm not proving it to
you rigorously.

00:06:06.780 --> 00:06:09.130
And you'll see this in a lot
of statistics books.

00:06:09.130 --> 00:06:12.330
Now what I want to show you is
that the variance of random

00:06:12.330 --> 00:06:15.440
variable a is actually this
exact same thing.

00:06:15.440 --> 00:06:17.400
And that's the interesting
thing, because you might say,

00:06:17.400 --> 00:06:18.640
hey, why wouldn't it
be the difference?

00:06:18.640 --> 00:06:20.100
We had the differences
over here.

00:06:20.100 --> 00:06:22.330
So let's experiment with
this a little bit.

00:06:22.330 --> 00:06:28.670
The variance-- so I'll just
write this-- the variance of

00:06:28.670 --> 00:06:32.750
random variable a is the same
thing as the variance of--

00:06:32.750 --> 00:06:39.110
I'll write it like this-- as x
minus y, which is equal to--

00:06:39.110 --> 00:06:45.270
you could view it this way--
which is equal to the variance

00:06:45.270 --> 00:06:50.630
of x plus negative y.

00:06:50.630 --> 00:06:53.240
These are equivalent
statements.

00:06:53.240 --> 00:06:56.550
So you could view this as being
equal to-- just using

00:06:56.550 --> 00:06:59.610
this over here, the sum of these
two variances, so it's

00:06:59.610 --> 00:07:05.050
going to be equal to the sum of
the variance of x plus the

00:07:05.050 --> 00:07:08.950
variance of negative y.

00:07:08.950 --> 00:07:11.100
Now what I need to show you is
that the variance of negative

00:07:11.100 --> 00:07:13.340
y, of the negative of that
random variables are going to

00:07:13.340 --> 00:07:15.750
be the same thing as
the variance of y.

00:07:15.750 --> 00:07:18.280
So what is the variance
of negative y?

00:07:18.280 --> 00:07:23.010
The variance of negative y is
the same thing as the variance

00:07:23.010 --> 00:07:32.060
of negative y, which is equal
to the expected value of the

00:07:32.060 --> 00:07:42.370
distance between negative y
and the expected value of

00:07:42.370 --> 00:07:45.970
negative y squared.

00:07:45.970 --> 00:07:50.080
That's all the variance
actually is.

00:07:50.080 --> 00:07:56.650
Now what is the expected value
of negative y right over here?

00:07:56.650 --> 00:07:59.240
Actually, even better let me
factor out a negative 1.

00:07:59.240 --> 00:08:01.420
So what's in the parentheses
right here, this is the exact

00:08:01.420 --> 00:08:07.460
same thing as negative 1 squared
times y plus the

00:08:07.460 --> 00:08:11.430
expected value of negative y.

00:08:11.430 --> 00:08:13.110
So that's the same exact
same thing in the

00:08:13.110 --> 00:08:14.670
parentheses, squared.

00:08:14.670 --> 00:08:17.120
So everything in magenta is
everything in magenta here,

00:08:17.120 --> 00:08:19.090
and it is the expected
value of that thing.

00:08:22.610 --> 00:08:26.956
Now what is the expected
value of negative y?

00:08:26.956 --> 00:08:30.390
The expected value of negative
y-- I'll do it over here-- the

00:08:30.390 --> 00:08:33.770
expected value of the negative
of a random variable is just a

00:08:33.770 --> 00:08:37.000
negative of the expected value
of that random variable.

00:08:37.000 --> 00:08:40.350
So if you look at this we can
re-write this-- I'll give

00:08:40.350 --> 00:08:42.860
myself a little bit more space--
we can re-write this

00:08:42.860 --> 00:08:46.240
as the expected value-- the
variance of negative y is the

00:08:46.240 --> 00:08:47.850
expected value--
this is just 1.

00:08:47.850 --> 00:08:49.930
Negative 1 squared is just 1.

00:08:49.930 --> 00:08:54.750
And over here you have y, and
instead just write plus the

00:08:54.750 --> 00:08:57.530
expected value of negative y,
that's the same thing as minus

00:08:57.530 --> 00:09:00.270
the expected value of y.

00:09:00.270 --> 00:09:03.140
So you have that, and then
all of that squared.

00:09:03.140 --> 00:09:10.160
Now notice, this is the exact
same thing by definition as

00:09:10.160 --> 00:09:13.020
the variance of y.

00:09:13.020 --> 00:09:15.000
So what we just showed you
just now, so this is the

00:09:15.000 --> 00:09:15.720
variance of y.

00:09:15.720 --> 00:09:22.340
So we just showed you is that
the variance of the difference

00:09:22.340 --> 00:09:27.770
of two independent random
variables is equal to the sum

00:09:27.770 --> 00:09:30.950
of the variances.

00:09:30.950 --> 00:09:33.810
You could definitely believe
this, it's equal to the sum of

00:09:33.810 --> 00:09:37.090
the variance of the first one
plus the variance of the

00:09:37.090 --> 00:09:38.720
negative of the second one.

00:09:38.720 --> 00:09:41.250
And we just showed that that
variance is the same thing as

00:09:41.250 --> 00:09:43.510
the variance of the positive
version of that variable,

00:09:43.510 --> 00:09:44.230
which makes sense.

00:09:44.230 --> 00:09:47.810
Your distance from the mean is
going to be-- it doesn't

00:09:47.810 --> 00:09:49.610
matter whether you're taking the
positive or the negative

00:09:49.610 --> 00:09:50.020
of the variable.

00:09:50.020 --> 00:09:51.410
You just cared about
absolute distance.

00:09:51.410 --> 00:09:54.510
So it makes complete sense that
that quantity and that

00:09:54.510 --> 00:09:56.860
quantity is going to
be the same thing.

00:09:56.860 --> 00:09:59.040
Now the whole reason why I went
through this exercise,

00:09:59.040 --> 00:10:03.360
kind of the important takeaways
here is that the

00:10:03.360 --> 00:10:08.810
mean of differences right over
here-- so I could re-write it

00:10:08.810 --> 00:10:12.650
as the differences of the random
variable is the same

00:10:12.650 --> 00:10:14.760
thing as the differences
of their means.

00:10:14.760 --> 00:10:16.600
And then the other important
takeaway, and I'm going to

00:10:16.600 --> 00:10:20.220
build on this in the next few
videos, is that the variance

00:10:20.220 --> 00:10:24.350
of the difference-- if I define
a new random variable

00:10:24.350 --> 00:10:27.420
is the difference of two other
random variables, the variance

00:10:27.420 --> 00:10:29.790
of that random variable is
actually the sum of the

00:10:29.790 --> 00:10:31.850
variances of the two
random variables.

00:10:31.850 --> 00:10:35.030
So these are the two important
takeaways that we'll use to

00:10:35.030 --> 00:10:37.010
build on in future videos.

00:10:37.010 --> 00:10:39.930
Anyway, hopefully that
wasn't too confusing.

00:10:39.930 --> 00:10:42.270
If it was, you can kind of
just accept these at face

00:10:42.270 --> 00:10:44.430
value and just assume
that these are tools

00:10:44.430 --> 00:10:46.090
that you can use.

