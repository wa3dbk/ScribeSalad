WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.780
 in this capsule we will see the concept 

00:00:02.100 --> 00:00:07.980
 of generalization of the flu 

00:00:03.780 --> 00:00:09.480
 learning then we saw different 

00:00:07.980 --> 00:00:11.969
 learning apparatus commented this 

00:00:09.480 --> 00:00:13.950
 that they are able to learn to 

00:00:11.969 --> 00:00:15.690
 from data from the set 

00:00:13.950 --> 00:00:18.090
 training now commented this 

00:00:15.690 --> 00:00:19.439
 that we evaluate the success of the algorithm to 

00:00:18.090 --> 00:00:20.699
 find us a model that will us 

00:00:19.439 --> 00:00:22.789
 allow to make predictions that 

00:00:20.699 --> 00:00:24.900
 are valid 

00:00:22.789 --> 00:00:26.039
 intuitively the first thing to 

00:00:24.900 --> 00:00:29.340
 which one could think of is 

00:00:26.039 --> 00:00:31.980
 look at their ways but committed on 

00:00:29.340 --> 00:00:33.420
 training examples 

00:00:31.980 --> 00:00:36.149
 however this mistake there she goes 

00:00:33.420 --> 00:00:40.290
 necessarily is optimistic why 

00:00:36.149 --> 00:00:42.750
 but from a higher level we know that 

00:00:40.290 --> 00:00:45.090
 our model h the model that makes them 

00:00:42.750 --> 00:00:46.920
 predictions and had a very exciting 

00:00:45.090 --> 00:00:49.370
 who was calculating hdx to make a 

00:00:46.920 --> 00:00:52.260
 prediction of the target I'm going to have 

00:00:49.370 --> 00:00:53.640
 actually hot already seen the right answer 

00:00:52.260 --> 00:00:57.120
 on these examples 

00:00:53.640 --> 00:00:58.590
 ok so in fact all we are in 

00:00:57.120 --> 00:01:00.989
 front train and measure the capacity 

00:00:58.590 --> 00:01:04.470
 of a gorith atom to find a model 

00:01:00.989 --> 00:01:07.590
 who is able to memorize parker 

00:01:04.470 --> 00:01:10.170
 what is the answer for them so the 

00:01:07.590 --> 00:01:13.380
 target y for external examples to his 

00:01:10.170 --> 00:01:15.420
 drive for example if we take the 

00:01:13.380 --> 00:01:17.820
 coc case closer neighbors but 

00:01:15.420 --> 00:01:19.470
 for that equals 1 to do on the room 

00:01:17.820 --> 00:01:21.540
 drive his mistake is always going 

00:01:19.470 --> 00:01:24.030
 to be zero because if I give him 

00:01:21.540 --> 00:01:26.909
 an andrée xx that is in the 

00:01:24.030 --> 00:01:29.070
 training room and well then if 

00:01:26.909 --> 00:01:29.970
 all my entries are always good 

00:01:29.070 --> 00:01:32.610
 labeled as a whole 

00:01:29.970 --> 00:01:34.320
 training but necessarily because 

00:01:32.610 --> 00:01:36.150
 that he memorized essentially 

00:01:34.320 --> 00:01:37.200
 remember all the examples 

00:01:36.150 --> 00:01:39.600
 drive he will be able to find 

00:01:37.200 --> 00:01:40.799
 the one that fits my entry is ok 

00:01:39.600 --> 00:01:43.110
 to be his closest neighbor for a 

00:01:40.799 --> 00:01:45.060
 distance from zero with that there and goes 

00:01:43.110 --> 00:01:47.939
 necessarily well predict his ethics 

00:01:45.060 --> 00:01:49.049
 ok so the drive error is 

00:01:47.939 --> 00:01:50.700
 really not a good measure for 

00:01:49.049 --> 00:01:52.680
 know which point an algorithm 

00:01:50.700 --> 00:01:54.000
 learning find a good model 

00:01:52.680 --> 00:01:56.219
 since we can have an age 

00:01:54.000 --> 00:01:57.930
 who makes good predictions made 

00:01:56.219 --> 00:01:59.610
 what really interests us is the 

00:01:57.930 --> 00:02:02.399
 ability of an algorithm to find a 

00:01:59.610 --> 00:02:05.490
 model that will generalize about 

00:02:02.399 --> 00:02:07.390
 new examples and finally sahara does 

00:02:05.490 --> 00:02:09.210
 much better 

00:02:07.390 --> 00:02:11.290
 the context in which we will use 

00:02:09.210 --> 00:02:12.459
 people back learning little 

00:02:11.290 --> 00:02:14.680
 model driven by an organ 

00:02:12.459 --> 00:02:16.720
 learning so if I'm trying to 

00:02:14.680 --> 00:02:18.040
 solve to render on data by 

00:02:16.720 --> 00:02:19.690
 example a neural network that makes 

00:02:18.040 --> 00:02:20.920
 of character recognition me 

00:02:19.690 --> 00:02:23.050
 what interests me is to know 

00:02:20.920 --> 00:02:25.000
 solve it further if I deploy it in 

00:02:23.050 --> 00:02:27.820
 an application then he receives from 

00:02:25.000 --> 00:02:29.260
 new new character images 

00:02:27.820 --> 00:02:30.790
 who will be able to recognize these 

00:02:29.260 --> 00:02:32.110
 new images there a new 

00:02:30.790 --> 00:02:33.940
 Sunday and character and a risk 

00:02:32.110 --> 00:02:35.560
 to be different anyway 

00:02:33.940 --> 00:02:38.290
 exactly the same as those that the 

00:02:35.560 --> 00:02:39.459
 world training set so that's 

00:02:38.290 --> 00:02:42.630
 what's called the ability to 

00:02:39.459 --> 00:02:45.430
 generalize to new examples and 

00:02:42.630 --> 00:02:47.050
 to measure this ability to 

00:02:45.430 --> 00:02:48.670
 generalization there what are we going to do in 

00:02:47.050 --> 00:02:52.630
 practice is that we will put 

00:02:48.670 --> 00:02:53.830
 examples enacted aside so they go 

00:02:52.630 --> 00:02:55.870
 not to be in the whole 

00:02:53.830 --> 00:02:59.080
 training and 6 example that I would have 

00:02:55.870 --> 00:03:00.640
 put aside I will call them the 

00:02:59.080 --> 00:03:01.930
 examples from my test set 

00:03:00.640 --> 00:03:03.100
 I'm going to create a set that we're going to 

00:03:01.930 --> 00:03:05.350
 call the test set that is going 

00:03:03.100 --> 00:03:06.670
 contain examples that are not in 

00:03:05.350 --> 00:03:09.489
 my training set that will be 

00:03:06.670 --> 00:03:12.810
 not used to train my model 

00:03:09.489 --> 00:03:12.810
 with my seasoned to take 7 

00:03:13.350 --> 00:03:17.799
 so what is the relationship between 

00:03:16.030 --> 00:03:22.540
 the drive error the error of 

00:03:17.799 --> 00:03:24.519
 tests in practice actually for example if 

00:03:22.540 --> 00:03:26.230
 we had several neural networks 

00:03:24.519 --> 00:03:26.950
 with a number of neurons cachet 

00:03:26.230 --> 00:03:28.450
 different 

00:03:26.950 --> 00:03:29.970
 so we trained a network of neurons 

00:03:28.450 --> 00:03:32.680
 they have with one layer hide and 

00:03:29.970 --> 00:03:36.220
 5.9 in cash and in the 19th rank to hide 

00:03:32.680 --> 00:03:38.380
 therefore 5.9 in cash and 10 9 in cash and 15.9 

00:03:36.220 --> 00:03:41.890
 in cash and so killed ten years up to 109 

00:03:38.380 --> 00:03:44.049
 in cash and that we can expect 

00:03:41.890 --> 00:03:46.600
 level of the difference between the error 

00:03:44.049 --> 00:03:48.130
 of classification that I'm going to measure this 

00:03:46.600 --> 00:03:49.720
 moment in training and the one that I 

00:03:48.130 --> 00:03:52.510
 am going to measure on my a no doubt is this 

00:03:49.720 --> 00:03:54.280
 that I'm going to call the test error this 

00:03:52.510 --> 00:03:57.489
 what we are going to observe is 

00:03:54.280 --> 00:04:00.100
 firstly the training ro vote 

00:03:57.489 --> 00:04:01.959
 mans horseback riding to be smaller than 

00:04:00.100 --> 00:04:05.560
 the test error so the number 

00:04:01.959 --> 00:04:07.150
 of errors that will make no my network 

00:04:05.560 --> 00:04:09.459
 neurons regardless of the number of 

00:04:07.150 --> 00:04:11.170
 hidden areas that I'm going to use on 

00:04:09.459 --> 00:04:12.790
 the training area this number of 

00:04:11.170 --> 00:04:15.550
 relay will always be smaller than 

00:04:12.790 --> 00:04:17.709
 on the test set ok so here the 

00:04:15.550 --> 00:04:20.049
 dotted curve is the error of 

00:04:17.709 --> 00:04:22.960
 test for different by dunod 

00:04:20.049 --> 00:04:27.580
 laurent to hide then the full curve 

00:04:22.960 --> 00:04:29.789
 that's for the test set so 

00:04:27.580 --> 00:04:31.860
 you were in the gym 

00:04:29.789 --> 00:04:34.090
 lines full of teeth no doubt is this 

00:04:31.860 --> 00:04:37.150
 just the first thing he always goes there 

00:04:34.090 --> 00:04:39.129
 have a space so a difference 

00:04:37.150 --> 00:04:40.360
 between workouts and and celeron 

00:04:39.129 --> 00:04:43.479
 drive is always worth a lower 

00:04:40.360 --> 00:04:45.340
 what the sco will also see is at 

00:04:43.479 --> 00:04:47.169
 as we go up 

00:04:45.340 --> 00:04:49.150
 number of neurons actually hide this 

00:04:47.169 --> 00:04:51.729
 difference the antrum the two courses is going 

00:04:49.150 --> 00:04:54.400
 to grow so as we 

00:04:51.729 --> 00:04:57.009
 is going to have more neurons hide we're going 

00:04:54.400 --> 00:05:00.039
 see the difference between the two 

00:04:57.009 --> 00:05:05.500
 curves get bigger at the point that it goes there 

00:05:00.039 --> 00:05:08.379
 possibly have a point where there in 

00:05:05.500 --> 00:05:10.719
 made the mistake of tests going to start 

00:05:08.379 --> 00:05:12.370
 increase because the rounds 

00:05:10.719 --> 00:05:14.289
 drive she's always going down 

00:05:12.370 --> 00:05:17.409
 their workout decreases because 

00:05:14.289 --> 00:05:19.210
 our neural network for different 

00:05:17.409 --> 00:05:21.340
 number of hiding neurons they are trying 

00:05:19.210 --> 00:05:22.930
 to minimize this maple more I him 

00:05:21.340 --> 00:05:24.580
 give nothing to hide anymore there is 

00:05:22.930 --> 00:05:26.380
 memory are wants more is able to 

00:05:24.580 --> 00:05:27.880
 memorize information before center 

00:05:26.380 --> 00:05:29.229
 drive so more be able 

00:05:27.880 --> 00:05:32.110
 to have a small child mistake 

00:05:29.229 --> 00:05:34.719
 on the other hand, on the test error 

00:05:32.110 --> 00:05:37.419
 will observe is that in fact the 

00:05:34.719 --> 00:05:39.430
 performance on the stage error 

00:05:37.419 --> 00:05:40.810
 drive is going to be less have 

00:05:39.430 --> 00:05:43.509
 thought less and less the 

00:05:40.810 --> 00:05:45.340
 performance on our set of tests 

00:05:43.509 --> 00:05:46.569
 so why is it seen on 

00:05:45.340 --> 00:05:50.199
 why are the breeds but 

00:05:46.569 --> 00:05:51.879
 increase a bath it's a concept that 

00:05:50.199 --> 00:05:53.680
 is a little difficult to grasp well a 

00:05:51.879 --> 00:05:55.750
 costume this would be the case where I would have a 

00:05:53.680 --> 00:05:57.039
 training set where there would be 

00:05:55.750 --> 00:05:58.960
 some of the examples would be wrong 

00:05:57.039 --> 00:06:01.120
 labeled so because my networks of 

00:05:58.960 --> 00:06:03.250
 neurons for a different number 

00:06:01.120 --> 00:06:04.659
 of neurons hide trying to memorize 

00:06:03.250 --> 00:06:07.000
 information through training 

00:06:04.659 --> 00:06:08.469
 but if I give a lot of neurons it 

00:06:07.000 --> 00:06:11.259
 is going to need to memorize these errors 

00:06:08.469 --> 00:06:13.870
 there and on mezin example in my 

00:06:11.259 --> 00:06:16.060
 set of tests that may have been 

00:06:13.870 --> 00:06:16.960
 well labeled for some examples 

00:06:16.060 --> 00:06:19.990
 lodges 

00:06:16.960 --> 00:06:22.090
 I would certainly have a general 

00:06:19.990 --> 00:06:25.240
 what we are going to observe is that 

00:06:22.090 --> 00:06:26.800
 if there are certain variations does 

00:06:25.240 --> 00:06:29.050
 to memorize these variations 

00:06:26.800 --> 00:06:31.330
 the other one in the whole workouts that 

00:06:29.050 --> 00:06:33.699
 not going to bring my neural network to 

00:06:31.330 --> 00:06:36.340
 to learn what is the reason 

00:06:33.699 --> 00:06:37.449
 fundamental that makes that one by 

00:06:36.340 --> 00:06:39.639
 example a character image 

00:06:37.449 --> 00:06:43.000
 corresponds to two ways will put 

00:06:39.639 --> 00:06:45.009
 then on trained on details 

00:06:43.000 --> 00:06:47.800
 associated these examples that not allow 

00:06:45.009 --> 00:06:51.580
 to generalize to examples of 

00:06:47.800 --> 00:06:54.190
 test so what are we going to say in fact this is 

00:06:51.580 --> 00:06:56.889
 that this number of neurons hidden here here 

00:06:54.190 --> 00:06:58.840
 which is a bit the ideal ie 

00:06:56.889 --> 00:07:00.370
 if I do not do it anymore I would have asked 

00:06:58.840 --> 00:07:01.750
 my test error but if I had 

00:07:00.370 --> 00:07:03.759
 does it less I will have a test error 

00:07:01.750 --> 00:07:07.030
 worry because was bigger so if I 

00:07:03.759 --> 00:07:07.720
 find me to the left of this point there we go 

00:07:07.030 --> 00:07:09.820
 to say that I am below 

00:07:07.720 --> 00:07:12.460
 learning not say that I could 

00:07:09.820 --> 00:07:14.020
 increase the capacity of my model by 

00:07:12.460 --> 00:07:15.970
 example to neural networks and then 

00:07:14.020 --> 00:07:17.409
 improve my performance on my 

00:07:15.970 --> 00:07:20.110
 set of tests 

00:07:17.409 --> 00:07:21.610
 I find myself right on the e-v time on 

00:07:20.110 --> 00:07:24.310
 learning ie we have 

00:07:21.610 --> 00:07:25.270
 started learning from the parkers of 

00:07:24.310 --> 00:07:27.490
 details that are overall 

00:07:25.270 --> 00:07:30.699
 training and that does not put well 

00:07:27.490 --> 00:07:33.940
 predict well performed on my old 

00:07:30.699 --> 00:07:38.400
 nantais what made me me but the least 

00:07:33.940 --> 00:07:40.630
 well performed on my set so for 

00:07:38.400 --> 00:07:41.650
 a neural network at a different 

00:07:40.630 --> 00:07:44.020
 in cash and that's what we're going 

00:07:41.650 --> 00:07:47.469
 observe more have neurons cachet 

00:07:44.020 --> 00:07:49.180
 the more I will be able to optimize my 

00:07:47.469 --> 00:07:50.590
 drive error possibly I 

00:07:49.180 --> 00:07:52.479
 will reach a point where I'm going 

00:07:50.590 --> 00:07:54.430
 pass under learning ensures 

00:07:52.479 --> 00:07:56.139
 learning where I'm going but will put 

00:07:54.430 --> 00:08:00.820
 less well performed at risk my 

00:07:56.139 --> 00:08:03.219
 performance on my test set on 

00:08:00.820 --> 00:08:04.960
 could say the same thing but on the 

00:08:03.219 --> 00:08:06.789
 number of iterations of a network of 

00:08:04.960 --> 00:08:08.560
 hikes with a fixed man from 

00:08:06.789 --> 00:08:10.870
 neurons hide so if I take my 

00:08:08.560 --> 00:08:12.759
 optimization algorithm by december 

00:08:10.870 --> 00:08:14.229
 stochastic gaza we want to measure 

00:08:12.759 --> 00:08:15.849
 that I'm going to train a currency makes 

00:08:14.229 --> 00:08:17.860
 your minuee necessarily because 

00:08:15.849 --> 00:08:19.270
 that's what I'm trying to optimize by 

00:08:17.860 --> 00:08:21.580
 against it will happen a number 

00:08:19.270 --> 00:08:23.740
 of iterations there I'm actually going to 

00:08:21.580 --> 00:08:25.599
 take particularities my old 

00:08:23.740 --> 00:08:27.639
 drive that will not allow me 

00:08:25.599 --> 00:08:29.340
 to generalize on my agenda 

00:08:27.639 --> 00:08:31.770
 tests but which in the reverse 

00:08:29.340 --> 00:08:33.630
 empire and my performances on my 

00:08:31.770 --> 00:08:38.370
 examples that are different that are 

00:08:33.630 --> 00:08:42.750
 in all of all and so 

00:08:38.370 --> 00:08:45.690
 general so sure my axis dxy if I 

00:08:42.750 --> 00:08:48.120
 put any parameter option 

00:08:45.690 --> 00:08:51.270
 from my grandmother's apprenticeship who 

00:08:48.120 --> 00:08:53.670
 gives more ability to memorize this 

00:08:51.270 --> 00:08:55.890
 that in the training set this 

00:08:53.670 --> 00:08:57.570
 that I observed these two curves it is 

00:08:55.890 --> 00:08:58.650
 to say a curve for ranks 

00:08:57.570 --> 00:09:01.080
 training that constantly sees 

00:08:58.650 --> 00:09:03.300
 decreased but for the test dresses so 

00:09:01.080 --> 00:09:04.860
 my generalization performance that is going 

00:09:03.300 --> 00:09:08.400
 decrease but eventually who will 

00:09:04.860 --> 00:09:11.010
 put to increase and so if I have a 

00:09:08.400 --> 00:09:13.320
 regret is that 4 of the options that allows 

00:09:11.010 --> 00:09:15.240
 to control the ability of my model to 

00:09:13.320 --> 00:09:17.220
 memorize the data what I would like 

00:09:15.240 --> 00:09:18.540
 it is to find this point which is the 

00:09:17.220 --> 00:09:21.150
 point where I have the best compromise 

00:09:18.540 --> 00:09:22.950
 between soft or learning and on 

00:09:21.150 --> 00:09:27.120
 learning that is found at 

00:09:22.950 --> 00:09:29.160
 to be there points that separate diets 

00:09:27.120 --> 00:09:31.430
 source or learning and on 

00:09:29.160 --> 00:09:31.430
 learning 

00:09:32.660 --> 00:09:39.360
 ok so those options there that I'm talking about 

00:09:37.560 --> 00:09:40.980
 and that allows me to control the 

00:09:39.360 --> 00:09:42.089
 ability to memorize on my set 

00:09:40.980 --> 00:09:43.589
 training 

00:09:42.089 --> 00:09:47.010
 if what we will call hypers 

00:09:43.589 --> 00:09:49.320
 so allow the options but 

00:09:47.010 --> 00:09:53.100
 for the nearest neighbors eco label 

00:09:49.320 --> 00:09:55.260
 that would be the case value for the 

00:09:53.100 --> 00:09:56.940
 hut of the collector to swells algre 

00:09:55.260 --> 00:09:59.459
 logistic regression 

00:09:56.940 --> 00:10:01.500
 I have to if I want to run it determine 

00:09:59.459 --> 00:10:02.880
 what is the alpha learning rate 

00:10:01.500 --> 00:10:05.670
 what is the number of iterations 

00:10:02.880 --> 00:10:06.779
 also for a neural network that 

00:10:05.670 --> 00:10:08.339
 would it be the rate 

00:10:06.779 --> 00:10:09.900
 iphone learning the number 

00:10:08.339 --> 00:10:11.400
 iterations but also the number of 

00:10:09.900 --> 00:10:13.950
 neurons hide the number of layers 

00:10:11.400 --> 00:10:15.360
 hide or if the choice of the function 

00:10:13.950 --> 00:10:17.610
 activation we saw only the 

00:10:15.360 --> 00:10:20.190
 sigmoid but overall it's 

00:10:17.610 --> 00:10:22.140
 possible of this high function so 

00:10:20.190 --> 00:10:24.480
 all these are options that I have 

00:10:22.140 --> 00:10:26.100
 specified hamon has little learning 

00:10:24.480 --> 00:10:27.959
 so that I can execute it and its 

00:10:26.100 --> 00:10:30.570
 options there we will call them zips 

00:10:27.959 --> 00:10:33.420
 apparently so it's for the 

00:10:30.570 --> 00:10:34.259
 distinguish parameters from a model 

00:10:33.420 --> 00:10:36.389
 as 

00:10:34.259 --> 00:10:38.879
 a neural network that would have said that 

00:10:36.389 --> 00:10:40.319
 weights that the weight of 

00:10:38.879 --> 00:10:41.759
 connections between neurons worry 

00:10:40.319 --> 00:10:44.009
 parameters that are adjusted by 

00:10:41.759 --> 00:10:46.079
 pride learning the parameters 

00:10:44.009 --> 00:10:47.459
 and option plans that are not 

00:10:46.079 --> 00:10:51.209
 adjusted by learning flu 

00:10:47.459 --> 00:10:53.729
 we must specify aprilia so 

00:10:51.209 --> 00:10:56.160
 comment what his settings choose 

00:10:53.729 --> 00:10:58.979
 there to avoid doing the over 

00:10:56.160 --> 00:10:59.970
 learning then or doing sudan 

00:10:58.979 --> 00:11:04.109
 learning to have the best 

00:10:59.970 --> 00:11:07.049
 compromise on the good learning da 

00:11:04.109 --> 00:11:08.729
 I can not choose from the eren 

00:11:07.049 --> 00:11:10.559
 drive choose camera option 

00:11:08.729 --> 00:11:12.809
 soaping 

00:11:10.559 --> 00:11:14.009
 necessarily hard learning 

00:11:12.809 --> 00:11:15.899
 so we have seen for example the number 

00:11:14.009 --> 00:11:17.879
 Iterations more I have but more my 

00:11:15.899 --> 00:11:19.619
 training ground your base so 

00:11:17.879 --> 00:11:21.089
 I would simply choose number 

00:11:19.619 --> 00:11:23.160
 of iterations that is bigger than them 

00:11:21.089 --> 00:11:24.419
 but I could actually so it's going 

00:11:23.160 --> 00:11:26.189
 bring necessarily hard learning 

00:11:24.419 --> 00:11:29.759
 a point where they eventually want 

00:11:26.189 --> 00:11:31.049
 the test rom will increase today 

00:11:29.759 --> 00:11:32.249
 only to the nearest neighbors if 

00:11:31.049 --> 00:11:34.949
 I used the training role 

00:11:32.249 --> 00:11:37.109
 since klm gives an error of 0 

00:11:34.949 --> 00:11:38.910
 I will always use cepoun 

00:11:37.109 --> 00:11:41.730
 oath what is the best for good 

00:11:38.910 --> 00:11:43.679
 generalize it would be very tempting 

00:11:41.730 --> 00:11:45.179
 to use my set of tests that 

00:11:43.679 --> 00:11:47.129
 I set aside to measure the 

00:11:45.179 --> 00:11:50.939
 generalization ability but that and 

00:11:47.129 --> 00:11:52.379
 it's very important it would be cheating 

00:11:50.939 --> 00:11:56.100
 these examples of tesco it really has to 

00:11:52.379 --> 00:11:58.559
 represent examples that I 

00:11:56.100 --> 00:12:01.129
 do not know yet and who 

00:11:58.559 --> 00:12:03.119
 would face my system 

00:12:01.129 --> 00:12:04.230
 when they would be deployed so I 

00:12:03.119 --> 00:12:06.839
 should not use it nor could 

00:12:04.230 --> 00:12:08.359
 coach nor to choose my options if 

00:12:06.839 --> 00:12:14.399
 no actually I could choose 

00:12:08.359 --> 00:12:15.660
 so in fact choose my examples me the 

00:12:14.399 --> 00:12:17.909
 value of the hyper parameters based 

00:12:15.660 --> 00:12:20.249
 of the set of tests so kind of like 

00:12:17.909 --> 00:12:22.409
 driven these options the seizure is not going 

00:12:20.249 --> 00:12:22.769
 put the food on my set of 

00:12:22.409 --> 00:12:26.339
 tests 

00:12:22.769 --> 00:12:28.139
 so that would be it would give me a 

00:12:26.339 --> 00:12:29.479
 optimistic measure of my ability to 

00:12:28.139 --> 00:12:32.249
 generalize 

00:12:29.479 --> 00:12:34.169
 so in fact what are we going to do a little 

00:12:32.249 --> 00:12:36.269
 as for the tabs test set 

00:12:34.169 --> 00:12:38.759
 different examples of the room 

00:12:36.269 --> 00:12:40.019
 drive the shadow of the examples are 

00:12:38.759 --> 00:12:41.339
 different sets of test and 

00:12:40.019 --> 00:12:42.640
 the coach set so we're going to have 

00:12:41.339 --> 00:12:43.960
 another set 

00:12:42.640 --> 00:12:46.720
 put aside we will call it 

00:12:43.960 --> 00:12:48.040
 the hockey validation set and so 

00:12:46.720 --> 00:12:52.690
 it will allow us not to use 

00:12:48.040 --> 00:12:54.310
 the test set for because I 

00:12:52.690 --> 00:12:55.570
 want to keep it without using it I'm going 

00:12:54.310 --> 00:12:58.720
 use it only the very end for 

00:12:55.570 --> 00:12:59.860
 measure the validation performance that 

00:12:58.720 --> 00:13:01.480
 lets me not use the whole 

00:12:59.860 --> 00:13:03.070
 drive also they 

00:13:01.480 --> 00:13:03.960
 would necessarily lead to hard 

00:13:03.070 --> 00:13:07.120
 learning 

00:13:03.960 --> 00:13:09.040
 so my validation set that's 

00:13:07.120 --> 00:13:12.610
 only goal is going to be to determine the 

00:13:09.040 --> 00:13:14.260
 father values ​​parameters and a destiny 

00:13:12.610 --> 00:13:17.140
 different by the way had to do settings 

00:13:14.260 --> 00:13:19.450
 on and evaluating it on performance 

00:13:17.140 --> 00:13:21.160
 on the validation set in fact one 

00:13:19.450 --> 00:13:23.110
 can see some form 

00:13:21.160 --> 00:13:24.910
 learning we will adapt my game 

00:13:23.110 --> 00:13:27.250
 our owl various settings but 

00:13:24.910 --> 00:13:31.510
 on the validation data rather than 

00:13:27.250 --> 00:13:34.620
 the input data ok so here we have the 

00:13:31.510 --> 00:13:37.720
 description of an evaluation procedure 

00:13:34.620 --> 00:13:40.390
 complete on a set despite 

00:13:37.720 --> 00:13:42.220
 learning and wishes to evaluate at 

00:13:40.390 --> 00:13:44.470
 what a point this tiger and allows them to 

00:13:42.220 --> 00:13:46.690
 to get the model that perform well 

00:13:44.470 --> 00:13:49.810
 sure of new outfits what are we going 

00:13:46.690 --> 00:13:51.070
 to do is we will take all our 

00:13:49.810 --> 00:13:53.170
 data we collected we want them 

00:13:51.070 --> 00:13:54.640
 divide into three sets my whole 

00:13:53.170 --> 00:13:57.790
 workout my set of 

00:13:54.640 --> 00:13:59.800
 validation and my set of tests 

00:13:57.790 --> 00:14:01.120
 an example of separation this would be 

00:13:59.800 --> 00:14:03.520
 take seventy for that of 

00:14:01.120 --> 00:14:06.190
 examples for training 15% for the 

00:14:03.520 --> 00:14:07.600
 15% validation for the test 

00:14:06.190 --> 00:14:10.540
 so we could have chosen other 

00:14:07.600 --> 00:14:11.890
 percentage it's an example that I 

00:14:10.540 --> 00:14:13.360
 I often use practices then who 

00:14:11.890 --> 00:14:16.270
 walk pretty well 

00:14:13.360 --> 00:14:18.910
 and so I have all my examples but I 

00:14:16.270 --> 00:14:20.410
 must you also me example I'm going 

00:14:18.910 --> 00:14:23.170
 represents by this rectangle 

00:14:20.410 --> 00:14:25.450
 I could take 70% for 

00:14:23.170 --> 00:14:30.400
 15% training for validation 

00:14:25.450 --> 00:14:32.230
 then 15% for the test then and that's 

00:14:30.400 --> 00:14:33.760
 what I'm going to do is a list of 

00:14:32.230 --> 00:14:36.790
 all the values ​​said not to promise 

00:14:33.760 --> 00:14:38.790
 that I should try so if I want 

00:14:36.790 --> 00:14:41.970
 lead a 

00:14:38.790 --> 00:14:43.110
 the flu of a model with the algorithm 

00:14:41.970 --> 00:14:45.210
 the taxman 

00:14:43.110 --> 00:14:47.790
 I could be choosing the rate 

00:14:45.210 --> 00:14:51.210
 learning for example 0.1 then to 

00:14:47.790 --> 00:14:54.300
 try to say 100 iteration where I 

00:14:51.210 --> 00:14:56.520
 may take 0.01 pieces and also for 

00:14:54.300 --> 00:14:58.740
 santhera sion I could try 

00:14:56.520 --> 00:15:01.230
 other fathers different settings 

00:14:58.740 --> 00:15:03.810
 like that where I can vary also the 

00:15:01.230 --> 00:15:08.100
 number of iterations for a network of 

00:15:03.810 --> 00:15:09.930
 neurons it could be the so let's say 

00:15:08.100 --> 00:15:11.220
 I have a training criterion that 

00:15:09.930 --> 00:15:12.540
 allows to specify my number 

00:15:11.220 --> 00:15:14.760
 of iterations ben that might be the 

00:15:12.540 --> 00:15:17.310
 learning rate and the number of 

00:15:14.760 --> 00:15:18.960
 neurons hide so like that I do 

00:15:17.310 --> 00:15:20.310
 a list of all the values 

00:15:18.960 --> 00:15:22.050
 possible configurations possible 

00:15:20.310 --> 00:15:25.260
 great settings I make a list 

00:15:22.050 --> 00:15:27.570
 that I wish to evaluate the choice of 

00:15:25.260 --> 00:15:29.610
 validated alas for each element of 

00:15:27.570 --> 00:15:31.800
 this list there i will run my 

00:15:29.610 --> 00:15:34.680
 learning pride about my old 

00:15:31.800 --> 00:15:36.020
 drive then once the place 

00:15:34.680 --> 00:15:38.580
 learning 

00:15:36.020 --> 00:15:40.500
 once he has finished his execution 

00:15:38.580 --> 00:15:42.540
 I will evaluate the performance of the model 

00:15:40.500 --> 00:15:45.570
 driven on the validation set 

00:15:42.540 --> 00:15:47.460
 so for each of the father's choices 

00:15:45.570 --> 00:15:50.100
 settings i will keep in memory 

00:15:47.460 --> 00:15:53.340
 what are the performance maybe 

00:15:50.100 --> 00:15:54.900
 that if I had 10% errors on 

00:15:53.340 --> 00:15:56.730
 my validation set maybe here 

00:15:54.900 --> 00:15:58.170
 I would have said 5% the ranks the 

00:15:56.730 --> 00:16:00.150
 giants figures like that but for 

00:15:58.170 --> 00:16:02.520
 all but my father's configuration 

00:16:00.150 --> 00:16:05.430
 allow I'll put aside the 

00:16:02.520 --> 00:16:07.980
 measuring the performance on my old 

00:16:05.430 --> 00:16:10.260
 validation and finally the end 

00:16:07.980 --> 00:16:12.180
 what I'm going to do is I'm going 

00:16:10.260 --> 00:16:15.630
 succeed executed my opinion and 

00:16:12.180 --> 00:16:17.310
 learning but only for 

00:16:15.630 --> 00:16:18.690
 hypers allow with the best 

00:16:17.310 --> 00:16:21.390
 performance in validation 

00:16:18.690 --> 00:16:23.490
 so in this case if it had been submitted 

00:16:21.390 --> 00:16:25.290
 but resumes but because I know that 

00:16:23.490 --> 00:16:27.860
 manage would run my art algorithms 

00:16:25.290 --> 00:16:30.330
 showing off parameter there and that's the model 

00:16:27.860 --> 00:16:32.400
 resulting from these learnings there that 

00:16:30.330 --> 00:16:34.080
 I will use to evaluate the performance 

00:16:32.400 --> 00:16:36.720
 on my set of tests 

00:16:34.080 --> 00:16:37.830
 so I say run them what I 

00:16:36.720 --> 00:16:40.040
 could do is keep in memory 

00:16:37.830 --> 00:16:42.450
 also all models 

00:16:40.040 --> 00:16:45.630
 if I did a taxman so have here 

00:16:42.450 --> 00:16:47.070
 I reindeer worth my father mw that I 

00:16:45.630 --> 00:16:48.660
 could put in memory in addition to his 

00:16:47.070 --> 00:16:50.400
 performance I could take these 

00:16:48.660 --> 00:16:51.800
 super allow them to put the value of 

00:16:50.400 --> 00:16:53.209
 w parameter 

00:16:51.800 --> 00:16:55.130
 with w in memory in addition to the 

00:16:53.209 --> 00:16:57.170
 performance is rather re executed 

00:16:55.130 --> 00:17:00.279
 the algorithm I just think I use my 

00:16:57.170 --> 00:17:03.260
 model that I would have saved on 10 

00:17:00.279 --> 00:17:04.699
 but often practicing doing so we 

00:17:03.260 --> 00:17:06.380
 is certainly of the temperament 

00:17:04.699 --> 00:17:08.300
 not want to keep all the parameters but 

00:17:06.380 --> 00:17:10.600
 me it would be too much it would take too much 

00:17:08.300 --> 00:17:13.040
 dead so often we performed well 

00:17:10.600 --> 00:17:14.720
 holger note of learning but there with 

00:17:13.040 --> 00:17:17.000
 the iron mayors allow one to have 

00:17:14.720 --> 00:17:19.870
 found and one evaluates the model 

00:17:17.000 --> 00:17:22.850
 results on the test set times 

00:17:19.870 --> 00:17:24.319
 and that measure on all of 

00:17:22.850 --> 00:17:27.620
 all and for all purposes it's going to be our 

00:17:24.319 --> 00:17:28.880
 estimate that will not be optimistic so 

00:17:27.620 --> 00:17:31.610
 we're going to say formally that it's a 

00:17:28.880 --> 00:17:34.760
 unbiased estimate of performance 

00:17:31.610 --> 00:17:36.020
 of generalization of my home so if 

00:17:34.760 --> 00:17:39.710
 this procedure there that will allow me 

00:17:36.020 --> 00:17:41.480
 to evaluate the capacity of an algorithm 

00:17:39.710 --> 00:17:44.240
 learning allow to do well 

00:17:41.480 --> 00:17:45.320
 perform in generalization and I 

00:17:44.240 --> 00:17:48.140
 could use but you have this 

00:17:45.320 --> 00:17:50.480
 procedure there to compare different 

00:17:48.140 --> 00:17:52.940
 learning algorithms if I 

00:17:50.480 --> 00:17:55.160
 wanted calculation compared to that range 

00:17:52.940 --> 00:17:59.660
 super scepter meyers for this problem there 

00:17:55.160 --> 00:18:01.850
 that the 2-2 algorithm for 

00:17:59.660 --> 00:18:03.380
 the training of wall networks have 

00:18:01.850 --> 00:18:05.270
 compared with a co nearest neighbors 

00:18:03.380 --> 00:18:07.520
 but I will repeat this procedure 

00:18:05.270 --> 00:18:09.590
 individually for the collector have 

00:18:07.520 --> 00:18:10.820
 for the nearest neighbors case and for 

00:18:09.590 --> 00:18:13.070
 the neural network and then I could 

00:18:10.820 --> 00:18:14.600
 compare at that moment the performance 

00:18:13.070 --> 00:18:18.320
 between metro learning algorithms 

00:18:14.600 --> 00:18:20.450
 to have an unbiased measure of 

00:18:18.320 --> 00:18:23.030
 kehl and the algorithm that has the best 

00:18:20.450 --> 00:18:24.850
 performance for my problem so that's 

00:18:23.030 --> 00:18:28.220
 like that we have to use 

00:18:24.850 --> 00:18:29.750
 this is the standard procedure for evaluating 

00:18:28.220 --> 00:18:32.410
 the performance of a shed 

00:18:29.750 --> 00:18:32.410
 learning 

