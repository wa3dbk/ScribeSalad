WEBVTT
Kind: captions
Language: en

00:00:00.380 --> 00:00:04.540
So, should we do a video about the three laws of robotics, then?

00:00:04.620 --> 00:00:07.740
Because it keeps coming up in the comments.

00:00:07.740 --> 00:00:13.980
Okay, so the thing is, you won't hear serious AI researchers talking about the three laws of robotics

00:00:13.980 --> 00:00:16.080
because they don't work. They never worked.

00:00:16.300 --> 00:00:24.080
So I think people don't see the three laws talked about, because they're not serious.

00:00:24.080 --> 00:00:28.780
They haven't been relevant for a very long time and they're out of a science fiction book, you know?

00:00:30.720 --> 00:00:37.340
So, I'm going to do it. I want to be clear that I'm not taking these seriously, right?

00:00:38.180 --> 00:00:41.500
I'm going to talk about it anyway, because it needs to be talked about.

00:00:44.040 --> 00:00:50.200
So these are some rules that science fiction author Isaac Asimov came up with, in his stories,

00:00:51.120 --> 00:00:59.320
as an attempted sort of solution to the problem of making sure that artificial intelligence did

00:00:59.320 --> 00:01:00.980
what we want it to do.

00:01:00.980 --> 00:01:03.680
Shall we read them out then and see what they are?

00:01:03.680 --> 00:01:06.180
Oh yeah, I'll look them- Give me a second.

00:01:06.180 --> 00:01:08.440
I've looked them up. Okay, right, so they are:

00:01:08.440 --> 00:01:14.240
Law Number 1: A robot may not injure a human being or, through inaction allow a human being

00:01:14.240 --> 00:01:15.280
to come to harm.

00:01:15.280 --> 00:01:21.360
Law Number 2: A robot must obey orders given it by human beings except where such orders would

00:01:21.360 --> 00:01:23.100
conflict with the first law.

00:01:23.100 --> 00:01:29.240
Law Number 3: A robot must protect its own existence as long as such protection does not conflict

00:01:29.240 --> 00:01:31.240
with the first or second laws.

00:01:31.240 --> 00:01:33.180
I think there was a zeroth one later as well.

00:01:33.360 --> 00:01:40.620
Law 0: A robot may not harm humanity or, by inaction, allow humanity to come to harm.

00:01:40.780 --> 00:01:47.140
So it's weird that these keep coming up because, okay, so firstly they were made by someone

00:01:47.140 --> 00:01:52.100
who is writing stories, right? And they're optimized for story-writing.

00:01:52.520 --> 00:01:56.920
But they don't even work in the books, right? If you read the books, they're all about

00:01:56.920 --> 00:02:02.600
the ways that these rules go wrong, the various, various negative consequences.

00:02:02.860 --> 00:02:09.100
The most unrealistic thing, in my opinion, about the way Asimov did his stuff was

00:02:09.640 --> 00:02:13.760
the way that things go wrong and then get fixed, right?

00:02:13.760 --> 00:02:18.640
Most of the time, if you have a super-intelligence, that is doing something you don't want it to do,

00:02:19.200 --> 00:02:26.000
there's probably no hero who's going to save the day with cleverness. Real life doesn't work that way,

00:02:26.000 --> 00:02:32.480
generally speaking, right? Because they're written in English. How do you define these things?

00:02:32.680 --> 00:02:38.760
How do you define human without having to first take an ethical stand on almost every issue?

00:02:38.760 --> 00:02:42.540
And if human wasn't hard enough, you then have to define harm, right?

00:02:42.720 --> 00:02:48.480
And you've got the same problem again. Almost any definitions you give for those words,

00:02:48.480 --> 00:02:52.900
really solid, unambiguous definitions that don't rely on human intuition,

00:02:54.220 --> 00:03:01.220
result in weird quirks of philosophy, resulting in your AI doing something you really don't want it to do.

00:03:01.220 --> 00:03:06.700
The thing is, in order to encode that rule, "Don't allow a human being to come to harm",

00:03:06.700 --> 00:03:11.380
in a way that means anything close to what we intuitively understand it to mean,

00:03:11.500 --> 00:03:18.620
you would have to encode within the words 'human' and 'harm' the entire field of ethics, right?

00:03:18.620 --> 00:03:23.780
You have to solve ethics, comprehensively, and then use that to make your definitions.

00:03:23.780 --> 00:03:27.920
So it doesn't solve the problem, it pushes the problem back one step

00:03:27.920 --> 00:03:30.580
into now, well how do we define these terms?

00:03:30.580 --> 00:03:34.440
When I say the word human, you know what I mean, and that's not because either of us

00:03:34.440 --> 00:03:39.940
have a rigorous definition of what a human is. We've just sort of learned by general association

00:03:39.940 --> 00:03:43.520
what a human is, and then the word 'human' points to that structure in your brain,

00:03:43.520 --> 00:03:46.260
but I'm not really transferring the content to you.

00:03:46.680 --> 00:03:56.620
So, you can't just say 'human' in the utility function of an AI and have it know what that means.

00:03:56.620 --> 00:03:58.940
You have to specify. You have to come up with a definition.

00:03:58.940 --> 00:04:03.880
And it turns out that coming up with a definition, a good definition, of something like 'human'

00:04:04.000 --> 00:04:11.880
is extremely difficult, right? It's a really hard problem of, essentially, moral philosophy.

00:04:11.880 --> 00:04:16.360
You would think it would be semantics, but it really isn't because,

00:04:16.360 --> 00:04:20.180
okay, so we can agree that I'm a human and you're a human. That's fine.

00:04:20.180 --> 00:04:24.040
And that this, for example, is a table, and therefore not a human.

00:04:24.140 --> 00:04:29.260
You know, the easy stuff, the central examples of the classes are obvious.

00:04:29.260 --> 00:04:34.800
But, the edge cases, the boundaries of the classes, become really important.

00:04:34.800 --> 00:04:38.600
The areas in which we're not sure exactly what counts as a human.

00:04:38.600 --> 00:04:46.900
So, for example, people who haven't been born yet, in the abstract, like people who hypothetically

00:04:46.900 --> 00:04:48.940
could be born ten years in the future, do they count?

00:04:48.940 --> 00:04:54.220
People who are in a persistent vegetative state don't have any brain activity.

00:04:54.240 --> 00:05:02.240
Do they fully count as people? People who have died or unborn fetuses, right?

00:05:02.240 --> 00:05:06.940
I mean, there's a huge debate even going on as we speak about whether they count as people.

00:05:06.940 --> 00:05:11.480
The higher animals, you know, should we include maybe dolphins, chimpanzees, something like that?

00:05:11.480 --> 00:05:18.220
Do they have weight? And so it it turns out you can't program in, you can't make your specification

00:05:18.220 --> 00:05:21.900
of humans without taking an ethical stance on all of these issues.

00:05:21.900 --> 00:05:26.340
All kinds of weird, hypothetical edge cases become relevant when you're talking about

00:05:26.340 --> 00:05:31.080
a very powerful machine intelligence, which you otherwise wouldn't think of.

00:05:31.080 --> 00:05:34.520
So for example, let's say we say that dead people don't count as humans.

00:05:34.520 --> 00:05:39.260
Then you have an AI which will never attempt CPR. This person's died.

00:05:39.260 --> 00:05:41.320
They're gone, forget about it, done, right?

00:05:41.340 --> 00:05:47.560
Whereas we would say, no, hang on a second, they're only dead temporarily. We can bring them back, right?

00:05:47.560 --> 00:05:54.000
Okay, fine, so then we'll say that people who are dead, if they haven't been dead for- Well, how long?

00:05:54.000 --> 00:05:58.540
How long do you have to be dead for? I mean, if you get that wrong and you just say, oh it's fine,

00:05:58.540 --> 00:06:02.180
do try to bring people back once they're dead, then you may end up with a machine

00:06:02.180 --> 00:06:05.320
that's desperately trying to revive everyone who's ever died in all of history,

00:06:05.440 --> 00:06:09.280
because there are people who count who have moral weight.

00:06:09.280 --> 00:06:13.860
Do we want that? I don't know, maybe. But you've got to decide, right?

00:06:13.860 --> 00:06:18.480
And that's inherent in your definition of human. You have to take a stance on all kinds of moral issues

00:06:18.480 --> 00:06:23.780
that we don't actually know with confidence what the answer is, just to program the thing in.

00:06:23.920 --> 00:06:31.560
And then it gets even harder than that, because there are edge cases which don't exist right now.

00:06:31.560 --> 00:06:38.060
Like, talking about living people, dead people, unborn people, that kind of thing.

00:06:38.060 --> 00:06:42.600
Fine, animals. But there are all kinds of hypothetical things which could exist

00:06:42.620 --> 00:06:49.240
which may or may not count as human. For example, emulated or simulated brains, right?

00:06:49.240 --> 00:06:54.400
If you have a very accurate scan of someone's brain and you run that simulation, is that a person?

00:06:54.560 --> 00:06:55.680
Does that count?

00:06:57.100 --> 00:07:00.740
And whichever way you slice that, you get interesting outcomes.

00:07:01.220 --> 00:07:07.680
So, if that counts as a person, then your machine might be motivated to bring out a situation

00:07:07.680 --> 00:07:13.000
in which there are no physical humans because physical humans are very difficult to provide for.

00:07:13.000 --> 00:07:18.040
Whereas simulated humans, you can simulate their inputs and have a much nicer environment for everyone.

00:07:18.040 --> 00:07:22.600
Is that what we want? I don't know. Is it, maybe? I don't know.

00:07:24.160 --> 00:07:27.480
I don't think anybody does. But the point is, you're trying to write an AI here, right?

00:07:27.480 --> 00:07:30.720
You're an AI developer. You didn't sign up for this.

00:07:33.540 --> 00:07:37.120
We'd like to thank Audible.com for sponsoring this episode of Computerphile.

00:07:37.120 --> 00:07:41.240
And if you like books, check out Audible.com's huge range of audiobooks.

00:07:41.240 --> 00:07:46.060
And if you go to Audible.com/computerphile, there's a chance to download one for free.

00:07:46.060 --> 00:07:50.800
Callum Chase has written a book called Pandora's Brain, which is a thriller centered around

00:07:50.800 --> 00:07:56.080
artificial general intelligence, and if you like that story, then there's a supporting nonfiction book

00:07:56.080 --> 00:07:59.620
called Surviving AI which is also worth checking out.

00:07:59.620 --> 00:08:05.080
So thanks to Audible for sponsoring this episode of Computerphile. Remember, audible.com/computerphile.

00:08:05.080 --> 00:08:06.520
Download a book for free.

