WEBVTT
Kind: captions
Language: en

00:00:04.140 --> 00:00:08.050
&gt;&gt;Eric: Good afternoon, everyone. It's my
pleasure to introduce to you Dean Karlan and

00:00:08.050 --> 00:00:13.969
Jacob Appel of Innovations for Poverty Action.
Dean is the president and the founder by IPA.

00:00:13.969 --> 00:00:19.470
Their group is doing very impressive work
in measuring the effectiveness of aid dollars

00:00:19.470 --> 00:00:23.689
and using that knowledge to figure out how
to design aid programs that have really good

00:00:23.689 --> 00:00:29.260
bang for the buck. Currently in the aid world,
a lot of aid programs are evaluated on their

00:00:29.260 --> 00:00:34.380
effectiveness by figuring out, "Well, we had
$5 million. Did we spend it all? Yes. Great!

00:00:34.380 --> 00:00:38.870
We're effective." But you don't really know.
Did you get $5 million worth of aid for that

00:00:38.870 --> 00:00:45.399
or, in fact, did you do more harm than good
with that? Dean and Jacob have written a book

00:00:45.399 --> 00:00:51.730
called 'More Than Good Intentions.' They talk
about what goes into designing their aid experiments

00:00:51.730 --> 00:00:56.670
and some of the surprisingly small differences
and some of the very little things that one

00:00:56.670 --> 00:01:04.190
can do to make aid more effective and to get
a lot more aid for a lot fewer dollars. Please

00:01:04.190 --> 00:01:06.000
welcome Dean Karlan and Jacob Appel.

00:01:06.000 --> 00:01:07.000
[applause]

00:01:07.000 --> 00:01:15.880
&gt;&gt;Dean Karlan: Hi, everyone. It's great to
be here. Thanks a lot for for coming and thanks,

00:01:15.880 --> 00:01:21.680
Eric, for putting this on and hosting. So,
there's a few different themes to I want to

00:01:21.680 --> 00:01:29.400
talk about that are related but might sound
very different. The first theme that I want

00:01:29.400 --> 00:01:35.189
to talk about is exactly what Eric said. Which
is, how do we... what are we doing to take

00:01:35.189 --> 00:01:39.860
data to help figure out what really works
and what does not? What is... what is this

00:01:39.860 --> 00:01:43.049
journey about? What is it that we're doing
differently than what's been done before?

00:01:43.049 --> 00:01:48.030
The second thing I want to talk about us how
we're thinking about designing some of the

00:01:48.030 --> 00:01:51.810
programs and the interventions that we're
looking into. And specifically a lot of the

00:01:51.810 --> 00:01:55.680
work we're doing uses lessons from behavioral
economics, but not always behavioral. Sometimes

00:01:55.680 --> 00:01:59.750
it's just plain vanilla economics. This is
one of the themes in the book, too, and we

00:01:59.750 --> 00:02:04.469
go back and forth between behavioral and more
traditional economics, mixing the two up.

00:02:04.469 --> 00:02:08.630
And to understand, in a given situation what's
the what's the right way of thinking about

00:02:08.630 --> 00:02:17.140
how to solve a particular problem.
The third theme that we'll talk about, and

00:02:17.140 --> 00:02:21.940
Jake will come up and speak about as well,
has to do with how do we sift through of all

00:02:21.940 --> 00:02:24.840
this information that's out there? This is
something near and dear to all of us here

00:02:24.840 --> 00:02:30.819
at Google, obviously, is, you know, tons and
tons of information can come flooding to us

00:02:30.819 --> 00:02:34.569
about different charities and doing different
work. How do we sift through that to figure

00:02:34.569 --> 00:02:43.430
out what actually... what actually works?
So sometimes I think it helps to be a little

00:02:43.430 --> 00:02:49.930
bit personal for a moment and let me tell
you about how I got into doing this. In 1992,

00:02:49.930 --> 00:02:55.060
I just finished two years of investment banking
and I moved to El Salvador. I was heading,

00:02:55.060 --> 00:02:59.281
from there I was going to go back to get an
MBA, probably go work in a hedge fund or something

00:02:59.281 --> 00:03:05.769
of that nature. And I took a detour, a detour
for a year to work for an organization that

00:03:05.769 --> 00:03:10.200
-- this is pre-Internet days -- that I had
seen a brochure about. And it seemed very

00:03:10.200 --> 00:03:15.670
appealing to me and very attractive. It really
hit my... It both hit my heart and my mind.

00:03:15.670 --> 00:03:19.709
But not my mind in a data sense, my mind in
the sense that the theory of what they're

00:03:19.709 --> 00:03:23.480
doing me sense to me. And what they were doing
was microcredit.

00:03:23.480 --> 00:03:29.909
Now what's the canonical microcredit story?
The canonical microcredit story says we're

00:03:29.909 --> 00:03:36.060
going to take money and we're going to lend
it out to women who have enterprising ideas,

00:03:36.060 --> 00:03:40.900
and are entrepreneurial, are a high-energy,
are really motivated to do something about

00:03:40.900 --> 00:03:44.849
the problem they're facing in their life -- the
poverty that they're stricken with. And they're

00:03:44.849 --> 00:03:48.510
doing this in order to fight for the family,
for their children, to improve the health

00:03:48.510 --> 00:03:51.720
and education of their children. It's a very
inspiring story that we've heard probably

00:03:51.720 --> 00:03:55.940
many... I'm guessing many people in the room
have heard this story told this way. You know,

00:03:55.940 --> 00:03:59.650
here was I ... I was coming from a banking
background and this seemed perfect. Right?

00:03:59.650 --> 00:04:03.510
In banking, I had some computer backgrounds.
So I went down and I was developing software

00:04:03.510 --> 00:04:08.530
systems for managing their portfolios.
The thing that struck me the most was the

00:04:08.530 --> 00:04:13.531
conversations at night. So talking with people
who were working with this group. And a lot

00:04:13.531 --> 00:04:20.200
of people in this group -- very smart people
-- but not using any sort of data to help

00:04:20.200 --> 00:04:24.920
guide what they were doing. And I remember
asking, "What is the impact of this? What

00:04:24.920 --> 00:04:28.790
do we think is happening with the impact here?"
The first thing that struck me when I moved

00:04:28.790 --> 00:04:32.910
to El Salvador was what the interest rate
was. Right? So in this brochure that I saw

00:04:32.910 --> 00:04:36.440
that inspired me to move to El Salvador to
do this there was no mention of the interest

00:04:36.440 --> 00:04:41.160
rate. And if you'd asked me, I would have
said, "Well, you know, I understand the principle

00:04:41.160 --> 00:04:44.260
here is kind of sustainable development. There
has to be some revenue, so there's probably

00:04:44.260 --> 00:04:49.750
going to be some nominal interest rate fee
of 5%, 10%. It turns out it was 72% a year

00:04:49.750 --> 00:04:54.910
-- the interest rate. And, you know my first
reaction was ... I was kind of disheartened.

00:04:54.910 --> 00:04:59.420
I remember the feeling of my heart sinking
and thinking "What's going on here?"

00:04:59.420 --> 00:05:04.020
Now, that sounds bad, but, you know, then
we sat down and we talked with some of the

00:05:04.020 --> 00:05:07.690
clients who were engaging this program, and
we asked them the types of things that we

00:05:07.690 --> 00:05:11.770
saw them doing and how they were maybe doing
something differently as getting access to

00:05:11.770 --> 00:05:15.600
loans. It wasn't actually that hard to come
up with returns that high. Returns that high

00:05:15.600 --> 00:05:20.880
for a microenterprise could actually be reasonably
that high. So that's not actually enough to

00:05:20.880 --> 00:05:23.790
say ... just looking at that interest rate
is by no means enough to say, "Oh, this is

00:05:23.790 --> 00:05:28.850
a bad idea."
Now, when we asked the organization, "What

00:05:28.850 --> 00:05:36.810
is your impact?", this is what they gave us.
They gave us a study that asked their existing

00:05:36.810 --> 00:05:42.570
clients who had been with them for a couple
years, "Are you eating better now than you

00:05:42.570 --> 00:05:48.440
were two years ago?" OK. And that was it.
That was the extent. And then they ... some

00:05:48.440 --> 00:05:52.490
other questions, too. "Are you earning more
in your business?" Right? And this was the

00:05:52.490 --> 00:05:55.750
... this was the state of what they were doing
in terms of impact measurement. They were

00:05:55.750 --> 00:05:59.870
not doing anything different -- I have now
learned -- than anybody else in this space.

00:05:59.870 --> 00:06:05.560
So this is not a criticism of them. This is
a statement of where the world was in terms

00:06:05.560 --> 00:06:11.620
of measuring impact of microcredit. Now, I
had a major in political science. It didn't

00:06:11.620 --> 00:06:15.500
take ... it didn't take a ... you know, it
didn't take training to just kind of say,

00:06:15.500 --> 00:06:21.250
"OK, now wait a second." The world is happening
at the same time. So, did it ... is it ... can

00:06:21.250 --> 00:06:25.690
we really say that microcredit caused these
people to be eating better and earn more money?

00:06:25.690 --> 00:06:30.890
Or could it be the macroeconomic conditions?
Could it be just weather was good? Could it

00:06:30.890 --> 00:06:35.841
just be that they were in an upward trajectory
anyhow because they're individuals who were

00:06:35.841 --> 00:06:40.650
chosen because they are fighting individuals?
Inspiring individuals? Entrepreneurial individuals?

00:06:40.650 --> 00:06:44.370
That's what the brochure said. That's who
they're lending to. And so what is it that

00:06:44.370 --> 00:06:49.890
caused them to do better? So I remember thinking
at the time that, you know, what else could

00:06:49.890 --> 00:06:55.900
we put into this and claim microcredit causes?
Imagine that survey also said, "Are you feeling

00:06:55.900 --> 00:07:00.970
older now than you were two years ago?" Would
we go and then take this to the brochures

00:07:00.970 --> 00:07:05.750
and say that microcredit causes aging? Right?
The rigor of that conclusion would have been

00:07:05.750 --> 00:07:12.020
just as solid as the rigor of the conclusion
that they caused an increase in income. So

00:07:12.020 --> 00:07:16.880
the point of this is that, you know, we need
to have some basis of attribution, right?

00:07:16.880 --> 00:07:22.650
We need to know whether our aid is doing something.
And we need to ... when we went in and asked

00:07:22.650 --> 00:07:26.940
that question, what we really are asking is,
"How have the lives of the people in a program

00:07:26.940 --> 00:07:31.430
changed compared to how their lives would
have changed had this program not existed?"

00:07:31.430 --> 00:07:35.830
And to do that second part of that question,
you have to do something more than just follow

00:07:35.830 --> 00:07:40.600
your clients over time and see what types
of changes occur in their life. And this is

00:07:40.600 --> 00:07:45.020
the heart of what we're doing as an organization,
and, you know, as my research as a professor

00:07:45.020 --> 00:07:51.740
at Yale is very much dedicated to setting
up experimental processes so that we can measure

00:07:51.740 --> 00:07:55.250
what is the actual impact of a particular
type idea.

00:07:55.250 --> 00:08:06.760
Now, in [pause] in the case of microcredit,
we have now done rigorous impact evaluations

00:08:06.760 --> 00:08:12.640
of microcredit. We've done three from researchers
in our group, and the results that we get

00:08:12.640 --> 00:08:18.930
from this are striking. And the reaction is
actually much more striking, I find. So, prior

00:08:18.930 --> 00:08:23.710
to the research we'd been doing on microcredit,
the conventional wisdom and the rhetoric around

00:08:23.710 --> 00:08:28.580
microcredit said it was almost like it was
the panacea to solve poverty. It's the silver

00:08:28.580 --> 00:08:33.330
bullet. It's going to increase income, increase
health, increase education, increase female

00:08:33.330 --> 00:08:40.220
empowerment in the household. The list goes
on. And in ... What we are finding with the

00:08:40.220 --> 00:08:45.700
studies is that there were important benefits
that were accrued to the families who received

00:08:45.700 --> 00:08:51.250
access to microcredit compared to a randomly
assigned control group that did not get access

00:08:51.250 --> 00:08:55.380
to the microcredit. But we did not find that
these things solved poverty by any means.

00:08:55.380 --> 00:09:00.600
Now, the pendulum has swung and it's a frustrating
thing to see as a researcher when your research

00:09:00.600 --> 00:09:04.650
gets overinterpreted the wrong way, and then
people are saying that "Oh, microcredit's

00:09:04.650 --> 00:09:09.550
bad." The results that we're finding is that
microcredit does have some benefits, but it's

00:09:09.550 --> 00:09:15.600
really pointing out very, very poignantly
that we need to think about opportunity costs.

00:09:15.600 --> 00:09:19.900
It's not the silver bullet. And if we're doing
microcredit, that means our money's not going

00:09:19.900 --> 00:09:24.210
and doing something else. So what's better?
What's the right answer?

00:09:24.210 --> 00:09:29.080
And so this is the ... this is the heart of
the first theme. What really works? Right?

00:09:29.080 --> 00:09:34.930
And when I left El Salvador, I went back to
graduate school because this is what I saw

00:09:34.930 --> 00:09:40.270
as the biggest gap, was this lack of rigor
of analytics in terms of understanding what

00:09:40.270 --> 00:09:44.150
the impact was and understanding how programs
were designed. So I went back to graduate

00:09:44.150 --> 00:09:48.700
school to get a Ph.D. in economics to study
these types of questions.

00:09:48.700 --> 00:09:53.260
What I found when I went back to graduate
school was -- with certain exceptions like

00:09:53.260 --> 00:09:59.020
my advisers who I was working with -- the
debate was strikingly frustrating. The debate

00:09:59.020 --> 00:10:04.860
was answering the question "Does development
aid work?" And this is just a bad question

00:10:04.860 --> 00:10:11.720
-- particularly if you're going to expect
a "yes" or "no." OK. First, the answer ... The

00:10:11.720 --> 00:10:18.780
answer is, first of all, so quite reasonable:
that some aid works and some aid does not.

00:10:18.780 --> 00:10:23.760
And how can you argue with that? But yet,
that was not the debate that we were seeing

00:10:23.760 --> 00:10:29.630
... was this polar opposite of aid ... and
it was also this very monolithic view of poverty:

00:10:29.630 --> 00:10:33.560
that we were just measuring poverty at the
country level and then asking whether aid

00:10:33.560 --> 00:10:40.830
flows reduces poverty. And it was a very frustrating
set of research to try to absorb and figure

00:10:40.830 --> 00:10:46.700
out what to do with. And the revolution which
started taking place in the mid-1990s in development

00:10:46.700 --> 00:10:51.420
economics was to say, "Well, let's stop this
debate from the ivory tower, from these large

00:10:51.420 --> 00:10:55.650
cross-country datasets that are ... where
the data is aggregated up to a level where

00:10:55.650 --> 00:11:00.990
we've lost all understanding of what we're
really measuring, and let's go to the ground.

00:11:00.990 --> 00:11:05.180
And by going to the ground, we can set up
careful tests and collect data that we actually

00:11:05.180 --> 00:11:10.250
know what it is, know what it represents,
measure the actual changes in people's lives,

00:11:10.250 --> 00:11:14.410
measure the counterfactual of what whould
have happened by setting up randomized trials,

00:11:14.410 --> 00:11:18.950
and knowing what happened, what would have
happened, and measuring the actual change

00:11:18.950 --> 00:11:23.570
in people's lives as a result of access to
various services or programs or interventions."

00:11:23.570 --> 00:11:30.840
And this applies to government, NGO, as well
as for-profit entities. So that's the first

00:11:30.840 --> 00:11:36.300
theme of the book. And it ... we talk a lot
about the examples of this type of ground

00:11:36.300 --> 00:11:42.550
research in microfinance, in health, education,
etc.

00:11:42.550 --> 00:11:48.050
The second theme of the book is about behavioral
economics, and, specifically, lessons from

00:11:48.050 --> 00:11:52.870
behavioral economics that can help us think
about how to design better programs, and how

00:11:52.870 --> 00:11:58.360
behavioral economics changes, sometimes, ways
that we think about things in subtle, but

00:11:58.360 --> 00:12:03.810
really important, ways. So, one example which
stays true to the microcredit topic which

00:12:03.810 --> 00:12:11.640
I already raised is about what we call loans.
Do we call a loan "credit" or "debt?" Right?

00:12:11.640 --> 00:12:18.880
So the striking thing about microloans is
that we can take ... I can take the same exact

00:12:18.880 --> 00:12:24.410
loan, the same terms, conditions, interest
rates, everything, in a developing country,

00:12:24.410 --> 00:12:30.240
and describe it to you, and tell you the organization
to send money to where you can get a tax deduction

00:12:30.240 --> 00:12:35.070
for supporting that loan, that creation of
that loan. And you can get a tax deduction

00:12:35.070 --> 00:12:40.800
for this under the umbrella of a charity doing
good work. And that would be called microcredit.

00:12:40.800 --> 00:12:45.670
In the U.S., we can take the same exact loan
and now we'll call it debt. It's the same

00:12:45.670 --> 00:12:51.100
interest rate, same terms, same conditions.
And now what's ... what are the non-profit

00:12:51.100 --> 00:12:54.130
organizations going to be doing around that?
They're going to be trying to figure out how

00:12:54.130 --> 00:12:58.960
to get people to not do that. To not take
out that loan. To pay it down faster. To not

00:12:58.960 --> 00:13:02.870
take it out in the first place. Or to pass
regulation or laws to shut it down so they

00:13:02.870 --> 00:13:05.840
can't even have the loan if they want it;
that the lender is not allowed to make it

00:13:05.840 --> 00:13:11.411
for them ... by passing usury laws.
OK, so there's something ... there's three

00:13:11.411 --> 00:13:17.040
ways of thinking about that -- reconciling
those two things. One is to say, "Well, we're

00:13:17.040 --> 00:13:20.840
wrong to be doing it overseas. Those are bad
loans for people and they're making people

00:13:20.840 --> 00:13:24.300
worse off, and we're right about it in the
United States." A second is to say, "No, no,

00:13:24.300 --> 00:13:27.920
no, no. We really should free markets up in
the U.S. and, yes, maybe we can help people

00:13:27.920 --> 00:13:31.680
think about the right decision, whether they
should be borrowing or saving. But we shouldn't

00:13:31.680 --> 00:13:36.340
be restricting the market. We should be letting
it happen just the same way we do overseas."

00:13:36.340 --> 00:13:39.960
A third is to say that I'm being loose with
my analogy and that the two are ... other

00:13:39.960 --> 00:13:44.500
things are different. And I'll admit that
I'm being a little bit loose, but not so loose

00:13:44.500 --> 00:13:50.810
as to be ... I don't think I'm wrong. There
is an overlap in these two worlds. You know,

00:13:50.810 --> 00:13:54.050
there are some loans in the U.S. that you
wouldn't see overseas and vice-versa, that

00:13:54.050 --> 00:14:01.510
are supported. Now, but that's just an example
of word choice making such a powerful difference

00:14:01.510 --> 00:14:06.529
in how we perceive something. And this is
true in a lot of spaces. When we want to ... when

00:14:06.529 --> 00:14:11.089
we want to figure out how to sell charity,
or when we want to sell goods in developing

00:14:11.089 --> 00:14:14.260
countries, we need to think about that last
mile. How would you actually get products

00:14:14.260 --> 00:14:18.650
into the hands of individuals? Is it as good
as just create the intervention and then with

00:14:18.650 --> 00:14:22.800
that everybody will use it? Or do you actually
have to think about how to sell it? Right?

00:14:22.800 --> 00:14:27.010
In the same way that that word -- credit or
debt -- has a very different meaning to us.

00:14:27.010 --> 00:14:31.860
Credit is good. We earn credit. We don't earn
debt. Right? Debt is bad. That's something

00:14:31.860 --> 00:14:36.420
we owe. We're indebted to people or indebted
to firms. So that very ... that simple word

00:14:36.420 --> 00:14:41.650
choice, which has the same actual meaning
in terms of economics, you know, puts a very

00:14:41.650 --> 00:14:45.330
different mindset to us when we think about
whether we like it or not, whether it's a

00:14:45.330 --> 00:14:49.620
good or bad thing, whether we should encourage
people or not.

00:14:49.620 --> 00:14:54.151
One of the other themes in the book -- one
of the other behavioral examples in the book

00:14:54.151 --> 00:15:01.230
-- is one about temptation and how is it that
we deal with temptation in life. We all have

00:15:01.230 --> 00:15:07.710
different things that tempt us. How do we
go about figuring out how to resist temptation

00:15:07.710 --> 00:15:13.290
and are there ways of offering people products
and services that help people resist temptation

00:15:13.290 --> 00:15:17.700
better? So that they can achieve the things
they want to achieve. And the fun thing about

00:15:17.700 --> 00:15:21.270
this one is that this is a universal thing,
right? It's not to say that it's universal

00:15:21.270 --> 00:15:25.070
what tempts us, obviously. I have different
temptations than everybody else in this room,

00:15:25.070 --> 00:15:29.190
no doubt. Everybody's unique in what it is.
And, you know, maybe there's someone on this

00:15:29.190 --> 00:15:34.590
planet who has no temptations, but I haven't
met that person yet. So, for me, you know,

00:15:34.590 --> 00:15:40.240
I love peanut M&amp;Ms, and, you know, if I could
make it so that the peanut M&amp;M company would

00:15:40.240 --> 00:15:44.240
only sell them to me at a price of $100 a
bag, I would actually be really happy. I'd

00:15:44.240 --> 00:15:51.270
like that. I would eat many fewer peanut M&amp;Ms.
Now, you know, understanding how people deal

00:15:51.270 --> 00:15:55.480
with temptation solves a few problems all
at once. And this is kind of the fun part

00:15:55.480 --> 00:16:00.330
of thinking through the theory of how people
make decisions. We look at a project in the

00:16:00.330 --> 00:16:06.040
book in Kenya in which we use an understanding
of temptation and how people manage their

00:16:06.040 --> 00:16:12.180
money to help farmers invest more fertilizer
to increase yield in their farm. And it's

00:16:12.180 --> 00:16:17.170
the same principle that we go to the Philippines
and we look at a program that we tested that

00:16:17.170 --> 00:16:22.600
helped people stop smoking. And it's the same
type of thinking that takes us back to America

00:16:22.600 --> 00:16:28.370
to a website that I started called "stickK"
-- stickK.com -- which one of my co-founders,

00:16:28.370 --> 00:16:34.040
Ian Ayres, spoke here at Google, I think about
two years ago, and talked about this as well.

00:16:34.040 --> 00:16:37.250
And it's a website that we started in the
United States that can help people lose weight,

00:16:37.250 --> 00:16:43.160
stop smoking, or achieve more success at work,
or at home, etc. So what's the underlying

00:16:43.160 --> 00:16:48.089
prinicple is a committment contract. And it's
... the basic idea is "How do you make your

00:16:48.089 --> 00:16:52.640
vices more expensive? How can I make it so
that peanut M&amp;Ms cost me more money?" So an

00:16:52.640 --> 00:16:56.320
example of what I did last night at dinner
tells you exactly how this type of thing can

00:16:56.320 --> 00:17:02.420
work. So last night at dinner, I know if I
... my sweet spot when eating a nice meal

00:17:02.420 --> 00:17:07.929
is to have wine and no dessert. OK, so if
I can plan ahead and achieve exactly what

00:17:07.929 --> 00:17:13.079
I want to do, I will have wine and no dessert.
I do know myself well enough, unfortunately,

00:17:13.079 --> 00:17:19.870
that after I've had wine, dessert looks much
better. OK? So after I've had my wine, if

00:17:19.870 --> 00:17:25.020
I then am asked if I want dessert, a lot of
times I'll end up saying "yes" to the dessert.

00:17:25.020 --> 00:17:30.100
Right? So if I have to rank my preferences,
my first choice is wine, no dessert. My second

00:17:30.100 --> 00:17:36.110
choice is neither. And my third choice is
both. That's me speaking now about my dinner

00:17:36.110 --> 00:17:41.550
tonight. But I know that if I'm actually eating
dinner, that the wine plus dessert will sneak

00:17:41.550 --> 00:17:46.490
its way to the top, and that'll end up being
what I do. So, how do I ... how do I solve

00:17:46.490 --> 00:17:50.740
this? How can I get it so that wine and no
dessert ends up being what I actually do?

00:17:50.740 --> 00:17:55.280
So what I did last night ... I was exactly
in this setting and I can do it again because

00:17:55.280 --> 00:17:58.980
I'm here with two people who I'm going to
be having dinner with tonight, and so I will

00:17:58.980 --> 00:18:01.960
say the same thing to you now. So if you see
me eating dessert tonight at dinner, I owe

00:18:01.960 --> 00:18:10.480
you each $1000. OK, so now I can eat, drink
my wine and feel confident that I'm not going

00:18:10.480 --> 00:18:19.800
to order dessert for $2012.95. Right? $12.95
I will. $2012.95? No, not going to happen.

00:18:19.800 --> 00:18:25.480
OK, so now that's a problem solved. So I just
wrote a very simple commitment contract. Um,

00:18:25.480 --> 00:18:29.800
technically, I probably, like, need to pay
them a $1 ... they need to pay me $1 for it

00:18:29.800 --> 00:18:33.520
or something like that to make it legally
binding. But it's socially binding 'cause

00:18:33.520 --> 00:18:38.420
I will definitely hear the wrath from them
if I don't pay up and eat dessert. And so

00:18:38.420 --> 00:18:44.559
that's, that's an example of how we're thinking
about ways to help people modify behavior

00:18:44.559 --> 00:18:48.630
towards their own goals, towards things that
they preemptively said they want to do. And

00:18:48.630 --> 00:18:52.300
that's how the stickK website works. You can
go online. You write a contract. You say,

00:18:52.300 --> 00:18:56.910
"I'm going to lose weight, stop smoking."
One of my favorite contracts was someone who

00:18:56.910 --> 00:19:03.530
said, "I'm going to speak more slowly to foreigners
in New York City. I'm not going to date any

00:19:03.530 --> 00:19:12.020
more losers." And named one of their friends
as a referee who could hold them accountable.

00:19:12.020 --> 00:19:15.600
So [sips water] [pause] And then you put money
or social shame at stake. A lot of people

00:19:15.600 --> 00:19:19.110
just put their emails of friends and family
who then are informed of whether they succeed

00:19:19.110 --> 00:19:22.580
or failed. And that might work better for
some people than financial stakes. If you

00:19:22.580 --> 00:19:28.780
put financial stakes, the single most popular
option is the George Bush Presidential Library.

00:19:28.780 --> 00:19:33.220
So the idea being that the money gets zapped
away to something that they hate. Now we're

00:19:33.220 --> 00:19:36.900
not ... we're agnostic. We're not taking sides,
obviously, on a political issue. So we give

00:19:36.900 --> 00:19:40.309
people both. That just tells you something
about who happens to be using the site more.

00:19:40.309 --> 00:19:44.450
But we give the Bill Clinton and the George
Bush Presidential Library. And we take five

00:19:44.450 --> 00:19:48.200
other hot political issues and we give people
both sides. And these are called the anti-charities.

00:19:48.200 --> 00:19:52.720
So you pledge your money to the thing you
hate. For people from England or U.K. Premiere

00:19:52.720 --> 00:19:58.200
League fans, you can also choose major teams,
Man U, Arsenal, Chelsea. And again, you choose

00:19:58.200 --> 00:20:03.700
the team you hate and your money will go off
to that club. So this is a very simple commitment

00:20:03.700 --> 00:20:06.710
device where you just raised the price. You
said what you're going to do. And you put

00:20:06.710 --> 00:20:10.110
money at stake. And then, if you fail, your
money goes away. And it's a very simple way

00:20:10.110 --> 00:20:14.710
of raising the price of vice or you can also
think of it as lowering the price of virtue.

00:20:14.710 --> 00:20:22.640
It's the same ... it's the same basic formula.
OK. So that's ... That was theme two. I now

00:20:22.640 --> 00:20:27.910
want to invite Jake Appel to come on up, who
is going to talk about the third theme in

00:20:27.910 --> 00:20:32.420
the book. And then I will wrap up and also
plant some ideas with you on some things we

00:20:32.420 --> 00:20:33.420
want to be doing in the future.

00:20:33.420 --> 00:20:35.250
[pause]

00:20:35.250 --> 00:20:45.600
&gt;&gt;Jacob Appel: Thanks. [pause] So, um, as
Dean said, I want to talk a little bit more

00:20:45.600 --> 00:20:50.240
about why this book. So Dean, Dean told you
a little bit about what is in this book, and

00:20:50.240 --> 00:20:57.150
I want to say why we saw fit to write it.
So, that sort of brings the focus back to

00:20:57.150 --> 00:21:03.549
us here as, as donors, as practitioners, people
who are engaged with the issues of poverty

00:21:03.549 --> 00:21:10.001
and poverty alleviation, and, and what's the
sort of situation that we face. Well, a lot

00:21:10.001 --> 00:21:16.950
of us face a flood of information from charities
and from development organizations to, to

00:21:16.950 --> 00:21:22.200
just try to grab our attention and to eventually
try to grab our money. And the, the problem

00:21:22.200 --> 00:21:27.610
here is that the information calling out to
us is not always the right information. It's

00:21:27.610 --> 00:21:32.261
not, it's not what tells us about which programs
are going to be the most effective, which

00:21:32.261 --> 00:21:37.250
are going to create the biggest change in
the world per dollar. More often it, it goes

00:21:37.250 --> 00:21:42.470
back to the theme Dean talked about, which
is behavioral. So, charities are quite savvy.

00:21:42.470 --> 00:21:47.570
They know how to pull at our heartstrings.
They know how to show us photographs of people

00:21:47.570 --> 00:21:52.530
who are really suffering, fly-bitten babies.
They know how to sell us sponsor-a-child programs,

00:21:52.530 --> 00:21:56.821
where you can get a -- you can make a personal
connection with a person that you help. These

00:21:56.821 --> 00:22:02.100
kinds of strategies are really powerful. They,
they tug at our heartstrings and they, they

00:22:02.100 --> 00:22:07.140
really engage us on a personal level. And
in a sense, that's great. It's good to be

00:22:07.140 --> 00:22:13.160
engaged anyhow, but, but it's much better
to be engaged and be thinking proactively

00:22:13.160 --> 00:22:21.350
about what am I supporting? Where's my money
going? At least we think so. So, [pause] so

00:22:21.350 --> 00:22:30.429
the ways that charities can, can attract our
attention is by making it either more compelling

00:22:30.429 --> 00:22:37.220
or just easier to give. So, the one side is
these kind of personal stories and the compelling

00:22:37.220 --> 00:22:42.370
photographs that really arouse our sympathies.
Another way is by breaking down the barriers

00:22:42.370 --> 00:22:47.290
to giving. So that's, that's things like in
the checkout at Whole Foods, you can give

00:22:47.290 --> 00:22:53.650
a dollar to the Whole Planet Foundation -- so
microcredit fund -- and when you tack on $1

00:22:53.650 --> 00:22:58.510
to a $100 grocery bill, "Surprise, surprise!"
it doesn't hurt so much. It's very easy to

00:22:58.510 --> 00:23:04.210
give on the margin when you're already spending
a bunch of money. Along similar lines is the

00:23:04.210 --> 00:23:11.650
donate-by-text campaigns that we saw particularly
after the earthquake in Haiti. So, there's

00:23:11.650 --> 00:23:17.770
a very funny kind of anecdote to come out
of that which, which illustrates a basic point.

00:23:17.770 --> 00:23:22.950
And that's when it's too easy to give, sometimes
we make mistakes. We just, we give without

00:23:22.950 --> 00:23:28.820
thinking. Generally it's not so bad to make
a mistake and have it be charitable, but,

00:23:28.820 --> 00:23:33.890
but it's a mistake nonetheless. So, I want
to just read this anecdote -- which we talk

00:23:33.890 --> 00:23:38.000
about in the book -- which describes one of
these mistakes. And it's kind of funny. So

00:23:38.000 --> 00:23:43.540
this is from, from Facebook. So.
[reading] "Giving by text message takes a

00:23:43.540 --> 00:23:49.260
few seconds and is utterly gratifying. You
type in Haiti, press send, get an instant

00:23:49.260 --> 00:23:52.570
response thanking you for your generosity.
You hardly have time to think of the phone

00:23:52.570 --> 00:23:56.809
bill coming at the end of the month. And when
it does arrive, your $10 is easier to part

00:23:56.809 --> 00:24:00.850
with because it's tacked on to the cost of
phone service, a cost you're already prepared

00:24:00.850 --> 00:24:06.590
to bear. Unless, that is, you are Cara. The
following was pulled from a real Facebook

00:24:06.590 --> 00:24:14.790
page. Cara's profile said, 'I've texted Haiti
to 90999 over 200 times, that's over $2000

00:24:14.790 --> 00:24:21.070
donated to Haiti relief efforts. Join me.'
So she's excited. Then, underneath the status

00:24:21.070 --> 00:24:25.320
update were some comments from her friends.
And her friend, Noah, said, 'Your parents

00:24:25.320 --> 00:24:30.590
might not like your cell phone bill this month.'
Cara said, 'It's not my money. Ha! Wait a

00:24:30.590 --> 00:24:34.780
second. This doesn't get added to the phone
bill, does it? I thought it was just a free

00:24:34.780 --> 00:24:40.950
thing.' And her other friend says, 'Cara,
shoot, no. Every text is $10.' 'Oh, wow. Are

00:24:40.950 --> 00:24:45.960
you sure? This could be very bad for me.'
Then he says, 'Yeah, I saw it on the football

00:24:45.960 --> 00:24:50.140
game. They bill it to your cell phone bill.'
Another friend chimed in, 'Yeah, every text

00:24:50.140 --> 00:24:54.220
is 10 bucks. It said so when the Health and
Human Services lady came on and told people

00:24:54.220 --> 00:24:58.820
about it on Colbert Report. Uh, maybe you
should ask for people to help you pay your

00:24:58.820 --> 00:25:02.280
phone bill?' And Cara said, 'Well, thanks
for letting me know. Ha, ha. Haiti must love

00:25:02.280 --> 00:25:07.679
me.' [audience chuckles]
So, so that's kind of the, the point, right?

00:25:07.679 --> 00:25:14.440
It can be too easy to give and Cara's $2000
mistake is not a tragedy in the, in the big

00:25:14.440 --> 00:25:19.960
picture of the world. It's not a bad thing
to give $2000, even of her parents' money,

00:25:19.960 --> 00:25:26.970
even by accident. But, but optimally, we could
do better, right? We could, we could as donors

00:25:26.970 --> 00:25:31.780
be a little bit more critical and we could
ask proactively, "Which things do I want to

00:25:31.780 --> 00:25:37.970
support?" and, and ideally we could make that
decision on the basis of impacts. So, I'd

00:25:37.970 --> 00:25:43.370
like to use my dollars to make the most, the
biggest possible difference in the world on

00:25:43.370 --> 00:25:48.210
some set of outcomes that I care about. And,
and so we should look to the evidence of which

00:25:48.210 --> 00:25:55.390
programs achieve those goals most effectively.
And, and so this book is -- we hope -- helps

00:25:55.390 --> 00:26:01.890
people to do exactly that. So the, the sort
of first, most basic goal is to get people

00:26:01.890 --> 00:26:06.860
thinking more critically, and more proactively,
and asking questions. And then we close the

00:26:06.860 --> 00:26:11.830
book, the sort of last chapter is a little
bit of a cheat sheet where we, we name a few

00:26:11.830 --> 00:26:16.900
ideas that we're particularly excited about
which have sort of come out through this process

00:26:16.900 --> 00:26:21.840
of evaluation. And they're looking â€” some
of them are more proven than others, but,

00:26:21.840 --> 00:26:27.390
in general, evidence shows them to be quite
effective. So, one caveat there is that the

00:26:27.390 --> 00:26:33.670
book is printed and the ink is dry, but research
is always on-going. So, the ideas we talk

00:26:33.670 --> 00:26:39.090
about there mirror an initiative at IPA called
'The Proven Impact Initiative', where IPA

00:26:39.090 --> 00:26:46.049
is constantly combing through current research
and consolidating it into recommendations

00:26:46.049 --> 00:26:51.929
of ideas, particular approaches, that are
shown to be especially effective achieving

00:26:51.929 --> 00:26:57.710
particular outcomes. So, so that's something
to keep track of, too. So that's a little

00:26:57.710 --> 00:27:02.620
more about the why of this book, and now Dean
will tie it up.

00:27:02.620 --> 00:27:03.620
[pause]

00:27:03.620 --> 00:27:14.070
&gt;&gt;Dean Karlan: Thanks. Thanks, Jake. So, [pause]
so I want to just tie it up by telling you

00:27:14.070 --> 00:27:19.660
a little bit more on what we're doing, on
what Jake just said. So, Innovations for Poverty

00:27:19.660 --> 00:27:25.940
Action is a non-profit that I started in 2002.
And we're now have projects in 46 countries

00:27:25.940 --> 00:27:34.590
and almost 500 employees. About 250 employees
that are U.S. or European on U.S. payroll,

00:27:34.590 --> 00:27:39.940
and about half of our staff are permanent
staff, local, local nationals helping to manage

00:27:39.940 --> 00:27:46.309
our projects overseas. We've grown really
rapidly. And part of the reason for that is

00:27:46.309 --> 00:27:49.580
-- there's two reasons why we started the
pro -- started this in the first place. And

00:27:49.580 --> 00:27:53.570
one of the reasons for our growth, that our
rapid growth, is the first of these reasons.

00:27:53.570 --> 00:27:59.120
The first was doing this field work was not
what we learned to do when we were in graduate

00:27:59.120 --> 00:28:02.980
school. That's the reality. The reality is
that we learn in graduate school about theory,

00:28:02.980 --> 00:28:09.520
we learn about data analysis, and we do not
learn how to manage a team of surveyors and

00:28:09.520 --> 00:28:13.350
we do not learn survey design, and we do not
learn -- well, maybe a little survey design

00:28:13.350 --> 00:28:17.250
-- but we do not learn some of the day-to-day
management that needs to go into actually

00:28:17.250 --> 00:28:21.030
going and collecting this field work, or,
in many cases, if we're actually working on

00:28:21.030 --> 00:28:24.920
an intervention we're working with some group
and helping them think through what the actual

00:28:24.920 --> 00:28:28.480
product is that they're testing. And we're
getting involved in that space. And this is

00:28:28.480 --> 00:28:34.679
not our normal way of doing things in academia.
And one of the things that I recognized when

00:28:34.679 --> 00:28:39.830
I finished graduate school is that we needed
an entity that didn't just churn, in and out,

00:28:39.830 --> 00:28:44.330
undergrads and grad students for one year.
That we needed an entity with a life, that

00:28:44.330 --> 00:28:50.039
had its own internal capacity, its own internal
learning, that stayed with people. And so

00:28:50.039 --> 00:28:55.740
that was one of the reasons for creating Innovations
for Poverty Action. And what we found very

00:28:55.740 --> 00:29:00.870
quickly was once we created it, and people
came. I mean, the researchers really saw the

00:29:00.870 --> 00:29:04.360
value of this because if they're a researcher
and did not have that staff, and said all

00:29:04.360 --> 00:29:08.960
of a sudden "I want to do a project in Ghana.
Can you help me?" And we said, "Yes." And

00:29:08.960 --> 00:29:11.990
then we helped them. And then we saw, and
then they see that they can take their skill

00:29:11.990 --> 00:29:17.309
set as professors, and then there's this field
arm that can really go and execute very well

00:29:17.309 --> 00:29:23.539
the field, the field research. It was a great
marriage. And we grew without really trying,

00:29:23.539 --> 00:29:27.850
merely by doing what we were doing and becoming
an organization that groups, that researchers

00:29:27.850 --> 00:29:32.111
kept coming to us and saying, "Can you help?
Can you help? Can you help?" And, similarly,

00:29:32.111 --> 00:29:36.950
foundations kept coming ... and foundations
and saying, "Wow. We saw this research and

00:29:36.950 --> 00:29:40.200
we want to do more of it."
So we got really good at the knowledge creation

00:29:40.200 --> 00:29:46.660
side, in terms of being a conduit for the
researchers who wanted to do research, the

00:29:46.660 --> 00:29:52.460
foundations that wanted answers to questions.
The ... What do you do with all that information,

00:29:52.460 --> 00:29:56.679
though? That was one of the other key "ahas"
and inspiring moments for starting Innovations

00:29:56.679 --> 00:30:01.290
for Poverty Action was recognizing that if
I just respond to my academic incentives and

00:30:01.290 --> 00:30:05.340
other researchers just respond to their academic
incentives, we're not really gonna make the

00:30:05.340 --> 00:30:09.240
impact on the world that we want to make,
that for many of us is what inspired us to

00:30:09.240 --> 00:30:14.580
do this type of work. And for that we needed
an organization that is committed to a couple

00:30:14.580 --> 00:30:19.450
different things. One is to working with,
with donors and with organizations to get

00:30:19.450 --> 00:30:24.450
that information into the right hands. The
information overload problem that Jake mentioned

00:30:24.450 --> 00:30:28.220
with Cara and the things being shouted at
us. This is about donors, but it's also about

00:30:28.220 --> 00:30:32.299
the people who work for OxFam, work for CARE.
How do they sift through it all to figure

00:30:32.299 --> 00:30:36.270
out what's really gonna work and what they
should be doing? The problem is on, on many

00:30:36.270 --> 00:30:41.350
levels. And so that's where the, the information
dissemination is so important for us as an

00:30:41.350 --> 00:30:46.590
organization to work. And with donors -- small
and medium and as well as large. But the large

00:30:46.590 --> 00:30:50.710
donors, that's a conversation that as professors
we're often engaged in anyhow, conversations

00:30:50.710 --> 00:30:55.190
with the World Bank and Gates Foundation and
groups like that. But we're not engaged as

00:30:55.190 --> 00:31:00.690
a professor with conversations with people
who are, are in running small, medium family-sized

00:31:00.690 --> 00:31:08.240
foundations or even smaller retail type, type
audience. So it's that, that communication

00:31:08.240 --> 00:31:12.600
work and that outreach to organizations that's
so important to what we're doing now, and,

00:31:12.600 --> 00:31:17.309
and one of the main areas that we're pushing
to grow as an, as an organization.

00:31:17.309 --> 00:31:24.940
Now, um. So I want to close with giving you
three ideas. This is a ... I talked with a

00:31:24.940 --> 00:31:30.299
couple of people who, who, who told me that
it's really important when talking with people

00:31:30.299 --> 00:31:35.299
at Google to not just talk and say, "Woohoo,
look at what we -- Look at this book. And

00:31:35.299 --> 00:31:38.870
this is what we found," but to try to throw
out some ideas to the, to the audience to

00:31:38.870 --> 00:31:43.950
say that here's an example of, of very proactive
things that, that we're looking for help to

00:31:43.950 --> 00:31:46.750
do to see if there's anybody that's interested
in help.

00:31:46.750 --> 00:31:56.150
So, let me give you one. So one is: what's
striking to us is ... we are, we're an organization

00:31:56.150 --> 00:32:01.590
that's -- we're dead without data. And, and
by dead without data I also mean we're dead

00:32:01.590 --> 00:32:05.790
without a good way of collecting data. When
I first started doing survey work, it was

00:32:05.790 --> 00:32:10.610
pen and paper. And that was how we did it.
We went out and we printed paper, you know.

00:32:10.610 --> 00:32:14.290
We'd go out and collect it and then someone
sat there and did data input and that's how

00:32:14.290 --> 00:32:18.309
we did it. We're now seeing in the world,
there's a lot of different kinds of bootstrap

00:32:18.309 --> 00:32:22.760
things that are going on in a lot of different
platforms that are incomplete, but that are

00:32:22.760 --> 00:32:28.110
trying to solve this data collection problem
of having surveyors go out in the field and,

00:32:28.110 --> 00:32:33.679
and gather data that gets then fed back to
us. But there's not a really good platform

00:32:33.679 --> 00:32:38.550
for doing this that really cuts across all
of the various needs and, and types of circumstances

00:32:38.550 --> 00:32:42.549
that we find ourselves in. And we're not unique
in this. There's other groups that value this,

00:32:42.549 --> 00:32:46.550
too. This is not just a research tool. There's
a lot of organizations that need this type

00:32:46.550 --> 00:32:53.059
of data for their own internal monitoring
and accountability. And, and, and, so how

00:32:53.059 --> 00:32:57.049
is that ... what's the right platform for
that? And so we, we've grappled with a few

00:32:57.049 --> 00:33:00.929
pieces of software that are out there, but
nothing ever kind of fits the, fits the, fits

00:33:00.929 --> 00:33:10.200
the, the total answer.
A second thing that is, that, that is related

00:33:10.200 --> 00:33:18.700
to, to what I was talking about, to -- earlier
about opportunity costs. One of the things

00:33:18.700 --> 00:33:27.390
that's striking to me is how many people I,
I talk to that say that they have intentions

00:33:27.390 --> 00:33:33.179
for giving to charity, that don't. But then
when you look at what they actually do, they

00:33:33.179 --> 00:33:36.720
don't quite live up to what they said they're
going to do. And this is not much different

00:33:36.720 --> 00:33:40.700
than the temptation story I was talking about
earlier. You say you're going to do some things.

00:33:40.700 --> 00:33:44.520
Then you get tempted away by other things.
And then you look at your bank account. You're

00:33:44.520 --> 00:33:47.730
making your charitable donation and you don't
have as much in that bank account as you thought

00:33:47.730 --> 00:33:52.120
you were going to have. And so you end up
making less that was originally intended.

00:33:52.120 --> 00:33:57.980
How, how do we go about making that, that
link more, more salient between consumption

00:33:57.980 --> 00:34:03.929
and, and charity? So one -- there's an example
from my -- that I experienced that I think

00:34:03.929 --> 00:34:10.409
can -- tells a lot about this. So I was in,
I was in Peru. And I have a jacket that I

00:34:10.409 --> 00:34:16.179
bought in Peru about 8 years ago that's made
of alpaca wool. And it has holes in it now.

00:34:16.179 --> 00:34:19.690
And so I'm back in Peru for the first time
in a while and I'm thinking, "Oh, this would

00:34:19.690 --> 00:34:23.789
be nice to go get a new jacket 'cause I'm
sick and tired of walking down the street

00:34:23.789 --> 00:34:27.450
and having someone chase me down saying, 'Sir,
your wallet. You dropped your wallet.' 'Cause

00:34:27.450 --> 00:34:32.489
there's too many holes in my pocket. And what's
embarrassing, that's happened more than once.

00:34:32.489 --> 00:34:36.760
You'd think that once would be enough. But,
no, right? But, regardless. So I'm there and

00:34:36.760 --> 00:34:39.940
I'm thinking, you know, maybe I should go
ahead and get another jacket. So I go into

00:34:39.940 --> 00:34:44.609
a store which is where I think I bought it
8 years ago, and the jacket was $300. Now

00:34:44.609 --> 00:34:47.619
that's a bit more than I would typically spend
on a jacket. I'm not the type of person who

00:34:47.619 --> 00:34:51.480
spends money on clothing. And I'm -- So I'm
first thinking that maybe this wasn't the

00:34:51.480 --> 00:34:55.779
store. Maybe I just came into the store, and
bought it somewhere else. And so I'm shopping

00:34:55.779 --> 00:35:00.739
around and I'm not finding a jacket that I
like that's for, that's made of alpaca that's

00:35:00.739 --> 00:35:06.350
less than $300. And then I thought to myself,
"Do I really need this jacket?" I have a jacket.

00:35:06.350 --> 00:35:10.180
It wasn't the alpaca one. I had another one
that was just a simple North Face jacket that

00:35:10.180 --> 00:35:14.920
I like. Do I really need this second jacket?
And then I thought to myself, "Well, what's

00:35:14.920 --> 00:35:19.999
the opportunity cost? What, what else would
I do with $300? What else could $300 do?"

00:35:19.999 --> 00:35:25.859
Well, a simple trade-off is 60 bed nets. Right?
On average will 60 bed nets save a child's

00:35:25.859 --> 00:35:31.839
life? I don't know the exact math, but let's
say, roughly, yes. So I thought to myself,

00:35:31.839 --> 00:35:36.369
"OK. Great.That's a better choice." It is
a clear choice. I can't spend the $300 on

00:35:36.369 --> 00:35:40.190
both. It's not the ... we're not talking the
Kennedy Silver Magic bullet here that can

00:35:40.190 --> 00:35:45.069
go through both things. Right? So it's either-or.
So I made the choice on the bed nets. Did

00:35:45.069 --> 00:35:50.759
I send the money off to the bed net place
when I got home? No. Haven't done it yet.

00:35:50.759 --> 00:35:55.900
Not explicitly.
So, why not? What if I was able to have a

00:35:55.900 --> 00:36:00.319
platform at that very moment that I could
just send an email, log in to the website,

00:36:00.319 --> 00:36:05.060
where I make that transfer salient. I make
that mental account very clear that I am foregoing

00:36:05.060 --> 00:36:11.549
this item specifically in order to support
a particular cause. And now I'm holding myself

00:36:11.549 --> 00:36:17.391
accountable by having this, this mental accounting
process that is now on this platform and I

00:36:17.391 --> 00:36:21.239
can ... You can imagine many ways of. of feeding
information into it. You can imagine having

00:36:21.239 --> 00:36:27.180
social networks that, that agree on common
consumption goods to give up. Maybe it's the

00:36:27.180 --> 00:36:31.200
daily lattÃ© that when you add it up for the
entire year you go, "My gosh. Really? I'm

00:36:31.200 --> 00:36:36.609
spending $2000 a year on lattÃ©s?" You know,
maybe that's something I could give up and

00:36:36.609 --> 00:36:40.900
$2000 can do something else. So, so the tool
can do a lot to help people see the salience

00:36:40.900 --> 00:36:46.140
of what they're giving up. And, and this might
inspire more giving. And for some maybe it

00:36:46.140 --> 00:36:51.279
wouldn't. But for those who want to aspire
to be more charitable in their trade-offs,

00:36:51.279 --> 00:36:56.200
it would be a tool for helping them see and
make that sali-, make that trade-off very

00:36:56.200 --> 00:37:02.069
salient. So there's, there's an analogy that
comes from Peter Singer, who's a philosopher

00:37:02.069 --> 00:37:07.319
at Princeton, that tells a lot, that makes
this analogy exactly, perfectly. The analogy

00:37:07.319 --> 00:37:10.410
is you're walking down the street and there's
a child drowning in a lake. And you're on

00:37:10.410 --> 00:37:16.200
your way to a meeting. And if you jump in
that lake to save that child, you're going

00:37:16.200 --> 00:37:21.289
to miss that meeting. And let's say, for simplicity,
that meeting is going to earn you $300. So

00:37:21.289 --> 00:37:26.990
you have a trade-off right now. $300 or the
child. Most people would say that there's

00:37:26.990 --> 00:37:33.710
an ethical obligation for, certainly for people
who are of modest capabilities financially,

00:37:33.710 --> 00:37:41.019
to forego the $300 and save that child's life.
OK. So, right now in Africa the same logic

00:37:41.019 --> 00:37:46.410
could apply. Go ... I can give you the names
of 20 different organizations that are immunizing

00:37:46.410 --> 00:37:53.440
children, or passing out bed nets. $300. Child's
life. It's the same exact trade-off. There

00:37:53.440 --> 00:37:57.309
is one key difference. And this goes back
to the first theme of this entire, the entire

00:37:57.309 --> 00:38:00.700
book. So a lot of people responded and I'm
willing to be that a lot of people right in

00:38:00.700 --> 00:38:05.219
this room are saying to me, "No, no, no, Dean.
You're wrong. It's not the same. You told

00:38:05.219 --> 00:38:11.369
me that I can save that child. And I know
how to swim and so I will save that child.

00:38:11.369 --> 00:38:15.739
But how do I know the $300 is gonna save the
child in Africa? How do I really know that?"

00:38:15.739 --> 00:38:21.390
Right? And that's one of the reasons for doing
this type of data work, this type of evaluation.

00:38:21.390 --> 00:38:27.020
Is to remove some, at least some, of that
uncertainty so that you know that $300 can

00:38:27.020 --> 00:38:30.339
actually make a difference, and here's how
to do it. And that's one of the motivations

00:38:30.339 --> 00:38:35.930
for doing it. And also what the other message
from that analogy is about the salience of

00:38:35.930 --> 00:38:41.559
what we're giving up. And, and the key is:
how can we create an app or a tool to help

00:38:41.559 --> 00:38:48.580
people think more concretely about the trace,
the choices they make in life, so that they

00:38:48.580 --> 00:38:53.910
can adhere to the value system that they say
they have? And how can we make the data actually

00:38:53.910 --> 00:38:59.130
help people fit the values that they claim
to have and want to have? So that's, that's

00:38:59.130 --> 00:39:02.020
something that I'm very excited about. And
if there's people here interested in doing

00:39:02.020 --> 00:39:05.670
that, please, by all means, let me, let me
know.

00:39:05.670 --> 00:39:09.240
There's one other twist to that lake analogy,
just to, just to fill the loop in. So if you

00:39:09.240 --> 00:39:14.049
ever want to talk about that, it's-- the other
thing to do is to, to make it so that you

00:39:14.049 --> 00:39:20.519
can't swim. OK. But there's a lifesaver on
the other side of the lake. And then ask yourself,

00:39:20.519 --> 00:39:25.080
"Do you have an ethical obligation to miss
that meeting? To go all the way to the other

00:39:25.080 --> 00:39:30.950
side of the lake? Find that lifesaver? Throw
it into the lake and hope you hit the child?"

00:39:30.950 --> 00:39:34.690
Well, not hit them on the head, hopefully,
and pass them out. But hit them -- get it

00:39:34.690 --> 00:39:39.890
near the child. Right? So now, what have I
done? I've made both of them a, a, an uncertain

00:39:39.890 --> 00:39:44.910
event. You may save the child with the lifesaver.
You may save the child in Africa. Don't know

00:39:44.910 --> 00:39:49.299
about either. Right. So that, that's another
way of changing that analogy to kind of wreak

00:39:49.299 --> 00:39:55.329
havoc with, with why people answer them differently.
OK. The third thing -- just to let you know

00:39:55.329 --> 00:40:00.779
-- in terms of how IPA is structured. Right.
So we don't, we do a lot of projects where

00:40:00.779 --> 00:40:04.009
an organization comes to us and says, "Please
tell us if we're, if this works. We'd like

00:40:04.009 --> 00:40:08.019
to see an evaluation." And a donor comes to
us and says, "We want to know if that works.

00:40:08.019 --> 00:40:11.430
We're supporting that. And we want to know
whether we should support more of it. And

00:40:11.430 --> 00:40:15.009
so please do an evaluation of the pilot so
that we can whether we should scale it up."

00:40:15.009 --> 00:40:19.569
And we, we like those projects. But the projects
that we really love, that have much bigger

00:40:19.569 --> 00:40:23.180
impact in terms of numbers, is when we can
take a step back, working with the donors,

00:40:23.180 --> 00:40:26.349
working with the organizations doing things
and saying, "What are the questions that you

00:40:26.349 --> 00:40:31.779
need answering in order to crack this nut?
And now how can we set up evaluations and,

00:40:31.779 --> 00:40:36.520
and experiments that help answer those types
of problems?" And that's when we, we have

00:40:36.520 --> 00:40:42.809
initiatives that have pots of money -- usually
ranging from two to ten million dollars -- where

00:40:42.809 --> 00:40:47.170
we then have an ability to take a step back
and say, "We need research that's going to

00:40:47.170 --> 00:40:51.210
answer these five questions." And then we
open it up to researchers to propose specific

00:40:51.210 --> 00:40:54.950
settings in which they think they can nail
one of these five questions. And then we use

00:40:54.950 --> 00:41:00.039
this process to facilitate the funding of,
of research to answer particular questions.

00:41:00.039 --> 00:41:05.021
So, for instance, the small-medium enterprise
market is a huge market that we see a lot

00:41:05.021 --> 00:41:09.990
of stagnation, a lot of lack of growth. And
that's where jobs can get created. The, the

00:41:09.990 --> 00:41:14.470
term "entrepreneur" for microcredit, in microcredit
-- to bring us back to the microcredit conversation

00:41:14.470 --> 00:41:19.680
-- is a very sexy term to most people in America.
Certainly, I'm guessing, to everybody in this

00:41:19.680 --> 00:41:24.570
room. It's a, it is to me. 'Entrepreneur is
exciting.' It's not ... In developing countries,

00:41:24.570 --> 00:41:30.480
entrepreneur is synonymous with "I don't have
a job." That's a very different beast. Right?

00:41:30.480 --> 00:41:36.000
And it's really a misplaced goal to say that
microenterprise growth, the goal of microenterprise

00:41:36.000 --> 00:41:40.230
is to grow microenterprises into small-medium
enterprises. We want to take people with zero

00:41:40.230 --> 00:41:45.119
employees and somehow figure out how to get
them 10, or 20, or 100 employees. That's a,

00:41:45.119 --> 00:41:50.190
that's a diamond in the rough. But there are,
there are firms out there that are hiring

00:41:50.190 --> 00:41:54.410
some. Not too many. And then the question
is, "What's hindering their growth?" They've

00:41:54.410 --> 00:41:58.619
gotten over the hurdle of the zero employee
to some employees. Now, what's necessary to

00:41:58.619 --> 00:42:03.249
get them bigger so that they can create more
jobs? That's a huge nut that we're trying

00:42:03.249 --> 00:42:07.890
to crack. So, other ones that are in that
space are on the savings side. When we've

00:42:07.890 --> 00:42:13.470
seen the lackluster, the positive but still
not, not super-exciting results on impact

00:42:13.470 --> 00:42:18.289
of credit, that's focused our energy a lot
to ask, "Well, what about savings?" Savings,

00:42:18.289 --> 00:42:22.011
you don't have the higher interest rates.
You're -- and what do we need to do to then

00:42:22.011 --> 00:42:25.779
help people save up to things that they want
to do? And so we have an initiative that is

00:42:25.779 --> 00:42:30.650
about savings. Similarly, there's, there's
a lot of work that we've talked about. We've

00:42:30.650 --> 00:42:35.849
thought about having an initiative on mobile,
on mobile apps. There's a huge number of interventions

00:42:35.849 --> 00:42:41.210
that, that we can talk about that are interesting
and exciting. But at the same time we wonder,

00:42:41.210 --> 00:42:45.579
"OK. Is this just exciting 'cause it's kind
of cool or is it actually going to improve

00:42:45.579 --> 00:42:51.910
welfare in a way that's important?" And so,
how can technology be used to improve people's

00:42:51.910 --> 00:42:57.469
lives in, in rural areas of Africa or Latin
America, Asia? And again, there,it wouldn't,

00:42:57.469 --> 00:43:00.849
we want to be able to take the approach of
rather than just finding a particular thing

00:43:00.849 --> 00:43:03.999
and evaluating it, we want to be able to step
back and understand what are the, what are

00:43:03.999 --> 00:43:08.599
the key, over-arching questions we have to
ask. You know, one obvious one is can the

00:43:08.599 --> 00:43:13.369
technology alone do something? Or when do
we have to think about combining technology

00:43:13.369 --> 00:43:18.569
with a human touch in terms of the way things
are integrated into a development program?

00:43:18.569 --> 00:43:22.650
And so those, better understanding of how
the interaction has to take place in order

00:43:22.650 --> 00:43:27.401
to make sure the technology's used for good
is an important question that we want to ask.

00:43:27.401 --> 00:43:32.749
And so that's an example where we're looking
for a pot of funds that we can then work creatively

00:43:32.749 --> 00:43:36.950
to help answer the questions that the, the
researchers, the researchers in this case,

00:43:36.950 --> 00:43:41.959
what I mean, is the technology researchers,
and the organizations, and the donors are

00:43:41.959 --> 00:43:47.020
looking for answers to these questions in
order to know what to, what to scale up and

00:43:47.020 --> 00:43:51.700
what not. So, so with that, I will turn it
to Q &amp; A. [pause]

00:43:51.700 --> 00:43:52.700
[applause]

00:43:52.700 --> 00:44:11.670
&gt;&gt; Eric: If you have questions, please come
up to the mike so that the remote sites and

00:44:11.670 --> 00:44:15.920
the recording can get your question. [pause]
[Dean sips water.]

00:44:15.920 --> 00:44:21.569
&gt;&gt;Female Audience Member #1: Thank you. It
was a really good talk. Um, my question is

00:44:21.569 --> 00:44:24.140
regards to, well, a couple points you brought
up when you were talking about, you know,

00:44:24.140 --> 00:44:30.950
how easy it is to give, and how -- basically,
the message I took was it's important to research

00:44:30.950 --> 00:44:31.950
and make a good economic choice about where
you put your money and make sure that it works.

00:44:31.950 --> 00:44:33.890
But how do you reconcile that with, basically,
government charity where I really can't choose

00:44:33.890 --> 00:44:38.489
particularly where my money goes. It's taken
from me and it goes into some giant hole,

00:44:38.489 --> 00:44:44.380
and policy-wise things don't change very rapidly.
If something's discovered not to work, they

00:44:44.380 --> 00:44:50.690
can't change it very quickly, either. How
do those kind of reconcile those two viewpoints?

00:44:50.690 --> 00:44:55.910
&gt;&gt;Dean Karlan: So, I mean, there is, though,
I mean, so clearly. We don't have a -- I've

00:44:55.910 --> 00:45:00.599
heard people proposals of a taxation system
where we actually get to choose allocations.

00:45:00.599 --> 00:45:06.390
I actually think that might be a dangerous
thing. Having said that, there, there is a

00:45:06.390 --> 00:45:12.249
lot of work, and a lot of our researchers
are working in the developing countries. When

00:45:12.249 --> 00:45:15.779
we work, we are working with USAID or Millenium
Challenge Corporation, which are two U.S.

00:45:15.779 --> 00:45:21.969
government agencies, to help use this type
of evaluation to help them direct their resources

00:45:21.969 --> 00:45:26.799
to better decisions. The Department of Education,
the Department of Labor have been doing rigorous-type,

00:45:26.799 --> 00:45:32.030
randomized trials for a long time. Actually,
the first randomized trials in economics that

00:45:32.030 --> 00:45:36.579
were done were in the 1960s by the U.S. government.
So I'm not saying that it's like, like everything

00:45:36.579 --> 00:45:40.170
fits that bill, by any means. I don't want
to pass that. But I think the answer is the

00:45:40.170 --> 00:45:44.200
same. And it's not ... But I wouldn't put
that in the hands of the, of the taxpayer.

00:45:44.200 --> 00:45:48.609
Right? That's in the hands of, of the executive
branch of the government and the Congress

00:45:48.609 --> 00:45:52.739
to say, "Let's take evaluation more seriously
in how we think about what we're doing. And

00:45:52.739 --> 00:45:56.049
there are a lot of ... there are some ... a
lot of government programs which do do that.

00:45:56.049 --> 00:45:57.680
And, obviously, there's a lot that do not.

00:45:57.680 --> 00:46:00.910
&gt;&gt;Female Audience Member #1: Yeah. But I guess
my question is: From a government perspective

00:46:00.910 --> 00:46:07.220
they get the money. Where's their incentive
to provide this proof of value to us?

00:46:07.220 --> 00:46:12.680
&gt;&gt;Dean Karlan: So there is -- okay, so, there
is -- look. There's a political risk that

00:46:12.680 --> 00:46:17.209
obviously a lot have overcome. But, you know,
nobody wants their program to appear like

00:46:17.209 --> 00:46:23.739
a failure. So you have to, as a politician,
really promote the point that you are, you

00:46:23.739 --> 00:46:29.529
are exercising prudence in making sure that
you choose the right thing. And look, and

00:46:29.529 --> 00:46:33.789
look, just as a matter of fact, there are
a lot of randomized trials done on government

00:46:33.789 --> 00:46:38.019
funding projects. So, I mean it is, just in
terms of looking at the way the world does

00:46:38.019 --> 00:46:42.700
work, this has been overcome in a lot of situations.
And there are a lot of government funded programs

00:46:42.700 --> 00:46:46.729
which do go through that. But by any means
I am not trying to say there's, like, enough.

00:46:46.729 --> 00:46:51.539
I think more should. But it is the case that
there's a lot of government programs that

00:46:51.539 --> 00:46:54.640
receive that. Millenium Challenge Corporation
is a good example of one that from the day

00:46:54.640 --> 00:46:59.060
one of that group -- I'm not saying that everything
... that I like everything they do. But one

00:46:59.060 --> 00:47:04.430
thing they have taken very seriously is when
we can, we want to see randomized trials done

00:47:04.430 --> 00:47:10.839
to evaluate our aid. And that was done from
day one. And USAID is really moving in that

00:47:10.839 --> 00:47:14.910
direction. We'll see. It's a big ship. It
needs moving. But the people, they -- that

00:47:14.910 --> 00:47:19.940
Obama brought in at the top are really dedicated
to introducing strong evaluations to help

00:47:19.940 --> 00:47:24.680
guide their resources. So, you know, where
there's, where there's will, there's a way.

00:47:24.680 --> 00:47:30.019
And you, there is, there is a message that
needs to be managed. Because if the evaluations

00:47:30.019 --> 00:47:34.920
start proving that everything failed, they
have to make sure from the political messaging

00:47:34.920 --> 00:47:39.349
to get their, get their PR people kind of
figuring out how to deal with that problem

00:47:39.349 --> 00:47:44.259
to make sure that the message is that, like,
"Hey. We learned it didn't work and we changed

00:47:44.259 --> 00:47:48.640
our decision because of that. Because we're
smart and we learn." I'd like to think that's

00:47:48.640 --> 00:47:50.029
how the world could work.

00:47:50.029 --> 00:47:53.459
&gt;&gt;Female Audience Member #1: Yeah. It could.

00:47:53.459 --> 00:47:59.829
&gt;&gt;Male Audience Member #1: Thanks for [inaudible].
This is really fascinating. My question to

00:47:59.829 --> 00:48:05.680
you is related to how we as Google in U.S.
maybe an academic, like maybe kind of an academic

00:48:05.680 --> 00:48:09.579
institution love to see data. But if we ask,
like, non-profits on the ground who are doing

00:48:09.579 --> 00:48:13.860
aid work, would they see randomized trials
and sort of building counterfactuals as a

00:48:13.860 --> 00:48:20.160
sort of non-immediate achievement of their
goals? And sort of using resources maybe for

00:48:20.160 --> 00:48:26.549
an academic exercise where they could have
been used for direct impact [inaudible]

00:48:26.549 --> 00:48:32.049
&gt;&gt;Dean Karlan: Right. So, let me separate
your question into two parts. There's ... there's

00:48:32.049 --> 00:48:35.420
a question ... there's an ethics question
-- which I think about a lot -- which is "Should

00:48:35.420 --> 00:48:39.869
we be doing evaluation? Should we be doing
research on something?" Right? If there's

00:48:39.869 --> 00:48:45.480
an organization and we know it's doing well,
should we be doing any research on them, or

00:48:45.480 --> 00:48:49.180
should they be taking all of their available
resources and doing more? And that's not a

00:48:49.180 --> 00:48:53.239
question about randomized trials or other
research. That's just a question about should

00:48:53.239 --> 00:48:56.819
they be doing research? Are there questions
that they need answers to that we're actually

00:48:56.819 --> 00:49:01.569
informing them on? Or, you know, or if they
already have the answers, then why are we

00:49:01.569 --> 00:49:04.940
wasting our time and their money? And that's
an ethics issue. But that's a question about,

00:49:04.940 --> 00:49:10.479
about of just research. It's not about randomized
trials or not. Now, there's a second issue

00:49:10.479 --> 00:49:15.920
which is randomized trials does -- not all
-- There are many situations in which we can

00:49:15.920 --> 00:49:20.299
do randomized trials and we basically leave
the organization alone. And they, they really

00:49:20.299 --> 00:49:23.969
change very little about what they have to
do, that is intrusive. But, by all means,

00:49:23.969 --> 00:49:30.549
there are some times where it is intrusive
to what they're doing. And then, then the

00:49:30.549 --> 00:49:36.229
organization has to ask themselves, "Well,
you know, if I'm using the evaluation to make

00:49:36.229 --> 00:49:40.499
forward-looking decisions, like, what are
those forward-looking decisions? How important

00:49:40.499 --> 00:49:44.930
are they? How much future resources are at
stake here that we make sure we get it right?"

00:49:44.930 --> 00:49:48.799
The minute evaluation is used to look forward
rather than to look back, it becomes a much

00:49:48.799 --> 00:49:53.960
more appetizing process. Because we realize
it's not about saying are these million dollars

00:49:53.960 --> 00:49:57.739
being spent well? But it's saying, "We're
about to spend a hundred more million dollars

00:49:57.739 --> 00:50:01.910
in the next 20 years, and how should we best
spend that money?" And the minute that's the

00:50:01.910 --> 00:50:06.269
right mindset it becomes a very exciting process
for everybody because they start realizing

00:50:06.269 --> 00:50:11.279
the incredible power that the research can
have in helping them maximize their impact.

00:50:11.279 --> 00:50:15.920
The other thing to note is that there are
many situations we deal with where the for-profit

00:50:15.920 --> 00:50:21.079
firm -- or a non-profit firm could behave
exactly like a for-profit. There are lots

00:50:21.079 --> 00:50:24.130
of for-profits -- as you all know here at
Google -- that run randomized trials all the

00:50:24.130 --> 00:50:29.279
time. They just don't post, share the data
with dorky academics like me that go write

00:50:29.279 --> 00:50:34.449
papers. So there's a lot of situations where
the non-profit could behave exactly like the,

00:50:34.449 --> 00:50:40.529
like the for-profit, running internal randomized
trials to help improve their operations. And,

00:50:40.529 --> 00:50:44.569
and then the partnership with academia in
some of those situations is a little bit of

00:50:44.569 --> 00:50:49.690
a quid pro quo called "it's like we're giving
free consulting advice in exchange for data.

00:50:49.690 --> 00:50:53.130
They're agreeing to let their results be made
public in exchange for the free consulting

00:50:53.130 --> 00:50:58.299
advice." And so there's a, that often does,
we do, I have been involved in exchanges that

00:50:58.299 --> 00:50:59.299
are more or less of that nature. [pointing
to audience] Yeah?

00:50:59.299 --> 00:51:00.299
&gt;&gt;Male Audience Member #2: Hi. Thanks for
coming to talk to us. I was wondering if there

00:51:00.299 --> 00:51:06.430
was a mechanism that you had thought of that
in terms of that one philosophical problem

00:51:06.430 --> 00:51:15.319
that you posed earlier about child drowning
in the lake. I think often, or at least one

00:51:15.319 --> 00:51:17.410
major reason people might think of is, "Well,
someone else is going to save them." Is there

00:51:17.410 --> 00:51:21.440
kind -- I mean, have you kind of explored
that mechanism of social versus individual

00:51:21.440 --> 00:51:29.079
responsibility, and, you know, how that kind
of all ties together? And you know, you came

00:51:29.079 --> 00:51:33.230
up with a lot of kind of like opportunity
costs type individual responsibility scenarios.

00:51:33.230 --> 00:51:38.859
But, you know, is there one that kind of captures
this ... this kind of problem I said?

00:51:38.859 --> 00:51:43.010
&gt;&gt;Dean Karlan: So. I mean, I totally agree
that, I mean, I've definitely --. That is

00:51:43.010 --> 00:51:46.700
a common reaction to that, and there's a couple
... So I have two thoughts for you. One is

00:51:46.700 --> 00:51:50.920
I'm gonna change the analogy on you in order
to still make the opportunity cost fine. So,

00:51:50.920 --> 00:51:55.049
fine. There's three other people standing
by the side of the lake. Are you not going

00:51:55.049 --> 00:51:59.410
to jump? Are you really gonna just walk away
because there's three other people not jumping

00:51:59.410 --> 00:52:02.259
in, either? Right?

00:52:02.259 --> 00:52:04.459
&gt;&gt;Male Audience Member #2: [inaudible]

00:52:04.459 --> 00:52:08.279
&gt;&gt;Dean Karlan: So, but there's an element
there where there's now I've made it both

00:52:08.279 --> 00:52:13.029
the same. And the point is there are many
situations in, in developing countries where

00:52:13.029 --> 00:52:20.920
the equilibrium is what it is. And the situation
right now is very simple. $300 can save a

00:52:20.920 --> 00:52:24.309
child's life. Right? And so the fact that
there's a billion people out there that could

00:52:24.309 --> 00:52:31.209
also send $300 is neither here nor there.
The equilibrium right now is, is what it is.

00:52:31.209 --> 00:52:35.529
And on the margin, you're making a trade-off.
Now, you know, like I said, my other favorite

00:52:35.529 --> 00:52:39.979
way of dealing with it is, you know, put a
mob of people on the side of the lake and,

00:52:39.979 --> 00:52:44.150
and ask whether the ethical thing is to just
stand there even if nobody else is moving.

00:52:44.150 --> 00:52:47.579
And most people would say no, you know. You
might look around a little and hope someone

00:52:47.579 --> 00:52:51.980
else goes in first. But at the end of the
day, are you really just not going to go?

00:52:51.980 --> 00:52:56.589
Now, I think the other, the other take-away
I have from your, from your question when

00:52:56.589 --> 00:53:01.160
I think about this is, is, is what it tells
us about how we, how we can be motivating

00:53:01.160 --> 00:53:07.119
giving. So I go back to the, to the website
app that I want to see created. And I think

00:53:07.119 --> 00:53:12.329
there's a lot of lessons to learn from that
exact question you posed about ways to make

00:53:12.329 --> 00:53:16.869
that app better; ways to incorporate a social
dimension to it so that people don't feel

00:53:16.869 --> 00:53:21.539
like they can free-ride and they really make
it salient that, "No, your action and your

00:53:21.539 --> 00:53:25.869
action alone can do this. And others aren't
going to do it, and you can't just rely on

00:53:25.869 --> 00:53:31.069
others." And I think that, that kind of question
you posed is something that gives me an idea.

00:53:31.069 --> 00:53:34.769
You know, there's specific things within that
app that one might want to test in order to

00:53:34.769 --> 00:53:36.609
figure that out more. [pointing to audience]
Yeah?

00:53:36.609 --> 00:53:39.349
&gt;&gt;Male Audience Member #3: Hi. I ... As a
small-time donor, I think that it's really

00:53:39.349 --> 00:53:47.709
great to be able to feel like the dollars
I'm putting in are doing good. And so I was

00:53:47.709 --> 00:53:58.470
really excited when I discovered Give Well.
And I was wondering if you know of other organizations

00:53:58.470 --> 00:53:59.660
like that that are helping to ... I guess,
take your academic work and make it more accessible

00:53:59.660 --> 00:54:00.660
to people like me?

00:54:00.660 --> 00:54:06.059
&gt;&gt;Dean Karlan: Right. No, and so ... There's
one domestically which I'm, I'm familiar with

00:54:06.059 --> 00:54:12.680
but not as well as, not as much as Give Well
called Root Cause. There's -- Guide Star is,

00:54:12.680 --> 00:54:17.839
is, is changing its -- it's gonna be new soon.
We actually just met with some people yesterday.

00:54:17.839 --> 00:54:23.579
And they are trying to, trying to incorporate
information from groups like Give Well, Guide

00:54:23.579 --> 00:54:28.170
Star. I'm sorry, Give Well, Root Cause, Innovations
for Poverty Action and, and figure out how

00:54:28.170 --> 00:54:36.110
to get that out to people. I think the -- you
know, the challenge is that, you know, as

00:54:36.110 --> 00:54:40.930
a donor you want to be ... There's a very
simple formula that you're trying to solve.

00:54:40.930 --> 00:54:47.680
You're trying to maximize A times B. A is:
Does the idea work? Does the idea of what

00:54:47.680 --> 00:54:53.380
the program is doing ... Is it the right idea
in that place and that context? And B is:

00:54:53.380 --> 00:55:00.130
Do they do what they say they're going to
do? So, you know, they say they're doing bed

00:55:00.130 --> 00:55:04.349
nets and that's great. But, you know, you
sent $1000 or $10,000. How many bed nets actually

00:55:04.349 --> 00:55:09.969
got into the hands of individuals? Right.
And what you want to do as a donor is multiply

00:55:09.969 --> 00:55:16.469
those two together and maximize it. A zero
in either and you're out of luck. Right? Now

00:55:16.469 --> 00:55:21.920
the book and our work is very focused on that
first. There is some of work that we're now

00:55:21.920 --> 00:55:25.869
starting to do about the second to try to
understand more about transparency and governance

00:55:25.869 --> 00:55:31.799
of NGOs. But it's, it's much further back.
One of the nice things about groups like Give

00:55:31.799 --> 00:55:36.339
Well is they're trying to tackle both at the
same time. The other way of dealing with it

00:55:36.339 --> 00:55:42.309
is to think about projects. And this is why
we started the Proven Impact Fund that Jake

00:55:42.309 --> 00:55:48.210
mentioned. And this is saying, "Well, the
organizational assessment is a, is a tricky

00:55:48.210 --> 00:55:54.249
beast." What's much easier to say is, "This
organization in that particular location we

00:55:54.249 --> 00:55:57.980
know is doing a particular project and we
know what they are doing. And so let's go

00:55:57.980 --> 00:56:05.349
fund that project." And that's an easier beast
to, to think through and to, to analyze than

00:56:05.349 --> 00:56:09.930
an entire organization where you don't know
-- OK, I know this organization's one project

00:56:09.930 --> 00:56:15.140
in Ethiopia is good, but they have 17 other
programs. Can I assume that there's an overall

00:56:15.140 --> 00:56:20.180
organizational effectiveness or not? And so,
you know, the two approaches ... and I'm not

00:56:20.180 --> 00:56:23.479
sure which is the right one in the long run.
I can just tell you that we're taking -- with

00:56:23.479 --> 00:56:28.240
the Proven Impact Fund -- the project approach
of saying, "Is the idea proven? And then,

00:56:28.240 --> 00:56:33.579
here are some projects we know that are doing
this." I do like the other two. I think the

00:56:33.579 --> 00:56:36.729
world is good with both. [pause]

00:56:36.729 --> 00:56:48.869
&gt;&gt;Eric: Well, we've come to the end of our
time. Everyone, please join me in thanking

00:56:48.869 --> 00:56:49.869
Dean and Jacob for coming. [applause]

00:56:49.869 --> 00:56:54.880
&gt;&gt;Dean Karlan: Come on up. Thank you, everyone.
It was great to be here.

