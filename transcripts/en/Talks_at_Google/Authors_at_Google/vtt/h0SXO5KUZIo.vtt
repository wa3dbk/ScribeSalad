WEBVTT
Kind: captions
Language: en

00:00:01.166 --> 00:00:02.540
MALE SPEAKER:
Welcome, everybody,

00:00:02.540 --> 00:00:06.160
to yet another
Authors@Google Talk.

00:00:06.160 --> 00:00:10.940
Today with us is best-selling
New York Times author Peter

00:00:10.940 --> 00:00:12.240
Singer.

00:00:12.240 --> 00:00:16.970
He wrote a very important
book on cyberwar.

00:00:16.970 --> 00:00:21.990
Our own Eric Schmidt says,
"this is an essential read."

00:00:21.990 --> 00:00:24.580
My dear colleagues, I
will repeat this for you.

00:00:24.580 --> 00:00:28.160
This is an essential read.

00:00:28.160 --> 00:00:32.369
So we do have copies
of the book on sale.

00:00:32.369 --> 00:00:35.290
This is the way it looks like.

00:00:35.290 --> 00:00:40.980
Please go and buy the book,
even if you've seen this talk.

00:00:40.980 --> 00:00:46.940
Peter is a great author
in many respects.

00:00:46.940 --> 00:00:50.890
But what he did for
cybersecurity and cyberwar

00:00:50.890 --> 00:00:53.720
for, in particular,
is he expanded

00:00:53.720 --> 00:00:56.920
on a field that is growing.

00:00:56.920 --> 00:01:01.470
And that we know is becoming
increasingly important not only

00:01:01.470 --> 00:01:04.209
from the infrastructure
standpoint,

00:01:04.209 --> 00:01:06.430
but also for
international relations.

00:01:06.430 --> 00:01:09.362
I'm going to let Peter
actually take it from here.

00:01:09.362 --> 00:01:10.070
Thank you, Peter.

00:01:15.410 --> 00:01:17.770
PETER SINGER: Thank you
for the kind introduction.

00:01:17.770 --> 00:01:20.320
So it's a little
bit daunting to be

00:01:20.320 --> 00:01:23.490
talking on this topic
at a company like this,

00:01:23.490 --> 00:01:25.750
because I remember
the very first time

00:01:25.750 --> 00:01:28.530
that I ever saw a computer.

00:01:28.530 --> 00:01:33.140
My father took me to a science
center down in North Carolina.

00:01:33.140 --> 00:01:37.490
And I got to see a Commodore,
if you remember those.

00:01:37.490 --> 00:01:41.510
And I took a class
on how to program,

00:01:41.510 --> 00:01:45.440
learning an entire new
language for the sole purpose

00:01:45.440 --> 00:01:51.080
of making a smiley face out of
the letter m that printed out

00:01:51.080 --> 00:01:53.620
on one of those
old spool printers

00:01:53.620 --> 00:01:55.945
that you tore the perforated
paper off the sides.

00:01:55.945 --> 00:01:57.530
Remember that?

00:01:57.530 --> 00:02:03.020
Now since then, the centrality
of computers to my life,

00:02:03.020 --> 00:02:05.030
your life, the
entire world, it's

00:02:05.030 --> 00:02:07.330
almost impossible to fathom.

00:02:07.330 --> 00:02:10.949
We live in a world where
more than 40 trillion emails

00:02:10.949 --> 00:02:14.030
are sent every single year.

00:02:14.030 --> 00:02:17.150
The first website
was made in 1991.

00:02:17.150 --> 00:02:19.500
Now, according to
your own analytics,

00:02:19.500 --> 00:02:23.310
there's more than 30 trillion
individual web pages out there.

00:02:23.310 --> 00:02:25.110
Moreover, the
internet is not just

00:02:25.110 --> 00:02:28.570
about compiling and
sharing information.

00:02:28.570 --> 00:02:32.190
It's also having impact out on
the real world via the emerging

00:02:32.190 --> 00:02:33.640
Internet of Things.

00:02:33.640 --> 00:02:37.100
According to Cisco, we'll see
more than 40 billion devices

00:02:37.100 --> 00:02:39.410
internet-enabled over
the next five years,

00:02:39.410 --> 00:02:44.550
as everything from thermostats
to cars to refrigerators

00:02:44.550 --> 00:02:48.210
to technologies literally not
yet invented or imagined all

00:02:48.210 --> 00:02:49.750
come online and
all start to carry

00:02:49.750 --> 00:02:52.050
on conversations without us.

00:02:52.050 --> 00:02:57.120
So in short, domains that range
from communication to commerce

00:02:57.120 --> 00:03:00.080
to critical infrastructure
to even conflict.

00:03:00.080 --> 00:03:02.670
98% of US military
communications

00:03:02.670 --> 00:03:05.260
goes over the civilian-owned
and operated internet.

00:03:05.260 --> 00:03:08.150
All of these spaces
are dependent on it.

00:03:08.150 --> 00:03:10.810
So we're in an age
of cyber dependency.

00:03:10.810 --> 00:03:13.820
But in the short
history of the internet,

00:03:13.820 --> 00:03:18.910
I would argue that we've reached
a critical turning point.

00:03:18.910 --> 00:03:22.160
And it's because while the
positive side of cyberspace

00:03:22.160 --> 00:03:25.317
is rippling out, so too are
the risks, the negative side.

00:03:25.317 --> 00:03:27.400
There's all sorts of ways
you can illustrate this.

00:03:27.400 --> 00:03:30.490
You can illustrate it
with the raw numbers.

00:03:30.490 --> 00:03:34.890
Every second, nine new pieces
of malware are discovered.

00:03:34.890 --> 00:03:39.860
97% of Fortune 500 companies
know that they've been hacked.

00:03:39.860 --> 00:03:42.100
And the other 3% have
been, too, they just

00:03:42.100 --> 00:03:44.400
aren't willing to
admit it to themselves.

00:03:44.400 --> 00:03:48.380
More than 100 governments
have created some kind

00:03:48.380 --> 00:03:53.080
of cyber military command, some
kind of military unit designed

00:03:53.080 --> 00:03:56.080
to fight and win wars in
cyberspace and beyond.

00:03:56.080 --> 00:04:00.450
And indeed, the very first
Pew poll to kick off 2014

00:04:00.450 --> 00:04:04.490
found that Americans are
more afraid of a cyber attack

00:04:04.490 --> 00:04:07.800
than they are of North
Korean nuclear weapons,

00:04:07.800 --> 00:04:11.600
Iranian nuclear weapons,
the rise of China, Russia,

00:04:11.600 --> 00:04:13.470
or climate change.

00:04:13.470 --> 00:04:15.610
So these fears,
they've coalesced

00:04:15.610 --> 00:04:17.950
into one of the most
rapidly growing industries

00:04:17.950 --> 00:04:19.320
in the entire world.

00:04:19.320 --> 00:04:23.840
They've also driven a
massive bureaucratic growth

00:04:23.840 --> 00:04:25.840
at the national
governmental level

00:04:25.840 --> 00:04:27.090
not just in the United States.

00:04:27.090 --> 00:04:28.590
Just earlier today,
France announced

00:04:28.590 --> 00:04:30.540
that it was spending
another $2 billion

00:04:30.540 --> 00:04:34.150
in its military on cybersecurity
issues and cyberwar.

00:04:34.150 --> 00:04:36.850
But also we see it
at the state level.

00:04:36.850 --> 00:04:38.800
And even at the
local level, where

00:04:38.800 --> 00:04:40.920
you see cities like Los
Angeles, for example,

00:04:40.920 --> 00:04:44.240
creating cybersecurity centers.

00:04:44.240 --> 00:04:48.270
What all this together means
is that for all the hope

00:04:48.270 --> 00:04:51.550
and promise of the
new digital age,

00:04:51.550 --> 00:04:55.660
we also live in an era
of cyber insecurity,

00:04:55.660 --> 00:04:57.800
if we're really being
honest about it.

00:04:57.800 --> 00:04:59.342
And so before I go
much further, it's

00:04:59.342 --> 00:05:00.883
at this point I'm
going to try and do

00:05:00.883 --> 00:05:02.890
something that's a little
bit counterintuitive,

00:05:02.890 --> 00:05:07.380
but will maybe help make that
point about cyber insecurity.

00:05:07.380 --> 00:05:10.100
And a lot like the
challenge of trying

00:05:10.100 --> 00:05:12.160
to write a book
about cybersecurity

00:05:12.160 --> 00:05:14.210
and make it
interesting, you also

00:05:14.210 --> 00:05:16.600
have the challenge of how
do you give a talk about it

00:05:16.600 --> 00:05:19.000
and give visuals that
make it interesting.

00:05:19.000 --> 00:05:22.150
So what I did-- and
with Boris's help,

00:05:22.150 --> 00:05:23.630
hopefully it will
play for us here

00:05:23.630 --> 00:05:25.530
--is I've assembled
what I think are

00:05:25.530 --> 00:05:29.210
some of the best
illustrations of cyberwar art,

00:05:29.210 --> 00:05:31.457
and some of the worst
illustrations of it.

00:05:31.457 --> 00:05:33.040
And it's going to
play in front of me.

00:05:33.040 --> 00:05:33.540
I'm not going to speak to it.

00:05:33.540 --> 00:05:36.250
It's just going to continue
to flash for a couple reasons.

00:05:36.250 --> 00:05:40.420
One, to tell that story
of cyber insecurity.

00:05:40.420 --> 00:05:43.040
But also because data
has found that you're

00:05:43.040 --> 00:05:45.870
60% more likely
to retain what I'm

00:05:45.870 --> 00:05:47.840
saying if you look at a picture.

00:05:47.840 --> 00:05:50.750
Even if the picture has nothing
to do with what I'm saying,

00:05:50.750 --> 00:05:53.010
it's just the way
us humans work.

00:05:53.010 --> 00:05:54.760
And that actually goes
to a broader lesson

00:05:54.760 --> 00:05:56.790
that the book explores, and
we'll talk about it later on.

00:05:56.790 --> 00:05:59.330
Which is that we're humans,
we're strange, we're weird,

00:05:59.330 --> 00:06:01.680
but that's what drives
all of these things.

00:06:01.680 --> 00:06:04.660
So let's pull back on
all this and wrestle

00:06:04.660 --> 00:06:09.120
with the question of why a book
on cybersecurity and cyberwar,

00:06:09.120 --> 00:06:11.030
and why now?

00:06:11.030 --> 00:06:14.170
There's two quotes
that motivated

00:06:14.170 --> 00:06:17.260
me that basically
encapsulate this.

00:06:17.260 --> 00:06:19.130
The first is from
President Obama,

00:06:19.130 --> 00:06:22.090
who declared that
cybersecurity risks pose quote,

00:06:22.090 --> 00:06:25.010
"The most serious economic
and national security

00:06:25.010 --> 00:06:27.860
challenges of the 21st century."

00:06:27.860 --> 00:06:30.750
The second quote is from
the former CIA director,

00:06:30.750 --> 00:06:34.990
who said quote, "Rarely has
something been so important

00:06:34.990 --> 00:06:39.340
and so talked about with less
and less clarity and less

00:06:39.340 --> 00:06:41.880
apparent understanding."

00:06:41.880 --> 00:06:44.430
And you can see, I really
do want to talk to this one,

00:06:44.430 --> 00:06:46.190
but we'll keep moving on.

00:06:46.190 --> 00:06:49.450
So let's explore this gap.

00:06:49.450 --> 00:06:51.750
We see it in all
sorts of fields.

00:06:51.750 --> 00:06:56.240
From the 70% of
business executives--

00:06:56.240 --> 00:07:00.390
not 70% of CTOs,
CSOs, CIOs --but 70%

00:07:00.390 --> 00:07:04.270
of business executives in
general, in any industry, who

00:07:04.270 --> 00:07:07.880
have made a cybersecurity
decision for their company

00:07:07.880 --> 00:07:11.460
despite the fact that no
major MBA program teaches it

00:07:11.460 --> 00:07:13.550
as part of your normal
business management

00:07:13.550 --> 00:07:15.530
training and responsibility.

00:07:15.530 --> 00:07:17.100
That same kind of
gap in training

00:07:17.100 --> 00:07:20.660
happens at the schools we teach
our diplomats, our lawyers,

00:07:20.660 --> 00:07:23.980
our journalists, our generals.

00:07:23.980 --> 00:07:25.870
Or anecdotes.

00:07:25.870 --> 00:07:28.080
And there's just
an array of funny,

00:07:28.080 --> 00:07:29.920
but in a certain way
sad, anecdotes that

00:07:29.920 --> 00:07:30.780
populate the book.

00:07:30.780 --> 00:07:33.410
From the opening of the book
where a Pentagon official is

00:07:33.410 --> 00:07:36.100
telling us how
important this all is,

00:07:36.100 --> 00:07:39.550
but he describes it
as "this cyber stuff."

00:07:39.550 --> 00:07:41.410
When you can only
call something stuff,

00:07:41.410 --> 00:07:42.970
but you know it's
important, that's

00:07:42.970 --> 00:07:44.580
not a good place to be in.

00:07:44.580 --> 00:07:49.240
Or the former Secretary
of Homeland Security,

00:07:49.240 --> 00:07:52.850
the agency that is ostensibly
in charge of cybersecurity

00:07:52.850 --> 00:07:54.820
on the civilian side
for the United States--

00:07:54.820 --> 00:07:57.444
who has actually now taken over
as Chancellor of the university

00:07:57.444 --> 00:08:00.100
system out here in California
--who proudly talked

00:08:00.100 --> 00:08:04.570
to us about the fact that
she doesn't use email.

00:08:04.570 --> 00:08:08.800
And in fact hasn't used social
media for over a decade.

00:08:08.800 --> 00:08:11.280
Not because she doesn't
think it's secure,

00:08:11.280 --> 00:08:14.570
but because she just
doesn't think it's useful.

00:08:14.570 --> 00:08:18.780
That same phenomena is happening
on the Judicial Branch.

00:08:18.780 --> 00:08:21.000
Where, for example, a
Supreme Court Justice

00:08:21.000 --> 00:08:23.250
talked about how they
quote, "Hadn't yet

00:08:23.250 --> 00:08:25.050
gotten around to email."

00:08:25.050 --> 00:08:27.304
Now this is obviously
worrisome to folks

00:08:27.304 --> 00:08:28.720
here working on
the Gmail account.

00:08:28.720 --> 00:08:30.550
But there's a broader
question of what

00:08:30.550 --> 00:08:33.110
does this mean for Justices
that in the upcoming year

00:08:33.110 --> 00:08:36.169
are going to decide everything
from maybe net neutrality

00:08:36.169 --> 00:08:38.610
questions to the legalities
of some of the things

00:08:38.610 --> 00:08:41.850
that the NSA was doing when they
just haven't yet gotten around

00:08:41.850 --> 00:08:43.710
to email.

00:08:43.710 --> 00:08:48.120
The cyber stuff problem is not
just an American phenomenon.

00:08:48.120 --> 00:08:50.280
We saw the same
thing in meetings

00:08:50.280 --> 00:08:55.400
with leaders in China,
UAE, France, Great Britain.

00:08:55.400 --> 00:08:59.160
The Head of Cybersecurity
in Australia

00:08:59.160 --> 00:09:02.220
had never heard
of Tor, obviously

00:09:02.220 --> 00:09:05.240
a critical technology
in this space.

00:09:05.240 --> 00:09:07.810
Now the result is
that cybersecurity

00:09:07.810 --> 00:09:11.980
is as crucial to areas as
intimate as your personal

00:09:11.980 --> 00:09:15.100
privacy, to the security
of your bank account,

00:09:15.100 --> 00:09:18.670
to as weighty as the future
of world politics itself.

00:09:18.670 --> 00:09:22.400
But it's been
treated as an issue

00:09:22.400 --> 00:09:27.250
only for the "it"
crowd, for the IT folks.

00:09:27.250 --> 00:09:30.050
In turn, the technical
community that

00:09:30.050 --> 00:09:35.550
understands the workings of
the software and the hardware

00:09:35.550 --> 00:09:38.750
hasn't dealt very
well with the wetware,

00:09:38.750 --> 00:09:40.490
with the human side,
and particularly

00:09:40.490 --> 00:09:43.140
the ripple effects of
this into other worlds,

00:09:43.140 --> 00:09:46.010
be it policy, law,
war, you name it.

00:09:46.010 --> 00:09:47.450
They've often
looked at the world

00:09:47.450 --> 00:09:49.075
through a very specific
lens and failed

00:09:49.075 --> 00:09:52.320
to appreciate some of the
broader pictures out there.

00:09:52.320 --> 00:09:54.480
Now the dangers of
this are diverse.

00:09:54.480 --> 00:09:56.920
Each of us, in whatever
role we play in life,

00:09:56.920 --> 00:09:59.210
must make decisions
about cybersecurity

00:09:59.210 --> 00:10:01.290
that shape the future
of the world well

00:10:01.290 --> 00:10:03.350
beyond just the online world.

00:10:03.350 --> 00:10:07.900
But too often we do so
without the proper tools.

00:10:07.900 --> 00:10:11.580
Basic terms and
essential definitions

00:10:11.580 --> 00:10:17.080
that define both what's
possible but also what's proper,

00:10:17.080 --> 00:10:23.160
what's right and wrong, are
missed or even worse distorted.

00:10:23.160 --> 00:10:27.740
Past myth and future hype
often weave together,

00:10:27.740 --> 00:10:31.930
obscuring what actually happened
with where we really are now.

00:10:31.930 --> 00:10:35.980
And so the result is that
some threats are overblown

00:10:35.980 --> 00:10:40.830
and overreacted to, and
other threats are ignored.

00:10:40.830 --> 00:10:43.340
So for example, as
someone who loves history,

00:10:43.340 --> 00:10:47.150
it absolutely pains
me when I hear

00:10:47.150 --> 00:10:48.690
people-- and people
who have done

00:10:48.690 --> 00:10:51.450
this range from senior
government leaders

00:10:51.450 --> 00:10:54.970
like senators to generals
to prominent news columnists

00:10:54.970 --> 00:10:58.830
--describe how we are in a
parallel to the Cold War.

00:10:58.830 --> 00:11:00.690
Or as a cabinet
official told me,

00:11:00.690 --> 00:11:04.085
that malware was
"just like a WMD."

00:11:04.085 --> 00:11:05.710
And that's why we
needed to approach it

00:11:05.710 --> 00:11:07.251
in the same kind of
deterrence theory

00:11:07.251 --> 00:11:09.040
that we used in the Cold War.

00:11:09.040 --> 00:11:11.200
What these people
fail to appreciate

00:11:11.200 --> 00:11:13.540
is the parallel to
the Cold War is not

00:11:13.540 --> 00:11:16.580
the one they think
they're making.

00:11:16.580 --> 00:11:18.330
If you understand
both the historic side

00:11:18.330 --> 00:11:20.720
and the technical side, the
best parallel to the Cold

00:11:20.720 --> 00:11:23.030
War-- actually,
those early days we

00:11:23.030 --> 00:11:26.480
didn't understand well either
the technology but even more so

00:11:26.480 --> 00:11:30.130
the political dynamics that it
was driving, the period of time

00:11:30.130 --> 00:11:34.260
where we took the real life
versions of Dr. Strangelove

00:11:34.260 --> 00:11:35.470
seriously.

00:11:35.470 --> 00:11:37.580
So as an illustration
in the book,

00:11:37.580 --> 00:11:41.780
we explore the episode where
the US Air Force actually

00:11:41.780 --> 00:11:45.090
had a serious plan
to nuke the moon

00:11:45.090 --> 00:11:46.690
to show the Soviets
that we could

00:11:46.690 --> 00:11:49.041
do interesting
stuff in space, too.

00:11:49.041 --> 00:11:51.540
Those are not historic lessons
we should be drawing in terms

00:11:51.540 --> 00:11:52.680
of the how-to's.

00:11:52.680 --> 00:11:55.760
But that's often what
the discourse is.

00:11:55.760 --> 00:11:57.830
Let me go into some
of the manifestations

00:11:57.830 --> 00:12:02.660
of this disconnect, and how they
play out, and why they matter.

00:12:02.660 --> 00:12:06.640
One in particular is that we
often lump things together

00:12:06.640 --> 00:12:11.280
that are unlike, simply because
they involve zeros and ones.

00:12:11.280 --> 00:12:14.820
So take that idea
of a cyber attack.

00:12:14.820 --> 00:12:17.910
General Alexander,
who is simultaneously

00:12:17.910 --> 00:12:22.510
the commander of US military
Cyber Command and double-hatted

00:12:22.510 --> 00:12:25.270
as the head of the
NSA-- which there

00:12:25.270 --> 00:12:28.130
are some very interesting
problems with that.

00:12:28.130 --> 00:12:29.810
But let's move beyond that.

00:12:29.810 --> 00:12:33.440
He testified to Congress,
quote, "Every day,

00:12:33.440 --> 00:12:38.790
America's armed forces face
millions of cyber attacks."

00:12:38.790 --> 00:12:42.170
But to get those
numbers he was combining

00:12:42.170 --> 00:12:45.800
a variety of like
and unlike things.

00:12:45.800 --> 00:12:49.720
He was combining everything from
probes and address scans that

00:12:49.720 --> 00:12:53.240
never entered networks
to unsuccessful attempts

00:12:53.240 --> 00:12:57.910
to get in that ranged from kids
carrying out pranks to attempts

00:12:57.910 --> 00:13:01.000
at political protests to
attempts to get in to carry out

00:13:01.000 --> 00:13:05.600
some kind of theft
or active espionage.

00:13:05.600 --> 00:13:10.680
But none of those millions of
attacks was what his listeners

00:13:10.680 --> 00:13:13.980
in Congress thought
he was talking about,

00:13:13.980 --> 00:13:17.060
which was the so-called
cyber-Pearl Harbor

00:13:17.060 --> 00:13:21.250
or cyber-9/11 that actually
there's been over a half

00:13:21.250 --> 00:13:25.241
million media and government
speech references to.

00:13:25.241 --> 00:13:27.240
And that's what his boss
as Secretary of Defense

00:13:27.240 --> 00:13:29.100
was warning everyone about.

00:13:29.100 --> 00:13:32.020
Essentially what we're doing
is that we're bundling together

00:13:32.020 --> 00:13:36.280
all of these activities simply
because they involve software.

00:13:36.280 --> 00:13:38.860
Which would be a lot
like bundling together

00:13:38.860 --> 00:13:43.540
the activities of a group of
teenagers with firecrackers,

00:13:43.540 --> 00:13:46.150
a group of political protesters
in the street with a smoke

00:13:46.150 --> 00:13:50.760
bomb, James Bond with
his Walther PPK missile,

00:13:50.760 --> 00:13:53.350
a terrorist with
a roadside bomb,

00:13:53.350 --> 00:13:55.600
and a Russian cruise missile,
and saying these are all

00:13:55.600 --> 00:13:59.780
the same because they involve
the chemistry of gunpowder.

00:13:59.780 --> 00:14:01.920
We've bundled them together
on the digital side,

00:14:01.920 --> 00:14:03.850
because they all
involve the internet.

00:14:03.850 --> 00:14:06.560
Or take the organizations.

00:14:06.560 --> 00:14:08.780
I had a senior US
military official

00:14:08.780 --> 00:14:14.510
argue with me that Anonymous and
Al Qaeda were the same thing.

00:14:14.510 --> 00:14:19.900
Now, however you come down on
Anonymous-- and I'm actually,

00:14:19.900 --> 00:14:23.140
I guess far more
empathetic towards them

00:14:23.140 --> 00:14:26.340
than what you'd expect
from people coming from DC.

00:14:26.340 --> 00:14:29.550
But the bottom line is,
wherever you come down on them,

00:14:29.550 --> 00:14:33.420
they have nothing to do
with Al Qaeda in terms

00:14:33.420 --> 00:14:37.220
of their organization,
their means, their ends,

00:14:37.220 --> 00:14:40.000
their causes-- basically the
only thing they're related

00:14:40.000 --> 00:14:42.820
is they're both non-state actors
that begin with the letter A.

00:14:42.820 --> 00:14:44.990
But that was the belief.

00:14:44.990 --> 00:14:47.440
Now these gaps in
understanding, these disconnects

00:14:47.440 --> 00:14:50.460
of policy and reality, mean
that we're not only seeing

00:14:50.460 --> 00:14:52.930
growing tension--
and we explore this

00:14:52.930 --> 00:14:55.745
in particular in meetings
with US and Chinese officials

00:14:55.745 --> 00:14:59.870
who would be negotiating on
core questions of cybersecurity.

00:14:59.870 --> 00:15:04.180
And yet, as an illustration,
one State Department official

00:15:04.180 --> 00:15:06.480
going off to one of these
negotiations actually

00:15:06.480 --> 00:15:09.890
asked us what an ISP was?

00:15:09.890 --> 00:15:11.642
Which to make that
Cold War parallel,

00:15:11.642 --> 00:15:13.850
would be like going off to
negotiate with the Soviets

00:15:13.850 --> 00:15:15.900
and not knowing what an ICBM is.

00:15:15.900 --> 00:15:18.650
But the point is, it's
not only driving tension,

00:15:18.650 --> 00:15:21.910
it's leading to us being
taken advantage of.

00:15:21.910 --> 00:15:25.390
And that can happen at
the individual level

00:15:25.390 --> 00:15:29.840
when you get tricked to send
your mom your bank account

00:15:29.840 --> 00:15:31.990
information because
she's stuck in Thailand.

00:15:31.990 --> 00:15:33.500
You didn't know she
was in Thailand,

00:15:33.500 --> 00:15:35.630
but gosh, you just
need to help her out.

00:15:35.630 --> 00:15:37.750
To more serious
illustrations of this.

00:15:37.750 --> 00:15:41.210
Like at the G-20 conference,
the most important

00:15:41.210 --> 00:15:43.400
international
conference of the year,

00:15:43.400 --> 00:15:48.940
diplomats were spearphished
by-- they received an email that

00:15:48.940 --> 00:15:50.740
had a wonderful offer for them.

00:15:50.740 --> 00:15:52.230
It said, if you
click this link you

00:15:52.230 --> 00:15:57.140
will be able to see nude photos
of the French First Lady.

00:15:57.140 --> 00:15:59.986
And many of these senior
diplomats clicked the link.

00:15:59.986 --> 00:16:02.360
And unfortunately they didn't
get to see the nude photos,

00:16:02.360 --> 00:16:05.860
but they did download
spyware onto their accounts.

00:16:05.860 --> 00:16:08.045
Again, senior
government officials

00:16:08.045 --> 00:16:10.520
to being taken advantage
of at the business

00:16:10.520 --> 00:16:12.320
organizational level.

00:16:12.320 --> 00:16:16.800
Either alternatively not doing
enough to protect the business

00:16:16.800 --> 00:16:24.540
or hiring hucksters
who offer 100% security

00:16:24.540 --> 00:16:28.740
with some kind of
silver bullet solution.

00:16:28.740 --> 00:16:30.280
Or frankly, being
taken advantage

00:16:30.280 --> 00:16:31.880
of at the national
political level.

00:16:31.880 --> 00:16:33.940
Which is, I think, behind
a number of the issues

00:16:33.940 --> 00:16:38.090
surrounding the current
Snowden-NSA scandal.

00:16:38.090 --> 00:16:39.980
This can even happen
to a president.

00:16:39.980 --> 00:16:42.330
Reportedly, Obama
expressed his, quote,

00:16:42.330 --> 00:16:45.400
"frustration that the
complexity of the technology

00:16:45.400 --> 00:16:48.140
was overwhelming policymakers."

00:16:48.140 --> 00:16:53.440
Now, our inability to have
a proper discussion on these

00:16:53.440 --> 00:16:58.420
means that we see a
distortion of threats.

00:16:58.420 --> 00:17:02.770
And in turn, a misapplication
of resources to face them.

00:17:02.770 --> 00:17:08.599
Perhaps the best illustration
of this is a number-- 31,300.

00:17:08.599 --> 00:17:13.050
That's the number of news
and academic journal articles

00:17:13.050 --> 00:17:17.589
that have explored the
phenomenon of cyberterrorism.

00:17:17.589 --> 00:17:19.004
Zero.

00:17:19.004 --> 00:17:20.920
That's the number of
people that have actually

00:17:20.920 --> 00:17:25.907
been hurt or killed by an actual
incident of cyberterrorism.

00:17:25.907 --> 00:17:27.490
In the book, we joke
that in many ways

00:17:27.490 --> 00:17:30.470
cyberterrorism is a lot like
Discovery Channel's Shark

00:17:30.470 --> 00:17:33.730
Week, where we obsess
about the danger of sharks

00:17:33.730 --> 00:17:35.870
even though you're 15,000
times more likely to be

00:17:35.870 --> 00:17:37.370
hurt on your toilet.

00:17:37.370 --> 00:17:41.100
Except the difference is that
Jaws actually did get someone,

00:17:41.100 --> 00:17:43.520
or the real world version
of Jaws did get someone.

00:17:43.520 --> 00:17:46.330
Whereas we've not seen
this in reality yet

00:17:46.330 --> 00:17:48.300
other than Die Hard 4.

00:17:48.300 --> 00:17:52.620
Now let me be clear,
I'm not saying

00:17:52.620 --> 00:17:56.670
that terrorists don't
use the internet.

00:17:56.670 --> 00:17:58.630
And in the book we
have several chapters

00:17:58.630 --> 00:18:00.671
that explore terrorists'
use of it, much of which

00:18:00.671 --> 00:18:02.500
is like how the
rest of us use it.

00:18:02.500 --> 00:18:05.320
And I'm not saying that there
is not interest in carrying out

00:18:05.320 --> 00:18:08.490
acts of cyberterrorism,
nor that there

00:18:08.490 --> 00:18:11.360
wouldn't be impactful
effects of them.

00:18:11.360 --> 00:18:13.830
Indeed, our
development of Stuxnet,

00:18:13.830 --> 00:18:17.177
a cyber weapon that finally
had physical powers, caused

00:18:17.177 --> 00:18:19.760
physical damage to the world,
is a great illustration of this.

00:18:19.760 --> 00:18:24.170
But in turn, Stuxnet illustrates
how an effective cyber attack

00:18:24.170 --> 00:18:29.600
that is real and consequential
is also quite difficult.

00:18:29.600 --> 00:18:34.340
To put it a different way, when
it comes to cyberterrorism Al

00:18:34.340 --> 00:18:38.230
Qaeda would like to, but can't.

00:18:38.230 --> 00:18:42.880
China could, but
doesn't want to yet.

00:18:42.880 --> 00:18:46.680
Now my point, rather,
is that strategy--

00:18:46.680 --> 00:18:50.270
whether it's at the national
level, at the business level,

00:18:50.270 --> 00:18:52.590
at the individual
level, strategy

00:18:52.590 --> 00:18:55.890
is about choices and priorities.

00:18:55.890 --> 00:18:58.760
And so we need to weigh
the centrality of what

00:18:58.760 --> 00:19:02.830
we talk about, what we obsess
about in our discussions

00:19:02.830 --> 00:19:07.680
versus what are arguably
not only very real, but more

00:19:07.680 --> 00:19:10.490
consequential cyber
threats out there.

00:19:10.490 --> 00:19:13.210
It ranges from something
that this organization

00:19:13.210 --> 00:19:16.430
is very familiar with--
the massive campaign

00:19:16.430 --> 00:19:21.270
of intellectual property
theft that by most measures

00:19:21.270 --> 00:19:24.260
you could judge to be
the largest theft in all

00:19:24.260 --> 00:19:28.180
of human history, that's
ongoing right now.

00:19:28.180 --> 00:19:30.250
And where is it coming from?

00:19:30.250 --> 00:19:31.760
If this was a
Harry Potter novel,

00:19:31.760 --> 00:19:33.630
we would describe it
as a large Asian power

00:19:33.630 --> 00:19:35.320
that shall not be named.

00:19:35.320 --> 00:19:37.820
To if we want to think
about the national security

00:19:37.820 --> 00:19:41.050
consequences-- not just looking
at the consequences of that IP

00:19:41.050 --> 00:19:43.580
theft and how it
plays out, but look

00:19:43.580 --> 00:19:47.800
beyond the sexy cyber-Pearl
Harbor descriptions

00:19:47.800 --> 00:19:51.017
and actually focus on how the
military uses this technology

00:19:51.017 --> 00:19:51.850
and wants to use it.

00:19:51.850 --> 00:19:55.050
And what is the future of
computer network operations

00:19:55.050 --> 00:19:57.990
in actual campaigns of warfare?

00:19:57.990 --> 00:20:00.430
To maybe moreso we should
be paying attention

00:20:00.430 --> 00:20:03.820
to the ripple effects, the
secondary effects of all

00:20:03.820 --> 00:20:04.860
these actions.

00:20:04.860 --> 00:20:07.710
Because if we use the
illustration of terrorism,

00:20:07.710 --> 00:20:09.490
one of the things
we've learned from 9/11

00:20:09.490 --> 00:20:12.380
is it's not merely
the attack itself,

00:20:12.380 --> 00:20:15.360
but how we react
to it that really

00:20:15.360 --> 00:20:17.360
stakes its place in history.

00:20:17.360 --> 00:20:20.700
And so I worry about some
of these secondary effects

00:20:20.700 --> 00:20:22.490
that are playing
out, and particularly

00:20:22.490 --> 00:20:26.930
how they are hammering away
at that crucial value that

00:20:26.930 --> 00:20:30.120
has basically underpinned
the internet of trust.

00:20:30.120 --> 00:20:31.940
And we can see
that being damaged

00:20:31.940 --> 00:20:34.640
by the massive campaigns
of cyber crime out there.

00:20:34.640 --> 00:20:37.420
Whether it's the IP theft
to credit card, and like.

00:20:37.420 --> 00:20:41.930
And that's affecting both trust
that consumers and users have

00:20:41.930 --> 00:20:44.460
with the network, and in
turn what the operators have

00:20:44.460 --> 00:20:45.880
towards consumers.

00:20:45.880 --> 00:20:50.780
To trust damaged by our
government's actions seeking

00:20:50.780 --> 00:20:54.270
to deal with
conventional terrorism.

00:20:54.270 --> 00:20:58.580
And what that has done to
both trust in those agencies,

00:20:58.580 --> 00:21:01.490
but also trust in America and
trust in American technology

00:21:01.490 --> 00:21:02.890
companies.

00:21:02.890 --> 00:21:06.710
To finally, what it's done to
the internet freedom agenda.

00:21:06.710 --> 00:21:09.210
And the trust in the
underlying governance structure

00:21:09.210 --> 00:21:11.790
of the internet that has
worked so effectively

00:21:11.790 --> 00:21:13.640
for our lifetime,
created this thing

00:21:13.640 --> 00:21:16.690
that's been arguably
the most powerful force

00:21:16.690 --> 00:21:20.190
for political,
economic, social change

00:21:20.190 --> 00:21:22.060
certainly in my
lifetime, maybe ever.

00:21:22.060 --> 00:21:24.515
And yet over the next year
could be seriously damaged

00:21:24.515 --> 00:21:26.140
by some international
negotiations that

00:21:26.140 --> 00:21:28.170
are playing out,
particularly pushed

00:21:28.170 --> 00:21:31.250
by authoritarian states
like Russia and China.

00:21:31.250 --> 00:21:36.650
If you like the idea of Russia's
82,000 blacklisted websites,

00:21:36.650 --> 00:21:41.246
or if you like the building
internet wall in China,

00:21:41.246 --> 00:21:43.120
this may be the future
if we don't watch out.

00:21:43.120 --> 00:21:44.900
Particularly as some
of the core swing

00:21:44.900 --> 00:21:47.430
states, the Brazils, the
Indias, the Germanys,

00:21:47.430 --> 00:21:50.380
may not be with us the
way they were previously.

00:21:50.380 --> 00:21:54.350
Now this gap in the
fields also means

00:21:54.350 --> 00:21:57.230
when it comes to
the warfare side,

00:21:57.230 --> 00:22:00.910
we act on bad assumptions.

00:22:00.910 --> 00:22:03.620
Or don't make connections
across domains

00:22:03.620 --> 00:22:05.420
in ways that truly matter.

00:22:05.420 --> 00:22:07.340
So take the notion of
something from the field

00:22:07.340 --> 00:22:09.840
of war applied here,
which is offense, defense,

00:22:09.840 --> 00:22:11.720
the balance between these.

00:22:11.720 --> 00:22:15.680
There is an idea that's taken
hold that cyber offense is

00:22:15.680 --> 00:22:17.030
inherently privileged.

00:22:17.030 --> 00:22:19.500
It's inherently dominant
against the defense.

00:22:19.500 --> 00:22:22.580
And not just now, but as
one US military report

00:22:22.580 --> 00:22:25.650
put it, quote, "For the
foreseeable future."

00:22:25.650 --> 00:22:28.850
So for as long as we can see
in the future cyber offense

00:22:28.850 --> 00:22:31.930
will be dominant, is the
assumption that's out there.

00:22:31.930 --> 00:22:34.800
This in turn has
driven the US military

00:22:34.800 --> 00:22:38.740
to spend roughly
four times as much

00:22:38.740 --> 00:22:41.840
on cyber offense
research and development

00:22:41.840 --> 00:22:45.480
as it has on cyber defense
research and development.

00:22:45.480 --> 00:22:48.290
Now the problem with
this is threefold.

00:22:48.290 --> 00:22:52.240
The first is that it
cyber offense is not

00:22:52.240 --> 00:22:55.050
as easy as it's
too often depicted.

00:22:55.050 --> 00:22:58.550
So for example, the former
number two in the Pentagon

00:22:58.550 --> 00:23:01.930
described how, quote, "A
couple of teenagers sitting

00:23:01.930 --> 00:23:04.480
in their parents'
basement, sipping Red Bull

00:23:04.480 --> 00:23:10.410
and wearing flip-flops, could
carry out a WMD-style attack."

00:23:10.410 --> 00:23:11.400
No.

00:23:11.400 --> 00:23:12.300
They couldn't.

00:23:12.300 --> 00:23:15.940
They could do a lot of
things, but not what he's

00:23:15.940 --> 00:23:17.120
for portraying.

00:23:17.120 --> 00:23:19.810
And Stuxnet is a great
illustration of that.

00:23:19.810 --> 00:23:22.140
In terms of the wide
variety of skill sets

00:23:22.140 --> 00:23:24.930
that were involved in this,
everything from intelligence

00:23:24.930 --> 00:23:28.070
analysts and collection to some
of the top technical talent

00:23:28.070 --> 00:23:31.710
in the world from multiple
nations, to nuclear physicists,

00:23:31.710 --> 00:23:35.570
to engineers, to then another
espionage effort to get it back

00:23:35.570 --> 00:23:36.210
in.

00:23:36.210 --> 00:23:39.220
It was a Manhattan
Project-style effort.

00:23:39.220 --> 00:23:43.010
Again, the barriers to entry
are lowering, but it's not just,

00:23:43.010 --> 00:23:45.262
oh I need a teenager
and some Red Bull

00:23:45.262 --> 00:23:47.260
and I can carry this out.

00:23:47.260 --> 00:23:51.020
The second is history
is replete with examples

00:23:51.020 --> 00:23:53.860
that every time a military
assumed the offense was

00:23:53.860 --> 00:23:57.070
inherently dominant, that it
turned out to be the opposite.

00:23:57.070 --> 00:23:59.420
And we're on the
100 year anniversary

00:23:59.420 --> 00:24:01.210
of probably the best
illustration of that.

00:24:01.210 --> 00:24:04.600
Where the nations of Europe,
prior to World War I,

00:24:04.600 --> 00:24:07.600
all assumed that the new
technologies of the day

00:24:07.600 --> 00:24:09.550
meant that the offense
was advantaged.

00:24:09.550 --> 00:24:11.530
And in fact, it
was so advantaged

00:24:11.530 --> 00:24:15.500
that you couldn't allow yourself
to be stuck on the defensive.

00:24:15.500 --> 00:24:18.730
So you had to go to war
before the other guy could.

00:24:18.730 --> 00:24:21.230
So that you wouldn't be
caught at a disadvantage.

00:24:21.230 --> 00:24:23.650
And as we saw play
out in World War I,

00:24:23.650 --> 00:24:26.690
actually it was the defense
that turned out to be dominant.

00:24:26.690 --> 00:24:28.740
But the final
issue with this is,

00:24:28.740 --> 00:24:32.700
even if it's true
it doesn't actually

00:24:32.700 --> 00:24:37.110
mean that we should be
acting the way we are.

00:24:37.110 --> 00:24:43.160
To give a metaphor, the idea
of sitting in your glass house

00:24:43.160 --> 00:24:44.830
and looking around
and saying, gosh

00:24:44.830 --> 00:24:49.270
I'm worried about all these
roving gangs of teens.

00:24:49.270 --> 00:24:55.049
Well, my best answer is to
buy a stone sharpening kit.

00:24:55.049 --> 00:24:57.090
That's not the logic that
we should be following,

00:24:57.090 --> 00:24:59.280
but that's what we're
doing right now.

00:24:59.280 --> 00:25:01.890
So what can we do instead?

00:25:01.890 --> 00:25:04.760
The last third of the
book is all about these

00:25:04.760 --> 00:25:08.080
what can we do questions,
everything from global level

00:25:08.080 --> 00:25:11.800
responses to national level
down to corporate to you

00:25:11.800 --> 00:25:13.350
and I. How can we
protect ourselves

00:25:13.350 --> 00:25:15.054
and the broader internet itself.

00:25:15.054 --> 00:25:16.470
I'm not going to
try and summarize

00:25:16.470 --> 00:25:18.360
that 100 pages up here.

00:25:18.360 --> 00:25:22.290
So I'll just hit on five themes
that cut through all of it.

00:25:22.290 --> 00:25:26.260
The first theme is
knowledge matters.

00:25:26.260 --> 00:25:30.870
It is vital that we demystify
this realm if we ever

00:25:30.870 --> 00:25:34.490
want to get anything done
effective in securing it.

00:25:34.490 --> 00:25:37.385
We have to move past
the situation now

00:25:37.385 --> 00:25:41.220
where, for example, a White
House official described this

00:25:41.220 --> 00:25:45.190
as quote, "only
understood by the nerds."

00:25:45.190 --> 00:25:48.820
Or when the President
himself received a briefing

00:25:48.820 --> 00:25:51.110
on cyber security questions.

00:25:51.110 --> 00:25:53.960
And at the end of the briefing,
reportedly asked for, repeated

00:25:53.960 --> 00:25:57.480
back, quote, "This
time in English."

00:25:57.480 --> 00:26:00.230
That's not to beat up on the
residents of the White House.

00:26:00.230 --> 00:26:05.800
That would happen in almost
any major company that's

00:26:05.800 --> 00:26:09.710
not in this space, not in
Silicon Valley, but also

00:26:09.710 --> 00:26:12.412
even small companies,
a cupcake stand.

00:26:12.412 --> 00:26:13.870
It would happen at
the White House.

00:26:13.870 --> 00:26:16.380
It would happen at my house.

00:26:16.380 --> 00:26:18.460
The second theme
leads from this.

00:26:18.460 --> 00:26:20.170
It's that people matter.

00:26:20.170 --> 00:26:22.910
Cybersecurity is one of those
wicked problem areas that's

00:26:22.910 --> 00:26:25.060
rife with complexities
and trade-offs.

00:26:25.060 --> 00:26:28.220
And this is in large part not
because of the technical side,

00:26:28.220 --> 00:26:31.890
which often gets too much focus,
but rather the people part.

00:26:31.890 --> 00:26:34.275
Now it's useful from a
writer's perspective,

00:26:34.275 --> 00:26:36.650
because that gives you all
the fun characters and stories

00:26:36.650 --> 00:26:38.420
to populate.

00:26:38.420 --> 00:26:41.520
My favorite being the time
that Pakistan accidentally

00:26:41.520 --> 00:26:45.650
kidnapped all the world's
cute cat videos for a day.

00:26:45.650 --> 00:26:47.330
But it also means
that if you want

00:26:47.330 --> 00:26:51.032
to set up best responses at the
global level, business level,

00:26:51.032 --> 00:26:52.740
all the way down to
the individual level,

00:26:52.740 --> 00:26:55.910
you need to recognize that
the people behind the machines

00:26:55.910 --> 00:26:59.330
are both part of
every single problem.

00:26:59.330 --> 00:27:02.690
And have to be part of
every single solution.

00:27:02.690 --> 00:27:04.410
This leads to the next theme.

00:27:04.410 --> 00:27:06.000
Incentives matter.

00:27:06.000 --> 00:27:09.330
If you want to understand why
something is or isn't happening

00:27:09.330 --> 00:27:12.890
in cybersecurity, look
to the motivations,

00:27:12.890 --> 00:27:15.630
the relative costs,
the organizations

00:27:15.630 --> 00:27:18.620
that people are in, the
tensions at play between them.

00:27:18.620 --> 00:27:21.960
There is a reason why, for
example, finance companies

00:27:21.960 --> 00:27:23.900
are doing better at
their cyber security--

00:27:23.900 --> 00:27:26.690
both in terms of defending
themselves, but also sharing

00:27:26.690 --> 00:27:29.310
information --versus
how, for example,

00:27:29.310 --> 00:27:34.340
critical infrastructure and
natural gas or the power grid,

00:27:34.340 --> 00:27:37.920
how they're not cooperating and
not defending themselves well.

00:27:37.920 --> 00:27:40.490
It's because they're
incentivized both to directly

00:27:40.490 --> 00:27:42.880
understand the cost,
but also there's

00:27:42.880 --> 00:27:46.410
a regulatory environment around
them that's driving that.

00:27:46.410 --> 00:27:48.585
And this points to the
role that government

00:27:48.585 --> 00:27:50.870
can, and frankly
should, be playing.

00:27:50.870 --> 00:27:52.970
And everything from being
a trusted information

00:27:52.970 --> 00:27:56.860
provider to setting standards
to-- in other situations,

00:27:56.860 --> 00:27:59.860
it's going to have to create
market incentives, which

00:27:59.860 --> 00:28:02.450
is another way of
saying regulation.

00:28:02.450 --> 00:28:05.400
The fourth is history matters.

00:28:05.400 --> 00:28:09.090
There is a history to how we
got here with the internet.

00:28:09.090 --> 00:28:11.565
And too often it's ignored.

00:28:11.565 --> 00:28:13.690
And that's when you hear
these sort of silly things

00:28:13.690 --> 00:28:17.640
like oh, well let's just build
a new, more secure internet,

00:28:17.640 --> 00:28:20.290
which is not a workable concept.

00:28:20.290 --> 00:28:23.770
And yet it's gotten a lot of
credence in policy circles.

00:28:23.770 --> 00:28:25.730
But more broadly, it
means that there's

00:28:25.730 --> 00:28:29.552
a wealth of lessons to learn
from history and other fields.

00:28:29.552 --> 00:28:31.010
So if we're exploring,
for example,

00:28:31.010 --> 00:28:35.570
how to deal with cyber crime,
but also patriotic hacker

00:28:35.570 --> 00:28:38.200
communities that are
linked to states,

00:28:38.200 --> 00:28:41.080
we look at the age of
sail as a parallel.

00:28:41.080 --> 00:28:44.110
Where you have this
domain in which

00:28:44.110 --> 00:28:47.200
commerce, communication,
and conflict all

00:28:47.200 --> 00:28:49.110
played out on the open sea.

00:28:49.110 --> 00:28:52.830
The conflict actors ranged
from state militaries

00:28:52.830 --> 00:28:55.310
to individual criminal
groups, pirates,

00:28:55.310 --> 00:28:57.720
to these fuzzy things in
the middle, privateers,

00:28:57.720 --> 00:28:59.970
that sort of gave you some
of the advantage of pirates

00:28:59.970 --> 00:29:01.550
but also state-linked as well.

00:29:01.550 --> 00:29:03.050
And that's a lesson
that we can look

00:29:03.050 --> 00:29:05.660
to in how we went
after that trade.

00:29:05.660 --> 00:29:09.530
To if we want to understand
good role for government, let's

00:29:09.530 --> 00:29:13.210
look at the most successful
government agencies in history.

00:29:13.210 --> 00:29:15.870
Like the Centers for Disease
Control, which literally

00:29:15.870 --> 00:29:17.450
started with a
couple of scientists

00:29:17.450 --> 00:29:21.340
taking a $10 collection,
a tin cup for $10.

00:29:21.340 --> 00:29:24.920
And that agency went on to
eradicate malaria inside United

00:29:24.920 --> 00:29:27.810
States, to fight smallpox on
an international level, to oh,

00:29:27.810 --> 00:29:31.640
by the way, served as a critical
back channel to the Soviets

00:29:31.640 --> 00:29:34.180
in the worst part
of the Cold War.

00:29:34.180 --> 00:29:39.470
This leads to the final lesson,
and it comes from the saying

00:29:39.470 --> 00:29:43.130
that Ben Franklin had, that
"An ounce of prevention

00:29:43.130 --> 00:29:45.480
is worth a pound of cure."

00:29:45.480 --> 00:29:50.230
What's fascinating is that
the CDC did studies and proved

00:29:50.230 --> 00:29:55.140
that Franklin's saying actually
is true in public health.

00:29:55.140 --> 00:29:59.940
It's also true in
cybersecurity and cyberwar.

00:29:59.940 --> 00:30:03.840
Very simple steps
of cyber hygiene

00:30:03.840 --> 00:30:06.950
would have an immense impact.

00:30:06.950 --> 00:30:09.530
Indeed, one study of
the top 20 controls

00:30:09.530 --> 00:30:13.200
found that they would stop
94% of all cyber attacks.

00:30:13.200 --> 00:30:15.280
Now some people react to
that, and they go well,

00:30:15.280 --> 00:30:16.570
I'm really special.

00:30:16.570 --> 00:30:17.900
I'm in the 6%.

00:30:17.900 --> 00:30:21.160
Well, statistically we
all can't be in the 6%.

00:30:21.160 --> 00:30:23.630
But even more so
they should talk

00:30:23.630 --> 00:30:27.570
to their technical
folks, their IT crowd.

00:30:27.570 --> 00:30:30.500
And they would quickly
learn how if they didn't

00:30:30.500 --> 00:30:33.670
have to spend so much time
dealing with the low level

00:30:33.670 --> 00:30:37.160
stuff, they could actually
focus on the more advanced

00:30:37.160 --> 00:30:39.207
persistent threats
that are out there.

00:30:39.207 --> 00:30:41.040
And a large part of
this, what's interesting

00:30:41.040 --> 00:30:43.860
is the data shows that senior
executives are actually

00:30:43.860 --> 00:30:46.920
twice as likely to be
behind one of these problems

00:30:46.920 --> 00:30:50.640
as junior folks, which makes it
even more difficult for the IT

00:30:50.640 --> 00:30:52.720
department to deal with.

00:30:52.720 --> 00:30:56.730
To give some
illustrations of this--

00:30:56.730 --> 00:30:58.110
let me add one more thing on it.

00:30:58.110 --> 00:31:00.665
The other challenge to this is
that there's this assumption

00:31:00.665 --> 00:31:02.040
that the advanced
threats are all

00:31:02.040 --> 00:31:04.910
using very advanced pathways in.

00:31:04.910 --> 00:31:07.200
And yet consistently,
they're coming

00:31:07.200 --> 00:31:11.000
in through rather
simple approaches.

00:31:11.000 --> 00:31:14.560
For example, the most
important outside penetration

00:31:14.560 --> 00:31:19.610
of US military classified
networks by a foreign espionage

00:31:19.610 --> 00:31:22.290
agency happened
when they conducted

00:31:22.290 --> 00:31:24.200
what's known as a candy drop.

00:31:24.200 --> 00:31:27.820
Basically they
dropped memory sticks

00:31:27.820 --> 00:31:31.420
in a parking lot outside
a US military base.

00:31:31.420 --> 00:31:33.990
And while we learn in
preschool don't take candy

00:31:33.990 --> 00:31:38.310
from strangers, a US soldier
saw the shiny memory stick

00:31:38.310 --> 00:31:39.240
in the dirt.

00:31:39.240 --> 00:31:41.000
Thought this was
really cool, picked

00:31:41.000 --> 00:31:42.677
it up, wanted to
see what was on it.

00:31:42.677 --> 00:31:44.510
So he took it inside
the base and plugged it

00:31:44.510 --> 00:31:45.750
into his computer.

00:31:45.750 --> 00:31:48.395
And that was actually the
most important penetration

00:31:48.395 --> 00:31:51.200
of US military networks
from the outside.

00:31:51.200 --> 00:31:57.510
To the insider threat, the
episodes of Manning or Snowden.

00:31:57.510 --> 00:32:01.840
Again, wherever you
come down on them,

00:32:01.840 --> 00:32:05.850
we can all agree that the
organizations were not

00:32:05.850 --> 00:32:09.680
following the kind of
internal security norms

00:32:09.680 --> 00:32:11.810
that a cupcake
store should have.

00:32:11.810 --> 00:32:15.010
Monitoring, for example,
massively anomalous traffic,

00:32:15.010 --> 00:32:17.020
things like that.

00:32:17.020 --> 00:32:22.780
Now this idea of hygiene
is important-- again,

00:32:22.780 --> 00:32:24.666
when I say hygiene,
picking up a memory stick

00:32:24.666 --> 00:32:25.790
that you found in the dirt.

00:32:25.790 --> 00:32:28.100
That's basic hygiene,
that's the five second rule,

00:32:28.100 --> 00:32:29.500
let alone cyber hygiene.

00:32:29.500 --> 00:32:34.410
But this idea of hygiene, I
think, is important not just

00:32:34.410 --> 00:32:38.920
because of that idea of
prevention, but even more so

00:32:38.920 --> 00:32:41.020
the ethic behind it.

00:32:41.020 --> 00:32:43.080
That we need--
again, whether we're

00:32:43.080 --> 00:32:45.455
talking about this on a global
level, a national level,

00:32:45.455 --> 00:32:48.420
a business level, to
an individual level.

00:32:48.420 --> 00:32:51.440
I teach my kids hygiene.

00:32:51.440 --> 00:32:54.110
Wash your hands, cover
your mouth when you cough.

00:32:54.110 --> 00:32:56.600
I teach them that not only
to protect themselves,

00:32:56.600 --> 00:32:59.650
but also that they
have a responsibility

00:32:59.650 --> 00:33:02.140
to protect all that they
connect with through the course

00:33:02.140 --> 00:33:03.260
of their day.

00:33:03.260 --> 00:33:04.980
That's the same kind
of ethic that we

00:33:04.980 --> 00:33:06.199
need in the online space.

00:33:06.199 --> 00:33:07.740
And we should be
pushing more of that

00:33:07.740 --> 00:33:11.040
rather than the fears that
are out there driving us.

00:33:11.040 --> 00:33:15.120
So to bring this
story full circle,

00:33:15.120 --> 00:33:19.470
in the beginning I talked about
how when I was seven years old

00:33:19.470 --> 00:33:21.650
I saw my first computer.

00:33:21.650 --> 00:33:25.630
Now if you had told
little seven-year-old me

00:33:25.630 --> 00:33:29.560
that one day this Commodore
or its descendants

00:33:29.560 --> 00:33:34.870
would allow someone
to steal your money,

00:33:34.870 --> 00:33:41.390
steal your identity, even become
a weapon of mass disruption,

00:33:41.390 --> 00:33:44.030
I would've begged and
pleaded with my dad

00:33:44.030 --> 00:33:47.280
not to turn on the power button.

00:33:47.280 --> 00:33:51.800
Don't let us go into this
dangerous, scary world.

00:33:51.800 --> 00:33:55.030
Today I wouldn't have
it any other way.

00:33:55.030 --> 00:33:59.360
Because that technology
has given me and all of us

00:33:59.360 --> 00:34:04.200
literally superpowers that
we didn't imagine back then.

00:34:04.200 --> 00:34:11.199
We can ask any question and
Google the answer to it.

00:34:11.199 --> 00:34:15.179
Any question, important
or not important.

00:34:15.179 --> 00:34:18.030
Yesterday I was looking
up the backstory

00:34:18.030 --> 00:34:22.102
of a minor noble in
the "Game of Thrones."

00:34:22.102 --> 00:34:23.685
That's actually the
important example.

00:34:26.530 --> 00:34:28.489
This technology has
given us the power

00:34:28.489 --> 00:34:36.040
to become friends with people
that we've literally never met.

00:34:36.040 --> 00:34:38.940
All of these great
steps forward.

00:34:38.940 --> 00:34:42.920
And so the same as it
was back then, I think,

00:34:42.920 --> 00:34:45.650
is the way it will
be in the future.

00:34:45.650 --> 00:34:50.100
We have to accept and manage
the risks of this world--

00:34:50.100 --> 00:34:54.800
whether it's the online world
or the real world, so to speak,

00:34:54.800 --> 00:34:57.730
--because of all that
can be achieved in it.

00:34:57.730 --> 00:35:00.950
And to steal the
title from the book,

00:35:00.950 --> 00:35:04.040
in the end, that's really
what everyone needs to know.

00:35:04.040 --> 00:35:04.850
Thank you.

00:35:04.850 --> 00:35:06.810
[APPLAUSE]

00:35:12.690 --> 00:35:14.620
MALE SPEAKER: We'll
do a short Q&amp;A. Please

00:35:14.620 --> 00:35:17.020
wait for the audience
mic to arrive to you.

00:35:17.020 --> 00:35:20.940
And I wanted to mention one more
thing that Peter told me about.

00:35:20.940 --> 00:35:22.970
And this is, there's a website.

00:35:22.970 --> 00:35:26.380
It's called
cybersecuritybook.com.

00:35:26.380 --> 00:35:31.520
And there's a cybersecurity
song playlist there.

00:35:31.520 --> 00:35:34.630
I'm curious myself
now what that is.

00:35:34.630 --> 00:35:35.510
Questions?

00:35:35.510 --> 00:35:37.510
AUDIENCE: It seemed like
one of the big problems

00:35:37.510 --> 00:35:39.990
you mentioned was a
problem of leadership.

00:35:39.990 --> 00:35:42.060
And the people who
are empowered just

00:35:42.060 --> 00:35:44.829
don't have the sophistication
to talk about these issues

00:35:44.829 --> 00:35:45.620
and make decisions.

00:35:45.620 --> 00:35:47.660
And I just was
wondering what you

00:35:47.660 --> 00:35:50.919
thought was the minimum
level of competence

00:35:50.919 --> 00:35:51.960
required by these people?

00:35:51.960 --> 00:35:54.481
Because realistically, they
seem to be pretty entrenched.

00:35:54.481 --> 00:35:56.230
And I don't think it's
realistic to expect

00:35:56.230 --> 00:35:57.650
a whole new breed of
people to come in and make

00:35:57.650 --> 00:35:58.580
these decisions.

00:35:58.580 --> 00:36:01.310
And on that point, as
well, how likely do

00:36:01.310 --> 00:36:03.675
think it is to be able to get
these people to that level

00:36:03.675 --> 00:36:06.050
of sophistication, given the
fact that these people don't

00:36:06.050 --> 00:36:08.420
know how to use email?

00:36:08.420 --> 00:36:11.570
PETER SINGER: It's
a great question.

00:36:11.570 --> 00:36:15.260
And one part of it,
sometimes people say well,

00:36:15.260 --> 00:36:17.880
isn't this just
a digital native,

00:36:17.880 --> 00:36:20.490
digital immigrant issue?

00:36:20.490 --> 00:36:25.380
That digital immigrants,
someone who grew up in a world

00:36:25.380 --> 00:36:26.840
without computers
and then now has

00:36:26.840 --> 00:36:30.840
moved into this world versus
a native who was born into it

00:36:30.840 --> 00:36:33.570
and it all seems
natural and intuitive.

00:36:33.570 --> 00:36:37.590
And so this problem, won't
it just solve itself,

00:36:37.590 --> 00:36:40.880
is how they sometimes
reference it.

00:36:40.880 --> 00:36:46.390
First, there's a long period
of time before the immigrants,

00:36:46.390 --> 00:36:50.350
so to speak, move out of
the positions of power.

00:36:50.350 --> 00:36:54.187
To put it a different
way-- there's

00:36:54.187 --> 00:36:55.645
a quote in the book
from a guy that

00:36:55.645 --> 00:36:58.720
talks about how the folks that
are sitting in the big boy

00:36:58.720 --> 00:37:00.320
chairs, is how he phrased it.

00:37:00.320 --> 00:37:03.640
The big boy chairs in
government or at CEOs

00:37:03.640 --> 00:37:07.260
of a lot of different companies
or the like, many of them

00:37:07.260 --> 00:37:09.950
didn't see or use
their first computer

00:37:09.950 --> 00:37:13.360
until they were in
their 30s or 40s.

00:37:13.360 --> 00:37:16.830
But it doesn't mean
one, they're going

00:37:16.830 --> 00:37:18.600
to be in those positions
for a long time.

00:37:18.600 --> 00:37:21.150
And so we've got this
gap, this period of time.

00:37:21.150 --> 00:37:22.400
We can't wait it out.

00:37:22.400 --> 00:37:26.050
The second is a lot
of digital natives

00:37:26.050 --> 00:37:28.950
don't have this intuitively
the way it's assumed.

00:37:28.950 --> 00:37:33.230
In large part because of how
we've stovepiped these issues.

00:37:33.230 --> 00:37:36.200
That's for the IT
folks to handle.

00:37:36.200 --> 00:37:38.960
Or the IT folks saying, oh,
well that's for legal to handle,

00:37:38.960 --> 00:37:41.270
that's not for us.

00:37:41.270 --> 00:37:46.900
And so to your question,
what's the level of expertise.

00:37:46.900 --> 00:37:49.910
I don't think there's a common
test that everyone has to pass,

00:37:49.910 --> 00:37:50.957
or something like that.

00:37:50.957 --> 00:37:53.290
I actually-- and this may be
a little bit controversial.

00:37:53.290 --> 00:37:54.740
I don't think it's
even about people

00:37:54.740 --> 00:37:57.156
knowing how to do things like
computer programming-- maybe

00:37:57.156 --> 00:37:59.380
it's controversial in this room.

00:37:59.380 --> 00:38:07.190
It's instead having familiarity
of the key concepts,

00:38:07.190 --> 00:38:11.230
the key terms, so
that frankly, they

00:38:11.230 --> 00:38:15.140
can have a good
argument about it.

00:38:15.140 --> 00:38:20.830
You can see this in what's
playing out with the NSA issues

00:38:20.830 --> 00:38:24.640
recently, where
both the mass media,

00:38:24.640 --> 00:38:28.820
but also both sides in
Congress that are arguing it,

00:38:28.820 --> 00:38:32.560
it's just so factually
disconnected.

00:38:32.560 --> 00:38:35.740
And so they're not able to even
have a good argument about it.

00:38:35.740 --> 00:38:38.560
To use that illustration
of offense, defense theory.

00:38:38.560 --> 00:38:40.830
It's a great way
of showing this.

00:38:40.830 --> 00:38:42.720
Where on one hand
the people that

00:38:42.720 --> 00:38:45.100
understand the
technical side don't

00:38:45.100 --> 00:38:47.650
know that there's actually
a very rich literature

00:38:47.650 --> 00:38:50.642
in international relations
of offense, defense,

00:38:50.642 --> 00:38:52.850
that doesn't lead you to
one conclusion or the other.

00:38:52.850 --> 00:38:55.020
And they were sort of-- they
picked one part of it and said,

00:38:55.020 --> 00:38:56.936
this is the conclusion
of what we should take.

00:38:56.936 --> 00:39:01.270
In turn the IR crowd doesn't
understand this all that well.

00:39:01.270 --> 00:39:05.670
The bigger thing is not a level
of knowledge, it's an attitude.

00:39:05.670 --> 00:39:10.370
There's too much Ludditism
out there that's celebrated.

00:39:10.370 --> 00:39:13.470
A senior government official
who held responsibility

00:39:13.470 --> 00:39:16.170
for this literally
saying, doesn't

00:39:16.170 --> 00:39:18.510
think it's all that useful.

00:39:18.510 --> 00:39:20.660
And she did the same
thing the SecDef did,

00:39:20.660 --> 00:39:22.990
where if their
email came in it's

00:39:22.990 --> 00:39:24.590
printed out by the assistant.

00:39:24.590 --> 00:39:25.840
They write their answer on it.

00:39:25.840 --> 00:39:27.550
And then they hand it back.

00:39:27.550 --> 00:39:30.349
You can't be effective if
that's the kind of attitude

00:39:30.349 --> 00:39:31.890
that you have, both
for your internal

00:39:31.890 --> 00:39:36.170
but you think it's OK to talk
to others about it that way.

00:39:36.170 --> 00:39:39.112
And so for me it's,
again, there's

00:39:39.112 --> 00:39:40.320
some base level of knowledge.

00:39:40.320 --> 00:39:42.820
But it's more about changing
the attitudes around it.

00:39:42.820 --> 00:39:47.500
And frankly stop looking at
this as just a highly technical

00:39:47.500 --> 00:39:52.470
issue for, again, the IT
crowd, or for the nerds.

00:39:52.470 --> 00:39:54.270
AUDIENCE: What
principles have we

00:39:54.270 --> 00:39:59.590
learned from the behavior of
immune systems and biology,

00:39:59.590 --> 00:40:02.315
and from the resilience
of biological networks

00:40:02.315 --> 00:40:03.940
all the way from the
metabolic networks

00:40:03.940 --> 00:40:08.110
up to ecologies, what principles
have we learned that we are not

00:40:08.110 --> 00:40:11.014
yet applying in cybersecurity?

00:40:11.014 --> 00:40:12.930
PETER SINGER: That's
actually a great question

00:40:12.930 --> 00:40:14.471
to bridge back to
the prior question.

00:40:14.471 --> 00:40:18.850
Because that all-important
word that you used,

00:40:18.850 --> 00:40:21.700
resilience, is
what I think should

00:40:21.700 --> 00:40:24.605
be at the centerpiece
of our approaches

00:40:24.605 --> 00:40:25.980
and our discussions
and the like.

00:40:29.940 --> 00:40:33.430
And you see this, again,
on the government side.

00:40:33.430 --> 00:40:37.130
But also on the business side.

00:40:37.130 --> 00:40:41.500
Basically there's this
mentality of offense, defense.

00:40:41.500 --> 00:40:46.060
And defense, it's build
higher or thicker walls.

00:40:48.660 --> 00:40:52.380
And then the offense
side is, weirdly enough,

00:40:52.380 --> 00:40:54.650
coming back into
the private sector

00:40:54.650 --> 00:40:57.350
with the emergence of the
potential hack back industry,

00:40:57.350 --> 00:41:00.110
of oh, the best way to
protect yourself is not just

00:41:00.110 --> 00:41:05.910
to build a high wall but we'll
go after the bad guys for you.

00:41:05.910 --> 00:41:09.000
It's basically a business
version of vigilantism.

00:41:09.000 --> 00:41:13.814
It has major concerns for
international relations,

00:41:13.814 --> 00:41:15.480
because it could
quickly escalate things

00:41:15.480 --> 00:41:17.100
in a way that's unplanned.

00:41:17.100 --> 00:41:21.900
It's also a horrible business
model for the client.

00:41:21.900 --> 00:41:26.500
Vigilantism only worked
for Charles Bronson.

00:41:26.500 --> 00:41:30.260
This idea of the best way
to defend yourself-- I'm

00:41:30.260 --> 00:41:31.480
going to go after this guy.

00:41:31.480 --> 00:41:32.240
And then oh, you're attacking?

00:41:32.240 --> 00:41:33.620
I'll go after this guy,
this guy, this guy.

00:41:33.620 --> 00:41:35.130
And so at the end of
the day all you're

00:41:35.130 --> 00:41:37.330
doing is paying someone to
go after others for you,

00:41:37.330 --> 00:41:39.390
not actually making
yourself secure.

00:41:39.390 --> 00:41:41.250
Instead of this
mentality, it goes

00:41:41.250 --> 00:41:43.290
to what you asked
about, resilience.

00:41:43.290 --> 00:41:47.940
And you can think about this
in the physiological way.

00:41:47.940 --> 00:41:51.010
And that turns on everything
from the notion of it being

00:41:51.010 --> 00:41:54.990
not a Cold War-- you
know, this idea of we're

00:41:54.990 --> 00:41:57.535
in a new Cold War is
literally a quote from folks.

00:42:00.110 --> 00:42:05.000
One, malware is not like the
physics of a nuclear weapon.

00:42:05.000 --> 00:42:09.930
Second, there's not the bipolar
relationship of two powers.

00:42:09.930 --> 00:42:12.600
The players in
cybersecurity are just

00:42:12.600 --> 00:42:14.130
like the players in cyberspace.

00:42:14.130 --> 00:42:17.770
It's everything from the
100 cyber military units

00:42:17.770 --> 00:42:21.290
out there to
non-state collectives

00:42:21.290 --> 00:42:25.330
interested in everything from
cute cats to online protest

00:42:25.330 --> 00:42:28.470
to corporations that range
from Google to Target

00:42:28.470 --> 00:42:29.980
to the cupcake store.

00:42:29.980 --> 00:42:33.850
And so it doesn't fit that
to the online battle of ideas

00:42:33.850 --> 00:42:37.330
is not the ideological Cold
War battle that it's framed.

00:42:37.330 --> 00:42:41.190
The online battle of
ideas are-- go on YouTube

00:42:41.190 --> 00:42:43.500
and you can see the
diversity of them.

00:42:43.500 --> 00:42:46.600
And so instead it's this
ecosystem of players.

00:42:46.600 --> 00:42:49.170
And then it goes to the idea
of the physiological approach

00:42:49.170 --> 00:42:50.110
of your own body.

00:42:50.110 --> 00:42:53.505
Our bodies are probably the most
resilient thing ever created.

00:42:57.210 --> 00:43:01.580
They're designed for a world
that's incredibly hostile.

00:43:01.580 --> 00:43:03.580
They expect that bad
things are going to happen.

00:43:03.580 --> 00:43:05.750
They have a really
great exterior line

00:43:05.750 --> 00:43:07.660
of defense, your skin.

00:43:07.660 --> 00:43:11.540
But they fully plan that
that skin, at some point,

00:43:11.540 --> 00:43:13.910
will definitely be penetrated.

00:43:13.910 --> 00:43:17.010
And it has all sorts of
systems to react to that.

00:43:17.010 --> 00:43:21.070
Everything from stemming the
flow to monitoring infection,

00:43:21.070 --> 00:43:24.040
internal monitoring systems,
to your body triages

00:43:24.040 --> 00:43:26.160
between what's
important what's not

00:43:26.160 --> 00:43:28.810
to-- guess what,
your body itself

00:43:28.810 --> 00:43:31.830
operates on the assumption that
something external is already

00:43:31.830 --> 00:43:32.790
inside.

00:43:32.790 --> 00:43:35.330
There's 10 times
as much-- when you

00:43:35.330 --> 00:43:37.490
look at the number
of cell counts,

00:43:37.490 --> 00:43:38.970
there's 10 times
as much bacteria

00:43:38.970 --> 00:43:41.720
and the like in your body
than there are human cells.

00:43:41.720 --> 00:43:46.120
And again compare that to the
typical oh, just buy my widget,

00:43:46.120 --> 00:43:49.420
or if I have a better, stronger
password I'll keep them out.

00:43:49.420 --> 00:43:51.840
But there's another
idea of resilience

00:43:51.840 --> 00:43:54.810
that I don't think we
pay enough attention to.

00:43:54.810 --> 00:43:58.420
And that's psychological
resilience.

00:43:58.420 --> 00:44:03.600
There's 3,000 books on
psychological resilience

00:44:03.600 --> 00:44:04.610
of some sort.

00:44:04.610 --> 00:44:09.650
Resilience in your job,
resilience in your love life,

00:44:09.650 --> 00:44:10.464
et cetera.

00:44:10.464 --> 00:44:11.880
And it's all built
around the idea

00:44:11.880 --> 00:44:14.560
that you can't go
through life thinking

00:44:14.560 --> 00:44:16.740
that bad things
will never happen,

00:44:16.740 --> 00:44:19.280
or they can all be
deterred or defeated.

00:44:19.280 --> 00:44:23.730
Instead, your success is
dependent on your assumption

00:44:23.730 --> 00:44:25.260
that bad things will play out.

00:44:25.260 --> 00:44:27.450
But it's all about how will
you power through them?

00:44:27.450 --> 00:44:29.930
How will you recover
quickly from them?

00:44:29.930 --> 00:44:32.920
How will you not allow them
to knock you down in the way

00:44:32.920 --> 00:44:33.587
that they could?

00:44:33.587 --> 00:44:35.253
All these different
way-- and again, you

00:44:35.253 --> 00:44:37.840
can think about in your love
life to your job, whatever.

00:44:37.840 --> 00:44:42.540
We need that same mentality
when it comes to cyber.

00:44:42.540 --> 00:44:48.045
So take cyberterrorism,
the central discussion

00:44:48.045 --> 00:44:52.180
of, oh my god, the power
grid might go down.

00:44:52.180 --> 00:44:54.920
And in fact, you've seen all
of these false news reports

00:44:54.920 --> 00:44:57.230
about times that cyber
attacks caused it.

00:44:57.230 --> 00:45:02.150
Which either in one situation
the power didn't go out,

00:45:02.150 --> 00:45:04.260
it's a false story
that-- guess what,

00:45:04.260 --> 00:45:07.580
"60 Minutes"
unsurprisingly covered.

00:45:07.580 --> 00:45:10.740
To another situation, things
that are described as cyber

00:45:10.740 --> 00:45:11.890
attacks that they're not.

00:45:11.890 --> 00:45:15.630
So two dudes with a rifle,
that's not a cyber attack.

00:45:15.630 --> 00:45:18.540
But that was recently in
the news covered as in this.

00:45:18.540 --> 00:45:21.700
The bottom line is that
squirrels have taken down

00:45:21.700 --> 00:45:26.330
more power grids than the
zero times that hackers have.

00:45:26.330 --> 00:45:27.740
Again, it could play out.

00:45:27.740 --> 00:45:29.890
But it's all about how
will we react to it.

00:45:32.430 --> 00:45:35.740
Where I live outside
Washington, the power

00:45:35.740 --> 00:45:37.870
went down multiple
times this summer.

00:45:41.040 --> 00:45:45.120
But if it had been a cyber
attack that caused it,

00:45:45.120 --> 00:45:47.740
we would have had a
congressional commission

00:45:47.740 --> 00:45:49.620
investigating who to blame.

00:45:49.620 --> 00:45:53.230
And we would have had
mass hysteria around it.

00:45:53.230 --> 00:45:56.860
And so what I would prefer-- and
I go back to that echo of 9/11

00:45:56.860 --> 00:46:01.660
and how you react --is the
British mentality to terrorism,

00:46:01.660 --> 00:46:03.370
keep calm and carry on.

00:46:03.370 --> 00:46:07.370
Rather than the
American model, which is

00:46:07.370 --> 00:46:12.650
we try and out-escalate the
hype and the fear around it.

00:46:12.650 --> 00:46:17.040
Because we're seeing more
gains in the fear and hype.

00:46:17.040 --> 00:46:20.270
And my worry is that's
carrying over to cyber side.

00:46:20.270 --> 00:46:23.770
AUDIENCE: The cyber crime that
really matters in the Snowden

00:46:23.770 --> 00:46:29.060
story is not what Snowden
did, but what he revealed.

00:46:29.060 --> 00:46:34.470
Alexander has two mandates,
both offense and defense.

00:46:34.470 --> 00:46:37.850
And as we've seen
and as you've said,

00:46:37.850 --> 00:46:42.050
the offense has dominated
in his activities.

00:46:42.050 --> 00:46:47.730
But whether offense
inevitably dominates,

00:46:47.730 --> 00:46:52.540
as they seem to think and as is
premised behind their actions,

00:46:52.540 --> 00:46:54.640
they've loaded the dice.

00:46:54.640 --> 00:46:56.960
Part of what they've
done is rather than

00:46:56.960 --> 00:47:00.660
also act on their
defensive mandate,

00:47:00.660 --> 00:47:05.690
they have purposely gone out
and inserted vulnerabilities,

00:47:05.690 --> 00:47:09.460
worked with vendors
of security software

00:47:09.460 --> 00:47:11.460
to purposely insert
vulnerabilities,

00:47:11.460 --> 00:47:14.330
making us more vulnerable.

00:47:14.330 --> 00:47:17.670
And as you said,
incentives matter.

00:47:17.670 --> 00:47:20.170
Let's take a look-- in
the absence of Snowden,

00:47:20.170 --> 00:47:22.910
let's take a look
at the incentives

00:47:22.910 --> 00:47:26.490
on secret intelligence
agencies themselves.

00:47:26.490 --> 00:47:30.300
What is the bureaucratic reward
for successfully carrying out

00:47:30.300 --> 00:47:31.580
an attack?

00:47:31.580 --> 00:47:34.770
And what is the
bureaucratic reward

00:47:34.770 --> 00:47:38.910
for successfully preventing
attacks that aren't visible

00:47:38.910 --> 00:47:41.010
because they couldn't happen?

00:47:41.010 --> 00:47:43.160
The second is invisible.

00:47:43.160 --> 00:47:45.810
The bureaucratic
reward structure

00:47:45.810 --> 00:47:48.180
has no means to reward it.

00:47:48.180 --> 00:47:52.450
And in the absence of Snowden,
the first is cost-free.

00:47:52.450 --> 00:47:55.930
PETER SINGER: I'm in agreement
with you on a couple of areas.

00:47:55.930 --> 00:47:58.090
One, on the notion
of incentives.

00:47:58.090 --> 00:48:00.510
And again, you can see
that whether you're

00:48:00.510 --> 00:48:04.220
talking about within that
intelligence agency to why

00:48:04.220 --> 00:48:07.380
we see on the defense side
certain industries cooperate

00:48:07.380 --> 00:48:07.909
or not.

00:48:07.909 --> 00:48:08.950
And it all turns on that.

00:48:08.950 --> 00:48:10.730
But then there's the
broader-- essentially

00:48:10.730 --> 00:48:14.300
you pulled the
bandage of Snowden.

00:48:14.300 --> 00:48:15.590
And so we've got to go at it.

00:48:15.590 --> 00:48:20.800
And you began by hitting
one part of his activity.

00:48:20.800 --> 00:48:22.930
And I think this is
the challenge right now

00:48:22.930 --> 00:48:27.065
in the discussion and
debate around him,

00:48:27.065 --> 00:48:30.850
the NSA, is he a traitor, is
he a whistleblower, should he

00:48:30.850 --> 00:48:36.960
get clemency or the like,
is that essentially he

00:48:36.960 --> 00:48:42.470
gathered and now is being
released-- actually not by him.

00:48:42.470 --> 00:48:44.500
This is one of the
myths that's out there.

00:48:44.500 --> 00:48:47.900
It's not him pulling
the strings right now.

00:48:47.900 --> 00:48:50.821
The journalists, they're
actually going through it.

00:48:50.821 --> 00:48:53.070
And the challenge for them
is because there's so much,

00:48:53.070 --> 00:48:55.750
it actually involves,
again, a very different set

00:48:55.750 --> 00:48:56.880
of expertise.

00:48:56.880 --> 00:49:01.510
So someone who understands
the technology will not

00:49:01.510 --> 00:49:05.760
get-- they'll see a name pop
up that, say, the Latin America

00:49:05.760 --> 00:49:08.210
beat reporter will go whoa,
whoa, whoa, that name.

00:49:08.210 --> 00:49:11.350
That guy's now the Deputy
Foreign Minister of Brazil.

00:49:11.350 --> 00:49:12.980
That name is meaningless
to the person

00:49:12.980 --> 00:49:16.250
who knows what this acronym
means that the Latin America

00:49:16.250 --> 00:49:17.632
reporter doesn't know that.

00:49:17.632 --> 00:49:19.590
And then in turn you need
the spy and the like.

00:49:19.590 --> 00:49:22.910
So they're actually having
these teams go through it

00:49:22.910 --> 00:49:27.340
and figuring out what's
newsworthy or not.

00:49:27.340 --> 00:49:30.390
But the bottom line is there's
such a mass of information

00:49:30.390 --> 00:49:32.840
and the wide variety of
stories that have come out

00:49:32.840 --> 00:49:37.740
and will continue to come
out is that it essentially

00:49:37.740 --> 00:49:42.170
falls into three very
different buckets of activity

00:49:42.170 --> 00:49:44.130
that has been disclosed.

00:49:44.130 --> 00:49:47.220
The first bucket of
activity is frankly

00:49:47.220 --> 00:49:52.330
what I would describe as
smart strategic espionage

00:49:52.330 --> 00:49:55.030
against American enemies.

00:49:55.030 --> 00:49:57.720
And you hit sort
of the mentality

00:49:57.720 --> 00:49:59.270
that drive some of that.

00:49:59.270 --> 00:50:01.226
Now there's an issue
of-- you said they

00:50:01.226 --> 00:50:03.350
when you're talking about
NSA versus cyber command.

00:50:03.350 --> 00:50:05.391
And they're the top military
intelligence agency.

00:50:05.391 --> 00:50:08.250
But the bottom line is
one bucket of activity

00:50:08.250 --> 00:50:15.160
was things that we would expect
and want an agency to do.

00:50:15.160 --> 00:50:20.300
Going after monitoring
terror rings in Pakistan,

00:50:20.300 --> 00:50:24.270
Iranian nuclear research,
China, et cetera.

00:50:24.270 --> 00:50:28.990
Bucket number two is what
I would term questionable.

00:50:28.990 --> 00:50:31.320
Activities that there
is a debate around

00:50:31.320 --> 00:50:35.430
because it involves US citizens
in some way, shape, or form.

00:50:35.430 --> 00:50:40.700
Either through legal approaches
on the front door to back door

00:50:40.700 --> 00:50:43.860
to running with an
authorization in a way

00:50:43.860 --> 00:50:47.030
that the policymaker that
authorized it didn't understand

00:50:47.030 --> 00:50:51.150
what was authorized
to essentially deals

00:50:51.150 --> 00:50:53.400
made with foreign
intelligence agencies

00:50:53.400 --> 00:50:55.400
where they were able to
collect things in a way

00:50:55.400 --> 00:50:57.920
that we couldn't, an exchange
of information and the like.

00:50:57.920 --> 00:51:01.870
But basically the debate around
involvement of US citizens.

00:51:01.870 --> 00:51:05.560
Category three is
the bucket that I

00:51:05.560 --> 00:51:09.590
would describe as unstrategic,
or more directly stupid.

00:51:09.590 --> 00:51:13.760
And that is targeting
of close American allies

00:51:13.760 --> 00:51:16.530
and American
technology companies.

00:51:16.530 --> 00:51:20.170
And the resonance of
that is everything

00:51:20.170 --> 00:51:22.390
from how I mentioned
the hammering

00:51:22.390 --> 00:51:25.510
to other kinds of
international negotiations that

00:51:25.510 --> 00:51:28.490
may matter more to
as you mentioned,

00:51:28.490 --> 00:51:33.049
the undermining of
cybersecurity for all of us.

00:51:33.049 --> 00:51:34.590
Particularly based
on this assumption

00:51:34.590 --> 00:51:36.298
that they were the
only ones smart enough

00:51:36.298 --> 00:51:40.180
to find the vulnerability,
but then more broadly what

00:51:40.180 --> 00:51:43.050
it's done to that
critical word, trust,

00:51:43.050 --> 00:51:45.182
trust in American
technology companies.

00:51:45.182 --> 00:51:46.640
And the resonance
of that, at least

00:51:46.640 --> 00:51:48.450
according to
Forrester Research, is

00:51:48.450 --> 00:51:51.520
that your industry will lose
approximately $180 billion

00:51:51.520 --> 00:51:52.750
worth of revenue.

00:51:52.750 --> 00:51:54.490
That's why people
here are pissed.

00:51:54.490 --> 00:51:59.310
The problem, though, is
that in the debate around it

00:51:59.310 --> 00:52:03.280
we pull from whichever
bucket we care most about.

00:52:03.280 --> 00:52:06.470
So if you care most about
classic national security,

00:52:06.470 --> 00:52:09.210
you go, this guy disclosed
things that are important.

00:52:09.210 --> 00:52:11.990
He is a traitor,
dada, dada, dada.

00:52:11.990 --> 00:52:15.520
If you care about the privacy
Fourth Amendment questions,

00:52:15.520 --> 00:52:18.380
you only talk about
those, and he's

00:52:18.380 --> 00:52:20.770
a whistleblower, and
clemency, and the like.

00:52:20.770 --> 00:52:23.190
We see it also in
how we defend it

00:52:23.190 --> 00:52:25.680
from the narrative on
the government side.

00:52:25.680 --> 00:52:31.670
So these kind of activities
are to prevent another 9/11.

00:52:31.670 --> 00:52:36.690
Which may describe bucket two
and the metadata and the like,

00:52:36.690 --> 00:52:38.930
but that doesn't make
the Germans feel better

00:52:38.930 --> 00:52:41.820
about why you were going
after Angela Merkel's

00:52:41.820 --> 00:52:43.480
messages, or the like.

00:52:43.480 --> 00:52:47.230
And so the problem is it's
all of these things at once.

00:52:47.230 --> 00:52:49.360
And it's muddied the
water of the discussions.

00:52:49.360 --> 00:52:51.100
And we can even see
this most recently

00:52:51.100 --> 00:52:54.440
in the President's speech,
which, again, focused primarily

00:52:54.440 --> 00:52:57.880
on one of the buckets,
mostly the privacy side.

00:52:57.880 --> 00:53:00.690
Because that's what matters
most in the American political

00:53:00.690 --> 00:53:02.540
debate, but actually
may not matter

00:53:02.540 --> 00:53:05.150
the most in the long
term national security

00:53:05.150 --> 00:53:07.420
and economic prosperity
of the nation, which

00:53:07.420 --> 00:53:11.050
is weird and scary to say.

00:53:11.050 --> 00:53:14.540
AUDIENCE: Just a question on
how Silicon Valley companies can

00:53:14.540 --> 00:53:18.270
partner with each other and
with the government to actually

00:53:18.270 --> 00:53:20.790
have better government
surveillance policies, right?

00:53:20.790 --> 00:53:23.735
Recently, we saw the
government surveillance reform,

00:53:23.735 --> 00:53:25.610
where like seven companies
have got together.

00:53:25.610 --> 00:53:28.310
And again, it's going back to
the notion of us versus them

00:53:28.310 --> 00:53:29.970
where instead of
partnering, it's

00:53:29.970 --> 00:53:33.050
now they're pushing
for like reforms,

00:53:33.050 --> 00:53:35.491
and wasting lobbying
dollars and stuff while it

00:53:35.491 --> 00:53:36.740
could be a better partnership.

00:53:36.740 --> 00:53:41.640
So what are you thoughts
on what we could do?

00:53:41.640 --> 00:53:44.160
PETER SINGER: There's
steps that can be taken.

00:53:44.160 --> 00:53:48.140
But one of the underlying
things this is attitude.

00:53:48.140 --> 00:53:54.680
And it's funny, I was out
here a little over a year ago.

00:53:54.680 --> 00:53:59.530
And there was sort of
an attitude towards,

00:53:59.530 --> 00:54:02.910
DC is so dysfunctional.

00:54:02.910 --> 00:54:04.600
Nothing could get done there.

00:54:04.600 --> 00:54:06.140
You guys are so problematic.

00:54:06.140 --> 00:54:09.930
We don't want anything
to do with you.

00:54:09.930 --> 00:54:12.890
And we don't need
anything to do with you.

00:54:12.890 --> 00:54:16.020
And then now we see the flip
side of that of actually

00:54:16.020 --> 00:54:18.780
what you do matters to us.

00:54:18.780 --> 00:54:20.520
You're still dysfunctional.

00:54:20.520 --> 00:54:22.880
But it matters to us.

00:54:22.880 --> 00:54:28.680
And in turn, you saw that
approach from-- again,

00:54:28.680 --> 00:54:34.240
this is from the stovepiping--
Individuals pursuing

00:54:34.240 --> 00:54:42.060
a certain political goal, and
within just a limited circle,

00:54:42.060 --> 00:54:45.710
not understanding the ripple
effects of what they were doing

00:54:45.710 --> 00:54:48.330
on lots of other
areas including one

00:54:48.330 --> 00:54:50.710
of the cornerstones of
American prosperity, which

00:54:50.710 --> 00:54:53.240
is our technology industry.

00:54:53.240 --> 00:54:57.275
So the problem is first
knocking down that attitude

00:54:57.275 --> 00:55:00.075
that neither side
matters to the other

00:55:00.075 --> 00:55:02.560
and doesn't need to
understand the other.

00:55:08.440 --> 00:55:10.870
Too often, Silicon
Valley-- and even sort

00:55:10.870 --> 00:55:16.050
of the reaction when I
said this in the speech

00:55:16.050 --> 00:55:19.160
--will offer a seeming
technologic solution

00:55:19.160 --> 00:55:22.030
to a problem.

00:55:22.030 --> 00:55:25.170
There's far more
engineers out here

00:55:25.170 --> 00:55:28.042
than almost any other specialty.

00:55:28.042 --> 00:55:29.500
And so there's
often-- you know, we

00:55:29.500 --> 00:55:32.790
can engineer our way out of
it some way, shape, or form.

00:55:32.790 --> 00:55:38.150
And we even see that now in
this discussion over privacy

00:55:38.150 --> 00:55:41.260
where it was OK, we can't
figure out what to do.

00:55:41.260 --> 00:55:47.770
But Attorney General and Head
of National Intelligence,

00:55:47.770 --> 00:55:51.020
you've got 60 days to
figure out this solution.

00:55:51.020 --> 00:55:52.530
And we see different
sort of things

00:55:52.530 --> 00:55:56.800
offered that are sort
of a technical solution.

00:55:56.800 --> 00:55:59.655
It's not going to be
a technical solution.

00:55:59.655 --> 00:56:04.060
It's going to be an awful,
painful grind of policy

00:56:04.060 --> 00:56:07.710
and votes and court
decisions and lobbying

00:56:07.710 --> 00:56:09.220
and all these other
things that go

00:56:09.220 --> 00:56:14.500
into the nasty sausage
of political process.

00:56:14.500 --> 00:56:17.190
But in turn, what I'm
getting at, too often

00:56:17.190 --> 00:56:20.810
we fail to look at the human
side of what can be done.

00:56:20.810 --> 00:56:23.410
And that would be
another aspect of it.

00:56:23.410 --> 00:56:26.670
But the bottom
line is we clearly

00:56:26.670 --> 00:56:28.670
have a shared stake in it.

00:56:28.670 --> 00:56:33.140
And I hope we can raise
the level of discourse

00:56:33.140 --> 00:56:35.930
and raise the level
of cooperation.

00:56:35.930 --> 00:56:36.800
AUDIENCE: Hi, Peter.

00:56:36.800 --> 00:56:38.424
Great to see the book
finally come out.

00:56:38.424 --> 00:56:41.655
What are you optimistic about?

00:56:41.655 --> 00:56:43.280
PETER SINGER: I
thought-- I mean, look,

00:56:43.280 --> 00:56:50.160
I'm actually hugely
optimistic about-- I mean,

00:56:50.160 --> 00:56:53.150
the possibilities of this
technology, what it's allowed

00:56:53.150 --> 00:57:00.600
to accomplish, and in turn
the people who misuse it,

00:57:00.600 --> 00:57:03.540
and what they're
costing themselves.

00:57:03.540 --> 00:57:09.260
And that misuse is
everything from-- there's

00:57:09.260 --> 00:57:13.900
a very real danger of the
balkanization of the internet.

00:57:13.900 --> 00:57:18.830
On the other hand, the cost
of that to those nations,

00:57:18.830 --> 00:57:21.650
it will be staggering.

00:57:21.650 --> 00:57:25.290
A flip way of putting
it is, there's

00:57:25.290 --> 00:57:27.170
one nation that has been
really, really great

00:57:27.170 --> 00:57:29.850
cyber security
protections-- North Korea.

00:57:29.850 --> 00:57:31.180
There's a cost to that.

00:57:34.300 --> 00:57:37.990
And we can see this in turn
on the debate around the NSA

00:57:37.990 --> 00:57:38.836
to businesses.

00:57:38.836 --> 00:57:41.460
I gave all of these anecdotes of
how they're not doing it well,

00:57:41.460 --> 00:57:43.620
but now they're
facing cost to it.

00:57:43.620 --> 00:57:47.030
The recent examples of, be it
Target or Snapchat or Neiman

00:57:47.030 --> 00:57:50.010
Marcus, is that there's
an ebb and flow.

00:57:50.010 --> 00:57:53.490
And people that
mishandle it face costs.

00:57:53.490 --> 00:57:56.250
And so to me, that's
where we'll see reactions.

00:57:56.250 --> 00:57:59.700
The incentives will drive it.

00:57:59.700 --> 00:58:01.550
If there's any
message from the book,

00:58:01.550 --> 00:58:04.890
it's that this is
seemingly scary stuff.

00:58:04.890 --> 00:58:06.720
And some of it should be scary.

00:58:06.720 --> 00:58:10.040
But on the other hand, we
can't have a good discussion

00:58:10.040 --> 00:58:13.170
if it's like Spinal Tap and
the volume's always at 11.

00:58:13.170 --> 00:58:16.100
Which has been how
we've talked about it.

00:58:16.100 --> 00:58:18.000
And so the goal of
the book was basically

00:58:18.000 --> 00:58:20.940
to fill this kind of sweet
spot where you either

00:58:20.940 --> 00:58:23.490
had this highly
technical discussion that

00:58:23.490 --> 00:58:27.840
was exclusionary or you
had the histrionic side.

00:58:27.840 --> 00:58:30.420
And instead, I think
this can be a topic--

00:58:30.420 --> 00:58:33.220
I think it has to be a topic
that we're all better equipped

00:58:33.220 --> 00:58:33.920
to talk about.

00:58:33.920 --> 00:58:36.760
And I'm optimistic that
when we do understand this,

00:58:36.760 --> 00:58:41.546
we can go much further than
where we're at right now.

00:58:41.546 --> 00:58:43.930
MALE SPEAKER: And on
this optimistic note,

00:58:43.930 --> 00:58:46.500
please give a hand
to Peter Singer.

