WEBVTT
Kind: captions
Language: en

00:00:04.200 --> 00:00:08.240
Funding for this program is provided by:

00:00:08.240 --> 00:00:15.240
Additional funding provided by

00:00:33.510 --> 00:00:37.750
This is a course about Justice and we begin
with a story

00:00:37.750 --> 00:00:40.180
suppose you're the driver of a trolley car,

00:00:40.180 --> 00:00:44.640
and your trolley car is hurdling down
the track at sixty miles an hour

00:00:44.640 --> 00:00:49.390
and at the end of the track you notice
five workers working on the track

00:00:49.390 --> 00:00:51.800
you tried to stop but you can't

00:00:51.800 --> 00:00:53.830
your brakes don't work

00:00:53.830 --> 00:00:56.780
you feel desperate because you know

00:00:56.780 --> 00:00:59.590
that if you crash into these five workers

00:00:59.590 --> 00:01:01.490
they will all die

00:01:01.490 --> 00:01:05.080
let's assume you know that for sure

00:01:05.080 --> 00:01:07.420
and so you feel helpless

00:01:07.420 --> 00:01:09.450
until you notice that there is

00:01:09.450 --> 00:01:11.290
off to the right

00:01:11.290 --> 00:01:13.170
a side track

00:01:13.170 --> 00:01:15.580
at the end of that track

00:01:15.580 --> 00:01:17.390
there's one worker

00:01:17.390 --> 00:01:19.080
working on track

00:01:19.080 --> 00:01:21.369
you're steering wheel works

00:01:21.369 --> 00:01:23.030
so you can

00:01:23.030 --> 00:01:26.010
turn the trolley car if you want to

00:01:26.010 --> 00:01:28.840
onto this side track

00:01:28.840 --> 00:01:30.399
killing the one

00:01:30.399 --> 00:01:33.270
but sparing the five.

00:01:33.270 --> 00:01:36.110
Here's our first question

00:01:36.110 --> 00:01:38.820
what's the right thing to do?

00:01:38.820 --> 00:01:40.479
What would you do?

00:01:40.479 --> 00:01:42.560
Let's take a poll,

00:01:42.560 --> 00:01:45.180
how many

00:01:45.180 --> 00:01:52.020
would turn the trolley car onto the side track?

00:01:52.020 --> 00:01:53.530
How many wouldn't?

00:01:53.530 --> 00:01:58.180
How many would go straight ahead

00:01:58.180 --> 00:02:04.050
keep your hands up, those of you who'd go straight
ahead.

00:02:04.050 --> 00:02:08.379
A handful of people would, the vast majority
would turn

00:02:08.379 --> 00:02:09.879
let's hear first

00:02:09.879 --> 00:02:14.149
now we need to begin to investigate the reasons
why you think

00:02:14.149 --> 00:02:19.799
it's the right thing to do. Let's begin with
those in the majority, who would turn

00:02:19.799 --> 00:02:22.059
to go onto side track?

00:02:22.059 --> 00:02:23.670
Why would you do it,

00:02:23.670 --> 00:02:25.809
what would be your reason?

00:02:25.809 --> 00:02:30.189
Who's willing to volunteer a reason?

00:02:30.189 --> 00:02:32.219
Go ahead, stand up.

00:02:32.219 --> 00:02:39.219
Because it can't be right to kill five people
when you can only kill one person instead.

00:02:39.579 --> 00:02:42.239
it wouldn't be right to kill five

00:02:42.239 --> 00:02:47.069
if you could kill one person instead

00:02:47.069 --> 00:02:48.529
that's a good reason

00:02:48.529 --> 00:02:52.959
that's a good reason

00:02:52.959 --> 00:02:53.779
who else?

00:02:53.779 --> 00:02:56.659
does everybody agree with that

00:02:56.659 --> 00:03:01.219
reason? go ahead.

00:03:01.219 --> 00:03:03.620
Well I was thinking it was the same reason it was on

00:03:03.620 --> 00:03:05.459
9/11 we regard the people who flew the plane

00:03:05.459 --> 00:03:08.009
who flew the plane into the

00:03:08.009 --> 00:03:09.529
Pennsylvania field as heroes

00:03:09.529 --> 00:03:11.719
because they chose to kill the people on the
plane

00:03:11.719 --> 00:03:14.419
and not kill more people

00:03:14.419 --> 00:03:16.109
in big buildings.

00:03:16.109 --> 00:03:19.260
So the principle there was the same on 9/11

00:03:19.260 --> 00:03:21.639
it's tragic circumstance,

00:03:21.639 --> 00:03:25.359
but better to kill one so that five can
live

00:03:25.359 --> 00:03:30.749
is that the reason most of you have, those
of you who would turn, yes?

00:03:30.749 --> 00:03:32.629
Let's hear now

00:03:32.629 --> 00:03:33.629
from

00:03:33.629 --> 00:03:35.509
those in the minority

00:03:35.509 --> 00:03:40.559
those who wouldn't turn. 

00:03:40.559 --> 00:03:45.709
Well I think that same type of mentality that
justifies genocide and totalitarianism

00:03:45.469 --> 00:03:50.359
in order to save one type of race you
wipe out the other.

00:03:50.359 --> 00:03:53.279
so what would you do in this case? You would

00:03:53.279 --> 00:03:55.379
to avoid

00:03:55.379 --> 00:03:57.799
the horrors of genocide

00:03:57.799 --> 00:04:03.989
you would crash into the five and kill them?

00:04:03.989 --> 00:04:07.629
Presumably yes.

00:04:07.629 --> 00:04:09.930
okay who else?

00:04:09.930 --> 00:04:14.499
That's a brave answer, thank you.

00:04:14.499 --> 00:04:16.079
Let's consider another

00:04:16.979 --> 00:04:20.239
trolley car case

00:04:20.239 --> 00:04:21.689
and see

00:04:21.689 --> 00:04:24.189
whether

00:04:24.189 --> 00:04:27.429
those of you in the majority

00:04:27.429 --> 00:04:30.990
want to adhere to the principle,

00:04:30.990 --> 00:04:33.500
better that one should die so that five
should live.

00:04:33.500 --> 00:04:38.529
This time you're not the driver of the trolley
car, you're an onlooker

00:04:38.529 --> 00:04:42.679
standing on a bridge overlooking a trolley car track

00:04:42.679 --> 00:04:45.659
and down the track comes a trolley car

00:04:45.659 --> 00:04:49.620
at the end of the track are five workers

00:04:49.620 --> 00:04:51.739
the brakes don't work

00:04:51.739 --> 00:04:55.949
the trolley car is about to careen into the
five and kill them

00:04:55.949 --> 00:04:57.219
and now

00:04:57.219 --> 00:04:58.789
you're not the driver

00:04:58.789 --> 00:05:01.309
you really feel helpless

00:05:01.309 --> 00:05:03.259
until you notice

00:05:03.259 --> 00:05:06.879
standing next to you

00:05:06.879 --> 00:05:08.639
leaning over

00:05:08.639 --> 00:05:09.889
the bridge

00:05:09.889 --> 00:05:16.889
is it very fat man.

00:05:17.219 --> 00:05:20.179
And you could

00:05:20.179 --> 00:05:22.669
give him a shove

00:05:22.669 --> 00:05:24.729
he would fall over the bridge

00:05:24.729 --> 00:05:27.949
onto the track

00:05:27.949 --> 00:05:29.539
right in the way of

00:05:29.539 --> 00:05:32.199
the trolley car

00:05:32.199 --> 00:05:33.419
he would die

00:05:33.419 --> 00:05:38.930
but he would spare the five.

00:05:38.930 --> 00:05:41.109
Now, how many would push

00:05:41.109 --> 00:05:48.090
the fat man over the bridge? Raise your hand. 

00:05:48.090 --> 00:05:51.199
How many wouldn't?

00:05:51.199 --> 00:05:54.069
Most people wouldn't.

00:05:54.069 --> 00:05:55.789
Here's the obvious question,

00:05:55.789 --> 00:05:56.879
what became

00:05:56.879 --> 00:06:00.169
of the principle

00:06:00.169 --> 00:06:05.110
better to save five lives even if it means
sacrificing one, what became of the principal

00:06:05.110 --> 00:06:07.490
that almost everyone endorsed

00:06:07.490 --> 00:06:09.240
in the first case

00:06:09.240 --> 00:06:12.809
I need to hear from someone who was in the
majority in both

00:06:12.809 --> 00:06:13.580
cases is

00:06:13.580 --> 00:06:17.759
how do you explain the difference between
the two?

00:06:17.759 --> 00:06:21.860
The second one I guess involves an
active choice of 

00:06:21.860 --> 00:06:22.510
pushing a person

00:06:22.510 --> 00:06:24.280
and down which

00:06:24.280 --> 00:06:25.199
I guess that

00:06:25.199 --> 00:06:29.830
that person himself would otherwise not 
have been involved in the situation at all

00:06:29.830 --> 00:06:31.270
and so

00:06:31.270 --> 00:06:33.770
to choose on his behalf I guess

00:06:33.770 --> 00:06:36.770
to 

00:06:36.770 --> 00:06:39.990
involve him in something that he otherwise would
have this escaped is

00:06:39.990 --> 00:06:41.789
I guess more than

00:06:41.789 --> 00:06:43.620
what you have in the first case where

00:06:43.620 --> 00:06:45.969
the three parties, the driver and

00:06:45.969 --> 00:06:47.669
the two sets of workers are

00:06:47.669 --> 00:06:50.599
already I guess in this situation.

00:06:50.599 --> 00:06:55.109
but the guy working, the one on the track
off to the side

00:06:55.109 --> 00:07:02.069
he didn't choose to sacrifice his life any
more than the fat guy did, did he?

00:07:02.069 --> 00:07:05.420
That's true, but he was on the tracks.

00:07:05.400 --> 00:07:10.439
this guy was on the bridge.

00:07:10.439 --> 00:07:13.550
Go ahead, you can come back if you want.

00:07:13.550 --> 00:07:15.330
Alright, it's a hard question

00:07:15.330 --> 00:07:18.659
but you did well you did very well it's a
hard question.

00:07:19.630 --> 00:07:21.180
who else

00:07:21.180 --> 00:07:22.599
can

00:07:22.599 --> 00:07:26.029
find a way of reconciling

00:07:26.029 --> 00:07:30.159
the reaction of the majority in these two cases? Yes?

00:07:30.159 --> 00:07:31.550
Well I guess

00:07:31.550 --> 00:07:32.540
in the first case where

00:07:32.540 --> 00:07:35.179
you have the one worker and the five

00:07:35.179 --> 00:07:37.430
it's a  choice between those two, and you have to 

00:07:37.430 --> 00:07:41.219
make a certain choice and people are going to die 
because of the trolley car 

00:07:41.219 --> 00:07:45.030
not necessarily because of your direct actions.
The trolley car is a runway,

00:07:45.030 --> 00:07:48.210
thing and you need to make in a split second choice

00:07:48.210 --> 00:07:52.659
whereas pushing the fat man over is an actual
act of murder on your part

00:07:52.659 --> 00:07:54.490
you have control over that

00:07:54.490 --> 00:07:57.279
whereas you may not have control over the trolley car.

00:07:57.279 --> 00:08:00.090
So I think that it's a slightly different situation.

00:08:00.090 --> 00:08:04.360
Alright who has a reply? Is that, who has a reply to that? 
no that was good, who has a way

00:08:04.360 --> 00:08:06.319
who wants to reply?

00:08:06.319 --> 00:08:09.490
Is that a way out of this?

00:08:09.490 --> 00:08:12.440
I don't think that's a very good reason because
you choose

00:08:12.440 --> 00:08:16.890
either way you have to choose who dies
because you either choose to turn and kill a person

00:08:16.890 --> 00:08:18.500
which is an act of conscious

00:08:18.500 --> 00:08:19.719
thought to turn,

00:08:19.719 --> 00:08:21.319
or you choose to push the fat man 

00:08:21.319 --> 00:08:23.609
over which is also an active

00:08:23.609 --> 00:08:27.689
conscious action so either way you're making a choice.

00:08:27.689 --> 00:08:29.729
Do you want to reply?

00:08:29.729 --> 00:08:34.190
Well I'm not really sure that that's the case, it just still
seems kind of different, the act of actually

00:08:34.190 --> 00:08:38.230
pushing someone over onto the tracks and killing them,

00:08:38.230 --> 00:08:42.600
you are actually killing him yourself, you're pushing
him with your own hands you're pushing and 

00:08:42.600 --> 00:08:43.710
that's different

00:08:43.710 --> 00:08:47.090
than steering something that is going to
cause death

00:08:47.090 --> 00:08:48.670
into another...you know

00:08:48.670 --> 00:08:52.830
it doesn't really sound right saying it now when I'm up here.

00:08:52.830 --> 00:08:54.740
No that's good, what's your name?

00:08:54.740 --> 00:08:55.570
Andrew. 

00:08:55.740 --> 00:08:59.570
Andrew and let me ask you this question Andrew,

00:08:59.570 --> 00:09:02.240
suppose

00:09:02.240 --> 00:09:03.680
standing on the bridge

00:09:03.680 --> 00:09:04.810
next to the fat man

00:09:04.810 --> 00:09:07.850
I didn't have to push him, suppose he was standing

00:09:07.850 --> 00:09:14.850
over a trap door that I could open by turning
a steering wheel like that

00:09:17.450 --> 00:09:18.690
would you turn it?

00:09:18.690 --> 00:09:20.960
For some reason that still just seems more 

00:09:20.960 --> 00:09:24.130
more wrong.

00:09:24.130 --> 00:09:30.350
I mean maybe if you just accidentally like leaned into
this steering wheel or something like that

00:09:30.350 --> 00:09:31.060
or but, 

00:09:31.060 --> 00:09:33.119
or say that the car is 

00:09:33.119 --> 00:09:37.850
hurdling towards a switch that will drop the trap

00:09:37.850 --> 00:09:39.230
then I could agree with that.

00:09:39.850 --> 00:09:42.230
Fair enough, it still seems 

00:09:42.230 --> 00:09:45.500
wrong in a way that it doesn't seem wrong in the
first case to turn, you say

00:09:45.500 --> 00:09:50.420
An in another way, I mean in the first situation you're
involved directly with the situation

00:09:50.420 --> 00:09:52.450
in the second one you're an onlooker as well.

00:09:52.450 --> 00:09:56.920
So you have the choice of becoming involved
or not by pushing the fat man.

00:09:56.920 --> 00:09:59.550
Let's forget for the moment about this case,

00:09:59.550 --> 00:10:01.270
that's good,

00:10:01.270 --> 00:10:06.460
but let's imagine a different case. This time
your doctor in an emergency room

00:10:06.460 --> 00:10:11.630
and six patients come to you

00:10:11.630 --> 00:10:18.500
they've been in a terrible trolley car wreck

00:10:18.500 --> 00:10:23.890
five of them sustained moderate injuries one
is severely injured you could spend all day

00:10:23.890 --> 00:10:27.720
caring for the one severely injured victim,

00:10:27.720 --> 00:10:32.330
but in that time the five would die, or you could
look after the five, restore them to health, but

00:10:32.330 --> 00:10:35.490
during that time the one severely injured

00:10:35.490 --> 00:10:36.470
person would die.

00:10:36.470 --> 00:10:37.660
How many would save 

00:10:37.660 --> 00:10:39.540
the five

00:10:39.540 --> 00:10:40.850
now as the doctor?

00:10:40.850 --> 00:10:44.480
How many would save the one?

00:10:44.480 --> 00:10:46.300
Very few people,

00:10:46.300 --> 00:10:49.270
just a handful of people.

00:10:49.270 --> 00:10:51.440
Same reason I assume,

00:10:51.440 --> 00:10:55.570
one life versus five.

00:10:55.570 --> 00:10:57.230
Now consider

00:10:57.230 --> 00:10:59.070
another doctor case

00:10:59.070 --> 00:11:02.080
this time you're a transplant surgeon

00:11:02.080 --> 00:11:06.160
and you have five patients each in desperate
need

00:11:06.160 --> 00:11:09.640
of an organ transplant in order to survive

00:11:09.640 --> 00:11:12.220
on needs a heart one a lung,

00:11:12.220 --> 00:11:13.550
one a kidney, 

00:11:13.550 --> 00:11:15.090
one a liver

00:11:15.090 --> 00:11:16.680
and the fifth

00:11:16.680 --> 00:11:20.210
a pancreas.

00:11:20.210 --> 00:11:22.780
And you have no organ donors

00:11:22.780 --> 00:11:24.910
you are about to

00:11:24.910 --> 00:11:27.650
see you them die

00:11:27.650 --> 00:11:28.860
and then

00:11:28.860 --> 00:11:30.650
it occurs to you

00:11:30.650 --> 00:11:32.470
that in the next room

00:11:32.470 --> 00:11:35.720
there's a healthy guy who came in for a checkup.

00:11:39.450 --> 00:11:43.740
and he is

00:11:43.740 --> 00:11:47.160
you like that

00:11:47.160 --> 00:11:50.740
and he's taking a nap

00:11:53.270 --> 00:11:56.770
you could go in very quietly

00:11:56.770 --> 00:12:00.600
yank out the five organs, that person would
die

00:12:00.600 --> 00:12:03.170
but you can save the five.

00:12:03.170 --> 00:12:10.170
How many would do it? Anyone?

00:12:10.390 --> 00:12:17.390
How many? Put your hands up if you would do it.

00:12:18.100 --> 00:12:21.840
Anyone in the balcony?

00:12:21.840 --> 00:12:24.280
You would? Be careful don't lean over too much

00:12:26.270 --> 00:12:29.340
How many wouldn't?

00:12:29.340 --> 00:12:30.360
All right.

00:12:30.360 --> 00:12:33.600
What do you say, speak up in the balcony, you
who would

00:12:33.600 --> 00:12:35.840
yank out the organs, why?

00:12:35.840 --> 00:12:38.629
I'd actually like to explore slightly alternate

00:12:38.629 --> 00:12:40.189
possibility of just taking the one

00:12:40.189 --> 00:12:44.420
of the five he needs an organ who dies first

00:12:44.420 --> 00:12:50.230
and using their four healthy organs to save the other
four

00:12:50.230 --> 00:12:54.640
That's a pretty good idea.

00:12:54.640 --> 00:12:57.830
That's a great idea

00:12:57.830 --> 00:13:00.160
except for the fact

00:13:00.160 --> 00:13:06.160
that you just wrecked the philosophical point.

00:13:06.160 --> 00:13:07.360
Let's step back

00:13:07.360 --> 00:13:10.240
from these stories and these arguments

00:13:10.240 --> 00:13:12.750
to notice a couple of things

00:13:12.750 --> 00:13:17.810
about the way the arguments have began to unfold.

00:13:17.810 --> 00:13:18.650
Certain

00:13:18.650 --> 00:13:20.360
moral principles

00:13:20.360 --> 00:13:23.340
have already begun to emerge

00:13:23.340 --> 00:13:25.880
from the discussions we've had

00:13:25.880 --> 00:13:27.570
and let's consider

00:13:27.570 --> 00:13:29.740
what those moral principles

00:13:29.740 --> 00:13:31.250
look like

00:13:31.250 --> 00:13:35.710
the first moral principle that emerged from the 
discussion said

00:13:35.710 --> 00:13:39.110
that the right thing to do the moral thing to do

00:13:39.110 --> 00:13:43.140
depends on the consequences that will result

00:13:43.140 --> 00:13:45.480
from your action

00:13:45.480 --> 00:13:47.080
at the end of the day

00:13:47.080 --> 00:13:49.250
better that five should live

00:13:49.250 --> 00:13:52.320
even if one must die.

00:13:52.320 --> 00:13:53.700
That's an example

00:13:53.700 --> 00:13:56.280
of consequentialist

00:13:56.280 --> 00:13:59.460
moral reasoning.

00:13:59.460 --> 00:14:04.310
consequentialist moral reasoning locates morality
in the consequences of an act. In the state of the 

00:14:04.310 --> 00:14:06.550
world that will result 

00:14:06.550 --> 00:14:09.050
from the thing you do

00:14:09.050 --> 00:14:12.790
but then we went a little further, we considered
those other cases

00:14:12.790 --> 00:14:15.320
and people weren't so sure 

00:14:15.320 --> 00:14:17.300
about

00:14:17.300 --> 00:14:20.530
consequentialist moral reasoning

00:14:20.530 --> 00:14:22.420
when people hesitated

00:14:22.420 --> 00:14:24.270
to push the fat man

00:14:24.270 --> 00:14:25.790
over the bridge

00:14:25.790 --> 00:14:28.640
or to yank out the organs of the innocent

00:14:28.640 --> 00:14:29.750
patient

00:14:29.750 --> 00:14:32.260
people gestured towards

00:14:32.260 --> 00:14:34.260
reasons

00:14:34.260 --> 00:14:35.380
having to do

00:14:35.380 --> 00:14:37.250
with the intrinsic

00:14:37.250 --> 00:14:39.230
quality of the act

00:14:39.230 --> 00:14:40.690
itself.

00:14:40.690 --> 00:14:42.770
Consequences be what they may.

00:14:42.770 --> 00:14:45.180
People were reluctant

00:14:45.180 --> 00:14:47.820
people thought it was just wrong

00:14:47.820 --> 00:14:49.480
categorically wrong

00:14:49.480 --> 00:14:50.500
to kill

00:14:50.500 --> 00:14:51.409
a person

00:14:51.409 --> 00:14:53.510
an innocent person

00:14:53.510 --> 00:14:54.690
even for the sake

00:14:54.690 --> 00:14:55.819
of saving

00:14:55.819 --> 00:14:58.500
five lives, at least these people thought that

00:14:58.500 --> 00:15:00.610
in the second

00:15:00.610 --> 00:15:05.120
version of each story we reconsidered

00:15:05.120 --> 00:15:06.520
so this points

00:15:06.520 --> 00:15:09.540
a second

00:15:09.540 --> 00:15:10.690
categorical

00:15:10.690 --> 00:15:12.660
way

00:15:12.660 --> 00:15:14.680
of thinking about

00:15:14.680 --> 00:15:16.430
moral reasoning

00:15:16.430 --> 00:15:22.430
categorical moral reasoning locates morality
in certain absolute moral requirements in

00:15:22.430 --> 00:15:24.440
certain categorical duties and rights

00:15:24.440 --> 00:15:27.440
regardless of the consequences.

00:15:27.440 --> 00:15:29.230
We're going to explore

00:15:29.230 --> 00:15:33.070
in the days and weeks to come the contrast
between

00:15:33.070 --> 00:15:36.540
consequentialist and categorical moral principles.

00:15:36.540 --> 00:15:38.370
The most influential

00:15:38.370 --> 00:15:40.420
example of

00:15:40.420 --> 00:15:45.920
consequential moral reasoning is utilitarianism,
a doctrine invented by

00:15:45.920 --> 00:15:51.100
Jeremy Bentham, the eighteenth century English
political philosopher.

00:15:51.100 --> 00:15:54.140
The most important

00:15:54.140 --> 00:15:56.850
philosopher of categorical moral reasoning

00:15:56.850 --> 00:15:58.170
is the

00:15:58.170 --> 00:16:02.590
eighteenth century German philosopher
Emmanuel Kant.

00:16:02.590 --> 00:16:03.860
So we will look

00:16:03.860 --> 00:16:07.260
at those two different modes of moral reasoning

00:16:07.260 --> 00:16:08.300
assess them

00:16:08.300 --> 00:16:10.670
and also consider others.

00:16:10.670 --> 00:16:16.100
If you look at the syllabus, you'll notice
that we read a number of great and famous books.

00:16:16.100 --> 00:16:18.310
Books by Aristotle

00:16:18.310 --> 00:16:19.890
John Locke

00:16:19.890 --> 00:16:22.080
Emanuel Kant, John Stuart Mill,

00:16:22.080 --> 00:16:24.030
and others.

00:16:24.030 --> 00:16:28.170
You'll notice too from the syllabus that
we don't only read these books,

00:16:28.170 --> 00:16:30.070
we also all

00:16:30.070 --> 00:16:32.010
take up

00:16:32.010 --> 00:16:37.000
contemporary political and legal controversies
that raise philosophical questions.

00:16:37.000 --> 00:16:40.190
We will debate equality and inequality,

00:16:40.190 --> 00:16:41.490
affirmative action,

00:16:41.490 --> 00:16:43.699
free speech versus hate speech,

00:16:43.699 --> 00:16:47.040
same sex marriage, military conscription,

00:16:47.040 --> 00:16:50.980
a range of practical questions, why

00:16:50.980 --> 00:16:55.350
not just to enliven these abstract and distant
books

00:16:55.350 --> 00:17:01.040
but to make clear to bring out what's at stake
in our everyday lives including our political

00:17:01.040 --> 00:17:03.520
lives,

00:17:03.520 --> 00:17:05.640
for philosophy.

00:17:05.640 --> 00:17:07.720
So we will read these books

00:17:07.720 --> 00:17:09.819
and we will debate these

00:17:09.819 --> 00:17:15.480
issues and we'll see how each informs and
illuminates the other.

00:17:15.480 --> 00:17:17.600
This may sound appealing enough

00:17:17.600 --> 00:17:19.110
but here

00:17:19.110 --> 00:17:22.550
I have to issue a warning,

00:17:22.550 --> 00:17:25.210
and the warning is this

00:17:25.210 --> 00:17:28.500
to read these books

00:17:28.500 --> 00:17:31.550
in this way,

00:17:31.550 --> 00:17:34.120
as an exercise in self-knowledge,

00:17:34.120 --> 00:17:38.850
to read them in this way carry certain risks

00:17:38.850 --> 00:17:42.120
risks that are both personal and political,

00:17:42.120 --> 00:17:47.800
risks that every student of political philosophy have known.

00:17:47.800 --> 00:17:50.690
These risks spring from that fact

00:17:50.690 --> 00:17:52.580
that philosophy

00:17:52.580 --> 00:17:54.220
teaches us

00:17:54.220 --> 00:17:56.400
and unsettles us

00:17:56.400 --> 00:18:01.390
by confronting us with what we already know.

00:18:01.390 --> 00:18:03.390
There's an irony

00:18:03.390 --> 00:18:09.790
the difficulty of this course consists in the
fact that it teaches what you already know.

00:18:09.790 --> 00:18:12.160
It works by taking

00:18:12.160 --> 00:18:16.470
what we know from familiar unquestioned settings,

00:18:16.470 --> 00:18:20.370
and making it strange.

00:18:20.370 --> 00:18:22.390
That's how those examples worked

00:18:22.390 --> 00:18:23.120
worked

00:18:23.120 --> 00:18:29.030
the hypotheticals with which we began with their
mix of playfulness and sobriety.

00:18:29.030 --> 00:18:33.940
it's also how these philosophical books work. Philosophy 

00:18:33.940 --> 00:18:35.640
estranges us

00:18:35.640 --> 00:18:37.520
from the familiar

00:18:37.520 --> 00:18:40.390
not by supplying new information

00:18:40.390 --> 00:18:41.780
but by inviting

00:18:41.780 --> 00:18:43.600
and provoking

00:18:43.600 --> 00:18:47.420
a new way of seeing

00:18:47.420 --> 00:18:49.970
but, and here's the risk,

00:18:49.970 --> 00:18:50.720
once

00:18:50.720 --> 00:18:54.300
the familiar turns strange,

00:18:54.300 --> 00:18:58.260
it's never quite the same again.

00:18:58.260 --> 00:19:00.210
Self-knowledge

00:19:00.210 --> 00:19:03.200
is like lost innocence,

00:19:03.200 --> 00:19:04.720
however unsettling

00:19:04.720 --> 00:19:06.030
you find it,

00:19:06.030 --> 00:19:07.290
it can never

00:19:07.290 --> 00:19:09.720
be unthought

00:19:09.720 --> 00:19:13.390
or unknown

00:19:13.390 --> 00:19:17.260
what makes this enterprise difficult

00:19:17.260 --> 00:19:19.970
but also riveting,

00:19:19.970 --> 00:19:20.880
is that

00:19:20.880 --> 00:19:25.400
moral and political philosophy is a story

00:19:25.400 --> 00:19:29.340
and you don't know where this story will lead
but what you do know

00:19:29.340 --> 00:19:31.320
is that the story

00:19:31.320 --> 00:19:34.480
is about you.

00:19:34.480 --> 00:19:37.080
Those are the personal risks,

00:19:37.080 --> 00:19:40.220
now what of the political risks.

00:19:40.220 --> 00:19:43.039
one way of introducing of course like this

00:19:43.039 --> 00:19:44.730
would be to promise you

00:19:44.730 --> 00:19:46.460
that by reading these books

00:19:46.460 --> 00:19:48.070
and debating these issues

00:19:48.070 --> 00:19:51.770
you will become a better more responsible
citizen.

00:19:51.770 --> 00:19:56.450
You will examine the presuppositions of
public policy, you will hone your political

00:19:56.450 --> 00:19:57.290
judgment

00:19:57.290 --> 00:20:02.820
you'll become a more effective participant
in public affairs

00:20:02.820 --> 00:20:06.870
but this would be a partial and misleading promise

00:20:06.870 --> 00:20:11.490
political philosophy for the most part hasn't
worked that way.

00:20:11.490 --> 00:20:14.660
You have to allow for the possibility

00:20:14.660 --> 00:20:19.100
that political philosophy may make you a worse
citizen

00:20:19.100 --> 00:20:21.970
rather than a better one

00:20:21.970 --> 00:20:23.810
or at least a worse citizen

00:20:23.810 --> 00:20:25.630
before it makes you

00:20:25.630 --> 00:20:27.950
a better one

00:20:27.950 --> 00:20:30.400
and that's because philosophy

00:20:30.400 --> 00:20:32.620
is a distancing

00:20:32.620 --> 00:20:34.710
even debilitating

00:20:34.710 --> 00:20:36.600
activity

00:20:36.600 --> 00:20:37.760
And you see this

00:20:37.760 --> 00:20:39.890
going back to Socrates

00:20:39.890 --> 00:20:42.290
there's a dialogue, the Gorgias

00:20:42.290 --> 00:20:44.680
in which one of Socratesâ€™ friends

00:20:44.680 --> 00:20:45.620
Calicles

00:20:45.620 --> 00:20:47.240
tries to talk him out

00:20:47.240 --> 00:20:49.970
of philosophizing.

00:20:49.970 --> 00:20:54.400
calicles tells Socrates philosophy is a pretty toy

00:20:54.400 --> 00:20:57.980
if one indulges in it with moderation at
the right time of life

00:20:57.980 --> 00:21:03.780
but if one pursues it further than one should
it is absolute ruin.

00:21:03.780 --> 00:21:06.820
Take my advice calicles says,

00:21:06.820 --> 00:21:08.370
abandon argument

00:21:08.370 --> 00:21:11.670
learn the accomplishments of active
life, take

00:21:11.670 --> 00:21:16.880
for your models not those people who spend
their time on these petty quibbles,

00:21:16.880 --> 00:21:20.080
but those who have a good livelihood and reputation

00:21:20.080 --> 00:21:22.320
and many other blessings.

00:21:22.320 --> 00:21:26.809
So Calicles is really saying to Socrates

00:21:26.809 --> 00:21:28.690
quit philosophizing,

00:21:28.690 --> 00:21:30.550
get real

00:21:30.550 --> 00:21:35.170
go to business school

00:21:35.170 --> 00:21:38.300
and calicles did have a point

00:21:38.300 --> 00:21:39.690
he had a point

00:21:39.690 --> 00:21:42.210
because philosophy distances us

00:21:42.210 --> 00:21:45.010
from conventions from established assumptions

00:21:45.010 --> 00:21:46.750
and from settled beliefs.

00:21:46.750 --> 00:21:48.650
those are the risks,

00:21:48.650 --> 00:21:49.970
personal and political

00:21:49.970 --> 00:21:54.090
and in the face of these risks there is a
characteristic evasion,

00:21:54.090 --> 00:21:57.470
the name of the evasion is skepticism. It's
the idea

00:21:57.470 --> 00:21:58.990
well it goes something like this

00:21:58.990 --> 00:22:03.660
we didn't resolve, once and for all,

00:22:03.660 --> 00:22:09.760
either the cases or the principles we were
arguing when we began

00:22:09.760 --> 00:22:11.390
and if Aristotle

00:22:11.390 --> 00:22:17.220
and Locke and Kant and Mill haven't solved these questions
after all of these years

00:22:17.220 --> 00:22:19.590
who are we to think

00:22:19.590 --> 00:22:23.730
that we here in Sanders Theatre over the
course a semester

00:22:23.730 --> 00:22:26.430
can resolve them

00:22:26.430 --> 00:22:29.460
and so maybe it's just a matter of

00:22:29.460 --> 00:22:33.520
each person having his or her own principles
and there's nothing more to be said about

00:22:33.520 --> 00:22:34.170
it

00:22:34.170 --> 00:22:36.590
no way of reasoning

00:22:36.590 --> 00:22:37.650
that's the

00:22:37.650 --> 00:22:39.440
evasion. The evasion of skepticism 

00:22:39.440 --> 00:22:41.280
to which I would offer the following

00:22:41.280 --> 00:22:42.660
reply:

00:22:42.660 --> 00:22:43.720
it's true

00:22:43.720 --> 00:22:47.560
these questions have been debated for a very
long time

00:22:47.560 --> 00:22:49.060
but the very fact

00:22:49.060 --> 00:22:52.700
that they have reoccurred and persisted

00:22:52.700 --> 00:22:54.650
may suggest

00:22:54.650 --> 00:22:57.169
that though they're impossible in one sense

00:22:57.169 --> 00:22:59.800
their unavoidable in another

00:22:59.800 --> 00:23:02.300
and the reason they're unavoidable

00:23:02.300 --> 00:23:06.500
the reason they're inescapable is that we live
some answer

00:23:06.500 --> 00:23:10.070
to these questions every day.

00:23:10.070 --> 00:23:16.130
So skepticism, just throwing up their hands
and giving up on moral reflection,

00:23:16.130 --> 00:23:18.280
is no solution

00:23:18.280 --> 00:23:19.990
Emanuel Kant

00:23:19.990 --> 00:23:23.710
described very well the problem with skepticism
when he wrote

00:23:23.710 --> 00:23:26.309
skepticism is a resting place for human reason 

00:23:26.309 --> 00:23:29.100
where it can reflect upon its dogmatic wanderings

00:23:29.100 --> 00:23:33.070
but it is no dwelling place for permanent settlement.

00:23:33.070 --> 00:23:35.940
Simply to acquiesce in skepticism, Kant wrote,

00:23:35.940 --> 00:23:42.660
can never suffice to overcome the restless
of reason.

00:23:42.660 --> 00:23:47.040
I've tried to suggest through theses stories
and these arguments

00:23:47.040 --> 00:23:49.899
some sense of the risks and temptations

00:23:49.899 --> 00:23:55.590
of the perils and the possibilities I would
simply conclude by saying

00:23:55.590 --> 00:23:58.100
that the aim of this course

00:23:58.100 --> 00:23:59.630
is to awaken

00:23:59.630 --> 00:24:02.240
the restlessness of reason

00:24:02.240 --> 00:24:04.380
and to see where it might lead

00:24:04.380 --> 00:24:11.380
thank you very much.

00:24:15.440 --> 00:24:16.980
Like, in a situation that desperate,

00:24:16.980 --> 00:24:21.260
you have to do what you have to do to survive.
You have to do what you have to do you? You've gotta do

00:24:21.260 --> 00:24:22.780
What you 

00:24:22.780 --> 00:24:23.620
gotta do. pretty much, 

00:24:23.620 --> 00:24:25.700
If you've been going nineteen days without any food

00:24:25.700 --> 00:24:32.700
someone has to take the sacrifice, someone has to make the sacrifice  
and people can survive. Alright that's good, what's your name? Marcus.

00:24:33.570 --> 00:24:40.350
Marcus, what do you say to Marcus?

00:24:40.350 --> 00:24:44.570
Last time

00:24:44.570 --> 00:24:46.960
we started out last time

00:24:46.960 --> 00:24:48.980
with some stores

00:24:48.980 --> 00:24:51.020
with some moral dilemmas

00:24:51.020 --> 00:24:53.050
about trolley cars

00:24:53.050 --> 00:24:54.510
and about doctors

00:24:54.510 --> 00:24:56.270
and healthy patients

00:24:56.270 --> 00:24:57.560
vulnerable

00:24:57.560 --> 00:25:00.910
to being victims of organ transplantation

00:25:00.910 --> 00:25:04.090
we noticed two things

00:25:04.090 --> 00:25:06.890
about the arguments we had

00:25:06.890 --> 00:25:10.830
one had to do with the way we were arguing

00:25:10.830 --> 00:25:13.570
it began with our judgments in particular cases

00:25:13.570 --> 00:25:18.470
we tried to articulate the reasons or the
principles

00:25:18.470 --> 00:25:22.550
lying behind our judgments

00:25:22.550 --> 00:25:25.400
and then confronted with a new case

00:25:25.400 --> 00:25:30.760
we found ourselves re-examining those principles

00:25:30.760 --> 00:25:34.020
revising each in the light of the other

00:25:34.020 --> 00:25:38.920
and we noticed the built-in pressure to try
to bring into alignment

00:25:38.920 --> 00:25:41.680
our judgments about particular cases

00:25:41.680 --> 00:25:43.940
and the principles we would endorse

00:25:43.940 --> 00:25:46.360
on reflection

00:25:46.360 --> 00:25:50.590
we also noticed something about the substance
of the arguments

00:25:50.590 --> 00:25:55.250
that emerged from the discussion.

00:25:55.250 --> 00:26:00.860
We noticed that sometimes we were tempted
to locate the morality of an act in the consequences

00:26:00.860 --> 00:26:06.520
in the results, in the state of the world that
it brought about.

00:26:06.520 --> 00:26:09.020
We called is consequentialist

00:26:09.020 --> 00:26:11.690
moral reason.

00:26:11.690 --> 00:26:13.240
But we also noticed that

00:26:13.240 --> 00:26:16.440
in some cases

00:26:16.440 --> 00:26:18.610
we weren't swayed only 

00:26:18.610 --> 00:26:22.090
by the results

00:26:22.090 --> 00:26:23.400
sometimes,

00:26:23.400 --> 00:26:25.350
many of us felt,

00:26:25.350 --> 00:26:31.670
that not just consequences but also the intrinsic
quality or character of the act

00:26:31.670 --> 00:26:35.370
matters morally.

00:26:35.370 --> 00:26:40.980
Some people argued that there are certain things
that are just categorically wrong

00:26:40.980 --> 00:26:42.580
even if they bring about

00:26:42.580 --> 00:26:44.460
a good result

00:26:44.460 --> 00:26:45.490
even

00:26:45.490 --> 00:26:47.200
if they save five people

00:26:47.200 --> 00:26:49.789
at the cost of one life.

00:26:49.789 --> 00:26:52.730
So we contrasted consequentialist

00:26:52.730 --> 00:26:54.570
moral principles

00:26:54.570 --> 00:26:58.140
with categorical ones.

00:26:58.140 --> 00:26:59.790
Today

00:26:59.790 --> 00:27:00.820
and in the next few days

00:27:00.820 --> 00:27:06.570
we will begin to examine one of the
most influential

00:27:06.570 --> 00:27:08.840
versions of consequentialist

00:27:08.840 --> 00:27:10.970
moral theory

00:27:10.970 --> 00:27:16.330
and that's the philosophy of utilitarianism.

00:27:16.330 --> 00:27:17.430
Jeremy Bentham,

00:27:17.430 --> 00:27:19.080
the eighteenth century

00:27:19.080 --> 00:27:21.690
English political philosopher

00:27:21.690 --> 00:27:22.910
gave first

00:27:22.910 --> 00:27:26.630
the first clear systematic expression

00:27:26.630 --> 00:27:28.670
to the utilitarian

00:27:28.670 --> 00:27:32.210
moral theory.

00:27:32.210 --> 00:27:36.400
And Bentham's idea,

00:27:36.400 --> 00:27:38.440
his essential idea

00:27:38.440 --> 00:27:42.930
is a very simple one

00:27:42.930 --> 00:27:44.809
with a lot of 

00:27:44.809 --> 00:27:46.330
morally

00:27:46.330 --> 00:27:48.430
intuitive appeal.

00:27:48.430 --> 00:27:50.450
Bentham's idea is

00:27:50.450 --> 00:27:51.590
the following

00:27:51.590 --> 00:27:54.440
the right thing to do

00:27:54.440 --> 00:27:57.800
the just thing to do

00:27:57.800 --> 00:27:58.820
it's to

00:27:58.820 --> 00:28:01.370
maximize

00:28:01.370 --> 00:28:02.340
utility.

00:28:02.340 --> 00:28:06.330
What did he mean by utility?

00:28:06.330 --> 00:28:11.490
He meant by utility the balance

00:28:11.490 --> 00:28:14.000
of pleasure over pain,

00:28:14.000 --> 00:28:16.779
happiness over suffering.

00:28:16.779 --> 00:28:18.210
Here's how we arrived 

00:28:18.210 --> 00:28:19.259
at the principle

00:28:19.259 --> 00:28:22.309
of maximizing utility.

00:28:22.309 --> 00:28:24.450
He started out by observing

00:28:24.450 --> 00:28:26.399
that all of us

00:28:26.399 --> 00:28:27.750
all human beings

00:28:27.750 --> 00:28:31.490
are governed by two sovereign masters,

00:28:31.490 --> 00:28:34.620
pain and pleasure.

00:28:34.620 --> 00:28:37.480
We human beings

00:28:37.480 --> 00:28:42.309
like pleasure and dislike pain

00:28:42.309 --> 00:28:45.840
and so we should base morality

00:28:45.840 --> 00:28:49.170
whether we are thinking of what to do in our own lives

00:28:49.170 --> 00:28:50.490
or whether

00:28:50.490 --> 00:28:52.580
as legislators or citizens

00:28:52.580 --> 00:28:57.030
we are thinking about what the law should be,

00:28:57.030 --> 00:29:02.070
the right thing to do individually or collectively

00:29:02.070 --> 00:29:05.839
is to maximize, act in a way that maximizes

00:29:05.839 --> 00:29:07.660
the overall level

00:29:07.660 --> 00:29:11.520
of happiness.

00:29:11.520 --> 00:29:15.430
Bentham's utilitarianism is sometimes summed
up with the slogan

00:29:15.430 --> 00:29:18.870
the greatest good for the greatest number.

00:29:18.870 --> 00:29:20.399
With this

00:29:20.399 --> 00:29:22.990
basic principle of utility on hand,

00:29:22.990 --> 00:29:26.410
let's begin to test it and to examine it

00:29:26.410 --> 00:29:28.409
by turning to another case

00:29:28.409 --> 00:29:30.620
another story but this time

00:29:30.620 --> 00:29:32.640
not a hypothetical story,

00:29:32.640 --> 00:29:34.120
a real-life story

00:29:34.120 --> 00:29:35.060
the case of

00:29:35.060 --> 00:29:38.330
the Queen versus Dudley and Stephens.

00:29:38.330 --> 00:29:41.890
This was a nineteenth-century British law case

00:29:41.890 --> 00:29:44.010
that's famous

00:29:44.010 --> 00:29:47.539
and much debated in law schools.

00:29:47.539 --> 00:29:50.060
Here's what happened in the case

00:29:50.060 --> 00:29:51.870
I'll summarize the story

00:29:51.870 --> 00:29:54.910
and then I want to hear

00:29:54.910 --> 00:29:57.640
how you would rule

00:29:57.640 --> 00:30:04.350
imagining that you are the jury.

00:30:04.350 --> 00:30:06.179
A newspaper account of the time

00:30:06.179 --> 00:30:09.290
described the background:

00:30:09.290 --> 00:30:11.460
A sadder story of disaster at sea

00:30:11.460 --> 00:30:12.790
was never told

00:30:12.790 --> 00:30:15.289
than that of the survivors of the yacht

00:30:15.289 --> 00:30:16.240
Mignonette.

00:30:16.240 --> 00:30:19.110
The ship foundered in the south Atlantic

00:30:19.110 --> 00:30:21.790
thirteen hundred miles from the cape

00:30:21.790 --> 00:30:24.000
there were four in the crew,

00:30:24.000 --> 00:30:26.500
Dudley was the captain

00:30:26.500 --> 00:30:28.400
Stephens was the first mate

00:30:28.400 --> 00:30:30.220
Brooks was a sailor,

00:30:30.220 --> 00:30:31.280
all men of

00:30:31.280 --> 00:30:32.490
excellent character,

00:30:32.490 --> 00:30:34.249
or so the newspaper account

00:30:34.249 --> 00:30:35.820
tells us.

00:30:35.820 --> 00:30:38.640
The fourth crew member was the cabin boy,

00:30:38.640 --> 00:30:40.300
Richard Parker

00:30:40.300 --> 00:30:42.850
seventeen years old.

00:30:42.850 --> 00:30:44.630
He was an orphan

00:30:44.630 --> 00:30:46.930
he had no family

00:30:46.930 --> 00:30:51.410
and he was on his first long voyage at sea.

00:30:51.410 --> 00:30:53.700
He went, the news account tells us,

00:30:53.700 --> 00:30:56.730
rather against the advice of his friends.

00:30:56.730 --> 00:31:00.210
He went in the hopefulness of youthful ambition

00:31:00.210 --> 00:31:03.390
thinking the journey would make a man of him.

00:31:03.390 --> 00:31:05.140
Sadly it was not to be,

00:31:05.140 --> 00:31:07.780
the facts of the case were not in dispute,

00:31:07.780 --> 00:31:08.970
a wave hit the ship

00:31:08.970 --> 00:31:12.000
and the Mignonette went down.

00:31:12.000 --> 00:31:14.860
The four crew members escaped to a lifeboat

00:31:14.860 --> 00:31:16.200
the only

00:31:16.200 --> 00:31:18.380
food they had

00:31:18.380 --> 00:31:19.659
were two

00:31:19.659 --> 00:31:20.990
cans of preserved

00:31:20.990 --> 00:31:21.780
turnips

00:31:21.780 --> 00:31:23.980
no fresh water

00:31:23.980 --> 00:31:26.610
for the first three days they ate nothing

00:31:26.610 --> 00:31:30.460
on the fourth day that opened one of the cans of
turnips

00:31:30.460 --> 00:31:31.590
and ate it.

00:31:31.590 --> 00:31:34.450
The next day they caught a turtle

00:31:34.450 --> 00:31:36.960
together with the other can of turnips 

00:31:36.960 --> 00:31:38.550
the turtle

00:31:38.550 --> 00:31:40.059
enabled them to subsist

00:31:40.059 --> 00:31:43.070
for the next few days and then for eight days

00:31:43.070 --> 00:31:44.039
they had nothing

00:31:44.039 --> 00:31:47.070
no food no water.

00:31:47.070 --> 00:31:50.070
Imagine yourself in a situation like that

00:31:50.070 --> 00:31:52.850
what would you do?

00:31:52.850 --> 00:31:55.120
Here's what they did

00:31:55.120 --> 00:32:00.970
by now the cabin boy Parker is lying at the
bottom of the lifeboat in a corner

00:32:00.970 --> 00:32:03.230
because he had drunk sea water

00:32:03.230 --> 00:32:05.490
against the advice of the others

00:32:05.490 --> 00:32:07.230
and he had become ill

00:32:07.230 --> 00:32:10.669
and he appeared to be dying

00:32:10.669 --> 00:32:14.619
so on the nineteenth day Dudley, the captain, suggested

00:32:14.619 --> 00:32:17.350
that they should all

00:32:17.350 --> 00:32:18.760
have a lottery. That they should

00:32:18.760 --> 00:32:19.620
all draw lots to see

00:32:19.620 --> 00:32:20.860
who would die

00:32:20.860 --> 00:32:24.080
to save the rest.

00:32:24.080 --> 00:32:25.210
Brooks

00:32:25.210 --> 00:32:26.540
refused

00:32:26.540 --> 00:32:29.140
he didn't like the lottery idea

00:32:29.140 --> 00:32:30.619
we don't know whether this

00:32:30.619 --> 00:32:35.999
was because he didn't want to take that chance
or because he believed in categorical moral

00:32:35.999 --> 00:32:36.870
principles

00:32:36.870 --> 00:32:38.620
but in any case

00:32:38.620 --> 00:32:42.130
no lots were drawn.

00:32:42.130 --> 00:32:43.230
The next day

00:32:43.230 --> 00:32:45.029
there was still no ship in sight

00:32:45.029 --> 00:32:48.470
so a Dudley told Brooks to avert his gaze 

00:32:48.470 --> 00:32:50.720
and he motioned to Stephens

00:32:50.720 --> 00:32:53.929
that the boy Parker had better be killed.

00:32:53.929 --> 00:32:55.850
Dudley offered a prayer

00:32:55.850 --> 00:32:58.480
he told a the boy his time had come

00:32:58.480 --> 00:33:00.679
and he killed him with a pen knife

00:33:00.679 --> 00:33:03.900
stabbing him in the jugular vein.

00:33:03.900 --> 00:33:09.750
Brooks emerged from his conscientious objection
to share in the gruesome bounty.

00:33:09.750 --> 00:33:11.030
For four days

00:33:11.030 --> 00:33:15.230
the three of them fed on the body and blood
of the cabin boy.

00:33:15.230 --> 00:33:17.220
True story.

00:33:17.220 --> 00:33:19.390
And then they were rescued.

00:33:19.390 --> 00:33:22.840
Dudley describes their rescue

00:33:22.840 --> 00:33:24.679
in his diary

00:33:24.679 --> 00:33:27.570
with staggering euphemism, quote:

00:33:27.570 --> 00:33:29.649
"on the twenty fourth day

00:33:29.649 --> 00:33:34.750
as we were having our breakfast

00:33:34.750 --> 00:33:38.600
a ship appeared at last."

00:33:38.600 --> 00:33:44.310
The three survivors were picked up by a German ship.
They were taken back to Falmouth in England

00:33:44.310 --> 00:33:47.080
where they were arrested and tried

00:33:47.080 --> 00:33:47.830
Brooks

00:33:47.830 --> 00:33:49.950
turned state's witness

00:33:49.950 --> 00:33:54.450
Dudley and Stephens went to trial. They didn't
dispute the facts

00:33:54.450 --> 00:33:55.390
they claimed

00:33:55.390 --> 00:33:58.140
they had acted out of necessity

00:33:58.140 --> 00:33:59.430
that was their defense

00:33:59.430 --> 00:34:01.220
they argued in effect

00:34:01.220 --> 00:34:03.250
better that one should die

00:34:03.250 --> 00:34:06.420
so that three could survive

00:34:06.420 --> 00:34:08.619
the prosecutor

00:34:08.619 --> 00:34:10.849
wasn't swayed by that argument

00:34:10.849 --> 00:34:12.509
he said murder is murder

00:34:12.509 --> 00:34:16.429
and so the case went to trial. Now imagine
you are the jury

00:34:16.429 --> 00:34:19.489
and just to simplify the discussion

00:34:19.489 --> 00:34:21.989
put aside the question of law,

00:34:21.989 --> 00:34:23.010
and let's assume that

00:34:23.010 --> 00:34:25.879
you as the jury

00:34:25.879 --> 00:34:28.279
are charged with deciding

00:34:28.279 --> 00:34:31.009
whether what they did was morally

00:34:31.009 --> 00:34:34.379
permissible or not.

00:34:34.379 --> 00:34:36.609
How many

00:34:36.609 --> 00:34:39.809
would vote

00:34:39.809 --> 00:34:46.809
not guilty, that what they did was morally
permissible?

00:34:49.529 --> 00:34:51.640
And how many would vote guilty

00:34:51.640 --> 00:34:54.859
what they did was morally wrong?

00:34:54.859 --> 00:34:57.999
A pretty sizable majority.

00:34:57.999 --> 00:35:03.809
Now let's see what people's reasons are, and let me
begin with those who are in the minority.

00:35:03.809 --> 00:35:07.739
Let's hear first from the defense

00:35:07.739 --> 00:35:10.099
of Dudley and Stephens.

00:35:10.099 --> 00:35:14.160
Why would you morally exonerate them?

00:35:14.160 --> 00:35:17.989
What are your reasons?

00:35:17.989 --> 00:35:20.799
I think it's I think it is morally reprehensible

00:35:20.799 --> 00:35:24.349
but I think that there's a distinction between
what's morally reprehensible

00:35:24.349 --> 00:35:26.609
what makes someone legally accountable

00:35:26.609 --> 00:35:30.690
in other words the night as the judge said
what's  always moral isn't necessarily

00:35:30.690 --> 00:35:34.789
against the law and while I don't think that
necessity

00:35:34.789 --> 00:35:36.169
justifies

00:35:36.169 --> 00:35:38.579
theft or murder any illegal act, 

00:35:38.579 --> 00:35:43.509
at some point your degree of necessity does
in fact

00:35:43.509 --> 00:35:45.849
exonerate you form any guilt. ok.

00:35:45.849 --> 00:35:50.589
other defenders, other voices for the defense?

00:35:50.589 --> 00:35:53.039
Moral justifications for

00:35:53.039 --> 00:35:56.989
what they did?

00:35:56.989 --> 00:35:57.619
yes, thank you

00:35:58.799 --> 00:35:59.570
I just feel like 

00:35:59.570 --> 00:36:03.139
in a situation that desperate you have to do
what you have to do to survive.

00:36:03.139 --> 00:36:04.679
You have to do what you have to do

00:36:04.679 --> 00:36:06.789
ya, you gotta do what you gotta do, pretty much.

00:36:06.789 --> 00:36:07.849
If you've been

00:36:07.849 --> 00:36:09.899
going nineteen days without any food

00:36:09.899 --> 00:36:14.710
you know someone just has to take the sacrifice
has to make sacrifices and people can survive

00:36:14.710 --> 00:36:16.140
and furthermore from that

00:36:16.140 --> 00:36:21.300
let's say they survived and then they become productive
members of society who go home and then start like

00:36:21.300 --> 00:36:26.230
a million charity organizations and this and that and this and that,
I mean they benefit everybody in the end so

00:36:26.230 --> 00:36:28.520
I mean I don't know what they did afterwards, I mean
they might have

00:36:28.520 --> 00:36:30.479
gone on and killed more people

00:36:30.479 --> 00:36:32.890
but whatever.

00:36:32.890 --> 00:36:35.709
what? what if they were going home and turned out to be assassins?

00:36:35.709 --> 00:36:38.909
What if they were going home and turned out to be assassins?

00:36:38.909 --> 00:36:42.879
You would want to know who they assassinated.

00:36:42.879 --> 00:36:45.849
That's true too, that's fair

00:36:45.849 --> 00:36:49.609
I would wanna know who they assassinated.

00:36:49.609 --> 00:36:50.709
alright that's good, what's your name? Marcus.

00:36:50.709 --> 00:36:52.489
We've heard a defense

00:36:52.489 --> 00:36:54.050
a couple voices for the defense

00:36:54.050 --> 00:36:55.660
now we need to hear

00:36:55.660 --> 00:36:57.299
from the prosecution

00:36:57.299 --> 00:36:59.339
most people think

00:36:59.339 --> 00:37:05.069
what they did was wrong, why?

00:37:05.069 --> 00:37:09.899
One of the first things that I was thinking was, oh well if they  
haven't been eating for a really long time, 

00:37:09.899 --> 00:37:11.410
maybe

00:37:11.410 --> 00:37:12.469
then

00:37:12.469 --> 00:37:15.140
they're mentally affected

00:37:15.140 --> 00:37:16.409
that could be used for the defense, 

00:37:16.409 --> 00:37:20.619
a possible argument that oh,

00:37:20.619 --> 00:37:24.179
that they weren't in a proper state of mind, they were making

00:37:24.179 --> 00:37:28.519
decisions that they otherwise wouldn't be making, and if that's an  
appealing argument

00:37:28.519 --> 00:37:33.609
that you have to be in an altered mindset to do something
like that it suggests that

00:37:33.609 --> 00:37:36.109
people who find that argument convincing

00:37:36.109 --> 00:37:40.089
do you think that they're acting immorally.
But I want to know what you think you're defending

00:37:40.089 --> 00:37:41.249
you k
0:37:41.249,0:37:45.549
you voted to convict right? yeah
 I don't think that they acted in morally 

00:37:45.549 --> 00:37:49.449
appropriate way. And why not? What do you say,
Here's Marcus

00:37:49.449 --> 00:37:51.089
he just defended them,

00:37:51.089 --> 00:37:52.909
he said,

00:37:52.909 --> 00:37:53.880
you heard what he said,

00:37:53.880 --> 00:37:55.249
yes I did

00:37:55.249 --> 00:37:56.559
yes

00:37:56.559 --> 00:38:00.119
that you've got to do what you've got to do in a
case like that.

00:38:00.119 --> 00:38:04.789
What do you say to Marcus?

00:38:04.789 --> 00:38:06.439
They didn't,

00:38:06.439 --> 00:38:13.439
that there is no situation that would allow human
beings to take 

00:38:13.579 --> 00:38:17.959
the idea of fate or the other people's
lives into their own hands that we don't have

00:38:17.959 --> 00:38:19.329
that kind of power.

00:38:19.329 --> 00:38:21.399
Good, okay

00:38:21.399 --> 00:38:24.130
thanks you, and what's your name?

00:38:24.130 --> 00:38:24.549
Britt? okay.

00:38:24.549 --> 00:38:26.029
who else?

00:38:26.029 --> 00:38:28.160
What do you say? Stand up

00:38:28.160 --> 00:38:35.160
I'm wondering if Dudley and Stephens had asked for Richard Parker's  
consent in, you know, dying, 

00:38:35.430 --> 00:38:37.569
if that would

00:38:37.569 --> 00:38:41.229
would that exonerate them

00:38:41.229 --> 00:38:45.449
from an act of murder, and if so is that still morally
justifiable?

00:38:45.449 --> 00:38:51.720
That's interesting, alright consent, now hang on, what's your name?
Kathleen.

00:38:51.720 --> 00:38:56.089
Kathleen says suppose so what would that scenario look like?

00:38:56.089 --> 00:38:56.619
so in the story

00:38:56.619 --> 00:39:00.410
Dudley is there, pen knife in hand,

00:39:00.410 --> 00:39:02.609
but instead of the prayer

00:39:02.609 --> 00:39:04.589
or before the prayer,

00:39:04.589 --> 00:39:07.599
he says, Parker,

00:39:07.599 --> 00:39:11.519
would you mind

00:39:11.519 --> 00:39:14.349
we're desperately hungry,

00:39:14.349 --> 00:39:17.679
as Marcus empathizes with

00:39:17.679 --> 00:39:19.769
we're desperately hungry

00:39:19.769 --> 00:39:22.169
you're not going to last long anyhow,

00:39:22.169 --> 00:39:23.499
you can be a martyr,

00:39:23.499 --> 00:39:25.749
would you be a martyr

00:39:25.749 --> 00:39:29.469
how about it Parker?

00:39:29.469 --> 00:39:33.220
Then, then

00:39:33.220 --> 00:39:37.640
then what do you think, would
be morally justified then? Suppose

00:39:37.640 --> 00:39:38.220
Parker

00:39:38.220 --> 00:39:40.169
in his semi-stupor 

00:39:40.169 --> 00:39:42.499
says okay

00:39:42.499 --> 00:39:47.890
 I don't think it'll be morally justifiable but I'm wondering.
Even then, even then it wouldn't be? No

00:39:47.890 --> 00:39:50.650
You don't think that even with consent

00:39:50.650 --> 00:39:52.490
it would be morally justified.

00:39:52.490 --> 00:39:54.569
Are there people who think

00:39:54.569 --> 00:39:56.369
who want to take up Kathleen's 

00:39:56.369 --> 00:39:57.200
consent idea

00:39:57.200 --> 00:40:01.689
and who think that that would make it
morally justified? Raise your hand if it would

00:40:01.689 --> 00:40:05.869
if you think it would.

00:40:05.869 --> 00:40:07.589
That's very interesting

00:40:07.589 --> 00:40:09.169
Why would consent 

00:40:09.169 --> 00:40:15.889
make a moral difference? Why would it?

00:40:15.889 --> 00:40:18.509
Well I just think that if he was making his own original
idea

00:40:18.509 --> 00:40:20.969
and it was his idea to start with

00:40:20.969 --> 00:40:23.779
then that would be the only situation in which I
would

00:40:23.779 --> 00:40:25.940
see it being appropriate in anyway
 
0:40:25.940,0:40:28.359
because that way you couldn't make the argument
that

00:40:28.359 --> 00:40:30.580
he was pressured you know itâ€™s three

00:40:30.580 --> 00:40:32.759
to one or whatever the ratio was,

00:40:32.759 --> 00:40:34.070
and I think that

00:40:34.070 --> 00:40:38.009
if he was making a decision to give his life
then he took on the agency

00:40:38.009 --> 00:40:42.669
to sacrifice himself which some 
people might see as admirable and other people

00:40:42.669 --> 00:40:45.449
 might disagree with that decision.

00:40:45.449 --> 00:40:49.099
So if he came up with the idea

00:40:49.099 --> 00:40:52.820
that's the only kind of consent we could have
confidence in

00:40:52.820 --> 00:40:55.359
morally, then it would be okay

00:40:55.359 --> 00:40:57.269
otherwise

00:40:57.269 --> 00:40:59.789
it would be kind of coerced consent

00:40:59.789 --> 00:41:01.469
under the circumstances

00:41:01.469 --> 00:41:05.279
you think.

00:41:05.279 --> 00:41:07.349
Is there anyone who thinks

00:41:07.349 --> 00:41:10.979
that the even the consent of Parker

00:41:10.979 --> 00:41:13.420
would not justify

00:41:13.420 --> 00:41:15.479
their killing him?

00:41:15.479 --> 00:41:18.089
Who thinks that?

00:41:18.089 --> 00:41:19.539
Yes, tell us why, stand up

00:41:19.539 --> 00:41:21.260
I think that Parker

00:41:21.260 --> 00:41:22.319
would be killed

00:41:22.319 --> 00:41:26.559
with the hope that the other crew members
would be rescued so

00:41:26.559 --> 00:41:29.250
there's no definite reason that he should
be killed

00:41:29.250 --> 00:41:31.199
because you don't know 

00:41:31.199 --> 00:41:35.809
when they're going to get rescued so if you kill him you're killing him  
in vain

00:41:35.809 --> 00:41:38.039
do you keep killing a crew member until you're rescued and then you're  
left with no one?

00:41:38.039 --> 00:41:40.309
because someone's going to die eventually?

00:41:40.309 --> 00:41:44.199
Well the moral logic of the situation seems to
be that.

00:41:44.199 --> 00:41:45.829
That they would

00:41:45.829 --> 00:41:50.319
keep on picking off the weakest maybe, one by
one,

00:41:50.319 --> 00:41:51.819
until they were

00:41:51.819 --> 00:41:57.679
rescued and in this case luckily when three at least were still alive.

00:41:57.679 --> 00:41:58.880
Now if

00:41:58.880 --> 00:42:01.299
if Parker did give his consent

00:42:01.299 --> 00:42:04.069
would it be all right do you think or not?

00:42:04.069 --> 00:42:06.329
No, it still wouldn't be right.

00:42:06.329 --> 00:42:08.030
Tell us why wouldn't be all right.

00:42:08.030 --> 00:42:10.029
First of all, cannibalism, I believe

00:42:10.029 --> 00:42:13.229
is morally incorrect

00:42:13.229 --> 00:42:14.510
so you shouldnâ€™t be eating a human anyway.

00:42:14.510 --> 00:42:17.449
So

00:42:17.449 --> 00:42:19.380
cannibalism is morally objectionable outside

00:42:19.380 --> 00:42:22.400
so then even in the scenario

00:42:22.400 --> 00:42:24.569
of waiting until someone died

00:42:24.569 --> 00:42:27.019
still it would be objectionable.

00:42:27.019 --> 00:42:27.930
Yes, to me personally

00:42:27.930 --> 00:42:29.739
I feel like of

00:42:29.739 --> 00:42:31.200
it all depends on

00:42:31.200 --> 00:42:35.289
one's personal morals, like we can't just, like this is just my opinion

00:42:35.289 --> 00:42:39.339
of course other people are going to disagree.

00:42:39.339 --> 00:42:41.499
Well let's see, let's hear what their disagreements
are

00:42:41.499 --> 00:42:42.640
and then we'll see

00:42:42.640 --> 00:42:44.259
if they have reasons

00:42:44.259 --> 00:42:46.229
that can persuade you or not.

00:42:46.229 --> 00:42:48.359
Let's try that

00:42:48.359 --> 00:42:50.099
Let's

00:42:50.099 --> 00:42:53.249
now is there someone

00:42:53.249 --> 00:42:57.909
who can explain, those of you who are tempted
by consent

00:42:57.909 --> 00:42:59.779
can you explain

00:42:59.779 --> 00:43:02.029
why consent makes

00:43:02.029 --> 00:43:03.359
such a moral difference,

00:43:03.359 --> 00:43:05.650
what about the lottery idea

00:43:05.650 --> 00:43:08.930
does that count as consent. Remember at
the beginning

00:43:08.930 --> 00:43:11.309
Dudley proposed a lottery

00:43:11.309 --> 00:43:13.839
suppose that they had agreed

00:43:13.839 --> 00:43:16.339
to a lottery

00:43:16.339 --> 00:43:17.369
then

00:43:17.369 --> 00:43:20.799
how many would then say

00:43:20.799 --> 00:43:23.929
it was all right. Say there was a lottery,

00:43:23.929 --> 00:43:25.380
cabin boy lost,

00:43:25.380 --> 00:43:32.380
and the rest of the story unfolded. How
many people would say it's morally permissible?

00:43:33.199 --> 00:43:37.030
So the numbers are rising if we add a lottery,
let's hear from one of you

00:43:37.030 --> 00:43:41.609
for whom the lottery would make a moral difference

00:43:41.609 --> 00:43:43.459
why would it?

00:43:43.459 --> 00:43:44.740
I think the essential

00:43:44.740 --> 00:43:45.719
element,

00:43:45.719 --> 00:43:47.859
in my mind that makes it a crime is

00:43:47.859 --> 00:43:53.849
the idea that they decided at some point that
their lives were more important than his, and that

00:43:53.849 --> 00:43:56.609
I mean that's kind of the basis for really
any crime

00:43:56.609 --> 00:43:57.689
right? It's like

00:43:57.689 --> 00:44:01.949
my needs, my desire is a more important than yours
and mine take precedent

00:44:01.949 --> 00:44:04.799
and if they had done a lottery were everyone
consented

00:44:04.799 --> 00:44:06.479
that someone should die

00:44:06.479 --> 00:44:09.240
and it's sort of like they're all sacrificing 
themselves,

00:44:09.240 --> 00:44:11.009
to save the rest,

00:44:11.009 --> 00:44:12.949
Then it would be all right?

00:44:12.949 --> 00:44:15.880
A little grotesque but,

00:44:15.880 --> 00:44:18.959
But morally permissible? Yes.

00:44:18.959 --> 00:44:22.689
what's your name? Matt.

00:44:22.689 --> 00:44:25.579
so, Matt for you

00:44:25.579 --> 00:44:27.329
what bothers you is not

00:44:27.329 --> 00:44:31.389
the cannibalism, but the lack of due process.

00:44:31.389 --> 00:44:34.689
I guess you could say that

00:44:34.689 --> 00:44:38.170
And can someone who agrees with Matt

00:44:38.170 --> 00:44:40.499
say a little bit more

00:44:40.499 --> 00:44:41.379
about why 

00:44:41.379 --> 00:44:43.690
a lottery

00:44:43.690 --> 00:44:47.099
would make it, in your view,

00:44:47.099 --> 00:44:50.979
morally permissible.

00:44:50.979 --> 00:44:55.569
The way I understood it originally was that that was the
whole issue is that the cabin boy was never

00:44:55.569 --> 00:44:56.399
consulted

00:44:56.399 --> 00:45:00.479
about whether or not it something was going
to happen to him even though with the original

00:45:00.479 --> 00:45:01.089
lottery

00:45:01.089 --> 00:45:04.420
whether or not he would be a part of that
it was just decided

00:45:04.420 --> 00:45:08.170
that he was the one that was going to die. 
Yes that's what happened in the actual case

00:45:08.170 --> 00:45:11.900
but if there were a lottery and they all agreed
to the procedure

00:45:11.900 --> 00:45:13.540
you think that would be okay?

00:45:13.540 --> 00:45:16.419
Right, because everyone knows that there's gonna be
a death

00:45:16.419 --> 00:45:17.080
whereas

00:45:17.080 --> 00:45:18.879
you know the cabin boy didn't know that

00:45:18.879 --> 00:45:21.039
this discussion was even happening

00:45:21.039 --> 00:45:21.919
there was no

00:45:21.919 --> 00:45:23.579
you know forewarning

00:45:23.579 --> 00:45:28.829
for him to know that hey, I may be the one
that's dying. Okay, now suppose the everyone agrees

00:45:28.829 --> 00:45:35.089
to the lottery they have the lottery the cabin
boy loses any changes his mind.

00:45:35.089 --> 00:45:40.989
You've already decided, it's like a verbal contract, you can't go back  
on that. You've decided the decision was made

00:45:40.989 --> 00:45:45.139
you know if you know you're dying for the 
 reason for at others to live,

00:45:45.139 --> 00:45:45.999
you would, you know

00:45:45.999 --> 00:45:47.689
if the someone else had died

00:45:47.689 --> 00:45:51.969
you know that you would consume them, so

00:45:51.969 --> 00:45:57.429
But then he could say I know, but I lost.

00:45:57.429 --> 00:46:01.939
I just think that that's the whole moral issue is that there was
no consulting of the cabin boy and that that's

00:46:01.939 --> 00:46:04.299
what makes it the most horrible

00:46:04.299 --> 00:46:08.909
is that he had no idea what was even
going on, that if he had known what was going on

00:46:08.909 --> 00:46:10.599
it would

00:46:10.599 --> 00:46:13.109
be a bit more understandable.

00:46:13.109 --> 00:46:14.510
Alright, good, now I want to hear

00:46:14.510 --> 00:46:17.049
so there's some who think

00:46:17.049 --> 00:46:18.709
it's morally permissible

00:46:18.709 --> 00:46:24.049
but only about twenty percent,

00:46:24.049 --> 00:46:26.559
led by Marcus,

00:46:26.559 --> 00:46:28.439
then there are some who say

00:46:28.439 --> 00:46:30.209
the real problem here

00:46:30.209 --> 00:46:32.839
is the lack of consent

00:46:32.839 --> 00:46:37.139
whether the lack of consent to a lottery to
a fair procedure

00:46:37.139 --> 00:46:38.589
or

00:46:38.589 --> 00:46:39.690
Kathleen's idea,

00:46:39.690 --> 00:46:40.749
lack of consent

00:46:40.749 --> 00:46:42.929
at the moment

00:46:42.929 --> 00:46:45.139
of death

00:46:45.139 --> 00:46:48.319
and if we add consent

00:46:48.319 --> 00:46:49.020
then

00:46:49.020 --> 00:46:51.889
more people are willing to consider

00:46:51.889 --> 00:46:54.529
the sacrifice morally justified.

00:46:54.529 --> 00:46:56.640
I want to hear now finally

00:46:56.640 --> 00:46:58.549
from those of you who think

00:46:58.549 --> 00:47:00.199
even with consent

00:47:00.199 --> 00:47:01.899
even with a lottery

00:47:01.899 --> 00:47:02.520
even with

00:47:02.520 --> 00:47:04.579
a final 

00:47:04.579 --> 00:47:06.950
murmur of consent from Parker

00:47:06.950 --> 00:47:08.019
at the

00:47:08.019 --> 00:47:09.180
very last moment

00:47:09.180 --> 00:47:10.839
it would still

00:47:10.839 --> 00:47:12.640
be wrong

00:47:12.640 --> 00:47:14.249
and why would it be wrong

00:47:14.249 --> 00:47:16.999
that's what I want to hear.

00:47:16.999 --> 00:47:18.819
well the whole time

00:47:18.819 --> 00:47:22.639
I've been leaning towards the categorical moral reasoning

00:47:22.639 --> 00:47:25.569
and I think that

00:47:25.569 --> 00:47:29.609
there's a possibility I'd be okay with the
idea of the lottery and then loser

00:47:29.609 --> 00:47:31.440
taking into their own hands to

00:47:31.440 --> 00:47:32.749
kill themselves

00:47:33.680 --> 00:47:37.359
so there wouldn't be an act of murder but
I still think that

00:47:37.359 --> 00:47:42.279
even that way it's coerced and also I don't
think that there's any remorse like in

00:47:42.279 --> 00:47:43.339
Dudley's diary

00:47:43.339 --> 00:47:44.909
we're getting our breakfast

00:47:44.909 --> 00:47:47.659
it seems as though he's just sort of like, oh,

00:47:47.659 --> 00:47:51.440
you know that whole idea of not valuing someone else's life

00:47:51.440 --> 00:47:53.639
so that makes me

00:47:53.639 --> 00:47:57.969
feel like I have to take the categorical stance. You want to throw the  
book at him.

00:47:57.969 --> 00:48:02.299
when he lacks remorse or a sense of having done
anything wrong. Right.

00:48:02.299 --> 00:48:06.969
Alright, good so are there any other

00:48:06.969 --> 00:48:08.769
defenders who

00:48:08.769 --> 00:48:13.269
who say it's just categorically wrong, with or without consent, yes  
stand up. Why?

00:48:13.269 --> 00:48:17.289
 I think undoubtedly the way our society is shaped, murder
is murder

00:48:17.289 --> 00:48:21.829
murder is murder and every way our society looks down at it in the same  
light

00:48:21.829 --> 00:48:24.780
and I don't think it's any different in any case. Good now let 
me ask you a question,

00:48:24.780 --> 00:48:27.119
there were three lives at stake

00:48:27.119 --> 00:48:30.489
versus one,

00:48:30.489 --> 00:48:33.029
the one, that the cabin boy, he  had no family

00:48:33.029 --> 00:48:34.509
he had no dependents,

00:48:34.509 --> 00:48:38.739
these other three had families back home
in England they had dependents

00:48:38.739 --> 00:48:41.419
they had wives and children

00:48:41.419 --> 00:48:43.329
think back to Bentham,

00:48:43.329 --> 00:48:44.989
Bentham says we have to consider

00:48:44.989 --> 00:48:48.049
the welfare, the utility, the happiness

00:48:48.049 --> 00:48:51.289
of everybody. We have to add it all up

00:48:51.289 --> 00:48:54.640
so it's not just numbers three against one

00:48:54.640 --> 00:48:58.759
it's also all of those people at home

00:48:58.759 --> 00:49:00.910
in fact the London newspaper at the time

00:49:00.910 --> 00:49:04.249
and popular opinion sympathized with them

00:49:04.249 --> 00:49:05.479
Dudley in Stephens

00:49:05.479 --> 00:49:07.920
and the paper said if they weren't

00:49:07.920 --> 00:49:08.280
motivated

00:49:08.280 --> 00:49:09.640
by affection

00:49:09.640 --> 00:49:13.489
and concern for their loved ones at
home and dependents, surely they wouldn't have

00:49:13.489 --> 00:49:15.969
done this. Yeah, and how is that any different from people

00:49:15.969 --> 00:49:17.369
on the corner

00:49:17.369 --> 00:49:21.109
trying to having the same desire to feed their family,
I don't think it's any different. I think in any case

00:49:21.109 --> 00:49:25.280
if I'm murdering you to advance my status, that's murder and I think  
that we should look at all

00:49:25.280 --> 00:49:28.430
of that in the same light. Instead of criminalizing certain

00:49:28.430 --> 00:49:30.279
activities

00:49:30.279 --> 00:49:33.760
and making certain things seem more
violent and savage

00:49:33.760 --> 00:49:36.760
when in that same case it's all the same act and mentality 

00:49:36.760 --> 00:49:40.150
that goes into the murder, a necessity
to feed their families.

00:49:40.150 --> 00:49:43.029
Suppose there weren't three, supposed there were thirty,

00:49:43.029 --> 00:49:44.609
three hundred,

00:49:44.609 --> 00:49:47.359
one life to save three hundred

00:49:47.359 --> 00:49:48.350
or in more time,

00:49:48.350 --> 00:49:49.590
three thousand

00:49:49.590 --> 00:49:51.040
or suppose the stakes were even bigger.

00:49:51.040 --> 00:49:52.779
Suppose the stakes were even bigger

00:49:52.779 --> 00:49:54.609
I think it's still the same deal.

00:49:54.609 --> 00:49:58.109
Do you think Bentham was wrong to say the right thing
to do

00:49:58.109 --> 00:49:58.929
is to add

00:49:58.929 --> 00:50:02.479
up the collected happiness, you think he's
wrong about that?

00:50:02.479 --> 00:50:06.729
I don't think he is wrong, but I think murder is murder in any case.
Well then Bentham has to be wrong

00:50:06.729 --> 00:50:09.569
if you're right he's wrong. okay then he's wrong.

00:50:09.569 --> 00:50:12.819
Alright thank you, well done.

00:50:12.819 --> 00:50:14.359
Alright, let's step back

00:50:14.359 --> 00:50:16.390
from this discussion

00:50:16.390 --> 00:50:19.729
and notice

00:50:19.729 --> 00:50:23.259
how many objections have we heard to what they did.

00:50:23.259 --> 00:50:26.049
we heard some defenses of what they did

00:50:26.049 --> 00:50:28.509
the defense has had to do with 

00:50:28.509 --> 00:50:28.919
necessity

00:50:28.919 --> 00:50:32.589
the dire circumstance and,

00:50:32.589 --> 00:50:33.410
implicitly at least,

00:50:33.410 --> 00:50:36.019
the idea that numbers matter

00:50:36.019 --> 00:50:37.859
and not only numbers matter

00:50:37.859 --> 00:50:40.380
but the wider effects matter

00:50:40.380 --> 00:50:43.459
their families back home, their dependents

00:50:43.459 --> 00:50:44.769
Parker was an orphan,

00:50:44.769 --> 00:50:47.979
no one would miss him.

00:50:47.979 --> 00:50:49.579
so if you

00:50:49.579 --> 00:50:50.829
add up

00:50:50.829 --> 00:50:52.649
if you tried to calculate

00:50:52.649 --> 00:50:53.999
the balance

00:50:53.999 --> 00:50:56.599
of happiness and suffering

00:50:56.599 --> 00:50:58.839
you might have a case for 

00:50:58.839 --> 00:51:02.769
saying what they did was the right thing

00:51:02.769 --> 00:51:09.469
then we heard at least three different types
of objections,

00:51:09.469 --> 00:51:11.690
we heard an objection that's said

00:51:11.690 --> 00:51:14.109
what they did was categorically wrong,

00:51:14.109 --> 00:51:15.750
right here at the end

00:51:15.750 --> 00:51:17.389
categorically wrong.

00:51:17.389 --> 00:51:19.820
Murder is murder it's always wrong

00:51:19.820 --> 00:51:20.969
even if

00:51:20.969 --> 00:51:23.349
it increases the overall happiness

00:51:23.349 --> 00:51:25.639
of society

00:51:25.639 --> 00:51:28.499
the categorical objection.

00:51:28.499 --> 00:51:30.739
But we still need to investigate

00:51:30.739 --> 00:51:32.749
why murder

00:51:32.749 --> 00:51:35.449
is categorically wrong.

00:51:35.449 --> 00:51:38.579
Is it because

00:51:38.579 --> 00:51:42.339
even cabin boys have certain fundamental rights?

00:51:42.339 --> 00:51:44.400
And if that's the reason

00:51:44.400 --> 00:51:47.880
where do those rights come from if not from
some idea

00:51:47.880 --> 00:51:53.209
of the larger welfare or utility or happiness?
Question number one.

00:51:53.209 --> 00:51:56.309
Others said

00:51:56.309 --> 00:51:58.449
a lottery would make a difference

00:51:58.449 --> 00:52:00.039
a fair procedure,

00:52:00.039 --> 00:52:05.949
Matt said.

00:52:05.949 --> 00:52:08.769
And some people were swayed by that.

00:52:08.769 --> 00:52:12.189
That's not a categorical objection exactly

00:52:12.189 --> 00:52:13.829
it's saying

00:52:13.829 --> 00:52:16.799
everybody has to be counted as an equal

00:52:16.799 --> 00:52:18.469
even though, at the end of the day

00:52:18.469 --> 00:52:20.769
one can be sacrificed

00:52:20.769 --> 00:52:23.289
for the general welfare.

00:52:23.289 --> 00:52:26.059
That leaves us with another question to investigate,

00:52:26.059 --> 00:52:29.670
Why does agreement to certain procedure, 

00:52:29.670 --> 00:52:31.969
even a fair procedure,

00:52:31.969 --> 00:52:34.739
justify whatever result flows

00:52:34.739 --> 00:52:38.089
from the operation of that procedure?

00:52:38.089 --> 00:52:39.899
Question number two.

00:52:39.899 --> 00:52:42.399
and question number three

00:52:42.399 --> 00:52:45.339
the basic idea of consent.

00:52:45.339 --> 00:52:48.529
Kathleen got us on to this.

00:52:48.529 --> 00:52:52.719
If the cabin boy had agreed himself

00:52:52.719 --> 00:52:54.499
and not under duress

00:52:54.499 --> 00:52:57.069
as was added

00:52:57.069 --> 00:53:01.919
then it would be all right to take his life 
to save the rest.

00:53:01.919 --> 00:53:04.900
Even more people signed on to that idea

00:53:04.900 --> 00:53:06.630
but that raises

00:53:06.630 --> 00:53:08.529
a third philosophical question

00:53:08.529 --> 00:53:11.009
what is the moral work

00:53:11.009 --> 00:53:12.839
that consent

00:53:12.839 --> 00:53:14.439
does?

00:53:14.439 --> 00:53:16.979
Why does an act of consent

00:53:16.979 --> 00:53:19.189
make such a moral difference

00:53:19.189 --> 00:53:23.819
that an act that would be wrong, taking a life,
without consent

00:53:23.819 --> 00:53:25.370
is morally

00:53:25.370 --> 00:53:26.359
permissible

00:53:26.359 --> 00:53:29.619
with consent?

00:53:29.619 --> 00:53:31.699
To investigate those three questions

00:53:31.699 --> 00:53:34.109
we're going to have to read some philosophers

00:53:34.109 --> 00:53:35.729
and starting next time

00:53:35.729 --> 00:53:36.940
we're going to read

00:53:36.940 --> 00:53:37.720
Bentham,

00:53:37.720 --> 00:53:43.799
and John Stuart Mill, utilitarian philosophers.

00:53:43.799 --> 00:53:47.299
Don't miss the chance to interact online with other viewers
of Justice

00:53:43.799 --> 00:53:47.299
join the conversation,

00:53:49.869 --> 00:53:56.869
take a pop quiz, watch lectures you've missed, and a lot more. Visit  
www.justiceharvard.org. It's the right thing to do.

00:54:36.219 --> 00:54:40.269
Funding for the program is provided by 

00:54:40.269 --> 00:54:41.679
Additional funding provided by

