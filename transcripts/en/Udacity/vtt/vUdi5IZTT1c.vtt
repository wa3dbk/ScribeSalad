WEBVTT
Kind: captions
Language: en

00:00:00.250 --> 00:00:01.740
You can do interesting
things with States.

00:00:01.740 --> 00:00:04.360
So, here we have the, the Head Contour.

00:00:04.360 --> 00:00:08.910
Another kind of model is,
suppose you've got some way

00:00:08.910 --> 00:00:12.670
of detecting colored blobs that
represent say, Hands and Head.

00:00:13.790 --> 00:00:17.830
Now your state would, might be the x,
y location of the hands and head.

00:00:17.830 --> 00:00:21.510
And your measurement might be where
you think the colored blobs are, and

00:00:21.510 --> 00:00:25.300
your sensor model might be how well
do predicted blobs match the color.

00:00:26.350 --> 00:00:29.900
Your, you can use sort of any model you
want as long as you can have a state and

00:00:29.900 --> 00:00:33.890
you can talk about how that
state expresses itself.

00:00:33.890 --> 00:00:36.090
In fact, here's an even better model.

00:00:36.090 --> 00:00:40.124
Suppose you want to track some region
of, of the distribution of colors.

00:00:40.124 --> 00:00:42.211
Right?
So like maybe this region here or

00:00:42.211 --> 00:00:45.971
this region here where you can see the
ellipse, that kind of gives away that

00:00:45.971 --> 00:00:49.150
the region that we're going to,
we're going to be tracking here.

00:00:49.150 --> 00:00:54.850
What would be a good model or
state for tracking that?

00:00:54.850 --> 00:00:55.660
Okay.

00:00:55.660 --> 00:00:58.190
Well, clearly you want the location,

00:00:58.190 --> 00:01:00.730
because after all I want
to know where the thing is.

00:01:00.730 --> 00:01:01.740
All right?

00:01:01.740 --> 00:01:06.130
And if I've defined a region,
I want to know, sort of,

00:01:06.130 --> 00:01:09.680
how that region is changing maybe
a little bit in orientation or size.

00:01:09.680 --> 00:01:12.550
But just to tell me this, the position
and the size, that's not enough.

00:01:12.550 --> 00:01:14.430
I need something about the appearance.

00:01:14.430 --> 00:01:15.530
In this particular method, and

00:01:15.530 --> 00:01:18.170
we're going to talk a lot
more about this next time.

00:01:18.170 --> 00:01:20.140
What they tracked is really cool.

00:01:20.140 --> 00:01:23.740
They tracked the distribution of colors.

00:01:23.740 --> 00:01:25.910
Right?
So if I look over, let's say.

00:01:25.910 --> 00:01:28.240
I'm not going to do that region,
I'm going to do this region over here.

00:01:28.240 --> 00:01:30.460
All right.
There are some white pixels,

00:01:30.460 --> 00:01:33.610
there are some reddish pixels,
there are some brownish pixels.

00:01:33.610 --> 00:01:38.930
And as that player moves,
the shape will change in the form etc.

00:01:38.930 --> 00:01:42.802
But the overall distribution of
colors might be quite similar.

00:01:42.802 --> 00:01:44.150
All right?

00:01:44.150 --> 00:01:46.620
And in fact what you
have to say is well,

00:01:46.620 --> 00:01:50.210
what's a reasonable sensor model and
we're going to get to that next time.

00:01:50.210 --> 00:01:52.550
But, basically what you're going to
do is you're going to compute

00:01:52.550 --> 00:01:57.220
the similarity of the color
distribution of what you had, and

00:01:57.220 --> 00:02:00.630
the color distribution
of your proposed state.

00:02:00.630 --> 00:02:01.220
All right?
And

00:02:01.220 --> 00:02:04.450
you'll need somewhere of comparing
those color distributions.

00:02:04.450 --> 00:02:07.720
And just as a way of showing
you how well this thing works,

00:02:07.720 --> 00:02:10.500
this, here's two examples.

00:02:10.500 --> 00:02:13.760
This is actually done using mean-shift
filtering, but could also have

00:02:13.760 --> 00:02:18.690
been done, using particle filtering, and
we'll talk about that, both, next time.

