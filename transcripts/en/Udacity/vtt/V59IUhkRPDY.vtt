WEBVTT
Kind: captions
Language: en

00:00:00.001 --> 00:00:03.537
Okay, so we've got these three little
bits of machine learning here.

00:00:03.537 --> 00:00:06.005
And there are a lot of tools and
techniques that are inside that.

00:00:06.005 --> 00:00:07.550
&gt;&gt; Mm-hm.
&gt;&gt; And I think that's great.

00:00:07.550 --> 00:00:09.870
And we're going to be trying to
teach you a lot of those tools and

00:00:09.870 --> 00:00:11.979
techniques and
sort of ways to connect them together.

00:00:11.979 --> 00:00:13.476
So by the way,
as Michael was pointing out,

00:00:13.476 --> 00:00:15.584
there are kind of ways that these
things might help each other,

00:00:15.584 --> 00:00:17.585
unsupervised learning might
help supervised learning.

00:00:17.585 --> 00:00:19.080
It's actually much deeper than that.

00:00:19.080 --> 00:00:21.817
It turns out you, even though
unsupervised learning is clearly not

00:00:21.817 --> 00:00:24.507
the same as supervised learning at
the level that we described it,

00:00:24.507 --> 00:00:26.394
in some ways they're
exactly the same thing.

00:00:26.394 --> 00:00:28.530
Supervised learning you have some bias.

00:00:28.530 --> 00:00:31.232
Oh, it's a quadratic function,
induction make sense.

00:00:31.232 --> 00:00:32.920
All these kind of assumptions you make.

00:00:32.920 --> 00:00:36.643
And in unsupervised learning, I told you
that we don't know whether this clusters

00:00:36.643 --> 00:00:40.258
is better than this cluster, dividing by
sex is better than dividing by height,

00:00:40.258 --> 00:00:41.709
or, or hair color or whatever.

00:00:41.709 --> 00:00:42.977
&gt;&gt; Mm.

00:00:42.977 --> 00:00:46.285
&gt;&gt; But ultimately you make some
decision about how to cluster, and

00:00:46.285 --> 00:00:49.050
that means implicitly
there's some assume signal.

00:00:49.050 --> 00:00:50.790
There's some assume set of labels
that you think make sense.

00:00:50.790 --> 00:00:54.722
Oh, I think things that look alike
should somehow be clustered together.

00:00:54.722 --> 00:00:55.289
&gt;&gt; Mm.

00:00:55.289 --> 00:00:57.810
&gt;&gt; Or things that are near one
another should cluster together.

00:00:57.810 --> 00:00:59.695
So in some ways it's still kind
of like supervised learning.

00:00:59.695 --> 00:01:00.694
You can certainly turn any

00:01:00.694 --> 00:01:03.481
supervised learning problem into
an unsupervised learning problem.

00:01:03.481 --> 00:01:04.031
&gt;&gt; Mm, mm.
&gt;&gt; Right?

00:01:04.031 --> 00:01:07.250
So in fact, all of these problems
are really the same kind of problem.

00:01:07.250 --> 00:01:08.536
&gt;&gt; Yeah, well there's two things
that I'd want to add to that.

00:01:08.536 --> 00:01:10.800
One is that in some sense,
in many cases,

00:01:10.800 --> 00:01:15.580
you can formulate all these different
problems as some form of optimization.

00:01:15.580 --> 00:01:17.778
In supervised learning
you want something that,

00:01:17.778 --> 00:01:21.178
that labels data well and so you're,
the thing you're trying to optimize is

00:01:21.178 --> 00:01:23.818
find me a function that,
that does that scores it.

00:01:23.818 --> 00:01:27.290
In reinforcement learning we're trying
to find a behavior that scores well.

00:01:27.290 --> 00:01:30.396
And unsu, unsupervised learning we
usually have to make up some kind of

00:01:30.396 --> 00:01:32.982
a criterion, and then we find
a way of clustering the data,

00:01:32.982 --> 00:01:34.929
organizing the data so
that it scores well.

00:01:34.929 --> 00:01:36.490
So that was the first
point I wanted to make.

00:01:36.490 --> 00:01:39.508
The other one is if you
divide things by sex and

00:01:39.508 --> 00:01:43.571
your a virgin then there's
numerical instability issues.

00:01:46.210 --> 00:01:48.409
&gt;&gt; Do you learn about
that on the street?

00:01:48.409 --> 00:01:50.011
&gt;&gt; I learned it in a Math book.

00:01:50.011 --> 00:01:54.281
&gt;&gt; Yes you [NOISE] I'm, I'm going to
move on And so here's the thing.

00:01:54.281 --> 00:01:55.722
&gt;&gt; All right.
&gt;&gt; Everything just Michael just

00:01:55.722 --> 00:01:57.800
said except the last part is true.

00:01:57.800 --> 00:01:59.887
But there's actually a sort of
deeper thing going on here.

00:01:59.887 --> 00:02:03.523
To me, if you think about the
commonalities of everything we've just

00:02:03.523 --> 00:02:07.420
said, it boils down to one thing data,
data, data, data, data, data.

00:02:07.420 --> 00:02:08.461
Data is king in machine learning.

00:02:08.461 --> 00:02:10.600
Now Michael would call
himself a computer scientist.

00:02:10.600 --> 00:02:11.429
&gt;&gt; Oh, yeah.
&gt;&gt; And I would call

00:02:11.429 --> 00:02:13.185
myself a computationalist.

00:02:13.185 --> 00:02:14.258
&gt;&gt; What?
&gt;&gt; What if I'm in a college of

00:02:14.258 --> 00:02:16.470
computing at a department
of computer science?

00:02:16.470 --> 00:02:19.940
I believe in computing and
computation as being the ultimate thing.

00:02:19.940 --> 00:02:22.073
So I would call myself
a computationalist, and

00:02:22.073 --> 00:02:25.980
Michael would probably agree with that
just to keep this discussion moving.

00:02:25.980 --> 00:02:26.747
&gt;&gt; Let's say.

00:02:26.747 --> 00:02:27.650
&gt;&gt; Right.

00:02:27.650 --> 00:02:28.450
So we're computationalists.

00:02:28.450 --> 00:02:29.880
We believe in computing.

00:02:29.880 --> 00:02:31.180
That's a good thing.
&gt;&gt; Sure.

00:02:31.180 --> 00:02:33.759
&gt;&gt; Many of our colleagues who do
computations tend to think in terms of

00:02:33.759 --> 00:02:34.321
algorithms.

00:02:34.321 --> 00:02:37.177
They think in terms of what
are the series of steps I

00:02:37.177 --> 00:02:39.560
need to do in order to
solve some problem?

00:02:39.560 --> 00:02:40.361
Or they.

00:02:40.361 --> 00:02:42.680
&gt;&gt; [CROSSTALK].
&gt;&gt; Might think in terms of theorems.

00:02:42.680 --> 00:02:45.277
If I try to describe this
problem in a particular way,

00:02:45.277 --> 00:02:47.601
is it solvable quizzically
by some algorithm?

00:02:47.601 --> 00:02:48.470
&gt;&gt; Yeah.

00:02:48.470 --> 00:02:50.337
&gt;&gt; And, truthfully,
machine learning is a lot of that.

00:02:50.337 --> 00:02:53.771
But the difference between the person
who's trying to solve our problem as

00:02:53.771 --> 00:02:55.569
an AI person or
as a computing person and

00:02:55.569 --> 00:02:59.005
somebody who's trying to solve our
problem as a machine learning person is

00:02:59.005 --> 00:03:02.416
that the algorithm stops being central,
the data starts being central.

00:03:02.416 --> 00:03:05.347
And so what I hope you get out of this
class, or at least part of the stuff

00:03:05.347 --> 00:03:08.082
that you do, is understanding that
you have to believe the data,

00:03:08.082 --> 00:03:11.740
you have to do something with the data,
you have to be consistent with the data.

00:03:11.740 --> 00:03:15.057
The algorithms that fall out of
all that are algorithms, but

00:03:15.057 --> 00:03:19.099
they're algorithms that take the data
as primary or at least important.

00:03:19.099 --> 00:03:20.902
&gt;&gt; I'm going to go with co-equal.

00:03:20.902 --> 00:03:22.369
&gt;&gt; So the algorithms and
data are co-equal.

00:03:22.369 --> 00:03:23.170
&gt;&gt; Co-equal.

00:03:23.170 --> 00:03:25.639
&gt;&gt; Well if you believe in Lisp,
they're the same thing.

00:03:25.639 --> 00:03:26.280
&gt;&gt; Exactly!

00:03:26.280 --> 00:03:26.941
&gt;&gt; All right.
So there you go.

00:03:26.941 --> 00:03:28.709
&gt;&gt; They knew back in the 70s.

00:03:28.709 --> 00:03:30.110
&gt;&gt; So
it turns out we do agree on most things.

00:03:30.110 --> 00:03:31.378
&gt;&gt; [NOISE] That was close.

00:03:31.378 --> 00:03:31.879
&gt;&gt; Excellent!

00:03:31.879 --> 00:03:36.283
So, the rest of the semester
will go exactly like this.

00:03:36.283 --> 00:03:38.620
&gt;&gt; [LAUGH].
&gt;&gt; [LAUGH] Except you won't see us.

00:03:38.620 --> 00:03:40.588
You'll see our hands though.

00:03:40.588 --> 00:03:41.255
&gt;&gt; This side.

00:03:41.255 --> 00:03:41.989
This side.

00:03:41.989 --> 00:03:43.190
&gt;&gt; You'll see our hands, though.

00:03:43.190 --> 00:03:45.025
Thank you, Michael.

00:03:45.025 --> 00:03:46.594
&gt;&gt; It's all right.

00:03:46.594 --> 00:03:48.762
&gt;&gt; [LAUGH] What?

00:03:48.762 --> 00:03:49.530
[LAUGH].

00:03:49.530 --> 00:03:50.965
&gt;&gt; What?
[LAUGH].

00:03:50.965 --> 00:03:53.100
&gt;&gt; That was good,
that took me back to when I was four.

00:03:53.100 --> 00:03:54.304
Okay, so.

00:03:54.304 --> 00:03:54.868
&gt;&gt; Senor Wences.

00:03:54.868 --> 00:03:55.836
&gt;&gt; Hm?
&gt;&gt; It's called SeÃ±or Wences.

00:03:55.836 --> 00:03:56.403
&gt;&gt; Yes, I know.

00:03:56.403 --> 00:03:57.404
&gt;&gt; Yeah, okay.
&gt;&gt; I remember that.

00:03:57.404 --> 00:03:58.112
&gt;&gt; Mm-hm, yeah.
&gt;&gt; I'm not

00:03:58.112 --> 00:03:59.173
that much younger than you are.

00:03:59.173 --> 00:03:59.940
&gt;&gt; Little bit.

00:03:59.940 --> 00:04:01.175
&gt;&gt; Ten, 12 years only.

00:04:01.175 --> 00:04:02.243
&gt;&gt; No come on.

00:04:02.243 --> 00:04:03.277
&gt;&gt; You can count gray hairs.

00:04:03.277 --> 00:04:07.036
Anyway the point is the rest of
the semester will go like this.

00:04:07.036 --> 00:04:10.084
We will talk about supervised learning
and a whole series of algorithms.

00:04:10.084 --> 00:04:13.423
Step back a little bit and talk about
the theory behind them, and try to

00:04:13.423 --> 00:04:16.935
connect theory of machine learning
with theory of computing notions, or

00:04:16.935 --> 00:04:18.535
at least that kind of basic idea.

00:04:18.535 --> 00:04:20.961
What does it mean to be a hard
problem versus an easier problem?

00:04:20.961 --> 00:04:24.273
Will move into randomized optimization
and unsupervised learning where we

00:04:24.273 --> 00:04:27.530
will talk about all the issues that we
brought up here and try to connect them

00:04:27.530 --> 00:04:31.025
back to some of the things that we did
in the section on supervised learning.

00:04:31.025 --> 00:04:33.974
And then finally, we will spend our
time on reinforcement learning.

00:04:33.974 --> 00:04:37.160
And a generalization of these
traditional reinforcement learning,

00:04:37.160 --> 00:04:38.870
which involves multiple agents.

00:04:38.870 --> 00:04:39.480
So we'll talk about a little bit of.

00:04:39.480 --> 00:04:40.354
&gt;&gt; Mm-hm.
&gt;&gt; Game theory,

00:04:40.354 --> 00:04:42.210
which Michael loves to talk about.

00:04:42.210 --> 00:04:42.883
I love to talk about.

00:04:42.883 --> 00:04:46.074
And the applications of all the stuff
that we've been learning to

00:04:46.074 --> 00:04:48.520
solving problems of how to
actually act in the world?

00:04:48.520 --> 00:04:50.225
How to build that world
out to do something?

00:04:50.225 --> 00:04:51.878
Or build that agent to play a game or

00:04:51.878 --> 00:04:55.600
to teach you how to do whatever you,
you need to be taught how to do?

00:04:55.600 --> 00:04:58.601
But at the end of the day,
we're going to teach you how to

00:04:58.601 --> 00:05:01.672
think about data,
how to think about algorithms, and

00:05:01.672 --> 00:05:04.571
how to build artifacts that you know,
will learn?

00:05:04.571 --> 00:05:05.970
&gt;&gt; Let's do this thing.

00:05:05.970 --> 00:05:07.174
&gt;&gt; Excellent.
All right.

00:05:07.174 --> 00:05:08.008
Well thank you Michael.

00:05:08.008 --> 00:05:08.842
&gt;&gt; Sure.

00:05:08.842 --> 00:05:13.314
&gt;&gt; I will see you next time we're
in the same place at the same time.

