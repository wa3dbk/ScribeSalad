WEBVTT
Kind: captions
Language: en

00:00:00.260 --> 00:00:06.070
This next material is bonus. It's optional but it's really cool and it will

00:00:06.070 --> 00:00:10.370
only take a few minutes to go through. So far we've only used one predictor

00:00:10.370 --> 00:00:15.390
variable. In the case before, the predictor was the distance traveled, and we

00:00:15.390 --> 00:00:19.490
were using that to predict the flight cost. But often times there are many

00:00:19.490 --> 00:00:23.390
factors that can predict the dependent variable. When we use multiple

00:00:23.390 --> 00:00:28.390
predictors, we're doing multiple regression. The purpose is to explain more of

00:00:28.390 --> 00:00:35.750
the variation in y. Basically we regress y on predictor 1, predictor 2, all the

00:00:35.750 --> 00:00:39.960
way to however many predictors we want. Why don't we do a simple linear

00:00:39.960 --> 00:00:43.905
regression for each one? The reason we just have one equation is not only

00:00:43.905 --> 00:00:49.410
because it's easier but also because when we include several predictors, we can

00:00:49.410 --> 00:00:54.020
calculate the relationship that each predictor has with y independently of the

00:00:54.020 --> 00:00:58.740
other predictors. For example, in my master's thesis, I wanted to look at

00:00:58.740 --> 00:01:03.350
things that impact student effort in math class. Remember that effort is the

00:01:03.350 --> 00:01:06.870
construct. So I measured this by things like whether or not they did their

00:01:06.870 --> 00:01:11.470
homework and time spent studying. A lot of things can influence effort.

00:01:11.470 --> 00:01:16.300
Specifically, I was interested in how much students value mathematics. Did they

00:01:16.300 --> 00:01:20.500
believe that math will help them in their careers? And also how much they enjoy

00:01:20.500 --> 00:01:25.420
math class. And their relationship with their teachers. It's possible that

00:01:25.420 --> 00:01:29.050
having a good relationship with their teachers can make them enjoy school more.

00:01:29.050 --> 00:01:33.570
And if they enjoy school more, they'll put forth more effort. Or maybe students

00:01:33.570 --> 00:01:38.265
place value on subjects that they enjoy. When we include all of these as

00:01:38.265 --> 00:01:43.660
predictors we no longer have a simple slope. Instead we get regression

00:01:43.660 --> 00:01:48.720
coefficients for each predictor variable. These will be numbers. Just like how

00:01:48.720 --> 00:01:54.155
before when we had one variable we found the slope and that was our regression

00:01:54.155 --> 00:01:58.960
coefficient. In this case we'll have three. These coefficients tell us the rate

00:01:58.960 --> 00:02:04.880
of change in y, given a one unit change in each respective variable holding the

00:02:04.880 --> 00:02:09.586
others constant. In other words we see the mathematical influence of one

00:02:09.586 --> 00:02:13.999
variable while statistically controlling for the influence of the other

00:02:13.999 --> 00:02:18.220
variables. We could go into more details but we won't. Multiple regression is

00:02:18.220 --> 00:02:22.760
not usually covered in elementary statistics courses. But it's still valuable

00:02:22.760 --> 00:02:26.990
to know about, and be able to interpret. Apart from the regression

00:02:26.990 --> 00:02:31.610
coefficients, most programs that conduct multiple regression will also provide

00:02:31.610 --> 00:02:36.520
a multiple correlation coefficient called r. This is similar to interpretation

00:02:36.520 --> 00:02:41.720
to Pierson's r. But it involves one outcome or response variable and more than

00:02:41.720 --> 00:02:46.310
one predictor variable. This tells us the strength of the relationship between

00:02:46.310 --> 00:02:51.585
y and the combined set of predictors. Usually we're more interested in r

00:02:51.585 --> 00:02:57.020
squared, which tells us the proportion of variability in y explained by our set

00:02:57.020 --> 00:02:57.960
of predictors.

