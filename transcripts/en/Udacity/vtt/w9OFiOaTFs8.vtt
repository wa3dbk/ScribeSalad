WEBVTT
Kind: captions
Language: en

00:00:00.470 --> 00:00:04.530
Alright so we're now in a great position to talk about what the network part of

00:00:04.530 --> 00:00:06.530
the neural network is about. So now the

00:00:06.530 --> 00:00:09.720
idea is that we can construct using exactly these

00:00:09.720 --> 00:00:14.520
kind of sigmoid units, a chain of relationships between the input

00:00:14.520 --> 00:00:18.940
layer, which are the different components of x, with the output.

00:00:20.870 --> 00:00:25.320
Y, and the way this is going to happen is, there's u,

00:00:25.320 --> 00:00:27.800
other layers of, of units in between. That each

00:00:27.800 --> 00:00:31.720
one is computing the weighted sum, signoided, of the

00:00:31.720 --> 00:00:35.800
layer before it. These other layers of units are

00:00:35.800 --> 00:00:38.060
often referred to as hidden layers, because you can kind

00:00:38.060 --> 00:00:39.850
of see the inputs, you can see the outputs.

00:00:39.850 --> 00:00:42.850
This, this other stuff is, is less constrained. Or indirectly

00:00:42.850 --> 00:00:45.750
constrained. And what's happening is that each of these

00:00:45.750 --> 00:00:50.180
units, it's, it's running exactly that kind of, you know,

00:00:50.180 --> 00:00:53.020
take the weights, multiply by the things coming into it,

00:00:53.020 --> 00:00:55.090
put it through the sigmoid and that's your activation, that's your

00:00:55.090 --> 00:00:58.880
output. So, so what's cool about this is, in the case

00:00:58.880 --> 00:01:02.220
where all these are sigmoid units this mapping from input to

00:01:02.220 --> 00:01:07.250
output. Is differentiable in terms of the weights, and by saying

00:01:07.250 --> 00:01:10.530
the whole thing is differentiable, what I'm saying is that we

00:01:10.530 --> 00:01:13.320
can figure out for any given weight in the network how

00:01:13.320 --> 00:01:15.270
moving it up or down a little bit is going to

00:01:15.270 --> 00:01:17.190
change the mapping from inputs to outputs. So we can

00:01:17.190 --> 00:01:19.600
move all those weights in the direction of producing something more

00:01:19.600 --> 00:01:21.710
like the output that we want. Even though that there's

00:01:21.710 --> 00:01:26.740
all these sort of crazy non linearities in between. And so,

00:01:26.740 --> 00:01:30.440
this leads to an idea called back propagation, which is

00:01:30.440 --> 00:01:34.380
really just at its heart, a computationally beneficial organization of the

00:01:34.380 --> 00:01:37.640
chain rule. We're just computing the derivatives with respect to

00:01:37.640 --> 00:01:40.690
all the different weights in the network, all in one convenient

00:01:40.690 --> 00:01:45.500
way, that has, this, this lovely interpretation of having information flowing

00:01:45.500 --> 00:01:48.900
from the inputs to the outputs. And then error information flowing

00:01:48.900 --> 00:01:52.040
back from the outputs towards the inputs, and that tells you

00:01:52.040 --> 00:01:54.740
how to compute all the derivatives. And then, therefore how to make

00:01:54.740 --> 00:01:58.050
all the weight updates to make, the network produce something more

00:01:58.050 --> 00:02:00.020
like what you wanted it to produce. So this is where

00:02:00.020 --> 00:02:03.340
learning is actually taking place, and it's really neat! You know,

00:02:03.340 --> 00:02:05.770
this back propagation is referring to the fact that the errors are

00:02:05.770 --> 00:02:08.910
flowing backwards. Sometimes it is even called error back propagation.

00:02:08.910 --> 00:02:16.480
&gt;&gt; Nice, so here's a question for you Michael. What happens if I replace

00:02:16.480 --> 00:02:20.880
the sigmoid units with some other function and, and let's say that function is

00:02:20.880 --> 00:02:23.970
also different Well, if it's differentiable, then

00:02:23.970 --> 00:02:25.490
we can still do this, this basic

00:02:25.490 --> 00:02:28.130
kind of trick that says we can

00:02:28.130 --> 00:02:30.900
compute derivatives, and therefore we can move weights

00:02:30.900 --> 00:02:34.220
around to try to get the network to produce what we want it to produce.

00:02:34.220 --> 00:02:37.270
&gt;&gt; Hmm. That's a big win. Does it still act like a preceptron?

00:02:37.270 --> 00:02:39.250
&gt;&gt; Well, even this doesn't act exactly

00:02:39.250 --> 00:02:41.340
like a preceptron, right? So it's really just

00:02:41.340 --> 00:02:43.370
analogous to a preceptron, because we're not really

00:02:43.370 --> 00:02:45.900
doing the hard thresholding, we don't have guarantees

00:02:45.900 --> 00:02:48.370
of, of convergence in finite time. In

00:02:48.370 --> 00:02:51.030
fact, the error function can have many local

00:02:51.030 --> 00:02:56.030
optima, and what, what we mean by that is this idea that we're trying to get

00:02:56.030 --> 00:02:57.750
the, we're trying to set the weight so that the error

00:02:57.750 --> 00:03:01.010
is low, but you can get to these situations where none of

00:03:01.010 --> 00:03:04.060
the weights can really change without making the error worse. And you'd

00:03:04.060 --> 00:03:06.350
like to think, well good, then we're done, we've made the error

00:03:06.350 --> 00:03:08.090
as low as we can make it, but in fact it

00:03:08.090 --> 00:03:10.920
could actually just be stuck in a local optima, that there's a

00:03:10.920 --> 00:03:13.320
much better way of setting the weights It's just we have to

00:03:13.320 --> 00:03:16.220
change more than just one weight at a time to get there.

00:03:16.220 --> 00:03:17.960
&gt;&gt; Oh so that makes sense, so if we think about

00:03:17.960 --> 00:03:21.450
sigmoid the sigmoid and the error function that we picked right.

00:03:21.450 --> 00:03:24.800
The error function was sum of squared airs,

00:03:24.800 --> 00:03:26.660
so that looks like a porabola in some

00:03:26.660 --> 00:03:29.310
high dimensional space, but once we start combining

00:03:29.310 --> 00:03:31.430
them with others like this over, over, and over

00:03:31.430 --> 00:03:35.860
again Then we have an error space where there may be lots of places that look

00:03:35.860 --> 00:03:37.610
low but only look low if you're standing

00:03:37.610 --> 00:03:40.870
there but globally would not be the lowest point.

00:03:40.870 --> 00:03:43.040
&gt;&gt; Right, exactly right and so you can get these

00:03:43.040 --> 00:03:46.580
situations in just the one unit version where the error

00:03:46.580 --> 00:03:48.920
function as you said is this nice little parabola and you can

00:03:48.920 --> 00:03:51.010
move down the gradient and when you get down to the bottom you're

00:03:51.010 --> 00:03:54.200
done. But now when we start throwing these networks of units together we

00:03:54.200 --> 00:03:57.170
can get an error surface that looks just in its cartoon form looks

00:03:57.170 --> 00:04:00.550
crazy like this, that there's, it's smooth but there's these Place where

00:04:00.550 --> 00:04:03.430
it goes down, comes up again and goes down maybe further, comes up

00:04:03.430 --> 00:04:07.160
again and doesn't come down as far and you could easily get yourself

00:04:07.160 --> 00:04:12.340
stuck at a point like this where you're not at the global minimum.

00:04:12.340 --> 00:04:13.630
Your at some local optimum.

