WEBVTT
Kind: captions
Language: en

00:00:00.710 --> 00:00:01.710
So, this is going to lead us to

00:00:01.710 --> 00:00:04.575
the concept of expectation maximization. So, expectation

00:00:04.575 --> 00:00:06.553
maximization is actually, at an algorithmic level,

00:00:06.553 --> 00:00:08.610
it's surprisingly similar to K means. So, what

00:00:08.610 --> 00:00:10.365
we are going to do is, we're going to

00:00:10.365 --> 00:00:12.520
tick-tock back and forth between 2 different

00:00:12.520 --> 00:00:14.830
probabilistic calculations. So, you see that? I

00:00:14.830 --> 00:00:15.890
kind of drew it like the other one.

00:00:15.890 --> 00:00:20.351
&gt;&gt; Mm Hm. The names of the 2 phases are expectation,

00:00:20.351 --> 00:00:24.680
and maximization. Sort of you know, our name is our algorithim.

00:00:24.680 --> 00:00:25.430
&gt;&gt; I like that.

00:00:26.490 --> 00:00:28.240
&gt;&gt; So, what they're going to do is, we're going to

00:00:28.240 --> 00:00:32.400
move back and forth between a soft clustering, and computing

00:00:32.400 --> 00:00:34.160
the means from the soft cluster. So the soft

00:00:34.160 --> 00:00:37.270
clustering goes like this. This probablisitic indicator variable, Z I

00:00:37.270 --> 00:00:43.290
J. Represents the likelihood that data element I comes

00:00:43.290 --> 00:00:45.850
from cluster J. And so, the way we're going to do

00:00:45.850 --> 00:00:48.440
that, since we're in the maximum likelihood setting, is to

00:00:48.440 --> 00:00:51.550
use Bayes' rule, and say, well, that's going to be proportional

00:00:51.550 --> 00:00:54.440
to the probability that data element I was

00:00:54.440 --> 00:00:56.460
produced by cluster J. And then we have a

00:00:56.460 --> 00:00:58.780
normalization factor. Normally, we'd also have the prior

00:00:58.780 --> 00:01:00.600
in there. So why is the prior gone Charles?

00:01:00.600 --> 00:01:03.880
&gt;&gt; Well, because you said it was the maximum likelihood. Scenario.

00:01:03.880 --> 00:01:05.420
&gt;&gt; Yeah, right. We talked about how that

00:01:05.420 --> 00:01:06.910
just meant that it was uniform and that

00:01:06.910 --> 00:01:08.730
allowed us to just leave that component out.

00:01:08.730 --> 00:01:10.520
It's not going to have any impact on the normalization.

00:01:10.520 --> 00:01:11.250
&gt;&gt; Right.

00:01:11.250 --> 00:01:13.030
&gt;&gt; So that's what the Z step is. Is if we had

00:01:13.030 --> 00:01:16.600
the clusters, if we knew where the means were, then we could compute

00:01:16.600 --> 00:01:18.210
how likely it is that the data would come from

00:01:18.210 --> 00:01:20.680
the means, and that's just this calculation here. So that's computing

00:01:20.680 --> 00:01:24.130
the expectation. Defining the Z varaibles from the muse. The centers.

00:01:24.130 --> 00:01:28.044
We're going to pass that information. That clustering information, Z, over to

00:01:28.044 --> 00:01:31.560
the maximization step. What the maximization is going to say is,

00:01:31.560 --> 00:01:33.780
okay, well if that's the clustering, we can compute the means

00:01:33.780 --> 00:01:36.550
from those clusters. All we have to do is just take

00:01:36.550 --> 00:01:41.720
the average variable value. Right? So the average of the Xi's.

00:01:41.720 --> 00:01:45.008
Within each cluster J. What's the likelihood it came from cluster J and

00:01:45.008 --> 00:01:48.370
then again, we have to normalize. If you think of this as being

00:01:48.370 --> 00:01:51.810
a 0 1 indicator variable, then really it is just the average of

00:01:51.810 --> 00:01:54.880
the things we assign to that cluster. But here, we actually are kind

00:01:54.880 --> 00:01:57.560
of soft assigning, so we could have half of one of the data

00:01:57.560 --> 00:02:00.060
points in there, and it only counts half towards the average, and we

00:02:00.060 --> 00:02:03.950
could have a tenth in another place, and a whole value in another

00:02:03.950 --> 00:02:06.610
place, and so we're just doing this weighted average of the data points.

00:02:06.610 --> 00:02:07.620
&gt;&gt; So,

00:02:07.620 --> 00:02:08.850
can I ask you a question, Michael?

00:02:08.850 --> 00:02:09.620
&gt;&gt; Yeah, shoot.

00:02:09.620 --> 00:02:10.860
&gt;&gt; So, this makes sense to me, and I, and I

00:02:10.860 --> 00:02:14.630
even get that for the Gaussian case, the z i variable will

00:02:14.630 --> 00:02:17.340
always be non 0 in the end, because there's always some

00:02:17.340 --> 00:02:21.350
probability. They come from some Gaussian because they have infinite extent. So

00:02:21.350 --> 00:02:23.510
I, this all makes sense to me. Is there a way

00:02:23.510 --> 00:02:26.740
to take exactly this algorithm and turn it into k means? I'm

00:02:26.740 --> 00:02:29.280
staring at it, and it feels like if all your probabilities

00:02:29.280 --> 00:02:33.120
were ones and zeroes, you would end up with exactly k means.

00:02:33.120 --> 00:02:33.440
I think.

00:02:33.440 --> 00:02:35.970
&gt;&gt; I dunno, I never really thought about that. Let's

00:02:35.970 --> 00:02:37.830
think about that for a moment. Certainly, the case, if

00:02:37.830 --> 00:02:40.750
all the z variables were 0, 1, then the maximization

00:02:40.750 --> 00:02:43.290
set would be the means, which is what k means does.

00:02:43.290 --> 00:02:48.080
&gt;&gt; Mm-hm. Then, what would happen? We send these means back, and what we do

00:02:48.080 --> 00:02:53.760
in k-means is we say, each data point belongs to it's closest center.

00:02:53.760 --> 00:02:54.541
&gt;&gt; Mm-hm.

00:02:54.541 --> 00:02:58.380
&gt;&gt; Which is very similar actually to what this does. Except that

00:02:58.380 --> 00:03:01.030
here we then make it proportional. So I guess it would

00:03:01.030 --> 00:03:04.668
exactly that if we made these clustering assignments, pushed them to

00:03:04.668 --> 00:03:08.090
0 or 1 depending on which was the most likely cluster.

00:03:08.090 --> 00:03:10.730
Right, so if you made it so that the probability of you

00:03:10.730 --> 00:03:14.470
being to a cluster actually depends upon all the clusters, and

00:03:14.470 --> 00:03:16.500
you always got a 1 over 0. Basically you did, this was

00:03:16.500 --> 00:03:18.550
like a hidden argmax kind of a thing, or a

00:03:18.550 --> 00:03:23.241
hidden max or something. Then you would end up with exactly k-means.

00:03:23.241 --> 00:03:23.570
&gt;&gt; I think

00:03:23.570 --> 00:03:23.860
you're right.

00:03:23.860 --> 00:03:24.250
&gt;&gt; Huh.

00:03:24.250 --> 00:03:25.150
&gt;&gt; Yeah, I never thought about that.

00:03:25.150 --> 00:03:25.880
&gt;&gt; Okay.

00:03:25.880 --> 00:03:27.924
&gt;&gt; So it really does end up being an awful lot like

00:03:27.924 --> 00:03:31.910
the k-means algorithm, which is improving

00:03:31.910 --> 00:03:33.690
in the error metric, this squared error

00:03:33.690 --> 00:03:37.270
metric. This is actually going to be improving in a probabilistic metric, right.

00:03:37.270 --> 00:03:40.210
The, the data is going to be more and more likely over time.

00:03:40.210 --> 00:03:41.230
&gt;&gt; That makes sense.

