WEBVTT
Kind: captions
Language: en

00:00:00.830 --> 00:00:02.240
Here's an example.

00:00:02.240 --> 00:00:05.740
Imagine your network is
a stack of simple operations.

00:00:05.740 --> 00:00:09.020
Like in your transforms,
whatever you want.

00:00:10.110 --> 00:00:15.990
Some have parameters like the matrix
transforms, some don't like the rellers.

00:00:15.990 --> 00:00:18.040
When you apply your
data to some input x,

00:00:18.040 --> 00:00:21.790
you have data flowing through
the stack up to your predictions y.

00:00:21.790 --> 00:00:25.820
To compute the derivatives, you create
another graph that looks like this.

00:00:25.820 --> 00:00:30.220
The data in the new graph flows
backwards through the network,

00:00:30.220 --> 00:00:35.000
get's combined using the chain rule that
we saw before and produces gradients.

00:00:35.000 --> 00:00:38.350
That graph can be derived completely
automatically from the individual

00:00:38.350 --> 00:00:40.260
operations in your network.

00:00:40.260 --> 00:00:43.348
So most deep learning frameworks
will just do it for you.

00:00:43.348 --> 00:00:48.200
This is called back-propagation,
and it's a very powerful concept.

00:00:48.200 --> 00:00:51.770
It makes computing derivatives of
complex function very efficient

00:00:51.770 --> 00:00:56.250
as long as the function is made up of
simple blocks with simple derivatives.

00:00:56.250 --> 00:01:00.910
Running the model up to the predictions
is often call the forward prop, and

00:01:00.910 --> 00:01:04.190
the model that goes backwards
is called the back prop.

00:01:04.190 --> 00:01:07.290
So, to recap,
to run stochastic gradient descent,

00:01:07.290 --> 00:01:11.220
for every single little batch of
your data in your training set,

00:01:11.220 --> 00:01:14.900
you're going to run the forward prop,
and then the back prop.

00:01:14.900 --> 00:01:19.270
And that will give you gradients for
each of your weights in your model.

00:01:19.270 --> 00:01:21.950
Then you're going to apply those
gradients with the learning weights

00:01:21.950 --> 00:01:24.710
to your original weights,
and update them.

00:01:24.710 --> 00:01:28.200
And you're going to repeat that
all over again, many, many times.

00:01:28.200 --> 00:01:31.610
This is how your entire
model gets optimized.

00:01:31.610 --> 00:01:34.610
I am not going to go through more
of the maths of what's going on in

00:01:34.610 --> 00:01:35.910
each of those blocks.

00:01:35.910 --> 00:01:38.780
Because, again, you don't typically
have to worry about that, and

00:01:38.780 --> 00:01:42.270
it's essentially the chain rule,
but keep in mind, this diagram.

00:01:42.270 --> 00:01:45.730
In particular each block of
the back prop often takes about

00:01:45.730 --> 00:01:50.395
twice the memory that's needed for
prop and twice the compute.

00:01:50.395 --> 00:01:52.520
That's important when you
want to size your model and

00:01:52.520 --> 00:01:53.940
fit it in memory for example.

