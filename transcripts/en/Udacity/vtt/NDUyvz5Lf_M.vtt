WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.000
All right. Let's try to work out the math.

00:00:02.000 --> 00:00:07.000
So what we have is, by assumption, we can run 1 test every 10 microseconds.

00:00:07.000 --> 00:00:11.000
Now, there are a million microseconds in a second, 60 seconds in a minute,

00:00:11.000 --> 00:00:15.000
and 60 minutes in an hour, and 24 hours in a day.

00:00:15.000 --> 00:00:17.000
So now we're going to do some unit cancellation.

00:00:17.000 --> 00:00:22.000
We can kill microseconds, we can kill minutes, we can kill seconds,

00:00:22.000 --> 00:00:24.000
and we can kill hours.

00:00:24.000 --> 00:00:27.000
So if we do the multiplication, we can get tests per day.

00:00:27.000 --> 00:00:33.000
And if we do that, we get 8,640,000,000 tests per day.

00:00:33.000 --> 00:00:36.000
If we multiply this testing throughput by the failure rate

00:00:36.000 --> 00:00:41.000
we're going to get 1 failure per 9 billion tests.

00:00:41.000 --> 00:00:48.000
We can cancel tests, do the division, and arrive at 0.96 expected failures per day.

00:00:48.000 --> 00:00:50.000
So under what I think are fairly modest assumptions here,

00:00:50.000 --> 00:00:53.000
if we perform completely random testing of the input space for fdiv,

00:00:53.000 --> 00:00:55.000
we should be able to find this bug in about a day.

00:00:55.000 --> 00:00:58.000
And so now this kind of testing is going to need some sort of an oracle.

00:00:58.000 --> 00:01:03.000
So we're going to need a way to tell if our particular output from fdiv is correct or not.

00:01:03.000 --> 00:01:06.000
And the way this is going to work is IEEE floating point,

00:01:06.000 --> 00:01:09.000
which is what fdiv is implementing, is specified very tightly.

00:01:09.000 --> 00:01:12.000
That is to say, one implementation of IEEE floating point division

00:01:12.000 --> 00:01:15.000
has to return the same bit pattern as another implementation.

00:01:15.000 --> 00:01:18.000
That's one of the nice things about that particular specification is that it's fairly tight.

00:01:18.000 --> 00:01:20.000
So we ask ourselves, what would have been the oracle for fdiv?

00:01:20.000 --> 00:01:26.000
And probably it would have been Intel's existing 487 floating point unit,

00:01:26.000 --> 00:01:29.000
which had been around for some years by the time they were developing the Pentium.

00:01:29.000 --> 00:01:32.000
So what I think this shows, unless I've messed up sort of egregiously on the math somewhere,

00:01:32.000 --> 00:01:35.000
is that random testing would have been a perfectly good way

00:01:35.000 --> 00:01:38.000
to find the Intel Pentium fdiv flaw, presuming, of course,

00:01:38.000 --> 00:01:41.000
that we could have found a way to rig up a Pentium in concert with a 487

00:01:41.000 --> 00:01:44.000
in such a way that they could have cooperated to do the testing.

