WEBVTT
Kind: captions
Language: en

00:00:00.600 --> 00:00:05.540
Some of the challenges for data center networking include traffic load balance,

00:00:05.540 --> 00:00:08.150
support for migrating virtual machines, in

00:00:08.150 --> 00:00:12.090
response to changing demands. Adjusting server

00:00:12.090 --> 00:00:15.320
and traffic placement to save power.

00:00:15.320 --> 00:00:19.270
Provisioning the network, when demands fluctuate,

00:00:19.270 --> 00:00:22.940
and providing various security guarantees, particularly

00:00:22.940 --> 00:00:25.790
in scenarios that involve multiple tenants.

00:00:25.790 --> 00:00:31.070
To understand these challenges, in a bit more detail, let's take a look at

00:00:31.070 --> 00:00:34.079
a typical data center topology. A topology

00:00:34.079 --> 00:00:37.274
typically has three layers: an access layer, which

00:00:37.274 --> 00:00:41.590
connect the servers themselves. An aggregation layer

00:00:41.590 --> 00:00:44.640
which connects the access layer, and then the

00:00:44.640 --> 00:00:48.180
core. Historically, the core of the network

00:00:48.180 --> 00:00:51.700
has been connected with layer three, but increasingly,

00:00:51.700 --> 00:00:54.320
modern data centers are connected with

00:00:54.320 --> 00:00:58.900
an entire layer-two topology. A layer-two topology

00:00:58.900 --> 00:01:04.730
makes it easier to perform migration of services from one part of the topology

00:01:04.730 --> 00:01:11.320
to another, since these services can stay on the same layer-two network and

00:01:11.320 --> 00:01:16.850
hence would not need new IP addresses when they moved. It also becomes easier to

00:01:16.850 --> 00:01:19.220
load balance traffic. On the other hand,

00:01:19.220 --> 00:01:23.970
a monolithic layer-two topology makes scaling difficult, since

00:01:23.970 --> 00:01:30.160
now we have tens of thousands of servers on a single flat topology. In other

00:01:30.160 --> 00:01:33.960
words, layer-two addresses are not topological. So

00:01:33.960 --> 00:01:36.580
the forwarding tables in these switches can't scale

00:01:36.580 --> 00:01:39.180
as easily, because they can't take advantage of

00:01:39.180 --> 00:01:42.560
the natural hierarchy that exists in the topology.

00:01:42.560 --> 00:01:44.838
Other problems that exist in this type of

00:01:44.838 --> 00:01:48.456
topology, is that the hierarchy can potentially create single

00:01:48.456 --> 00:01:51.560
points of failure. And links at the top of

00:01:51.560 --> 00:01:55.550
the topology, in the core, can be come oversubscribed.

00:01:55.550 --> 00:01:59.130
Modern data center operators have observed that as you

00:01:59.130 --> 00:02:02.500
move from the bottom of the hierarchy up towards

00:02:02.500 --> 00:02:04.790
the core, that the links at the top can

00:02:04.790 --> 00:02:08.060
carry as much as 200 times as much traffic

00:02:08.060 --> 00:02:09.258
as the links at the bottom of

00:02:09.258 --> 00:02:13.340
the hierarchy. So there's a serious capacity mismatch

00:02:13.340 --> 00:02:17.980
in that the top part of the topology has to carry a whole lot more

00:02:17.980 --> 00:02:20.980
traffic than the bottom. We'll explore how

00:02:20.980 --> 00:02:24.060
modern data center network architectures address these various

00:02:24.060 --> 00:02:26.880
challenges, but let's first take a quick look

00:02:26.880 --> 00:02:29.440
at one way of solving the scale problem.

