WEBVTT
Kind: captions
Language: en

00:00:00.360 --> 00:00:03.227
Now that you've seen what
a simple convnet looks like,

00:00:03.227 --> 00:00:05.794
there are many things that
we can do to improve it.

00:00:05.794 --> 00:00:10.112
We're going to talk about three of them,
pooling, one by one convolutions and

00:00:10.112 --> 00:00:13.821
something a bit more advanced
called the inception architecture.

00:00:13.821 --> 00:00:17.740
The first improvement is a better way
to reduce the spatial extent of your

00:00:17.740 --> 00:00:20.790
feature maps in
the convolutional pyramid.

00:00:20.790 --> 00:00:25.540
Until now, we've used striding to shift
the filters by a few pixel each time and

00:00:25.540 --> 00:00:27.570
reduce the future map size.

00:00:27.570 --> 00:00:30.860
This is a very aggressive
way to downsample an image.

00:00:30.860 --> 00:00:32.280
It removes a lot of information.

00:00:33.600 --> 00:00:36.660
What if instead of skipping
one in every two convolutions,

00:00:36.660 --> 00:00:40.690
we still ran with a very small stride,
say for example one.

00:00:40.690 --> 00:00:44.980
But then took all the convolutions in a
neighborhood and combined them somehow.

00:00:46.540 --> 00:00:50.780
That operation is called pooling, and
there are a few ways to go about it.

00:00:50.780 --> 00:00:52.850
The most common is max pooling.

00:00:52.850 --> 00:00:56.500
At every point in the future map,
look at a small neighborhood around that

00:00:56.500 --> 00:01:00.910
point and compute the maximum
of all the responses around it.

00:01:00.910 --> 00:01:03.800
There are some advantages
to using max pooling.

00:01:03.800 --> 00:01:06.400
First, it doesn't add to
your number of parameters.

00:01:06.400 --> 00:01:08.160
So you don't risk
an increasing over fitting.

00:01:09.190 --> 00:01:11.660
Second, it simply often
yields more accurate models.

00:01:12.880 --> 00:01:16.900
However, since the convolutions that
run below run at a lower stride,

00:01:16.900 --> 00:01:19.290
the model then becomes a lot
more expensive to compute.

00:01:20.400 --> 00:01:23.480
And now you have even more hyper
parameters to worry about.

00:01:23.480 --> 00:01:26.850
The pooling region size, and
the pooling stride, and no,

00:01:26.850 --> 00:01:27.880
they don't have to be the same.

00:01:28.910 --> 00:01:30.480
A very typical architecture for

00:01:30.480 --> 00:01:33.910
a covenant is a few layers
alternating convolutions and

00:01:33.910 --> 00:01:37.660
max pooling, followed by a few
fully connected layers at the top.

00:01:38.670 --> 00:01:43.472
The first famous model to use this
architecture was LENET-5 designed by

00:01:43.472 --> 00:01:47.356
Yann Lecun to the character
recognition back in 1998.

00:01:47.356 --> 00:01:50.276
Modern convolutional networks
such as ALEXNET, which

00:01:50.276 --> 00:01:54.687
famously won the competitive ImageNet
object recognition challenge in 2012,

00:01:54.687 --> 00:01:57.559
used a very similar architecture
with a few wrinkles.

00:01:58.810 --> 00:02:02.250
Another notable form of
pooling is average pooling.

00:02:02.250 --> 00:02:04.020
Instead of taking the max,

00:02:04.020 --> 00:02:08.970
just take an average over the window
of pixels around a specific location.

00:02:08.970 --> 00:02:12.423
It's a little bit like providing
a blurred low resolution view of

00:02:12.423 --> 00:02:13.694
the feature map below.

00:02:13.694 --> 00:02:15.500
We're going to take
advantage of that shortly.

