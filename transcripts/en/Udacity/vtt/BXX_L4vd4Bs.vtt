WEBVTT
Kind: captions
Language: en

00:00:00.200 --> 00:00:02.080
Ok, Michael, so I'm going to talk a little bit

00:00:02.080 --> 00:00:05.525
about bias. In particular, the preference bias for K

00:00:05.525 --> 00:00:08.950
[INAUDIBLE]. So, let me remind you what preference bias

00:00:08.950 --> 00:00:13.220
is. Preference bias is kind of our notion of

00:00:13.220 --> 00:00:16.700
why we would prefer one hypothesis over another. And

00:00:16.700 --> 00:00:19.000
they say all things, other things being equal. And

00:00:19.000 --> 00:00:21.950
what that really means is, it's the thing that

00:00:21.950 --> 00:00:26.250
encompasses our belief. About what makes a good hypothesis.

00:00:26.250 --> 00:00:27.570
So in some of the previous examples that

00:00:27.570 --> 00:00:30.790
we used it was things like shorter trees,

00:00:30.790 --> 00:00:36.518
smoother functions, simpler functions, Occam's Razor, those sorts of

00:00:36.518 --> 00:00:39.793
things were the ways that we expressed our preferences

00:00:39.793 --> 00:00:43.410
over various hypothesis. And kNN is no exception.

00:00:43.410 --> 00:00:45.070
It also has preference by its built in

00:00:45.070 --> 00:00:51.280
as does every algorithm of any note. So I just wanted to go through three that I

00:00:51.280 --> 00:00:54.770
thought of is as being indicitive of this bias. And they're,

00:00:54.770 --> 00:00:57.030
kind of all related to one another. So the first one

00:00:57.030 --> 00:01:01.460
is a notion of locality. Right? There's this idea that near

00:01:01.460 --> 00:01:04.650
points are similiar to one another. Does that make sense to you?

00:01:04.650 --> 00:01:07.150
&gt;&gt; Yeah. Yeah. That was really important. It came out

00:01:07.150 --> 00:01:09.950
nicely in the the real estate example. Right. So the

00:01:09.950 --> 00:01:12.610
whole idea. The whole thing we are using to generalize

00:01:12.610 --> 00:01:15.030
from one thing to another is this notion of nearness.

00:01:15.030 --> 00:01:16.580
&gt;&gt; Right. And exactly

00:01:16.580 --> 00:01:18.770
how this notion of nearness works out.

00:01:18.770 --> 00:01:20.910
Is embedded in whatever distance function we happen

00:01:20.910 --> 00:01:22.360
to be given. And so, there's further

00:01:22.360 --> 00:01:25.110
bias that might come out, based upon exactly

00:01:25.110 --> 00:01:26.780
the way we implement distances. So, in

00:01:26.780 --> 00:01:29.630
the example we just did, euclidian distance is

00:01:29.630 --> 00:01:31.820
making a different assumption about what nearness or

00:01:31.820 --> 00:01:34.730
similarity is, compared to Manhattan distance, for example.

00:01:34.730 --> 00:01:37.550
&gt;&gt; So is there like, a perfect distance function for a given problem?

00:01:37.550 --> 00:01:41.970
&gt;&gt; There's certainly a perfect distance function for any particular problem.

00:01:41.970 --> 00:01:44.250
&gt;&gt; Yeah, that's what I mean. Not one that works for the

00:01:44.250 --> 00:01:48.380
universe, but one, like, you know, if I give you a problem and

00:01:48.380 --> 00:01:50.600
you can work on it all day long. Can you find, is

00:01:50.600 --> 00:01:51.720
there a notion that there''s a

00:01:51.720 --> 00:01:54.720
distance function that would capture things perfectly?

00:01:54.720 --> 00:01:59.670
&gt;&gt; Well, it has to be the case for any given fixed problem. That there

00:01:59.670 --> 00:02:04.260
is some distance function that minimizes, say,

00:02:04.260 --> 00:02:07.580
sum of squared errors or something like that.

00:02:07.580 --> 00:02:09.680
First is some other distance function. Right?

00:02:09.680 --> 00:02:10.590
&gt;&gt; Okay.

00:02:10.590 --> 00:02:12.510
&gt;&gt; That has to be the case. So there, there always to

00:02:12.510 --> 00:02:16.590
be at least one best distance function given everything else is fixed.

00:02:16.590 --> 00:02:17.890
&gt;&gt; That makes sense.

00:02:17.890 --> 00:02:20.400
&gt;&gt; Right. Now, what that is, who

00:02:20.400 --> 00:02:24.690
knows. Maybe you finding it might be. Arbitrarily

00:02:24.690 --> 00:02:27.560
difficult. Because there's at least an infinite, there's

00:02:27.560 --> 00:02:29.915
at least an infinite number of distance functions.

00:02:29.915 --> 00:02:32.780
&gt;&gt; Well yeah, I was thinking that, that for latter to find distance functions

00:02:32.780 --> 00:02:36.350
to be anything we want. What about a distance function that said

00:02:36.350 --> 00:02:40.090
the distance between all the things that have the same answer, is zero.

00:02:40.090 --> 00:02:40.643
&gt;&gt; Mm-hm.

00:02:40.643 --> 00:02:42.180
&gt;&gt; And the distance between them and the ones that

00:02:42.180 --> 00:02:45.090
have different answers is you know, infinity or something big.

00:02:45.090 --> 00:02:45.310
&gt;&gt; Yeah.

00:02:45.310 --> 00:02:47.670
&gt;&gt; And then, then the distance function,

00:02:47.670 --> 00:02:49.680
like, somehow already has built in the solution

00:02:49.680 --> 00:02:51.350
to the problem because it's already put

00:02:51.350 --> 00:02:52.850
the things that have the same answers together.

00:02:52.850 --> 00:02:54.820
&gt;&gt; Right, you could do that, and of course, doing

00:02:54.820 --> 00:02:58.650
that would require again solving the original problem. But yeah, so.

00:02:58.650 --> 00:03:01.240
So, such a function has to exist, or, well, you

00:03:01.240 --> 00:03:03.650
know, there's always noise. What if there's noise in your data,

00:03:03.650 --> 00:03:05.870
you know? But some such function like that has to

00:03:05.870 --> 00:03:08.930
exist, the question is finding it. But I think the real

00:03:08.930 --> 00:03:11.150
point to take there is, there are some good distance

00:03:11.150 --> 00:03:13.460
functions for our problem and there are some bad distance functions

00:03:13.460 --> 00:03:17.010
for our problem. And how you pick those is really fundamental

00:03:17.010 --> 00:03:19.630
assumption your making about the domain. That's why it's domain knowledge.

00:03:19.630 --> 00:03:21.600
&gt;&gt; Yeah, that sounds right.

00:03:21.600 --> 00:03:23.780
&gt;&gt; Mm 'Kay. So, locality however it's expressed

00:03:23.780 --> 00:03:26.910
to the distance function, that is similarity. It's built

00:03:26.910 --> 00:03:29.244
in to kNN that we believe that near points

00:03:29.244 --> 00:03:32.290
are similar. Kind of by definition. That leads actually

00:03:32.290 --> 00:03:34.270
to the second preference bias which is this notion of

00:03:34.270 --> 00:03:39.870
smoothness. Alright. That we are by choosing to average.

00:03:39.870 --> 00:03:42.130
And by choosing to look at points that are similar

00:03:42.130 --> 00:03:45.070
to one another. We are expecting functions to behave,

00:03:45.070 --> 00:03:48.730
smoothly. Alright, so, you know, in the 2D case.

00:03:48.730 --> 00:03:52.530
It's, it's kind of easy to see, right? You, you, you, you have these,

00:03:52.530 --> 00:03:55.070
these sort of points and you're basically

00:03:55.070 --> 00:03:57.470
saying, look, these two points should somehow be

00:03:57.470 --> 00:04:02.600
related to one another more than this point and this point. And that sort

00:04:02.600 --> 00:04:06.370
of assumes kind of smoothly changing behavior

00:04:06.370 --> 00:04:08.560
as you move from one neighborhood to another.

00:04:11.390 --> 00:04:11.790
Does that make sense?

00:04:11.790 --> 00:04:15.240
&gt;&gt; I mean, it seems like we're defining to be pretty similar to locality.

00:04:15.240 --> 00:04:19.640
&gt;&gt; In this case. So I'm, I'm drawing an example, such that, you know,

00:04:19.640 --> 00:04:22.750
whatever we meant by locality has already been kind of expressed in the graph.

00:04:22.750 --> 00:04:23.450
&gt;&gt; Okay.

00:04:23.450 --> 00:04:25.940
&gt;&gt; And you know, by picking, you know, this is really for pedagogical

00:04:25.940 --> 00:04:28.725
reasons. You know can imagine, this you know, these are points that live in

00:04:28.725 --> 00:04:31.480
77,000 dimensions, and it's impossible to actually

00:04:31.480 --> 00:04:33.090
visualize them much less draw them. And

00:04:33.090 --> 00:04:34.510
I could try. [LAUGH] But here's, here's

00:04:34.510 --> 00:04:36.910
three dimensions and here's the fourth dimension.

00:04:37.920 --> 00:04:40.250
I think I'm going to get tired before I hit seven and seven thousand

00:04:40.250 --> 00:04:43.270
but, you know, you kind of, you kind of get the idea, right? That, if. In,

00:04:43.270 --> 00:04:45.450
you know, if you can imagine in your head points that are really

00:04:45.450 --> 00:04:47.590
near one another in some space, you

00:04:47.590 --> 00:04:50.270
kind of hope that they behave similarly. Right.

00:04:50.270 --> 00:04:53.350
&gt;&gt; Right. Okay, so locality and smoothness. And I think these make sense. I

00:04:53.350 --> 00:04:54.460
mean, these, this is hardly the only

00:04:54.460 --> 00:04:56.100
algorithm that makes these kind of assumptions.

00:04:56.100 --> 00:04:59.810
But there is another assumption which is a bit more subtle I think. Which

00:04:59.810 --> 00:05:03.370
is worth spending a second talking about, which is, for at least the distance

00:05:03.370 --> 00:05:07.520
functions we've looked at before. The Euclidian distance and

00:05:07.520 --> 00:05:11.500
the Manhattan distance. They all kind of looked at each

00:05:11.500 --> 00:05:14.600
of the dimensions, sort of, and subtracted them, and

00:05:14.600 --> 00:05:16.780
squared them, or didn't, or took their absolute value and

00:05:16.780 --> 00:05:19.810
added them all together. What that means is, we

00:05:19.810 --> 00:05:22.060
were treating, at least in those cases, that all the

00:05:22.060 --> 00:05:24.320
features mattered. And not only did they matter, they

00:05:24.320 --> 00:05:28.530
mattered equally. Right. So think about the the last quiz

00:05:28.530 --> 00:05:34.500
I gave you. Right. It said y equals x 1 squared plus x 2. And you noticed we

00:05:34.500 --> 00:05:36.320
got answers that were wildly off from what the

00:05:36.320 --> 00:05:38.470
actual answer was. Well if I know that the

00:05:38.470 --> 00:05:41.890
first dimension. The first feature is going to be squared

00:05:41.890 --> 00:05:43.950
and the second one is not going to be

00:05:43.950 --> 00:05:47.060
squared. Do you think either one of these features

00:05:47.060 --> 00:05:48.970
is more important or more important to get right?

00:05:50.190 --> 00:05:52.460
&gt;&gt; Okay. Right. Trying to think about what that might mean. So,

00:05:53.800 --> 00:05:57.380
if, yea its definitely the case that when you look

00:05:57.380 --> 00:06:00.110
for similar examples in the database you want to care

00:06:00.110 --> 00:06:03.020
more about X1 because a little bit of a difference

00:06:03.020 --> 00:06:06.580
in X1 gets squared out. Right? It can lead to a

00:06:06.580 --> 00:06:10.910
very large difference in the corresponding Y value. Whereas in

00:06:10.910 --> 00:06:13.790
the x2's, it's not quite as crucial. Th, th, the,

00:06:13.790 --> 00:06:15.830
if you're off a little bit more, then you're off

00:06:15.830 --> 00:06:18.900
a little bit more, it's just a linear relationship. So yeah,

00:06:18.900 --> 00:06:21.360
it does seems like that first dimension needs to be a lot

00:06:21.360 --> 00:06:25.150
more important, I guess, when you're doing the matching. Then the second one.

00:06:25.150 --> 00:06:26.670
&gt;&gt; Right so, we probably would have gotten

00:06:26.670 --> 00:06:27.860
different, I'm not going to go through this but,

00:06:27.860 --> 00:06:31.250
we probably would have gotten different answers if, in

00:06:31.250 --> 00:06:34.370
the Euclidian or Manhattan case we had instead of

00:06:34.370 --> 00:06:37.330
just taking the difference between the first two The

00:06:37.330 --> 00:06:40.000
first dimensions, we had taken that difference and squared

00:06:40.000 --> 00:06:41.810
it. And then in the case, including this and

00:06:41.810 --> 00:06:44.380
squaring it again, and then some of those things

00:06:44.380 --> 00:06:46.020
that were closer in the first dimension instead

00:06:46.020 --> 00:06:48.300
of the second dimension would've looked more similar and

00:06:48.300 --> 00:06:49.980
we might've gotten better answer. That's probably a good

00:06:49.980 --> 00:06:52.265
exercise to go back and do for someone else.

00:06:52.265 --> 00:06:54.150
&gt;&gt; [LAUGH] Yeah, I was thinking of doing it right

00:06:54.150 --> 00:06:56.160
now, but yeah, probably should leave it for other people.

00:06:56.160 --> 00:06:59.500
&gt;&gt; Well you can do it if you want to. So did you do it Michael?

00:06:59.500 --> 00:07:01.096
&gt;&gt; I did.

00:07:01.096 --> 00:07:02.289
&gt;&gt; And?

00:07:02.289 --> 00:07:05.140
&gt;&gt; So it's a kind of now a mix between the Manhattan distance

00:07:05.140 --> 00:07:06.700
and the Euclidian distance. So, I'm taking

00:07:06.700 --> 00:07:09.700
the first component, take the difference, square

00:07:09.700 --> 00:07:09.840
it.

00:07:09.840 --> 00:07:10.255
&gt;&gt; Mm-hm.

00:07:10.255 --> 00:07:12.350
&gt;&gt; Take the second component, take the difference,

00:07:12.350 --> 00:07:14.800
absolute value it. And add those two things together.

00:07:14.800 --> 00:07:15.870
&gt;&gt; Sure.

00:07:15.870 --> 00:07:18.440
&gt;&gt; All right. So if I do that, with one nearest neighbor,

00:07:18.440 --> 00:07:21.280
I still get that tie, but the output answer ends up being 12.

00:07:21.280 --> 00:07:21.780
&gt;&gt; Hm. Which

00:07:23.980 --> 00:07:25.199
is better than 24.7.

00:07:25.199 --> 00:07:27.490
&gt;&gt; And that's better than eight, which is what

00:07:27.490 --> 00:07:29.260
it was before. So the eight has gone up to

00:07:29.260 --> 00:07:32.040
12, which is better than the other one, which I

00:07:32.040 --> 00:07:34.600
think was 35.5, comes down to 29.5 Close here again

00:07:34.600 --> 00:07:37.120
to the correct answer which is eighteen. So in

00:07:37.120 --> 00:07:39.510
both cases it kind of pushed in the right direction,

00:07:39.510 --> 00:07:42.640
it was using more, of the, the answers that were

00:07:42.640 --> 00:07:44.680
relevant and fewer of the answers that were not relevant.

00:07:44.680 --> 00:07:49.140
&gt;&gt; Right. There you go. So the notion of relevance by the way,

00:07:49.140 --> 00:07:54.100
turns out to be very, very important. And highlights a weakness of

00:07:54.100 --> 00:07:58.220
kNN. So this brings me to a kind of theorem or fundamental

00:07:58.220 --> 00:08:03.320
results of a machine learning that is particularly relevant to kNN but

00:08:03.320 --> 00:08:07.060
its actually relevant everywhere. Do you think its worth while to mention it?

00:08:07.060 --> 00:08:08.130
&gt;&gt; Sure it sounds very relevant.

00:08:08.130 --> 00:08:08.790
&gt;&gt; Alright let's do it.

