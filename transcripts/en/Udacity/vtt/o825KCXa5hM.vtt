WEBVTT
Kind: captions
Language: en

00:00:00.130 --> 00:00:02.216
To analyze a cache-oblivious algorithm,

00:00:02.216 --> 00:00:04.420
we're going to need a model
of how a cache works.

00:00:04.420 --> 00:00:06.400
Here's the model I want you to assume,

00:00:06.400 --> 00:00:08.800
which we sometimes call
the ideal cache model.

00:00:08.800 --> 00:00:10.687
Start by assuming that slow and

00:00:10.687 --> 00:00:13.890
fast memory are divided into
blocks of size L words.

00:00:13.890 --> 00:00:16.210
This L is the same as the transfer size.

00:00:16.210 --> 00:00:21.200
We'll refer to one of these blocks in
fast memory or cache as a cache line.

00:00:21.200 --> 00:00:25.220
So you have a machine in which fast
memory is being managed automatically.

00:00:25.220 --> 00:00:27.340
Consider your algorithm or program.

00:00:27.340 --> 00:00:31.300
As it runs, it issues sequences
of load and store operations.

00:00:31.300 --> 00:00:34.890
These loads and stores reference
addresses in slow memory.

00:00:34.890 --> 00:00:35.776
For this lesson,

00:00:35.776 --> 00:00:39.510
let's assume that the algorithm
issues these operations sequentially.

00:00:39.510 --> 00:00:41.680
Now, consider some load operation.

00:00:41.680 --> 00:00:44.122
Suppose it reads from
an address call it a and

00:00:44.122 --> 00:00:47.690
wants to load the value into a register,
call it r.

00:00:47.690 --> 00:00:52.290
The hardware will check to see if
a copy of a is already in fast memory.

00:00:52.290 --> 00:00:55.490
If it's there, then it returns the value
and completes the store operation,

00:00:55.490 --> 00:00:57.560
which writes to register r.

00:00:57.560 --> 00:00:59.690
Let's refer to this case as a cache hit,

00:00:59.690 --> 00:01:02.580
because the value that
we want is in cache.

00:01:02.580 --> 00:01:06.250
If the value we want is not in cache,
then it's a cache miss.

00:01:06.250 --> 00:01:09.697
In this case, the hardware grabs
the value from slow memory, but

00:01:09.697 --> 00:01:11.460
also stashes a copy in the cache.

00:01:11.460 --> 00:01:15.510
Keep in mind that the hardware has
to transfer an entire cache line.

00:01:15.510 --> 00:01:20.363
Now, which L consecutive words around
the address a get transferred depends

00:01:20.363 --> 00:01:21.640
on how a is aligned.

00:01:21.640 --> 00:01:26.640
So, what about a store from say
a register s to a memory address b?

00:01:26.640 --> 00:01:29.500
It will behave kind of
like a load operation.

00:01:29.500 --> 00:01:32.681
There's a copy of b in cache,
then it's a cache hit and

00:01:32.681 --> 00:01:34.350
we update the cached value.

00:01:34.350 --> 00:01:38.000
Otherwise, there's no copy of b in
the cache and it's a cache miss.

00:01:38.000 --> 00:01:41.760
The hardware would load the block
from slow memory into cache.

00:01:41.760 --> 00:01:46.390
In other words, a store miss like
a load miss causes a memory transfer.

00:01:46.390 --> 00:01:49.710
So those were the basics of load and
store operations.

00:01:49.710 --> 00:01:52.670
Here's the next assumption
in the ideal cache model.

00:01:52.670 --> 00:01:54.880
Cache is fully associative.

00:01:54.880 --> 00:01:56.420
So, what does that mean?

00:01:56.420 --> 00:02:00.850
Remember that a cache consists of
a set of cache lines or cache blocks.

00:02:00.850 --> 00:02:05.505
Now suppose you load a new block from
slow memory, full associativity means

00:02:05.505 --> 00:02:09.590
that this block is allowed to go
into any block or line of the cache.

00:02:09.590 --> 00:02:14.170
Now you may know about set associative
caches and direct-mapped caches.

00:02:14.170 --> 00:02:17.770
If you do, then you know the real
caches typically don't implement full

00:02:17.770 --> 00:02:19.120
associativity.

00:02:19.120 --> 00:02:21.547
Rather, they implement
one of these schemes,

00:02:21.547 --> 00:02:24.338
which has the effect of
restricting the possible cache

00:02:24.338 --> 00:02:26.840
lines that are given memory
address can go into.

00:02:26.840 --> 00:02:29.630
Full associativity says,
you can ignore this restriction.

00:02:29.630 --> 00:02:32.309
It's a simplifying
assumption that will make

00:02:32.309 --> 00:02:35.545
our ideal cache model more
powerful than real cache is.

00:02:35.545 --> 00:02:39.625
Now at some point, the cache will
be full of previously used values.

00:02:39.625 --> 00:02:41.153
To make room for new values,

00:02:41.153 --> 00:02:45.277
the hardware will need to choose
some line to kick out or to evict.

00:02:45.277 --> 00:02:49.562
If the value being evicted hasn't been
written to main memory yet, because say,

00:02:49.562 --> 00:02:53.677
it was a store hit previously, then
that will cause another memory transfer.

00:02:53.677 --> 00:02:57.137
I'll refer to those transfers
as store-evictions.

00:02:57.137 --> 00:03:00.057
So if we have to kick something out,
what do we kick out?

00:03:00.057 --> 00:03:03.864
That leads us to the next assumption
of the ideal cache model,

00:03:03.864 --> 00:03:05.337
optimal replacement.

00:03:05.337 --> 00:03:09.538
Optimal replacement means that
the hardware managing the cache actually

00:03:09.538 --> 00:03:10.627
knows the future.

00:03:10.627 --> 00:03:14.580
In particular,
the hardware knows all future accesses.

00:03:14.580 --> 00:03:17.630
It looks at all the blocks
currently in the cache and then

00:03:17.630 --> 00:03:22.010
evicts the one that will be accessed
the most distantly in the future.

00:03:22.010 --> 00:03:26.170
At first glance, this might strike
you as being extremely idealistic or

00:03:26.170 --> 00:03:27.330
optimistic.

00:03:27.330 --> 00:03:29.936
But in fact,
we'll do an analysis of just how

00:03:29.936 --> 00:03:32.760
powerful this assumption
really is in a moment.

00:03:32.760 --> 00:03:36.100
Let's do a quick summary of all the
assumptions of the ideal cache model.

00:03:36.100 --> 00:03:38.900
We'll model the program as
issuing a sequence of load and

00:03:38.900 --> 00:03:41.420
store operations to slow memory.

00:03:41.420 --> 00:03:44.060
The hardware manages
the z words of cache,

00:03:44.060 --> 00:03:47.350
which is divided into lines
of size L words each.

00:03:47.350 --> 00:03:51.790
These Z over L cache lines
are sometimes called cache blocks.

00:03:51.790 --> 00:03:54.150
As in the conventional I/O model,

00:03:54.150 --> 00:03:58.880
slow memory to cache transfers will
happen in lines or blocks of size L.

00:03:58.880 --> 00:04:02.186
If the value for some slow memory
address is already in cache,

00:04:02.186 --> 00:04:03.210
it's a cache hit.

00:04:03.210 --> 00:04:05.630
And otherwise, it's a cache miss.

00:04:05.630 --> 00:04:08.630
The cache will assume
it's fully associated.

00:04:08.630 --> 00:04:11.007
Lastly, when we need
to evict a cache line,

00:04:11.007 --> 00:04:13.590
we'll assume an optimal
replacement policy.

00:04:13.590 --> 00:04:16.630
This policy sees the future.

00:04:16.630 --> 00:04:18.130
One final point.

00:04:18.130 --> 00:04:22.000
Remember that in the conventional I/O
model, we counted memory transfers.

00:04:22.000 --> 00:04:24.590
In the ideal cache model,
we do the same thing.

00:04:24.590 --> 00:04:28.597
The number of transfers is really equal
to the number of misses plus the number

00:04:28.597 --> 00:04:29.840
of store-evictions.

00:04:29.840 --> 00:04:31.492
Now I think is a good time to see,

00:04:31.492 --> 00:04:34.380
if you really understand how
an ideal cache might work.

