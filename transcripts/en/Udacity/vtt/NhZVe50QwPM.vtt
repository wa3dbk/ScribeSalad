WEBVTT
Kind: captions
Language: en

00:00:00.390 --> 00:00:03.320
There's another important technique for
regularization

00:00:03.320 --> 00:00:06.740
that only emerged relatively
recently and works amazingly well.

00:00:08.020 --> 00:00:12.220
It also looks insane the first
time you see it, so bear with me.

00:00:12.220 --> 00:00:13.760
It's called dropout.

00:00:13.760 --> 00:00:15.850
Dropout works likes this.

00:00:15.850 --> 00:00:19.340
Imagine that you have one layer
that connects to another layer.

00:00:19.340 --> 00:00:23.089
The values that go from one layer to
the next are often called activations.

00:00:24.350 --> 00:00:27.260
Now take those activations and
randomly for

00:00:27.260 --> 00:00:31.760
every example you train your network on,
set half of them to zero.

00:00:31.760 --> 00:00:34.740
Completely randomly, you basically take

00:00:34.740 --> 00:00:39.110
half of the data that's flowing through
your network and just destroy it.

00:00:39.110 --> 00:00:40.170
And then randomly again.

00:00:41.370 --> 00:00:45.380
If that doesn't sound crazy to you then
you might qualify to become a student of

00:00:45.380 --> 00:00:48.110
Jeffery Hinton who
pioneered the technique.

00:00:48.110 --> 00:00:50.080
So what happens with dropout?

00:00:50.080 --> 00:00:55.310
Your network can never rely on any
given activation to be present

00:00:55.310 --> 00:00:57.790
because they might be
squashed at any given moment.

00:00:58.870 --> 00:01:01.820
So it is forced to learn
a redundant representation for

00:01:01.820 --> 00:01:06.130
everything to make sure that at least
some of the information remains.

00:01:06.130 --> 00:01:08.160
It's like a game of whack-a-mole.

00:01:08.160 --> 00:01:11.400
One activations gets smashed,
but there's always one or

00:01:11.400 --> 00:01:15.320
more that do the same job and
that don't get killed.

00:01:15.320 --> 00:01:18.100
So everything remains fine at the end.

00:01:18.100 --> 00:01:21.540
Forcing your network to learn redundant
representations might sound very

00:01:21.540 --> 00:01:22.985
inefficient.

00:01:22.985 --> 00:01:27.215
But in practice, it makes things more
robust and prevents over fitting.

00:01:27.215 --> 00:01:30.685
It also makes your network act
as if taking the consensus

00:01:30.685 --> 00:01:32.505
over an ensemble of networks.

00:01:32.505 --> 00:01:35.080
Which is always a good way
to improve performance.

00:01:35.080 --> 00:01:38.405
Dropout is one of the most important
techniques to emerge in the last few

00:01:38.405 --> 00:01:39.225
years.

00:01:39.225 --> 00:01:40.375
If drop out doesn't work for

00:01:40.375 --> 00:01:43.005
you, you should probably
be using a bigger network.

