WEBVTT
Kind: captions
Language: en

00:00:00.137 --> 00:00:03.874
The computers we're using in this class are termed heterogeneous.

00:00:03.874 --> 00:00:08.023
They have two different processors in them, the CPU and the GPU.

00:00:08.023 --> 00:00:14.585
Now, if you write a plain C program, your code will only allow you to use the CPU to run your program.

00:00:14.585 --> 00:00:17.787
So how do we write code that will run on the GPU?

00:00:17.787 --> 00:00:20.092
That's where CUDA comes in.

00:00:20.092 --> 00:00:24.997
The CUDA programming model allows us to program both processors with one program

00:00:24.997 --> 00:00:28.170
so that we can use the power of the GPU in our programs.

00:00:28.170 --> 00:00:32.488
CUDA supports numerous languages, but in this class we're using C.

00:00:32.488 --> 00:00:37.160
Now, part of your CUDA program is plain C and it will run on your CPU.

00:00:37.160 --> 00:00:39.329
CUDA calls this the host.

00:00:39.329 --> 00:00:42.566
The other part of your problem will run on the GPU in parallel.

00:00:42.566 --> 00:00:46.485
It's also written in C but with some extensions that we use to express parallelism.

00:00:46.485 --> 00:00:49.673
The CUDA term for your GPU is the device.

00:00:49.673 --> 00:00:53.911
Then, the CUDA compiler will compile your program, split it into pieces

00:00:53.911 --> 00:00:58.614
that will run on the CPU and the GPU, and generate code for each.

00:00:58.614 --> 00:01:04.855
CUDA assumes that the device, the GPU, is a co-processor to the host, the CPU.

00:01:04.855 --> 00:01:11.064
It also assumes that both the host and the device have their own separate memories where they store data.

00:01:11.064 --> 00:01:14.593
In the systems we use in this class, both the CPU and the GPU

00:01:14.593 --> 00:01:18.432
have their own physical dedicated memory in the form of DRAM,

00:01:18.432 --> 00:01:22.934
with the GPU's memory typically being a very high performance block of memory.

00:01:22.934 --> 00:01:27.938
Now, in this relationship between CPU and GPU, the CPU is in charge.

00:01:27.938 --> 00:01:33.277
It runs the main program, and it sends directions to the GPU to tell it what to do.

00:01:33.277 --> 00:01:36.718
It's the part of the system that's responsible for the following.

00:01:36.718 --> 00:01:41.330
One, moving data from the CPU's memory to the GPU's memory.

00:01:41.330 --> 00:01:45.190
Two, moving data from the GPU back to the CPU.

00:01:45.190 --> 00:01:50.030
Now, in the C programming language, moving data from one place to another is called Memcpy.

00:01:50.030 --> 00:01:54.973
So, it makes sense that in CUDA, this command, either moving data from the CPU to the GPU

00:01:54.973 --> 00:01:58.875
or moving data from the GPU to the CPU, is called cudaMemcpy.

00:01:58.875 --> 00:02:05.285
Three, allocating memory on the GPU, and in C this command is Malloc, so in CUDA, it's cudaMalloc.

00:02:05.285 --> 00:02:10.155
And four, invoking programs on the GPU that compute things in parallel.

00:02:10.155 --> 00:02:12.261
These programs are called kernels.

00:02:12.261 --> 00:02:14.597
And, here's a lot of jargon in one phrase.

00:02:14.597 --> 00:02:18.566
We say that the host launches kernels on the device.

