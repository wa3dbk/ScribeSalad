WEBVTT
Kind: captions
Language: en

00:00:00.140 --> 00:00:02.700
So I just want to quickly also
now just review the whole

00:00:03.770 --> 00:00:08.778
process of how scale-invariant
features are computed SIFTs.

00:00:08.778 --> 00:00:12.110
First we want to find the orientation,
compute the test orientation for

00:00:12.110 --> 00:00:13.860
each keypoint region.

00:00:13.860 --> 00:00:17.762
And then we want to actually use
the method to kind of find the keypoints

00:00:17.762 --> 00:00:21.390
itself and we use local image
gradients at selected scale, and

00:00:21.390 --> 00:00:24.010
rotation to describe
each keypoint region.

00:00:25.590 --> 00:00:28.010
Here's an example of
invariant local features.

00:00:28.010 --> 00:00:30.760
I mean,
here are two examples of the same car.

00:00:30.760 --> 00:00:35.490
So here for example, this region
here is matched to this region here.

00:00:35.490 --> 00:00:38.570
It's the same car, but
of course, we notice

00:00:38.570 --> 00:00:41.960
this patch is exactly the same even with
orientation and all that kind of stuff.

00:00:41.960 --> 00:00:44.680
And it finds others
regions like the similar.

00:00:44.680 --> 00:00:46.630
This region here is matched to this.

00:00:46.630 --> 00:00:51.330
This part here is matched, well it's
available here but there's no match here

00:00:51.330 --> 00:00:54.520
because these two here are either
too small or not visible.

00:00:54.520 --> 00:00:57.743
The bigger patch here again,
has the same characteristics on both.

00:00:57.743 --> 00:01:01.389
So one of the biggest advantage of SIFT
is that basically image content is

00:01:01.389 --> 00:01:03.903
transformed into local
feature coordinates that

00:01:03.903 --> 00:01:08.168
are invariant to translation, rotation,
scale, and other imaging parameters.

00:01:08.168 --> 00:01:12.320
And therefore allows us to
take two pairs of images with

00:01:12.320 --> 00:01:16.390
similar appearances of objects and
stuff like that.

00:01:16.390 --> 00:01:19.550
And it starts finding the features that
might have seen and do different things,

00:01:19.550 --> 00:01:21.680
and that allows us to do the matching.

00:01:21.680 --> 00:01:23.120
And that becomes an important part.

00:01:23.120 --> 00:01:25.070
So let's say I have these two images.

00:01:25.070 --> 00:01:28.780
First, I can run the whole process
again to detect all the features.

00:01:28.780 --> 00:01:31.430
Here it found all the features
in these two images.

00:01:31.430 --> 00:01:33.070
This is of course the common regions.

00:01:33.070 --> 00:01:35.020
These are two images next to each other.

00:01:35.020 --> 00:01:38.360
Now of course, what I basically
after I have found the features,

00:01:38.360 --> 00:01:39.160
I want to match them.

00:01:39.160 --> 00:01:40.040
So here basically,

00:01:40.040 --> 00:01:43.570
it find and matches that these
are similar between these two.

00:01:43.570 --> 00:01:47.760
And once I match them,
I can also then register them together.

00:01:47.760 --> 00:01:51.400
And if I registered, then I can use
that to align the two images together,

00:01:51.400 --> 00:01:52.450
which I'm not actually showing.

00:01:52.450 --> 00:01:54.940
I'm just basically showing that
these two features are now

00:01:54.940 --> 00:01:56.410
matches between these two images.

