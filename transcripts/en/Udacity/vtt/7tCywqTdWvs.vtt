WEBVTT
Kind: captions
Language: en

00:00:00.190 --> 00:00:03.700
So let us look now at
the overall performance and

00:00:03.700 --> 00:00:06.590
reliability of a Raid 4 array.

00:00:06.590 --> 00:00:12.660
For reads, we are getting,
on average the throughput of N-1 disks.

00:00:12.660 --> 00:00:16.200
We can read data from all
of the data disks, but

00:00:16.200 --> 00:00:21.260
the parity disk has no useful data on
it, so for reads, we don't use it.

00:00:21.260 --> 00:00:25.150
This is significantly
improving RAID performance

00:00:25.150 --> 00:00:28.730
because usually we have more
than just two disks in an array.

00:00:28.730 --> 00:00:31.200
So for example with four disks
we are really getting for

00:00:31.200 --> 00:00:34.940
reads the throughput of three disks,
all of the data disks.

00:00:34.940 --> 00:00:40.750
For writes however, the data accesses
are distributed among the data disks.

00:00:40.750 --> 00:00:47.320
However, every write requires
the parity disk to read and then write.

00:00:47.320 --> 00:00:51.140
We need to read the old parity,
update it and then write it back.

00:00:52.140 --> 00:00:56.510
So we are getting only one half of
the throughput of a single disk,

00:00:56.510 --> 00:01:01.450
because the parity disk, again,
needs two accesses for every write.

00:01:01.450 --> 00:01:03.860
This is a significant disadvantage, and

00:01:03.860 --> 00:01:07.550
we will see that this is really the
primary reason why we need raid five.

00:01:07.550 --> 00:01:09.040
Now let's look at the reliability.

00:01:09.040 --> 00:01:12.150
What is the entity F of a raid for this?

00:01:12.150 --> 00:01:14.580
Again, we have two options.

00:01:14.580 --> 00:01:15.820
Just like raid one,

00:01:15.820 --> 00:01:20.270
the first period we get is while
all of the disks are working.

00:01:20.270 --> 00:01:24.200
The time you have during that period
is on average MTTF of the single disk

00:01:24.200 --> 00:01:30.630
divided by N, because that's how long we
have for the first of N disks to fail.

00:01:30.630 --> 00:01:35.630
Just like grade 1, after this first
failure, we did not lose the data yet.

00:01:35.630 --> 00:01:37.650
And now we have two options.

00:01:37.650 --> 00:01:40.982
If we do no repair of
the failed disk then,

00:01:40.982 --> 00:01:46.313
now we are operating with N minus 1
good disks, if we do no repair or

00:01:46.313 --> 00:01:51.931
the failed disks, what we get now is
at the point of the first failure,

00:01:51.931 --> 00:01:55.549
we are left with the N
minus 1 disk array, and

00:01:55.549 --> 00:02:01.210
any disk failure in that array
is going to result in data loss.

00:02:01.210 --> 00:02:06.006
So, if we don't do repair,
then to this, we add the expected time

00:02:06.006 --> 00:02:11.790
until the first failure among the N
minus 1 disks that we still have.

00:02:11.790 --> 00:02:15.930
This plus this, if N is, let's say, 4,

00:02:15.930 --> 00:02:20.400
it's not going to be better
than the MTTF of a single disk.

00:02:20.400 --> 00:02:25.360
So you don't want to even consider
using RAID 4 unless you're

00:02:25.360 --> 00:02:27.300
going to do repairs.

00:02:27.300 --> 00:02:31.340
So pretty much you don't do
this because it's a bad idea.

00:02:31.340 --> 00:02:33.960
You're not getting
an increased reliability.

00:02:33.960 --> 00:02:37.590
What you want to do is
repair the failed disk,

00:02:37.590 --> 00:02:43.190
in which case the chances
of a disk failing during

00:02:43.190 --> 00:02:48.010
the repair period is the MTTF
of what's left of our array.

00:02:48.010 --> 00:02:52.360
With RAID 1 we had one disk left
Here we have N-1 disk left, so

00:02:52.360 --> 00:02:56.930
the MTTF for the remaining disks
is lower than for one disk,

00:02:56.930 --> 00:03:01.600
because we are talking about
the first failure among the N disks.

00:03:01.600 --> 00:03:06.030
So for example if we have three disks,
the first failure among three

00:03:06.030 --> 00:03:10.120
is going to happen sooner than the first
failure among just the one disk.

00:03:10.120 --> 00:03:13.630
And then we divide that with
the mean time to repair.

00:03:13.630 --> 00:03:17.580
So this is the factor with which
we now multiply our, this is for

00:03:17.580 --> 00:03:21.710
how long we get a RAID 4 to
work until we lose a disk.

00:03:21.710 --> 00:03:26.920
So overall MTTF of our
RAID 4 array is going

00:03:26.920 --> 00:03:31.551
to be the entity of one disk times again

00:03:31.551 --> 00:03:36.326
the entity of one disk
divided by N time N

00:03:36.326 --> 00:03:40.845
minus 1 times the MTTR for the disk.

00:03:40.845 --> 00:03:44.445
So note that again this is
very small compared to this.

00:03:44.445 --> 00:03:47.745
So this particular ratio
is extremely large.

00:03:48.790 --> 00:03:51.320
The number of disks is, let's say, four,

00:03:51.320 --> 00:03:54.020
you don't want to have too
many disks in a RAID 4 array.

00:03:54.020 --> 00:03:58.970
If that is so, then what we get is
the MTTF of a single disk that is fairly

00:03:58.970 --> 00:04:03.040
large multiplied by something that
is usually in the thousands or

00:04:03.040 --> 00:04:07.360
more and divided by something
that is usually something like,

00:04:07.360 --> 00:04:10.380
if it's four disks, it's going to be 12.

00:04:10.380 --> 00:04:15.730
So this MTTF or RAID 4, if we repair
the failed disk every time it fails,

00:04:15.730 --> 00:04:18.890
we get an MTTF that is
still very very large.

