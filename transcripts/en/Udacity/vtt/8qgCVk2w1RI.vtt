WEBVTT
Kind: captions
Language: en

00:00:00.116 --> 00:00:03.036
We compile it and run it.

00:00:03.036 --> 00:00:09.203
I made a really small matrix to begin with so we can just see that it's acting correctly.

00:00:09.203 --> 00:00:17.869
So as you see our input matrix goes from 0 to 63, and the transposed version does the same thing down the columns instead of the rows.

00:00:17.869 --> 00:00:22.236
So we've written correct code, as we expected, and we're just going

00:00:22.236 --> 00:00:25.961
to store this routine for generating this transpose in the gold matrix,

00:00:25.961 --> 00:00:29.127
and we'll use that to compare so we don't have to keep printing out these matrices.

00:00:29.127 --> 00:00:35.200
For the rest of the lecture, I'll leave a smaller window so that we can see the results

00:00:35.200 --> 00:00:37.197
without actually printing out the matrices.

00:00:37.197 --> 00:00:42.628
Ok, so here's how you code transpose on a serial machine like a CPU, and here's how you call it.

00:00:42.628 --> 00:00:56.796
So let's do our first CUDA version. We'll copy this, paste it, make it a kernel, rename it.

00:00:56.796 --> 00:01:00.394
So now we've got a kernel that will operate on the GPU and call it.

00:01:00.394 --> 00:01:08.866
We need to do a few things. We need to allocate space for the input and output matrix on the device,

00:01:08.866 --> 00:01:16.595
so allocate those, and we need to copy the input matrix onto the device,

00:01:16.595 --> 00:01:20.828
call our kernel, so what you see here is I've taken that same kernel that we wrote,

00:01:20.828 --> 00:01:23.570
a serial kernel, I've called it on a single thread,

00:01:23.570 --> 00:01:29.801
running at a single block, passed in d_in and d_out and then I copy my result and we're done.

00:01:29.801 --> 00:01:34.812
See, that was quick. GPU programming is easy. I'm actually making an important point here.

00:01:34.812 --> 00:01:38.652
Okay, this code is going to run correctly, and it was really easy to write.

00:01:38.652 --> 00:01:41.815
And the problem, of course, is that it won't perform very well, right?

00:01:41.815 --> 00:01:45.481
It's using a single thread, which is a really small fraction of the horsepower of a GPU,

00:01:45.481 --> 00:01:49.550
especially a high-end GPU which can run tens of thousands of threads concurrently.

00:01:49.550 --> 00:01:54.018
So let's go ahead and time this. Okay, so I've added some timer code.

00:01:54.018 --> 00:01:56.760
Okay, we're going to start the timer, measure it,

00:01:56.760 --> 00:02:01.786
and I also added--I'm using a helper function here called compare matrices,

00:02:01.786 --> 00:02:08.816
which just compares the output to the golden matrix that we calculated to make sure that we did the transpose correctly.

00:02:08.816 --> 00:02:12.717
let's compile again, run again.

00:02:12.717 --> 00:02:21.314
Great, so, yes, we performed the transpose successfully, and it took some very small fraction of a millisecond,

00:02:21.314 --> 00:02:25.483
but this was a really small problem. let's make this problem bigger again,

00:02:25.483 --> 00:02:32.050
say 1024 by 1024 matrix. So now we've got roughly a million elements to touch,

00:02:32.050 --> 00:02:34.212
and we're going to do this in a single thread.

00:02:34.212 --> 00:02:38.212
Okay, now this is taking almost half a second; that's a long time.

