WEBVTT
Kind: captions
Language: en

00:00:00.270 --> 00:00:04.340
So this is called the first component, or the principle component, and this is

00:00:04.340 --> 00:00:07.100
called the second, or second principle component

00:00:07.100 --> 00:00:09.500
of this space. Okay, does that make sense?

00:00:09.500 --> 00:00:09.970
&gt;&gt; Yep.

00:00:09.970 --> 00:00:11.800
&gt;&gt; Now, here's what's interesting about principle

00:00:11.800 --> 00:00:13.440
components analysis. You might ask me exactly

00:00:13.440 --> 00:00:17.940
how you do this. There are several, several mechanisms for doing it. For those

00:00:17.940 --> 00:00:19.590
of you who have dealt with linear

00:00:19.590 --> 00:00:21.580
algebra before, something like this singular value

00:00:21.580 --> 00:00:24.070
decomposition might be familiar to you. It's

00:00:24.070 --> 00:00:25.720
one way of finding the principal component.

00:00:25.720 --> 00:00:28.860
But principal components analysis basically has a lot of really

00:00:28.860 --> 00:00:30.970
neat properties, so let me just describe some of those

00:00:30.970 --> 00:00:33.600
properties to you. The first property is, well, the two

00:00:33.600 --> 00:00:37.074
that I've written here. It finds directions that maximize variants and

00:00:37.074 --> 00:00:41.640
it finds directions that are mutually orthogonal. Mutually orthogonal means

00:00:41.640 --> 00:00:44.430
it's a global algorithm. And by global here I mean

00:00:44.430 --> 00:00:47.100
that all the directions, all the new features that they

00:00:47.100 --> 00:00:51.016
find have a big global constraint, namely that they must be

00:00:51.016 --> 00:00:55.720
mutually orthogonal. It also turns out that you can prove. Which

00:00:55.720 --> 00:00:57.994
I will try to give you a little bit of evidence

00:00:57.994 --> 00:01:02.960
for. But I'm going to prove formally, that the PCA actually gives

00:01:02.960 --> 00:01:05.410
you the best reconstruction. Now what do I mean by best

00:01:05.410 --> 00:01:08.120
reconstruction? What I mean is, if you think of each of

00:01:08.120 --> 00:01:10.860
these directions that it found, in this case. You found this

00:01:10.860 --> 00:01:13.650
one first and found this one second. The first thing you,

00:01:13.650 --> 00:01:16.170
I, I hope you see is that if I return these two

00:01:16.170 --> 00:01:20.250
dimensions, I have actually lost no information. That is, this

00:01:20.250 --> 00:01:25.550
is just a linear rotation of the original dimensions. So if

00:01:25.550 --> 00:01:28.630
I were to give you back these two different features,

00:01:28.630 --> 00:01:31.080
you could reconstruct all of your original data. You see that?

00:01:31.080 --> 00:01:33.110
&gt;&gt; Wait. When you say features you mean, what we're

00:01:33.110 --> 00:01:35.590
going to give it is, for each of those little black

00:01:35.590 --> 00:01:38.650
data points, we're going to say how far along the red

00:01:38.650 --> 00:01:41.190
axis is it and how far along the orange axis

00:01:41.190 --> 00:01:41.490
is it.

00:01:41.490 --> 00:01:41.970
&gt;&gt; Right.

00:01:41.970 --> 00:01:43.590
&gt;&gt; So it really is just a, a, a kind

00:01:43.590 --> 00:01:47.980
of a relabeling of the. of the dimensions. Right, so just

00:01:47.980 --> 00:01:50.330
like when I think about these points in x and

00:01:50.330 --> 00:01:53.740
y space, the original feature space, whenever I give you value

00:01:53.740 --> 00:01:56.090
here for this feature I'm just describing how far along

00:01:56.090 --> 00:01:59.390
a black dot is on this dimension or this projection. Now

00:01:59.390 --> 00:02:02.280
the second dimension or the second feature just tells me how

00:02:02.280 --> 00:02:06.550
far along a dot is. On this particular dimension or axis,

00:02:06.550 --> 00:02:08.600
and similarly about projecting on the read and on

00:02:08.600 --> 00:02:10.788
the orange, I'm telling how far along a point is

00:02:10.788 --> 00:02:15.190
along this axis and along that axis. So if I were to return, if I were to take X

00:02:15.190 --> 00:02:17.370
and Y and transform them into this new one

00:02:17.370 --> 00:02:19.660
and two, I would have given you different values than

00:02:19.660 --> 00:02:21.690
I did from X or Y, but they're actually just

00:02:21.690 --> 00:02:24.320
the same point. And so I've thrown away no information.

00:02:24.320 --> 00:02:26.140
&gt;&gt; So that's a pretty good reconstruction.

00:02:26.140 --> 00:02:26.800
&gt;&gt; That's a pretty good

00:02:26.800 --> 00:02:28.920
reconstruction. But what principle components analysis

00:02:28.920 --> 00:02:31.810
does for you. Is if I take just on of these dimensions,

00:02:31.810 --> 00:02:36.300
in particular, the first one, the principle component, I am

00:02:36.300 --> 00:02:39.630
guaranteed that if I project only in to this space

00:02:39.630 --> 00:02:42.800
and then try to reproject into the original space. I

00:02:42.800 --> 00:02:47.070
will minimize what's called L2 error. So do you understand that?

00:02:47.070 --> 00:02:52.230
&gt;&gt; I'm not sure let me check. So, you're saying if we instead of,

00:02:52.230 --> 00:02:56.860
all right we take the little black dots, they now have a red dimension and

00:02:56.860 --> 00:03:01.470
orange dimension one two, and now if we reconstruct using only the first

00:03:01.470 --> 00:03:04.910
dimension, I guess it just puts the black dots on the red line.

00:03:04.910 --> 00:03:09.550
&gt;&gt; Yep. And so there is no, they have no existance in that second dimension.

00:03:09.550 --> 00:03:10.490
&gt;&gt; Correct.

00:03:10.490 --> 00:03:13.380
&gt;&gt; And now you're saying of all the different ways

00:03:13.380 --> 00:03:15.370
that I could do that to kind of project it to

00:03:15.370 --> 00:03:17.660
a, to a linear sequence. This is the one that's

00:03:17.660 --> 00:03:22.880
going to have the smallest. L2 error which if im not mistaken

00:03:22.880 --> 00:03:24.940
is the same kind of reconstruction error we talked about

00:03:24.940 --> 00:03:28.580
in all the other times we talked about reconstrucion. So it's

00:03:28.580 --> 00:03:31.510
like squared error. Thats exactly right, its squared error. This

00:03:31.510 --> 00:03:34.230
particular notion is called a [UNKNOWN] norm but thats really just

00:03:34.230 --> 00:03:36.460
talking about distance. What this means is that if I

00:03:36.460 --> 00:03:39.820
project onto this single axis here, and then I compare it

00:03:39.820 --> 00:03:43.870
to where it was in the original space, the distance,

00:03:43.870 --> 00:03:48.130
the sum of all the distances between those points will actually

00:03:48.130 --> 00:03:51.250
be the minimal that I could get for any other projection.

00:03:51.250 --> 00:03:51.730
&gt;&gt; Cool.

00:03:51.730 --> 00:03:54.650
&gt;&gt; And you can, you can sort of prove this. It kind of makes sense if you

00:03:54.650 --> 00:03:56.840
just think about the fact that points. Always

00:03:56.840 --> 00:03:59.650
start out in some orthogonal space. And, I'm basically

00:03:59.650 --> 00:04:02.290
finding in scaling and a rotation such that

00:04:02.290 --> 00:04:05.530
I don't lose any information. And I maximize variance

00:04:05.530 --> 00:04:08.280
along the way. By maximizing variance, it turns out

00:04:08.280 --> 00:04:13.400
I'm maximizing or maintaining distances as best I can

00:04:13.400 --> 00:04:16.560
in any given dimension. And, so, that gives me the

00:04:16.560 --> 00:04:19.839
best reconstruction that I can imagine. Now, you might ask

00:04:19.839 --> 00:04:23.070
yourself, Is there anything else nice about principal components analysis

00:04:23.070 --> 00:04:26.720
given this reconstruction error? And there's another property of PCA that

00:04:26.720 --> 00:04:29.000
is very, very useful. And it boils down to the

00:04:29.000 --> 00:04:31.500
fact that it's an eigenproblem. What happens when you do

00:04:31.500 --> 00:04:34.370
principal components analysis is you get all of these axes

00:04:34.370 --> 00:04:38.360
back, and in fact, if you start out with. N dimensions,

00:04:38.360 --> 00:04:42.110
you get back N dimensions again, and the job

00:04:42.110 --> 00:04:44.960
here for a future transformation as you might recall, is

00:04:44.960 --> 00:04:48.300
you want to pick a subset M of them hopefully

00:04:48.300 --> 00:04:51.870
much smaller than N. Well, it turns out that associated

00:04:51.870 --> 00:04:53.870
with each one of these new dimensions that we

00:04:53.870 --> 00:04:57.815
get. Is its eigenvalue. That eigenvalue is guaranteed to be

00:04:57.815 --> 00:05:00.270
non-negative, it has a lot of other neat properties.

00:05:00.270 --> 00:05:04.120
But what matters to us here is that the eigenvalues

00:05:04.120 --> 00:05:08.640
monotonically non-increase, that is, they tend to get smaller

00:05:08.640 --> 00:05:10.860
as you move from the principal to the second

00:05:10.860 --> 00:05:12.400
principal, to the third, to the fourth, to the

00:05:12.400 --> 00:05:14.540
fifth, to the sixth, and so on to the nth

00:05:14.540 --> 00:05:18.490
dimension. And so, you can throw away the ones

00:05:18.490 --> 00:05:21.040
with the least eigenvalue. And that's a way of

00:05:21.040 --> 00:05:23.930
saying that you're throw awaying these projections, or the

00:05:23.930 --> 00:05:26.800
directions, or the features, with the least amount of variance.

