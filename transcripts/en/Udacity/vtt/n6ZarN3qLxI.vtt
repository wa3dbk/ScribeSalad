WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.385
We are ready to try to build various models.

00:00:02.385 --> 00:00:04.735
But before we get into the next steps,

00:00:04.735 --> 00:00:07.825
let's learn what it means to evaluate the model.

00:00:07.825 --> 00:00:11.502
As you must have noticed, we are constantly iterating to and

00:00:11.502 --> 00:00:16.540
fro from the validation phase and the questioning phase and the modeling phase.

00:00:16.540 --> 00:00:20.690
So what it is that we use to validate these models?

00:00:20.690 --> 00:00:25.170
We'll take a short digression from our example that we have been following

00:00:25.170 --> 00:00:29.910
to introduce some key concepts involved with the model validation techniques.

00:00:29.910 --> 00:00:34.809
We won't end up using all these techniques in our specific example, but

00:00:34.809 --> 00:00:39.869
this material is very important to clearly understand the concepts behind

00:00:39.869 --> 00:00:41.247
model validation.

00:00:41.247 --> 00:00:44.795
When we build models and train them,

00:00:44.795 --> 00:00:51.555
we need some way to measure how far out model is from the actual values.

00:00:51.555 --> 00:00:56.780
The loss function is just such a concept.

00:00:56.780 --> 00:01:03.790
It measures how far an estimated value of a quantity is from the true value.

00:01:03.790 --> 00:01:08.540
Let's take an example to see what the loss function is.

00:01:08.540 --> 00:01:10.990
Let's take the special case of parametric model.

00:01:10.990 --> 00:01:17.130
Let's say, the parameter is theta, which is the true value of a quantity.

00:01:17.130 --> 00:01:22.040
And then theta hat is an estimated value for the same quantity.

00:01:22.040 --> 00:01:27.660
Remember, theta hat is a function of the data that we collected.

00:01:27.660 --> 00:01:33.350
The idea is that we are fitting a parametric model to the data.

00:01:33.350 --> 00:01:37.500
And we always need a measure of how well we perform.

00:01:37.500 --> 00:01:41.360
We accomplish this by defining a loss function.

00:01:41.360 --> 00:01:46.310
It is up to you to choose how you define the loss function.

00:01:46.310 --> 00:01:50.860
The choice depends on the specific problem you're trying to solve.

00:01:50.860 --> 00:01:54.750
Here are some examples of loss functions you may encounter.

00:01:54.750 --> 00:01:59.300
I'm using the notation of theta and theta hat for a parametric model.

00:01:59.300 --> 00:02:02.110
One example is the squared error loss.

00:02:02.110 --> 00:02:04.490
In this case, you take the difference between theta and

00:02:04.490 --> 00:02:06.830
theta hat and square them.

00:02:06.830 --> 00:02:09.053
Another is the absolute error loss.

00:02:09.053 --> 00:02:13.500
In this case, you take the difference between theta and theta hat and

00:02:13.500 --> 00:02:16.116
the absolute value of that difference.

00:02:16.116 --> 00:02:22.770
An Lp Loss is essentially the Absolute Error Loss raised to the power p.

00:02:22.770 --> 00:02:26.670
Another very interesting quantity is the Kullback-Leibler Loss.

00:02:26.670 --> 00:02:28.970
It's a complicated formula and

00:02:28.970 --> 00:02:35.000
it's an information theoretic loss calculated for two different distributions.

00:02:35.000 --> 00:02:39.970
You will often encounter squared error loss in statistical learning.

00:02:39.970 --> 00:02:43.408
The square error loss is sensitive to outliers.

00:02:43.408 --> 00:02:47.896
For values of theta hat that are far from the values of theta,

00:02:47.896 --> 00:02:51.130
it contributes a large quantity to L.

00:02:51.130 --> 00:02:54.840
Also, notice this is always a positive quantity given it's

00:02:54.840 --> 00:02:56.900
a square of difference.

00:02:56.900 --> 00:03:00.910
The Absolute Error Loss is not so sensitive to outliers.

00:03:00.910 --> 00:03:06.370
However, it is not smooth at the value where theta hat is exactly

00:03:06.370 --> 00:03:10.770
equal to theta and thus, it's difficult to differentiate it at that point.

00:03:12.120 --> 00:03:15.770
We mentioned that the loss function depends on the selected data.

00:03:16.790 --> 00:03:21.080
Remember, theta-hat is a function of the data.

00:03:21.080 --> 00:03:27.290
We want a measurement that is averaged over many possible datasets.

00:03:27.290 --> 00:03:28.831
The risk is such a function.

00:03:28.831 --> 00:03:33.520
It is an average measure of the loss.

00:03:33.520 --> 00:03:39.580
And we calculate this by taking the expected value of the loss function.

00:03:39.580 --> 00:03:43.530
So as you see here, the risk is denoted by R which is

00:03:43.530 --> 00:03:48.600
the expected value of the loss function and it's calculated under the integral.

