WEBVTT
Kind: captions
Language: en

00:00:00.110 --> 00:00:03.100
So going back to our choices for

00:00:03.100 --> 00:00:06.970
reducing the AMAT, we have seen
that we can reduce the hit time.

00:00:06.970 --> 00:00:10.750
We have now seen that there are
techniques that reduce the miss rate.

00:00:10.750 --> 00:00:16.379
And the final set of techniques
are those that reduce the miss penalty.

00:00:16.379 --> 00:00:20.160
So when we miss,
we don't suffer as much as before.

00:00:20.160 --> 00:00:22.170
So the first technique for

00:00:22.170 --> 00:00:26.190
using the missed penalty is
to overlap multiple misses.

00:00:26.190 --> 00:00:28.260
If time goes in this direction,

00:00:28.260 --> 00:00:32.250
our processor does a lot of activity
multiple instructions per cycle.

00:00:32.250 --> 00:00:35.870
And at some point it does a load for
example.

00:00:35.870 --> 00:00:39.520
And now that load tries
to be found in the cache.

00:00:39.520 --> 00:00:44.170
And if not, we're going to go to
memory and it eventually comes back.

00:00:44.170 --> 00:00:46.830
Now if you have a fancy
out of order processor,

00:00:46.830 --> 00:00:48.900
it doesn't stop here and wait.

00:00:48.900 --> 00:00:51.768
What it does is it finds
other instructions to do.

00:00:51.768 --> 00:00:57.040
So even after we start
fetching the data from memory,

00:00:57.040 --> 00:00:58.830
the processor is continuing.

00:00:58.830 --> 00:01:02.860
But eventually it starts
running out of things to do.

00:01:02.860 --> 00:01:05.770
And probably before the load
comes back from memory,

00:01:05.770 --> 00:01:07.680
the processor runs out of resources.

00:01:07.680 --> 00:01:09.770
Remember that it cannot
commit this load.

00:01:09.770 --> 00:01:12.960
So eventually for example,
it will fill up the ROB or

00:01:12.960 --> 00:01:16.160
maybe even sooner than that it will
run out of reservation stations,

00:01:16.160 --> 00:01:17.990
if a lot of things depend on this load.

00:01:17.990 --> 00:01:21.960
So some part of this missed latency
is going to be directly added to

00:01:21.960 --> 00:01:23.340
the execution time, but

00:01:23.340 --> 00:01:27.660
some part of it is actually overlapping
with the processor activity.

00:01:27.660 --> 00:01:32.077
During the time, between,
trying to execute this load,

00:01:32.077 --> 00:01:34.509
and running out of things to do,

00:01:34.509 --> 00:01:40.296
this processor might actually issue
another load that will be a cache miss.

00:01:40.296 --> 00:01:43.801
So, if we have what is
called a blocking cache,

00:01:43.801 --> 00:01:48.536
then this load cannot be done until
the first load is finished and

00:01:48.536 --> 00:01:53.020
only at this point this load can
really be tried in the cache.

00:01:53.020 --> 00:01:56.332
We realize it's a miss,
we suffer the miss latency, and

00:01:56.332 --> 00:02:00.472
meanwhile because the processor can
commit these instructions here,

00:02:00.472 --> 00:02:05.240
the processor can overlap some of this
miss latency with some other activity.

00:02:05.240 --> 00:02:10.210
We can also have a non-blocking
cache and a non-blocking cache can

00:02:10.210 --> 00:02:15.410
support things like hit under a miss,
meaning while

00:02:15.410 --> 00:02:19.670
we are having a cache miss, hits to
other blocks in the cache that are sent

00:02:19.670 --> 00:02:24.412
by the processor will be serviced and
returned to the processor with data.

00:02:24.412 --> 00:02:29.270
And also we can have what is called
a miss under a miss, in which case

00:02:29.270 --> 00:02:33.070
while we are having a miss,
we can send another request to memory.

00:02:33.070 --> 00:02:35.280
So let's look at what that looks like.

00:02:35.280 --> 00:02:37.980
So our processor is
happily chugging along,

00:02:37.980 --> 00:02:40.510
it has this first load
that suffers a miss.

00:02:40.510 --> 00:02:42.560
We check in the cache, we wait for

00:02:42.560 --> 00:02:45.420
it to come back from memory,
we continue working,

00:02:45.420 --> 00:02:49.290
and eventually run out of things to do
because they depend on this first load.

00:02:49.290 --> 00:02:52.990
But the idea is that now this load here

00:02:52.990 --> 00:02:57.020
that also suffers a miss will have
its own check in the cache and

00:02:57.020 --> 00:02:59.910
when we realize it's a miss
we will send it to memory.

00:02:59.910 --> 00:03:01.780
So it will come back here.

00:03:01.780 --> 00:03:05.150
Now what we have is when the first
load comes back from memory,

00:03:05.150 --> 00:03:06.680
there is a burst of activity.

00:03:06.680 --> 00:03:11.600
It starts drying out because we
are still waiting for the second load.

00:03:11.600 --> 00:03:13.790
But then the second load comes back and

00:03:13.790 --> 00:03:16.880
we are very quickly back
to a normal operation.

00:03:16.880 --> 00:03:22.770
So now, as you can see, the inactivity
in the processor used to be this and

00:03:22.770 --> 00:03:27.580
this and now it's just this and
maybe a small amount here.

00:03:27.580 --> 00:03:31.980
So by overlapping the miss
time of the two loads,

00:03:31.980 --> 00:03:36.460
we have almost cut
the penalty to on performance

00:03:36.460 --> 00:03:41.530
to half of what it was because
before we had to wait twice this.

00:03:41.530 --> 00:03:45.470
Now we really wait once and
maybe some little more.

00:03:45.470 --> 00:03:49.560
If we manage to find three or
four rows that overlap.

00:03:49.560 --> 00:03:54.230
Then a blocking cacher will pay
the penalty three or four times.

00:03:54.230 --> 00:03:58.660
Here we might be paying one
penalty plus a little bit more.

00:03:58.660 --> 00:04:03.210
The property that the processor
is exploiting here is called

00:04:03.210 --> 00:04:05.700
memory level parallelism.

00:04:05.700 --> 00:04:09.970
Here, the memory never gets
more than one access at a time.

00:04:09.970 --> 00:04:13.070
Here, the memory gets accesses
to process them in parallel.

00:04:13.070 --> 00:04:15.640
So, of course,
our memory needs to be able to do this.

00:04:15.640 --> 00:04:19.920
But if it can't, then a non-blocking
cache that can do miss under miss,

00:04:19.920 --> 00:04:23.320
can dramatically cut down
on the cost of misses,

00:04:23.320 --> 00:04:28.260
instead of seeing the full miss latency
being added to the memory access time.

00:04:28.260 --> 00:04:31.780
We are really seeing that the penalties
of the two misses overlap.

00:04:31.780 --> 00:04:37.720
So really we paid a penalty once and
we get 1, 2, 15 misses in exchange.

