WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.000
So let's do a little demonstration of this idea.

00:00:02.000 --> 00:00:05.000
We've got a little flight graph from one of the previous examples,

00:00:05.000 --> 00:00:07.000
and we can compute the clustering coefficient for any node.

00:00:07.000 --> 00:00:11.000
Let's say node 2. We're going to print the clustering coefficient for node 2.

00:00:11.000 --> 00:00:16.000
And then what we're going to do is actually randomly pick two of these neighbors.

00:00:16.000 --> 00:00:20.000
Now, I ended up having to go a little bit crazy here because you need to pick the neighbors

00:00:20.000 --> 00:00:24.000
without replacement meaning that we pick one neighbor and then the second neighbor

00:00:24.000 --> 00:00:26.000
we pick has to be different from the first one.

00:00:26.000 --> 00:00:29.000
So I went out of my way a little bit and said okay here's what we're going to do.

00:00:29.000 --> 00:00:32.000
We're going to run through all of the neighbors of v—these two.

00:00:32.000 --> 00:00:37.000
And for each of them, we're going to stick them into an array called the index.

00:00:37.000 --> 00:00:40.000
And so instead of randomly choosing from this set, we're going to randomly choose

00:00:40.000 --> 00:00:43.000
indices from this array and that will make it easier to make sure we don't repeat.

00:00:43.000 --> 00:00:46.000
So d is going to be the degree of node V.

00:00:46.000 --> 00:00:51.000
And as long as that degree is greater than 1 we're going to pick a random neighbor

00:00:51.000 --> 00:00:56.000
so that now the neighbor here is being chosen by its ordering from 0 to d-1.

00:00:56.000 --> 00:01:01.000
So v1 is the neighbor—the actual node name associated with that pick and then we do

00:01:01.000 --> 00:01:06.000
a second pick and this time we're going to do—we're going to again choose from 1 to d-1.

00:01:06.000 --> 00:01:11.000
We're going to add that to the pick that we already got with modular d to make it wrap around.

00:01:11.000 --> 00:01:14.000
So this is going to make sure that we're going to pick something that is different

00:01:14.000 --> 00:01:17.000
from what we just picked and we look up the corresponding ID.

00:01:17.000 --> 00:01:19.000
So we've got v1 and v2 are the two neighbors.

00:01:19.000 --> 00:01:22.000
We check whether they're connected and we add 1 to the total if they are.

00:01:22.000 --> 00:01:26.000
We repeat this whole loop a thousand times and at each point in time

00:01:26.000 --> 00:01:29.000
we take the total—the total number of times things were connected

00:01:29.000 --> 00:01:31.000
divided by the total number that we tried.

00:01:31.000 --> 00:01:36.000
And I claim that it ought to be the case that this number converges to

00:01:36.000 --> 00:01:38.000
the actual clustering coefficient.

00:01:38.000 --> 00:01:43.000
So let's try it. So in this particular case the clustering coefficient for node v is 0.3.

00:01:43.000 --> 00:01:46.000
And now let's watch what happens to the estimate.

00:01:46.000 --> 00:01:50.000
Well, certainly when there's only one sample that we've done so far

00:01:50.000 --> 00:01:53.000
it's either going to be 1 or 0; it's not going to be very close to 0.3.

00:01:53.000 --> 00:01:56.000
But as we repeat this over and over again you can see it's kind of settling in.

00:01:56.000 --> 00:01:59.000
It actually hit 0.3 briefly on the 48th try.

00:01:59.000 --> 00:02:03.000
But it's kind of bouncing around a little bit above 3, a little bit below 3,

00:02:03.000 --> 00:02:07.000
and the longer we run this the kind of less it seems to be changing—

00:02:07.000 --> 00:02:14.000
0.26, 0.27, 0.26, 0.27—Oh, it wandered up to 0.3 again and ended up at 0.31.

00:02:14.000 --> 00:02:19.000
So if we do this long enough and repeat this in enough times what we'll find is that

00:02:19.000 --> 00:02:23.000
the number really is 0.3 but in any given run it's going to be a little bit more, a little bit less.

00:02:23.000 --> 00:02:26.000
The longer that it runs, the more digits are going to be correct.

00:02:26.000 --> 00:02:31.000
And so in this particular case, this is a terrible approximation because it's actually not very

00:02:31.000 --> 00:02:35.000
close to the real answer and it took us a long time to get this extremely bad answer.

00:02:35.000 --> 00:02:38.000
There's some nice mathematical tricks like the Chernoff bound that tells us

00:02:38.000 --> 00:02:42.000
how many times we have to do a random sample as a function of the variance

00:02:42.000 --> 00:02:46.000
of the distribution before we can get an estimate of a particular level of accuracy

00:02:46.000 --> 00:02:48.000
with a particular level of certainty.

00:02:48.000 --> 00:02:51.000
So if you want to be really careful about this, you can actually look up

00:02:51.000 --> 00:02:55.000
the Chernoff bound and apply it to figure out how many samples are needed

00:02:55.000 --> 00:02:59.000
but for current purposes run it as long as you can run it and hope for the best.

00:02:59.000 --> 00:03:02.000
All right. So that's really all I wanted to go over in Unit 5

00:03:02.000 --> 00:03:06.000
and then we pick it up again in Unit 6 talking about computational complexity

00:03:06.000 --> 00:03:09.000
and its relation to social network analysis. See you then.

