WEBVTT
Kind: captions
Language: en

00:00:00.500 --> 00:00:02.647
Two more technical details
about Word 2 Vec and

00:00:02.647 --> 00:00:05.270
about methods to learn
embeddings in general.

00:00:05.270 --> 00:00:09.320
First, because of the way embeddings are
trained, it's often better to measure

00:00:09.320 --> 00:00:13.430
the closeness using a cosine
distance instead of L2, for example.

00:00:13.430 --> 00:00:16.079
That's because the length
of the embedding vector

00:00:16.079 --> 00:00:18.340
is not relevant to the classification.

00:00:18.340 --> 00:00:21.910
In fact, it's often better to
normalize all embedding vectors

00:00:21.910 --> 00:00:23.930
to simply have unit norm.

00:00:23.930 --> 00:00:27.080
Second, we have the issue of
trying to predict words, and

00:00:27.080 --> 00:00:28.330
there are lots of them.

00:00:28.330 --> 00:00:32.450
So in Word 2 Vec we have this set up,
we have a word that we're going to embed

00:00:32.450 --> 00:00:37.070
into a small vector, then feed that into
a simple linear model with weights and

00:00:37.070 --> 00:00:41.360
biases and
that outputs a softmax probability.

00:00:41.360 --> 00:00:45.560
This is then compared to the target
which is another word in the context

00:00:45.560 --> 00:00:47.560
of the input word.

00:00:47.560 --> 00:00:49.550
The problem of course is
that there might be many,

00:00:49.550 --> 00:00:51.790
many words in our vocabulary.

00:00:51.790 --> 00:00:56.740
And computing the softmax function of
all those words can be very inefficient.

00:00:56.740 --> 00:00:58.190
But you can use a trick.

00:00:58.190 --> 00:01:02.120
Instead of treating the softmax as
if the label had probability of 1,

00:01:02.120 --> 00:01:04.730
and every other word
had probability of 0,

00:01:04.730 --> 00:01:07.960
you can sample the words
that are not the targets,

00:01:07.960 --> 00:01:12.880
pick only a handful of them and act
as if the other words were not there.

00:01:12.880 --> 00:01:15.430
This idea of sampling
the negative targets for

00:01:15.430 --> 00:01:18.770
each example is often
called sampled softmax.

00:01:18.770 --> 00:01:21.940
And it makes things faster
at no cost in performance.

00:01:21.940 --> 00:01:23.200
That's all there is to it.

00:01:23.200 --> 00:01:27.570
You take your large corpus of texts,
look at each word, embed it, and

00:01:27.570 --> 00:01:29.150
predict its neighbors.

00:01:29.150 --> 00:01:32.300
What you get out of that is
a mapping from words to vectors

00:01:32.300 --> 00:01:34.730
where similar words are going to
be close to each other.

