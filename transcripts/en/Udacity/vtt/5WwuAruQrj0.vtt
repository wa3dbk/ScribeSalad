WEBVTT
Kind: captions
Language: en

00:00:00.380 --> 00:00:03.390
Okay so Michael let's see if we can do something that

00:00:03.390 --> 00:00:06.520
might be faster and just as good as what we've been

00:00:06.520 --> 00:00:09.820
doing with value iterations. And what we're going to do is we're

00:00:09.820 --> 00:00:14.200
going to emphasize the fact that we care about policies, not values.

00:00:14.200 --> 00:00:16.540
Now it's true given the true utilities we can find a

00:00:16.540 --> 00:00:20.590
policy, but maybe we don't have to find the true utilities. In

00:00:20.590 --> 00:00:23.070
order to find the optimal policy. So, here's a little sketch

00:00:23.070 --> 00:00:25.500
of an algorithm okay, so it's going to look a lot like value

00:00:25.500 --> 00:00:28.290
to ratio. We are going to start with some initial

00:00:28.290 --> 00:00:31.330
policy, let's call it pi sub zero and that's just

00:00:31.330 --> 00:00:32.549
going to be a guess, so it's just going to be an

00:00:32.549 --> 00:00:35.470
arbitrary setting of actions that you should take in different

00:00:35.470 --> 00:00:40.540
states. Then we're going to evaluate how good that policy is, and

00:00:40.540 --> 00:00:45.660
the way we're going to it at time T. Is to calculate its utility,

00:00:45.660 --> 00:00:50.500
which I'm going to call U sub t. Which is equal to the utility you get by

00:00:50.500 --> 00:00:53.830
following that policy. Okay? And I, I'll show you in a moment

00:00:53.830 --> 00:00:56.390
exactly how you do that, alright? But I just want to make certain

00:00:56.390 --> 00:01:00.330
that you, that you, you kind of buy that maybe we can

00:01:00.330 --> 00:01:03.570
do that. So, We have, given a policy, we ought to be able

00:01:03.570 --> 00:01:06.030
to evaluate it by figuring out what the utility of that policy

00:01:06.030 --> 00:01:08.790
is. And, again, we'll talk about that in a second. And then, after

00:01:08.790 --> 00:01:11.690
we know what the utility of that policy is, we're actually going to

00:01:11.690 --> 00:01:15.630
improve that policy in a way similar to what we did with value

00:01:15.630 --> 00:01:19.330
iteration, we're going to update our policy, Time T plus one, to

00:01:19.330 --> 00:01:23.910
be the policy. That takes the actions that maximizes the expected

00:01:23.910 --> 00:01:27.920
utility based of what we just calculated for [INAUDIBLE] of T.

00:01:27.920 --> 00:01:30.370
Now notice it will allow us to change Pi over time

00:01:30.370 --> 00:01:34.020
because image that we discover that in some state that actually

00:01:34.020 --> 00:01:36.880
there is an action that is very good in that state.

00:01:36.880 --> 00:01:38.470
That gets you some place really nice gives you a really

00:01:38.470 --> 00:01:41.480
big reward and that went on and you do fairly well.

00:01:41.480 --> 00:01:44.800
Well then all other states that can reach that state. Might

00:01:44.800 --> 00:01:47.630
end up taking a different action than they did before, because

00:01:47.630 --> 00:01:50.420
now the best action would be to move towards that state.

00:01:50.420 --> 00:01:54.300
So these two steps will actually, or can actually lead to some

00:01:54.300 --> 00:01:58.150
improvement of the policy over time. But the key thing here

00:01:58.150 --> 00:02:00.470
is we have to figure out exactly how to compute u sub

00:02:00.470 --> 00:02:02.770
t. Well, the good news is we know how to do

00:02:02.770 --> 00:02:06.760
that, and it's actually pretty easy. And it boils down to our

00:02:06.760 --> 00:02:08.380
favorite equation, Doman's equation.

00:02:08.380 --> 00:02:09.520
&gt;&gt; [LAUGH]

00:02:09.520 --> 00:02:14.020
&gt;&gt; So our utility at time t, that is the utility that we

00:02:14.020 --> 00:02:18.060
get by following a policy at time t, is just well, the true reward

00:02:18.060 --> 00:02:19.220
that we're going to get by entering that

00:02:19.220 --> 00:02:24.330
state plus gamma times the expected utility,

00:02:25.360 --> 00:02:28.130
which now looks like this. There, does that make sense? Do you see that?

00:02:28.130 --> 00:02:29.830
&gt;&gt; Okay, hang on, it looks a little

00:02:29.830 --> 00:02:31.850
different from the other equation. So did you

00:02:31.850 --> 00:02:37.050
mean for it to have T UT is defined in terms of UT and not UT minus one?

00:02:37.050 --> 00:02:37.550
&gt;&gt; Yes.

00:02:37.550 --> 00:02:41.600
&gt;&gt; Okay, that's interesting. And the max is gone but

00:02:41.600 --> 00:02:45.410
instead of max, there's a policy. Stuck into the transition function.

00:02:45.410 --> 00:02:46.000
&gt;&gt; Yep.

00:02:46.000 --> 00:02:48.090
&gt;&gt; A choice of action is determined by the policy.

00:02:48.090 --> 00:02:52.280
&gt;&gt; Right. And that's actually the only difference between what we were doing

00:02:52.280 --> 00:02:56.860
before is that rather than having this max over actions, we already know what

00:02:56.860 --> 00:02:58.100
action we're going to take. It's determined

00:02:58.100 --> 00:02:59.600
by the policy we're currently following.

00:02:59.600 --> 00:03:01.380
&gt;&gt; Okay, but isn't this just as hard

00:03:01.380 --> 00:03:03.660
as solving? The thing with the max, you said?

00:03:03.660 --> 00:03:05.780
&gt;&gt; Well, what was the problem that we were solving before with the max?

00:03:05.780 --> 00:03:07.560
&gt;&gt; That was the Bellman equation.

00:03:07.560 --> 00:03:11.290
&gt;&gt; Yes, but we were solving a bunch of equations. How many of them?

00:03:11.290 --> 00:03:11.540
&gt;&gt; N.

00:03:11.540 --> 00:03:15.050
&gt;&gt; So we were solving n equations, and how many unknowns?

00:03:15.050 --> 00:03:15.380
&gt;&gt; N.

00:03:15.380 --> 00:03:18.260
&gt;&gt; What's the difference between this, n equations and

00:03:18.260 --> 00:03:20.870
n unknowns, and the other n equations and n unknowns?

00:03:20.870 --> 00:03:22.870
&gt;&gt; Well, n is the same.

00:03:22.870 --> 00:03:23.180
Mm hm.

00:03:23.180 --> 00:03:24.990
&gt;&gt; There's no max, though.

00:03:24.990 --> 00:03:28.500
&gt;&gt; There's no max. And what was it that made solving that hard before?

00:03:29.850 --> 00:03:31.950
&gt;&gt; It made it, the max made it non linear. The max is

00:03:31.950 --> 00:03:36.660
gone now. You're saying this is, this is a set of linear equations?

00:03:36.660 --> 00:03:40.490
&gt;&gt; Yeah. Because, well, there's, there's just a bunch of sums. And the

00:03:40.490 --> 00:03:44.250
pi is not like some weird function. This is just effectively a constant.

00:03:44.250 --> 00:03:45.640
&gt;&gt; I see.

00:03:45.640 --> 00:03:48.100
&gt;&gt; So now, I have n equations, and n unknowns.

00:03:48.100 --> 00:03:50.440
But it's in linear equations. And now that I

00:03:50.440 --> 00:03:53.310
have in linear equations and unknowns, I can actually compute

00:03:53.310 --> 00:03:55.600
this is a reasonable amount of time, by doing

00:03:55.600 --> 00:03:59.340
matrix inversions, and regression, and other magic hand wavey things.

00:03:59.340 --> 00:04:00.710
&gt;&gt; That's very slick.

00:04:00.710 --> 00:04:00.800
&gt;&gt; Yeah.

00:04:00.800 --> 00:04:03.180
&gt;&gt; It's seems, it's still more expensive than doing the

00:04:03.180 --> 00:04:07.960
valued [UNKNOWN], I guess. Yeah, but you don't have to, perhaps,

00:04:07.960 --> 00:04:10.840
do as many iterations as you were doing before. So

00:04:10.840 --> 00:04:13.360
once you've evaluated it, which we now know how to do,

00:04:13.360 --> 00:04:15.090
and you've improved it, you just keep

00:04:15.090 --> 00:04:16.829
doing that until your policy doesn't change.

00:04:16.829 --> 00:04:17.890
&gt;&gt; Very cool.

00:04:17.890 --> 00:04:21.480
&gt;&gt; Mmhm. And this does look a lot like value iteration to you, doesn't it?

00:04:21.480 --> 00:04:25.350
&gt;&gt; Yeah, though is seems like it's making bigger jumps somehow.

00:04:25.350 --> 00:04:26.970
&gt;&gt; It is, and that's because instead

00:04:26.970 --> 00:04:29.650
of making jumps. In value space, it's making

00:04:29.650 --> 00:04:31.830
jumps in policy space. Which is why

00:04:31.830 --> 00:04:34.290
we call this class of algorithms Policy Iteration.

00:04:34.290 --> 00:04:35.724
&gt;&gt; Cool.

00:04:35.724 --> 00:04:37.533
&gt;&gt; Right.

00:04:37.533 --> 00:04:43.350
Now, this inversion can still be fairly painful, it's, you

00:04:43.350 --> 00:04:46.010
know, if we don't worry about being highly efficient, you know,

00:04:46.010 --> 00:04:48.070
it's roughly n cubed, and if there are a lot of

00:04:48.070 --> 00:04:51.270
states, this can be kind of painful. But it turns out there's

00:04:51.270 --> 00:04:54.100
little tricks you can do, like do a little step

00:04:54.100 --> 00:04:58.020
evaluate iteration here for a while to get an estimate and

00:04:58.020 --> 00:05:00.550
then, you know, kind of cycle through. So there's all kinds of

00:05:00.550 --> 00:05:02.790
clever things you might want to do, but at a high level

00:05:02.790 --> 00:05:06.700
without worrying about, you know, the, the details of constants, this general

00:05:06.700 --> 00:05:10.620
process of moving through policy space. And taking advantage of the fact

00:05:10.620 --> 00:05:15.490
that by picking a specific policy you're able to turn your nonlinear

00:05:15.490 --> 00:05:18.470
equations into linear equations turns out to often to be very helpful.

00:05:18.470 --> 00:05:21.030
&gt;&gt; So, is it guaranteed to converge?

00:05:21.030 --> 00:05:21.810
&gt;&gt; Yes.

00:05:21.810 --> 00:05:22.760
&gt;&gt; Nice.

00:05:22.760 --> 00:05:25.770
&gt;&gt; Well there, that was easy. I'm, I'm not going

00:05:25.770 --> 00:05:27.910
to go into it but, you know, there's a finite number

00:05:27.910 --> 00:05:32.000
of policies. You're always getting better so eventually

00:05:32.000 --> 00:05:33.390
you have to converge. It's very similar to

00:05:33.390 --> 00:05:35.520
the, or at least intuitively it's very similar

00:05:35.520 --> 00:05:37.270
to the argument you might make for [UNKNOWN]. Cool.

