WEBVTT
Kind: captions
Language: en

00:00:00.370 --> 00:00:04.650
Consider our robot here that's
interacting with the world.

00:00:04.650 --> 00:00:10.920
The first thing it sees is some state,
and as we've talked about already,

00:00:10.920 --> 00:00:15.120
the thing to do now is to take that
state, go look in the Q table and

00:00:15.120 --> 00:00:19.710
find the action that corresponds
to the maximum Q value.

00:00:19.710 --> 00:00:21.860
And then take that action.

00:00:21.860 --> 00:00:25.130
Once that action is taken,
that results in two new things.

00:00:25.130 --> 00:00:32.530
One is a new state,
we call that S prime, and a reward.

00:00:32.530 --> 00:00:34.970
All that information
comes into the robot, and

00:00:34.970 --> 00:00:38.900
it needs to use that information
to update its Q table.

00:00:38.900 --> 00:00:43.770
So as a consequence of this interaction
with the world, it's got an s,

00:00:43.770 --> 00:00:46.840
an a, an s prime, and an r.

00:00:47.840 --> 00:00:52.060
How does it take that information
to improve this Q table?

00:00:52.060 --> 00:00:54.770
There are two main parts
to the update rule.

00:00:54.770 --> 00:00:59.612
The first is, what is the old
value that we used to have?

00:00:59.612 --> 00:01:01.692
And that's Q [s, a].

00:01:02.780 --> 00:01:04.920
And what is our improved estimate?

00:01:04.920 --> 00:01:07.320
And we want to blend them together.

00:01:07.320 --> 00:01:10.740
And to combine them,
we introduce this new variable, alpha.

00:01:10.740 --> 00:01:13.110
Alpha is the learning rate.

00:01:13.110 --> 00:01:16.890
Alpha can take on any value from 0 to 1.

00:01:16.890 --> 00:01:21.430
Usually, we use about 0.2.

00:01:21.430 --> 00:01:27.300
And what that means is,
in our new improved version of Q,

00:01:27.300 --> 00:01:32.410
which I indicate over here as Q prime,
it's a blend of alpha

00:01:32.410 --> 00:01:38.140
times the improved estimate,
plus 1 minus alpha of the old value.

00:01:38.140 --> 00:01:41.920
So larger values of alpha cause
us to learn more quickly,

00:01:41.920 --> 00:01:46.610
lower values of alpha cause
the learning to be more slow.

00:01:46.610 --> 00:01:50.460
So a low value of alpha, for instance,
means that in this update rule,

00:01:50.460 --> 00:01:55.250
the previous value for
Q of sa is more strongly preserved.

00:01:56.370 --> 00:01:58.840
So stretching it out
a little bit more detail.

00:01:58.840 --> 00:02:04.820
Again we have here,
our current value for the Q at s,a,

00:02:04.820 --> 00:02:12.830
plus alpha times the immediate reward,
plus gamma times later rewards.

00:02:12.830 --> 00:02:17.019
Now we're introducing gamma here, so I
promise there's only two new parameters

00:02:17.019 --> 00:02:19.330
here that you have to worry about.

00:02:19.330 --> 00:02:21.360
What gamma is is the discount rate.

00:02:22.640 --> 00:02:27.590
And similar to alpha,
gamma usually ranges from 0 to 1.

00:02:27.590 --> 00:02:35.060
A low value of gamma means that
we value later rewards less.

00:02:35.060 --> 00:02:38.108
Remember the discount rate when
we were talking about bonds?

00:02:38.108 --> 00:02:40.683
Same thing.

00:02:40.683 --> 00:02:48.660
A low value of gamma equates to
essentially a high discount rate.

00:02:48.660 --> 00:02:51.950
The high value of gamma,
in other words a gamma near 1,

00:02:51.950 --> 00:02:57.440
means that we value later
rewards very significantly.

00:02:57.440 --> 00:03:03.130
If we were to use 1.0, that means
a reward 20 steps in the future

00:03:03.130 --> 00:03:06.170
is worth just as much
as a reward right now.

00:03:06.170 --> 00:03:10.100
Now, we have to expand this component
here in a little bit more detail.

00:03:11.130 --> 00:03:15.020
This next part here is a little
bit tricky, but don't worry,

00:03:15.020 --> 00:03:17.710
we're going to step
through it step by step.

00:03:17.710 --> 00:03:22.550
This component represents our
future discounted rewards.

00:03:22.550 --> 00:03:26.744
In other words, we end up in state
s prime, and from then on out,

00:03:26.744 --> 00:03:31.439
we're going to act optimally, or
at least, the best that we know how to.

00:03:31.439 --> 00:03:36.076
And the question is,
what is the value of those future

00:03:36.076 --> 00:03:41.550
rewards if we reach state S prime and
we act appropriately?

00:03:41.550 --> 00:03:47.290
Well, it is simply that Q value, but
we have to find out what the action

00:03:47.290 --> 00:03:51.700
is that we would have taken, so that
we can reference the Q table properly.

00:03:51.700 --> 00:03:56.760
So, If we're in state s prime,
the action that

00:03:56.760 --> 00:04:01.760
we would take that would maximize
our future reward is argmax a prime,

00:04:01.760 --> 00:04:05.440
so a prime is the next action
that we're going to take.

00:04:05.440 --> 00:04:08.500
With regard to Q, s prime, a prime.

00:04:08.500 --> 00:04:11.540
So we're going to find
that best a prime,

00:04:11.540 --> 00:04:16.507
that best action, that maximizes
the value when we're in that state.

00:04:16.507 --> 00:04:21.500
So this collapses just to a prime, and

00:04:21.500 --> 00:04:25.960
then we look up the Q table value for
Q s prime, a prime.

00:04:27.050 --> 00:04:29.970
So that allows us to bring
it all together now.

00:04:29.970 --> 00:04:32.560
So our update rule is the following.

00:04:32.560 --> 00:04:37.334
Our new Q value in state s,
action a is that

00:04:37.334 --> 00:04:42.440
old value multiplied by 1 minus alpha.

00:04:42.440 --> 00:04:46.630
So depending on how large alpha is,
we valued that old value more or

00:04:46.630 --> 00:04:51.970
less, plus alpha times
our new best estimate.

00:04:53.360 --> 00:04:57.480
And our new best estimate is,
again, our immediate reward,

00:04:57.480 --> 00:05:02.350
plus the discounted reward for
all of our future actions.

00:05:02.350 --> 00:05:03.520
And that's it.

00:05:03.520 --> 00:05:06.150
This is the equation you need
to know to implement Q learning.

