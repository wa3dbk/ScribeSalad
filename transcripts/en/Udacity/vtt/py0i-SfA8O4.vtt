WEBVTT
Kind: captions
Language: en

00:00:00.080 --> 00:00:03.400
To see what the RAID 0
reliability looks like,

00:00:03.400 --> 00:00:09.160
let's assume that f is a failure
rate for a single disk.

00:00:09.160 --> 00:00:13.690
A failure rate is really how
many failures do we expect for

00:00:13.690 --> 00:00:17.220
a single working disk
to have in a second.

00:00:17.220 --> 00:00:20.800
Obviously, this number
will be extremely small.

00:00:20.800 --> 00:00:24.350
Usually we can assume that the failure
rate is constant over time,

00:00:24.350 --> 00:00:29.070
that is, every second, if the desk is
working at the beginning of the second,

00:00:29.070 --> 00:00:32.790
it has the same failure
rate during that second.

00:00:32.790 --> 00:00:35.500
For a single disk then, the MTTF,

00:00:35.500 --> 00:00:40.630
the mean time to failure,
will be 1 over f.

00:00:40.630 --> 00:00:44.780
So, for example, if the disk has one
millionth of a failure per second,

00:00:44.780 --> 00:00:48.830
then the MTTF will be 1 million seconds.

00:00:48.830 --> 00:00:54.090
For disks, the entity f is also
sometimes called mean time

00:00:54.090 --> 00:00:59.690
to data loss, or how long until
we lose some data on this disk.

00:00:59.690 --> 00:01:02.740
And for a single disk,
the mean time to data loss

00:01:02.740 --> 00:01:07.030
is the same as the MTTF of one disk.

00:01:07.030 --> 00:01:09.750
Now, let's look at what happens when we

00:01:09.750 --> 00:01:13.310
have N such disks in
a RAID 0 configuration.

00:01:13.310 --> 00:01:17.560
It turns out that the failure
rate when we have N working disks

00:01:17.560 --> 00:01:23.200
is equal to N times
the failure rate of one disk.

00:01:23.200 --> 00:01:24.580
This is because, for

00:01:24.580 --> 00:01:29.810
example, if we can expect one millionth
of a failure per working disk per second

00:01:29.810 --> 00:01:34.670
then we can expect one failure per
million working disks per second.

00:01:34.670 --> 00:01:37.870
So if we start the second
with a million working disks,

00:01:37.870 --> 00:01:41.150
chances are that one of them
will fail during the second.

00:01:41.150 --> 00:01:45.760
If we start the second with
half a million working disks,

00:01:45.760 --> 00:01:49.780
on average half of a single disk
will fail per second, and so on.

00:01:50.950 --> 00:01:57.250
It also turns out that the mean time to
the first failure among N working disks

00:01:57.250 --> 00:02:02.810
has the same distribution as the first
failure for the single disk, so

00:02:02.810 --> 00:02:09.419
we get that MTTF for
N disks in a RAID 0 configuration,

00:02:09.419 --> 00:02:14.800
where MTTF is really the mean
time to data loss in this case,

00:02:14.800 --> 00:02:20.890
is equal to the MTTF of
one disk divided by N.

00:02:20.890 --> 00:02:25.460
So if you have two disks,
the MTTF of the RAID 0 array with two

00:02:25.460 --> 00:02:30.230
disks will be half of
the MTTF of a single disk.

00:02:30.230 --> 00:02:33.050
A single disk typically
will have the MTTF

00:02:33.050 --> 00:02:37.020
on the order of ten-ish
years to maybe 100 years.

00:02:37.020 --> 00:02:41.550
So if we have too many disks in a raid
zero configuration, we can reduce

00:02:41.550 --> 00:02:46.170
the MTTF, so the average disk will not
fail in the five or six years that we

00:02:46.170 --> 00:02:50.850
actually end up using it, but if we have
two or three or four such disks then we

00:02:50.850 --> 00:02:55.120
can expect the array to fail at least
once until we actually get rid of it.

