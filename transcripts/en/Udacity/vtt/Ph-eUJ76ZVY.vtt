WEBVTT
Kind: captions
Language: en

00:00:00.180 --> 00:00:03.719
Modern parallel machines offer a lot of challenges

00:00:03.719 --> 00:00:07.020
in converting the algorithms and the techniques that

00:00:07.020 --> 00:00:11.355
we have learned so far into scalable implementations.

00:00:11.355 --> 00:00:13.850
Now what are some of these challenges? Well

00:00:13.850 --> 00:00:16.790
first of all, there's a size bloat of

00:00:16.790 --> 00:00:19.810
the operating system. And the size bloat comes

00:00:19.810 --> 00:00:22.930
because of additional features that we have to

00:00:22.930 --> 00:00:25.180
add to the operating system and so on.

00:00:25.180 --> 00:00:29.760
And that results in, in system software bottlenecks, especially

00:00:29.760 --> 00:00:32.930
for global data structures. And then, of course, we

00:00:32.930 --> 00:00:36.560
already have been discussing this quite a bit, that

00:00:36.560 --> 00:00:39.890
the memory latency to go from the processor to

00:00:39.890 --> 00:00:45.480
memory is, is huge. All the cores of the processor are on a chip, and if you go

00:00:45.480 --> 00:00:47.980
outside the chip to the memory, that latency is

00:00:47.980 --> 00:00:50.200
huge. 100 to one ratio is what we've been talking

00:00:50.200 --> 00:00:53.080
about and that latency is only growing. The

00:00:53.080 --> 00:00:55.370
other thing that happens in parallel machines is the

00:00:55.370 --> 00:00:58.200
fact that, this is a single node. And

00:00:58.200 --> 00:01:00.210
we're talking about the memory latency going from the

00:01:00.210 --> 00:01:05.519
processor to the memory. But in a parallel machine, it's typically constructed

00:01:05.519 --> 00:01:10.550
as a non-uniform memory access machine. And that is, you take

00:01:10.550 --> 00:01:15.280
individual nodes like this that contains a processor and memory and

00:01:15.280 --> 00:01:17.330
put all them together and connect them through an

00:01:17.330 --> 00:01:21.150
interconnection network. And what happens with this NUMA architecture

00:01:21.150 --> 00:01:25.520
is that access, there's differential access to memory whether

00:01:25.520 --> 00:01:29.840
this processor is accessing memory that is local to it,

00:01:29.840 --> 00:01:32.460
or it has to reach out into the network

00:01:32.460 --> 00:01:34.540
and access some memory that is farther away from

00:01:34.540 --> 00:01:37.150
where it is. In addition to the NUMA effect,

00:01:37.150 --> 00:01:40.570
there is also the memory hierarchy itself is very deep.

00:01:40.570 --> 00:01:43.570
We already talked about the fact a single

00:01:43.570 --> 00:01:48.180
processor these days contains multiple levels of caches before

00:01:48.180 --> 00:01:50.780
it goes to the memory. And this deep memory

00:01:50.780 --> 00:01:52.930
hierarchy is another thing that, that you have to

00:01:52.930 --> 00:01:55.350
worry about in building the operating system for a

00:01:55.350 --> 00:01:58.000
parallel machine. And there is the issue of false

00:01:58.000 --> 00:02:02.000
sharing. And false sharing is essentially saying that even

00:02:02.000 --> 00:02:06.080
though programmatically there is no connection between a piece

00:02:06.080 --> 00:02:09.330
of memory that is being touched by a

00:02:09.330 --> 00:02:12.530
particular thread executing on this core, another thread

00:02:12.530 --> 00:02:14.890
that is executing on, on this core. The

00:02:14.890 --> 00:02:19.300
cache hierarchy may make the block that contains the

00:02:19.300 --> 00:02:21.980
individual memory touched by different threads on different

00:02:21.980 --> 00:02:24.870
cores to be on the same cache block. So,

00:02:24.870 --> 00:02:28.050
programmatically, there's no sharing, but because of the

00:02:28.050 --> 00:02:31.150
fact that the memory that is being touched by

00:02:31.150 --> 00:02:33.210
a thread on this core, and a memory that

00:02:33.210 --> 00:02:35.430
is being touched by a thread on this core happen

00:02:35.430 --> 00:02:37.680
to be on the same cache line, they appear

00:02:37.680 --> 00:02:40.590
to be shared. That's what is false sharing. False sharing

00:02:40.590 --> 00:02:43.560
is essentially saying that there is no programmatic sharing

00:02:43.560 --> 00:02:47.930
But, because of the way the cache coherence mechanism operates,

00:02:47.930 --> 00:02:50.718
they appear shared. And this is happening more and more

00:02:50.718 --> 00:02:56.360
in modern processors, because modern processors tend to employ larger

00:02:56.360 --> 00:03:00.940
cache blocks. Why is that? Well, the analogy I'm

00:03:00.940 --> 00:03:03.050
gong to give you is that of a handyman. If

00:03:03.050 --> 00:03:05.940
you're good at doing chores around the house, then

00:03:05.940 --> 00:03:10.320
you might relate to this analogy quite well. You probably

00:03:10.320 --> 00:03:13.750
have a tool box if you're a handyman. And

00:03:13.750 --> 00:03:16.520
if you want to do some work, let's say a

00:03:16.520 --> 00:03:18.560
leaky faucet that you want to fix, what you

00:03:18.560 --> 00:03:21.600
do is you put the tools that you need into

00:03:21.600 --> 00:03:25.710
a, a tool tray and bring it from the tool box

00:03:25.710 --> 00:03:28.720
to the site where you're doing, doing the work. And basically, what

00:03:28.720 --> 00:03:31.790
you're doing there is, you know, collecting the set of tools

00:03:31.790 --> 00:03:34.020
that you need for the project so that you don't have to

00:03:34.020 --> 00:03:37.230
go running back to the tool tray all the time. That's

00:03:37.230 --> 00:03:40.888
the same sort of thing that's happening with caching and memory. Memory

00:03:40.888 --> 00:03:43.266
contains all this stuff but what I need, I want to

00:03:43.266 --> 00:03:46.620
bring it in. And the more I bring in from the memory,

00:03:46.620 --> 00:03:49.360
the less time that I have to go out to

00:03:49.360 --> 00:03:51.680
memory in order to fetch it. That means that I want

00:03:51.680 --> 00:03:54.480
to keep increasing the block size of the cache, in

00:03:54.480 --> 00:03:57.550
order to make sure that I take advantage of spatial locality

00:03:57.550 --> 00:04:00.830
in the cache design. And that increases the chances that

00:04:00.830 --> 00:04:04.320
false sharing is going to happen. The larger the cache line,

00:04:04.320 --> 00:04:08.090
the more chances are that memory that is being touched

00:04:08.090 --> 00:04:11.730
by different threads happen to be on the same cache block,

00:04:11.730 --> 00:04:14.460
and that results in false shading. So all of

00:04:14.460 --> 00:04:17.529
these effects, the NUMA effect, the deep memory hierarchy,

00:04:17.529 --> 00:04:21.290
and increasing block size leading to false sharing, all

00:04:21.290 --> 00:04:24.110
of these are things that the operating system designer

00:04:24.110 --> 00:04:27.430
has to worry about in making sure that the

00:04:27.430 --> 00:04:30.580
algorithms and the techniques that we have learned, when

00:04:30.580 --> 00:04:33.310
it is translated to a large scale parallel machine,

00:04:33.310 --> 00:04:36.960
it remains scalable. So that's really the challenge that

00:04:36.960 --> 00:04:39.730
the operating system designer faces. So some of

00:04:39.730 --> 00:04:42.260
the things that the OS designer would have to

00:04:42.260 --> 00:04:46.570
do is work hard to avoid false sharing, work

00:04:46.570 --> 00:04:51.620
hard to reduce write sharing the same cache line.

00:04:51.620 --> 00:04:53.220
Because if you write share the same cache

00:04:53.220 --> 00:04:55.680
line, then it is going to result among different

00:04:55.680 --> 00:04:57.950
cores of the same processor, then it's going to

00:04:57.950 --> 00:05:02.310
result in the cache line migrating from one processor

00:05:02.310 --> 00:05:05.510
to another. And even within the same core

00:05:05.510 --> 00:05:08.970
and even within the same processor, multiple cores,

00:05:08.970 --> 00:05:12.410
and across processors that are on different nodes

00:05:12.410 --> 00:05:14.970
of parallel machine, connected by the interconnection network.

