WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.780
So CUDA DMA is a template library designed to make it easier to use shared memory

00:00:04.780 --> 00:00:06.476
while achieving high performance.

00:00:06.476 --> 00:00:12.783
Now to use CUDA DMA, programmers declare CUDA DMA objects for each shared memory buffer that needs to be loaded or stored,

00:00:12.783 --> 00:00:17.887
and the cool thing is that CUDA DMA let's you explicitly describe the transfer pattern for that data.

00:00:17.887 --> 00:00:23.365
So, for example, you might be transferring one long sequential trunk of memory,

00:00:23.365 --> 00:00:26.730
you might be transferring strided trunks of memory,

00:00:26.730 --> 00:00:29.588
or you might be doing sort of indirect access to memory,

00:00:29.588 --> 00:00:32.073
such as you would find in a sparse matrix representation.

00:00:32.073 --> 00:00:37.600
As with CUB, decoupling the transfer patterns from the actual processing that we're going to do on each

00:00:37.600 --> 00:00:41.472
thread to achieve that transfer pattern has several benefits and improves

00:00:41.472 --> 00:00:45.039
programmability because the code is now simpler.

00:00:45.039 --> 00:00:52.822
You packaged away all of that logic for doing the transfer separately from the actual compute in your kernel.

00:00:52.822 --> 00:00:57.885
It improves portability because you can have the CUDA DMA ninjas or the CUB ninjas

00:00:57.885 --> 00:01:03.394
develop the very best implementations of these various transfer patterns

00:01:03.394 --> 00:01:07.336
for your situation and package that all up in a library for you,

00:01:07.336 --> 00:01:10.765
and because those CUDA ninjas are good at what they do, you get high performance.

00:01:10.765 --> 00:01:13.370
You're going to achieve high DRAM memory bandwidth,

00:01:13.370 --> 00:01:17.772
you're going to hide the global memory latency for kernels that don't have a lot of occupancy,

00:01:17.772 --> 00:01:21.944
and hopefully this will lead to better compiler-generated code.

00:01:21.944 --> 00:01:28.619
And as I said, these benefits really accrue to both CUB, which tackles the whole circle from bringing

00:01:28.619 --> 00:01:34.316
data in from global memory and doing the computation on it, as well as CUDA DMA,

00:01:34.316 --> 00:01:40.567
which is just tackling the top part of that cycle where you're bringing memory into shared memory,

00:01:40.567 --> 00:01:43.000
and then you're going to do your own operations on it,

00:01:43.000 --> 00:01:46.203
so these benefits really accrue to both approaches.

