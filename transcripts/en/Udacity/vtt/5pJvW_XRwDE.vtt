WEBVTT
Kind: captions
Language: en

00:00:00.120 --> 00:00:02.650
So I think there's one more class

00:00:02.650 --> 00:00:04.520
of problems that we didn't
really get a chance get.

00:00:04.520 --> 00:00:07.520
There are tons by the way just
to be clear, algorithms and

00:00:07.520 --> 00:00:10.070
representation that we
didn't really get into.

00:00:10.070 --> 00:00:13.907
A lot of them actually coming out of
big data like k-d trees for example.

00:00:13.907 --> 00:00:17.162
But there's a whole class of problems
we didn't really discuss that's

00:00:17.162 --> 00:00:18.970
actually quite important.

00:00:18.970 --> 00:00:22.390
And that's because it reveals one
of the assumptions that we've had.

00:00:22.390 --> 00:00:28.230
I'm going to call them
the assumption of balanced labels.

00:00:28.230 --> 00:00:29.190
&gt;&gt; Balanced labels.

00:00:29.190 --> 00:00:32.119
&gt;&gt; Yeah, which is related to
cost-sensitive learning and

00:00:32.119 --> 00:00:33.455
a variety of other things.

00:00:33.455 --> 00:00:36.972
So implicit in a lot of the work that
we talked about was this idea that

00:00:36.972 --> 00:00:40.552
if you're a big data set, about half
of them are labeled negative and

00:00:40.552 --> 00:00:42.734
about half of them are labeled positive.

00:00:42.734 --> 00:00:43.910
&gt;&gt; Sure.
&gt;&gt; Right?

00:00:43.910 --> 00:00:47.150
But in practice,
the world doesn't work that way, and

00:00:47.150 --> 00:00:50.010
you have to actually
take that into account.

00:00:50.010 --> 00:00:50.943
&gt;&gt; So, what would be an example?

00:00:50.943 --> 00:00:51.919
&gt;&gt; So, here's my favorite example.

00:00:51.919 --> 00:00:54.073
So imagine you have a camera.

00:00:54.073 --> 00:00:55.441
&gt;&gt; [LAUGH]
&gt;&gt; Close your eyes.

00:00:55.441 --> 00:00:57.948
Now imagine you have a camera, and

00:00:57.948 --> 00:01:02.470
let's say that it's in an airport,
like we are now.

00:01:02.470 --> 00:01:05.918
And people are going through constantly
and in fact, you're in Atlanta Airport,

00:01:05.918 --> 00:01:09.269
which is the world's busiest airport,
and there are cameras constantly taking

00:01:09.269 --> 00:01:12.480
pictures of you, and there are, and
we're trying to find terrorists.

00:01:12.480 --> 00:01:15.750
So we're taking pictures of people,
and of their faces say, and

00:01:15.750 --> 00:01:17.170
maybe what they're wearing, and

00:01:17.170 --> 00:01:19.828
we just want to label which ones
are terrorists and which ones aren't.

00:01:19.828 --> 00:01:23.430
Well the vast majority of
people are not terrorists,

00:01:23.430 --> 00:01:26.170
we hope,
even the ones going through airports.

00:01:26.170 --> 00:01:30.370
And so let's say 99% of
the people that come through,

00:01:30.370 --> 00:01:34.610
you would label negative, and
less than 1% you would label positive.

00:01:34.610 --> 00:01:36.260
Does that make sense as a problem?

00:01:36.260 --> 00:01:37.900
&gt;&gt; Yeah, though I'm thinking
about this issue about

00:01:37.900 --> 00:01:39.920
most people aren't terrorists.

00:01:39.920 --> 00:01:40.490
Like I feel like.

00:01:40.490 --> 00:01:42.050
&gt;&gt; That most people are?

00:01:42.050 --> 00:01:44.323
&gt;&gt; Are there really any non-terrorists,
or

00:01:44.323 --> 00:01:47.869
are we all just terrorists who
just haven't gotten inspired yet?

00:01:47.869 --> 00:01:48.783
&gt;&gt; You've met my son.

00:01:48.783 --> 00:01:49.677
&gt;&gt; [LAUGH]
&gt;&gt; So

00:01:49.677 --> 00:01:53.719
let's say terror is by some
definition ruling to accept.

00:01:53.719 --> 00:01:54.469
&gt;&gt; Yeah, yeah.

00:01:54.469 --> 00:01:55.782
Let's talk-
&gt;&gt; How about a threat?

00:01:55.782 --> 00:01:57.500
A threat-
&gt;&gt; That's where I was going to go.

00:01:57.500 --> 00:02:00.253
Yes, forget whether they're terrorists
or not, just In the airport, the airport

00:02:00.253 --> 00:02:02.600
terrorists, this person is threatening
to the safety of the airport.

00:02:02.600 --> 00:02:04.010
&gt;&gt; And in fact we can back
it up a little bit and

00:02:04.010 --> 00:02:06.160
say it was not even terrorists, it's
just someone's that's going to cause

00:02:06.160 --> 00:02:07.890
some kind of trouble as
defined by the airports.

00:02:07.890 --> 00:02:10.490
So you know we'll get irritated
as they go through the line.

00:02:10.490 --> 00:02:13.920
&gt;&gt; Okay, that explains why the last
time that we got together in Atlanta I

00:02:13.920 --> 00:02:15.310
was chased by police.

00:02:15.310 --> 00:02:16.560
&gt;&gt; You were chased by police.

00:02:16.560 --> 00:02:17.400
That's a funny story,

00:02:17.400 --> 00:02:20.040
he was literally chased by
the police in the Atlanta airport.

00:02:20.040 --> 00:02:21.320
You got outta that one
pretty well I thought.

00:02:21.320 --> 00:02:22.500
&gt;&gt; Thanks.

00:02:22.500 --> 00:02:23.260
&gt;&gt; Eventually.

00:02:23.260 --> 00:02:23.890
&gt;&gt; Yeah.

00:02:23.890 --> 00:02:25.387
&gt;&gt; If we're not happy with-

00:02:25.387 --> 00:02:26.770
&gt;&gt; [LAUGH]
&gt;&gt; Anyway.

00:02:26.770 --> 00:02:30.975
&gt;&gt; So anyway, let's say maybe 3%
of people cause trouble in a day.

00:02:30.975 --> 00:02:32.160
&gt;&gt; [LAUGH]
&gt;&gt; Michael.

00:02:32.160 --> 00:02:32.670
&gt;&gt; Yeah.

00:02:32.670 --> 00:02:34.340
&gt;&gt; And let's say the other 97% don't, so

00:02:34.340 --> 00:02:37.230
if you were going to throw this
at a learner, what would happen.

00:02:38.470 --> 00:02:40.440
&gt;&gt; Yeah.
&gt;&gt; The best learner is probably the one

00:02:40.440 --> 00:02:41.710
that just says.

00:02:41.710 --> 00:02:42.648
&gt;&gt; No one will cause trouble.

00:02:42.648 --> 00:02:43.462
&gt;&gt; No one will cause trouble.

00:02:43.462 --> 00:02:45.737
&gt;&gt; because it could be wrong sometimes,
yeah that's right.

00:02:45.737 --> 00:02:47.790
It'll be wrong sometimes,
but very rarely.

00:02:47.790 --> 00:02:51.210
&gt;&gt; Yeah, 97% is pretty good,
well that's the problem.

00:02:51.210 --> 00:02:53.090
That's what's called
the majority classifier.

00:02:53.090 --> 00:02:53.640
&gt;&gt; Yes.

00:02:53.640 --> 00:02:57.840
&gt;&gt; And it's a baseline,
which we've talked about a little bit.

00:02:57.840 --> 00:02:59.960
And the problem is that when
your algorithm does 97%,

00:02:59.960 --> 00:03:02.730
that's actually not very
impressive on the one hand.

00:03:02.730 --> 00:03:04.604
But there's another issue which is that.

00:03:04.604 --> 00:03:05.700
&gt;&gt; On this date it's not so impressive.

00:03:05.700 --> 00:03:06.795
&gt;&gt; On this date it's not so impressive.

00:03:06.795 --> 00:03:12.470
But this other issue which
is the cost of being wrong.

00:03:12.470 --> 00:03:16.250
When somebody really is going
to cause trouble is much higher.

00:03:16.250 --> 00:03:19.690
So it's actually very important
that you get that 3%,

00:03:19.690 --> 00:03:21.230
you can't just treat it as 3%.

00:03:21.230 --> 00:03:23.620
So I'm just going to just very
briefly go through this and

00:03:23.620 --> 00:03:24.630
then we're going to move on.

00:03:24.630 --> 00:03:26.248
There's a bunch of approaches to this.

00:03:26.248 --> 00:03:27.880
You could re-weight
the way you do error,

00:03:27.880 --> 00:03:29.840
there's all kinds of
clever things you can do.

00:03:29.840 --> 00:03:33.460
But there's one that I really like which
is do to Paul Viola and Mike Jones,

00:03:33.460 --> 00:03:35.115
it was applied to a vision problem.

00:03:35.115 --> 00:03:37.310
It's called cascade learning.

00:03:37.310 --> 00:03:38.750
I like this because,
it's a seminal paper.

00:03:38.750 --> 00:03:40.140
I like this because it
is rally good work.

00:03:40.140 --> 00:03:42.343
And also because Paul Viola
is one of my advisers and

00:03:42.343 --> 00:03:45.380
Mike Jones was my office mate
when I was in grad school.

00:03:45.380 --> 00:03:48.165
They did all of this while I was sitting
there and didn't put me on their paper.

00:03:48.165 --> 00:03:51.630
&gt;&gt; [LAUGH] You think you should be on
the paper just by virtue of the fact you

00:03:51.630 --> 00:03:53.450
were there inspiring
them with your presence.

00:03:53.450 --> 00:03:55.278
&gt;&gt; My [SOUND] agent
let's them go up by one.

00:03:55.278 --> 00:03:56.192
&gt;&gt; Yeah.

00:03:56.192 --> 00:03:57.040
&gt;&gt; And that's important.

