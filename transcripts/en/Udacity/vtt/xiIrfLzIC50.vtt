WEBVTT
Kind: captions
Language: en

00:00:00.430 --> 00:00:04.434
Let's shift gears now and talk about ways to avoid programming in the first place.

00:00:04.434 --> 00:00:07.294
There are many libraries available for CUDA, more come out every day.

00:00:07.294 --> 00:00:10.146
There are some libraries that are developed by NVIDIA, some by third parties,

00:00:10.146 --> 00:00:12.849
there's open source libraries, and there's commercial products,

00:00:12.849 --> 00:00:15.970
and of course there's no way I can cover all of the available libraries out there,

00:00:15.970 --> 00:00:18.450
but I do want to hit a few highlights here

00:00:18.450 --> 00:00:20.858
and talk about some of the more popular and useful libraries out there.

00:00:20.858 --> 00:00:23.775
The libraries I'm going to talk about are, for the most part, fairly mature.

00:00:23.775 --> 00:00:26.496
They're designed and optimized for performance by experts,

00:00:26.496 --> 00:00:30.284
and they get tuned up and re-released every time a new GPU architecture comes out.

00:00:30.284 --> 00:00:32.445
So if you can use these libraries, you should.

00:00:32.445 --> 00:00:35.720
So the first one I want to highlight is called cuBLAS,

00:00:35.720 --> 00:00:40.401
and cuBLAS is an implementation of the BLAS, or Basic Linear Algebra Subroutines.

00:00:40.401 --> 00:00:43.364
This is a venerable library that's been around for a long time.

00:00:43.364 --> 00:00:45.777
It's used in Fortran and C,

00:00:45.777 --> 00:00:51.660
and scientists and engineers everywhere use this as one of their go-to workhorses for dealing with dense linear algebra.

00:00:51.660 --> 00:00:53.923
Next is cuFFT.

00:00:53.923 --> 00:00:59.062
FFT stands for Fast Fourier Transform, and so not surprisingly, this is the CUDA Fast Fourier Transform.

00:00:59.062 --> 00:01:01.722
This includes various batched transforms,

00:01:01.722 --> 00:01:07.110
as well as support for various real to complex, complex to complex FFTs and so forth.

00:01:07.110 --> 00:01:10.952
It has an interface similar in ways to the FFTW,

00:01:10.952 --> 00:01:14.141
the popular Fastest Fourier Transform in the West routine,

00:01:14.141 --> 00:01:21.693
so it's a familiar interface to anybody who uses FFTs as part of their bread and butter toolbox.

00:01:21.693 --> 00:01:26.496
cuSPARSE is BLAS-like routines for doing linear algebra on sparse matrix formats.

00:01:26.496 --> 00:01:31.061
Sparse matrices, as you know, are matrices that are mostly 0 and therefore stored in some sort of compressed format,

00:01:31.061 --> 00:01:33.908
and cuSPARSE supports a variety of formats

00:01:33.908 --> 00:01:39.262
and includes higher level routines like incomplete LU factorization.

00:01:39.262 --> 00:01:44.240
cuRAND is a bunch of pseudo and quasi random number generation routines

00:01:44.240 --> 00:01:47.944
for making random numbers, and this includes device side functions as well as host interfaces

00:01:47.944 --> 00:01:51.738
for quickly filling arrays with numbers drawn from particular distributions,

00:01:51.738 --> 00:01:55.390
various high quality random number generations basically.

00:01:55.390 --> 00:01:58.576
NPP stands for NVIDIA Performance Primitives,

00:01:58.576 --> 00:02:02.466
and this is basically, for the most part, low-level image processing primitives,

00:02:02.466 --> 00:02:06.084
so highly optimized low-level primitives for image processing.

00:02:06.084 --> 00:02:09.202
Magma is developed by the same group that wrote the original LAPACK library,

00:02:09.202 --> 00:02:14.772
and LAPACK is another one of these tools in the toolbox of many scientists and engineers

00:02:14.772 --> 00:02:17.874
who simply use these tools for linear algebra all the time.

00:02:17.874 --> 00:02:23.719
So Magma provides GPU and multicore CPU implementations of many LAPACK routines,

00:02:23.719 --> 00:02:31.221
so it's sort of a modern parallel rethinking of our implementation of LAPACK style linear algebra.

00:02:31.221 --> 00:02:33.289
Another linear algebra tool is CULA,

00:02:33.289 --> 00:02:37.972
which is implementations of Eigensolvers and matrix factorizations and matrix solvers,

00:02:37.972 --> 00:02:45.735
and for dense matrices similar to the LAPACK API, and there's also a sparse CULA package as well.

00:02:45.735 --> 00:02:50.442
Array Fire is a framework for data parallel manipulation of different types of array data,

00:02:50.442 --> 00:02:56.980
including a wide variety of built-in operations from numerical linear algebra to signal processing to financial,

00:02:56.980 --> 00:03:02.413
so this is somewhere in between a domain-specific library like these others

00:03:02.413 --> 00:03:06.338
and programming power tool, which is sort of the next category we'll talk about.

