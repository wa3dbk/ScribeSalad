WEBVTT
Kind: captions
Language: en

00:00:00.025 --> 00:00:04.917
Finally, we need to make some assumptions, in order to use one way ANOVA. The

00:00:04.917 --> 00:00:08.870
first is normality. All the populations from which the samples are from are

00:00:08.870 --> 00:00:14.695
normally distributed. Another is homogeneity of variance. The data come from

00:00:14.695 --> 00:00:19.852
populations that have equal amounts of variability. And finally independence of

00:00:19.852 --> 00:00:25.115
observations. The results found from one sample won't affect the others.

00:00:25.115 --> 00:00:31.240
However, we can violate these assumptions under certain conditions. We can

00:00:31.240 --> 00:00:36.282
violate the normality assumption if the sample size is large. We can violate

00:00:36.282 --> 00:00:40.506
the homogeneity of variance assumption if all the samples have nearly equal

00:00:40.506 --> 00:00:46.534
sample sizes, and the ratio of any two variances does not exceed four. We have

00:00:46.534 --> 00:00:50.758
to maintain independence of observations, but we can use random assignment to

00:00:50.758 --> 00:00:56.154
conditions to help us meet this assumption. Let's do a quick summary of ANOVA

00:00:56.154 --> 00:01:01.296
to wrap up this lesson. I'm not actually going to rap this time though. We're

00:01:01.296 --> 00:01:06.032
just going to wrap up the lesson. If we have three or more samples, and we want

00:01:06.032 --> 00:01:10.119
to know if any two of them are significantly different, we look at both the

00:01:10.119 --> 00:01:16.783
between-group variability, and the within-group variability. Between-group

00:01:16.783 --> 00:01:21.327
variability is a measure of how spaced apart these sample means are from each

00:01:21.327 --> 00:01:28.049
other. And we do that by finding the grand mean and each squared deviation from

00:01:28.049 --> 00:01:34.578
the grand mean, for each sample mean. We multiply each sample size by the

00:01:34.578 --> 00:01:41.621
squared deviation of each sample mean from the grand mean. Then we add them up.

00:01:41.621 --> 00:01:47.099
Then we have to look at the within-group variability, which is essentially the

00:01:47.099 --> 00:01:52.328
squared deviation of each value in each sample, from the respective sample

00:01:52.328 --> 00:02:01.136
mean. So we add up all sums of squares from their respective sample mean. And

00:02:01.136 --> 00:02:04.918
then we have to find the average sum of squares for each, by dividing by the

00:02:04.918 --> 00:02:10.338
degrees of freedom. In the case of the between-groups, this is the number of

00:02:10.338 --> 00:02:16.322
samples minus 1. And for within-groups, this is the total number of values

00:02:16.322 --> 00:02:21.660
minus the number of groups. This is the same as adding the degrees of freedom

00:02:21.660 --> 00:02:27.022
for each group. There we have our F statistic. And if it falls out here in the

00:02:27.022 --> 00:02:32.502
critical region, past the F critical value, we'll reject the null. After making

00:02:32.502 --> 00:02:37.182
a statistical decision, we can use the multiple comparison test, one of which

00:02:37.182 --> 00:02:41.790
is Tukey's honestly significant difference, which is a value that if any two

00:02:41.790 --> 00:02:46.398
sample means have a difference greater than that value, they're considered

00:02:46.398 --> 00:02:53.750
honestly significantly different. You've also learned how to determine what

00:02:53.750 --> 00:02:58.910
proportion of the difference between means is due to the independent variable.

00:02:58.910 --> 00:03:01.600
That's eta squared. And that's a wrap.

