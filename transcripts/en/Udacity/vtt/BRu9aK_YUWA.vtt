WEBVTT
Kind: captions
Language: en

00:00:00.350 --> 00:00:06.730
I mentioned that any system design should be simple. And power of simplicity

00:00:06.730 --> 00:00:12.000
is the key for adoption. So in this case, let's revisit the sequential program

00:00:12.000 --> 00:00:13.740
for video analytics that I showed

00:00:13.740 --> 00:00:17.530
you earlier. Converting the sequential program for

00:00:17.530 --> 00:00:21.250
video analytics into a distributed program, using

00:00:21.250 --> 00:00:25.550
the get put primitives, provided by PTS,

00:00:25.550 --> 00:00:28.660
is fairly straightforward. What you do

00:00:28.660 --> 00:00:34.190
is, you interpose between these computations,

00:00:34.190 --> 00:00:39.860
that are there in the original pipeline, channels that are named entities

00:00:39.860 --> 00:00:45.030
that can be used to hold the temporal evolution of output

00:00:45.030 --> 00:00:50.460
from a particular computation. So in this case, camera capture is a thread,

00:00:50.460 --> 00:00:54.280
and this capture computation captures images from

00:00:54.280 --> 00:00:56.900
a camera with a certain periodicity. And

00:00:56.900 --> 00:00:59.090
places it into a name channel called

00:00:59.090 --> 00:01:03.002
frames, and so the frames channel abstraction is

00:01:03.002 --> 00:01:05.850
going to contain the temporal evolution of

00:01:05.850 --> 00:01:09.570
output produced by this capture computation. And

00:01:09.570 --> 00:01:13.340
the detector can discover where this frame

00:01:13.340 --> 00:01:16.390
channel is, connect to it, and get images

00:01:16.390 --> 00:01:21.060
from this channel, process them and produce

00:01:21.060 --> 00:01:25.030
blobs that characterize the objects that it

00:01:25.030 --> 00:01:28.780
sees in any particular given frame. So

00:01:28.780 --> 00:01:31.220
this is a temporal evolution of the detector's

00:01:31.220 --> 00:01:33.320
output corresponding to the frames that it

00:01:33.320 --> 00:01:36.880
sees. And the tracker takes these blobs.

00:01:36.880 --> 00:01:38.980
And interesting blobs from that are the

00:01:38.980 --> 00:01:42.430
objects that it is tracking and it produces

00:01:42.430 --> 00:01:47.420
a temporal evolution of the location of objects that it sees and

00:01:47.420 --> 00:01:52.560
places it in its output channel and a recognizer may then look at these

00:01:52.560 --> 00:01:57.740
objects and consults. A database of known objects to see

00:01:57.740 --> 00:02:02.690
whether any of those objects that are being continuously

00:02:02.690 --> 00:02:08.050
captured, detected, and tracked, correspond to anything

00:02:08.050 --> 00:02:11.000
that may indicate that there's an anomalous

00:02:11.000 --> 00:02:13.600
situation. And those are the events that it

00:02:13.600 --> 00:02:16.225
produces, and those events may trigger an

00:02:16.225 --> 00:02:18.840
alarm. So basically, what, what I'm showing you

00:02:18.840 --> 00:02:23.970
here is converting the sequential program for video analytics into a

00:02:23.970 --> 00:02:28.920
distributed program using the channel abstraction and the get/put primitive

00:02:28.920 --> 00:02:34.100
available in the PTS programming model. So the ovals

00:02:34.100 --> 00:02:39.670
are the threads of the PTS abstraction and these rectangles

00:02:39.670 --> 00:02:45.020
are the channels that are there between any two computational entities.

