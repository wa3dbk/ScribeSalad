WEBVTT
Kind: captions
Language: en

00:00:00.370 --> 00:00:04.650
okay, wait. So let me see if I can echo some of that back. So, it's almost as

00:00:04.650 --> 00:00:06.220
if what we're doing here is we're doing a

00:00:06.220 --> 00:00:10.800
transformation into a new space where feature selection can work.

00:00:10.800 --> 00:00:14.510
&gt;&gt; Exactly, and in fact, here's something kind of interesting for you.

00:00:14.510 --> 00:00:18.810
It turns out that if the ion value of some particular dimension

00:00:18.810 --> 00:00:22.590
is equal to zero, then it means it provides no information whatsoever

00:00:22.590 --> 00:00:25.410
in the original space. So, if I have a direction, if I have

00:00:25.410 --> 00:00:27.960
a dimension that has an eigen of zero, I can

00:00:27.960 --> 00:00:30.150
throw it away and it will not affect my reconstructioner.

00:00:30.150 --> 00:00:33.290
&gt;&gt; I'm trying to remember if that makes it irrelevant or useless.

00:00:33.290 --> 00:00:37.070
&gt;&gt; Well, it certainly makes it irrelevant. If there's no variance, that's the

00:00:37.070 --> 00:00:40.760
same thing as saying it has zero entropy, because it never changes. So it's

00:00:40.760 --> 00:00:44.580
irrelevant. Now whether it's useful or not, well, probably not, but it might

00:00:44.580 --> 00:00:48.005
be useful for something like, our simple,

00:00:48.005 --> 00:00:50.460
[INAUDIBLE] example that we used last time.

00:00:50.460 --> 00:00:51.460
&gt;&gt;

00:00:50.460 --> 00:00:50.720
Gotcha.

00:00:50.720 --> 00:00:53.190
&gt;&gt; Okay. So does this all make sense?

00:00:53.190 --> 00:00:55.300
&gt;&gt; So one question I have is in the example

00:00:55.300 --> 00:00:58.170
that we just kind of worked through there was a blob

00:00:58.170 --> 00:01:00.860
of data and when we drew the red line through

00:01:00.860 --> 00:01:05.120
the maximum variance direction it went through the. The xy origin.

00:01:05.120 --> 00:01:05.530
&gt;&gt; Um-huh.

00:01:05.530 --> 00:01:10.070
&gt;&gt; Does it have to? Is it necessarily the case that it's going to?

00:01:10.070 --> 00:01:12.070
Or you know is the algorithm restricted

00:01:12.070 --> 00:01:13.590
to have to put things through the origin?

00:01:13.590 --> 00:01:15.750
&gt;&gt; Well so my

00:01:15.750 --> 00:01:18.420
answer to you is that, that's actually a very complicated

00:01:18.420 --> 00:01:22.830
question. And in principle it, so to speak it, it

00:01:22.830 --> 00:01:26.220
doesn't really [LAUGH] matter. But in practice what people do

00:01:26.220 --> 00:01:29.010
and their doing something like PCA is they actually subtract the

00:01:29.010 --> 00:01:32.170
mean of the data. Or the central of the data

00:01:32.170 --> 00:01:33.880
from all the data points. So it ends up being

00:01:33.880 --> 00:01:38.370
centered around an origin, for the origin. But what this

00:01:38.370 --> 00:01:40.760
means is that you can then interprit what we're doing is

00:01:40.760 --> 00:01:43.350
finding maximum variants. As capturing correlation.

00:01:43.350 --> 00:01:44.510
&gt;&gt; Okay. That's helpful.

00:01:44.510 --> 00:01:46.930
&gt;&gt; And otherwise, if you don't do that, what you end up with is

00:01:46.930 --> 00:01:49.660
effectively a principle component that at least

00:01:49.660 --> 00:01:51.590
intuitively kind of captures the notion of where

00:01:51.590 --> 00:01:55.060
the origin should be, and it's not terribly helpful for what it is we're

00:01:55.060 --> 00:01:58.830
trying to do. So my answer is, no it doesn't, and yes it does.

00:01:58.830 --> 00:02:00.367
&gt;&gt; [LAUGH]

00:02:00.367 --> 00:02:01.650
&gt;&gt; Okay?

00:02:01.650 --> 00:02:01.770
&gt;&gt; Sure.

00:02:01.770 --> 00:02:04.190
&gt;&gt; Okay. So, that's, that's basically

00:02:04.190 --> 00:02:06.270
PCA. It's got all these neat properties.

00:02:06.270 --> 00:02:09.699
Let's sort of summarize them again. It's, well, actually.

00:02:09.699 --> 00:02:11.780
Let me add a couple beyond these. It is a

00:02:11.780 --> 00:02:14.420
global algorhythm. It does give you best reconstruction error. Which

00:02:14.420 --> 00:02:17.250
seems like a very nice thing to, thing to have.

00:02:17.250 --> 00:02:21.080
And, it tells you, which of the. New features

00:02:21.080 --> 00:02:23.680
that you get out are actually important with respect to

00:02:23.680 --> 00:02:27.190
this notion of reconstruction by simply looking at their corresponding

00:02:27.190 --> 00:02:31.370
eigenvalues. They also have a couple of other practical properties.

00:02:31.370 --> 00:02:34.130
In particular, it's very well studied. And what that means

00:02:34.130 --> 00:02:37.600
in this case is that there are very fast algorithms that

00:02:37.600 --> 00:02:41.160
do a very good job, even in weird cases. Even with

00:02:41.160 --> 00:02:44.520
large data sets, at least if they're appropriately sparse, of being

00:02:44.520 --> 00:02:46.710
able to compute these things. People have been working on this

00:02:46.710 --> 00:02:50.030
problem again, since before we were born Michael, and they've gotten

00:02:50.030 --> 00:02:53.690
really good at finding principle components even for what would be

00:02:53.690 --> 00:02:56.690
very difficult spaces. But this does lead me to a question

00:02:56.690 --> 00:02:59.840
though which is another question, which is okay, so

00:02:59.840 --> 00:03:01.900
you've got an algorithm that probably gives you the best

00:03:01.900 --> 00:03:05.730
reconstruction. But what does it have to do with classification?

00:03:05.730 --> 00:03:07.600
This is kind of like the question we had before

00:03:07.600 --> 00:03:11.440
about relevance versus usefulness. So we find a bunch

00:03:11.440 --> 00:03:15.770
of projections which are relevant in the sense that they

00:03:15.770 --> 00:03:18.880
allow you to do reconstruction. But it's not clear that

00:03:18.880 --> 00:03:21.840
if you threw away some of these projections, the ones

00:03:21.840 --> 00:03:24.090
with low eigenvalue, that even though you'd be

00:03:24.090 --> 00:03:27.510
able to reconstruct your. Original data is not

00:03:27.510 --> 00:03:30.180
clear that would help you do classification later,

00:03:30.180 --> 00:03:31.390
can you see how that would work out?

00:03:31.390 --> 00:03:33.900
&gt;&gt; Sure, I mean like, it just could be that that's

00:03:33.900 --> 00:03:37.050
not where the information is about what the labels ought to be.

00:03:37.050 --> 00:03:38.900
&gt;&gt; Right, so imagine for example that one

00:03:38.900 --> 00:03:42.380
of your original dimensions is in fact directly related

00:03:42.380 --> 00:03:43.680
to the label and all the rest are

00:03:43.680 --> 00:03:47.410
just, say goush and noise. But the, the variance

00:03:47.410 --> 00:03:51.170
of that particular direction is extremely small.

00:03:51.170 --> 00:03:52.850
&gt;&gt; It might end up throwing it away.

00:03:52.850 --> 00:03:55.580
&gt;&gt; It'll almost certainly end up throwing it away. Which means

00:03:55.580 --> 00:03:58.950
you'll end up with a bunch of random data that doesn't

00:03:58.950 --> 00:04:02.200
actually later help you do classification. And that's because this looks

00:04:02.200 --> 00:04:05.680
a lot like, if I can do the analogy, a filter method.

00:04:05.680 --> 00:04:06.700
&gt;&gt; I was going to say that, yeah.

00:04:06.700 --> 00:04:10.040
&gt;&gt; Oh. So this is kind of like filtering. And in fact it's going

00:04:10.040 --> 00:04:12.580
to turn out that the other two items we're looking at too are like

00:04:12.580 --> 00:04:15.150
filtering although their particular criterion might be

00:04:15.150 --> 00:04:17.579
more relevant. We'll see. Okay? Alright, so do

00:04:17.579 --> 00:04:20.160
you understand principal components analysis? Again, I'm not

00:04:20.160 --> 00:04:22.940
asking you to understand exactly how you would

00:04:22.940 --> 00:04:25.920
run a principal components analsyis algorithm. That

00:04:25.920 --> 00:04:28.020
stuff's in, in all the text, but just

00:04:28.020 --> 00:04:29.890
to sort of understand exactly what is is

00:04:29.890 --> 00:04:31.460
trying to do, what it's trying to accomplish.

00:04:31.460 --> 00:04:34.410
&gt;&gt; Yeah, I think so. So, it's, it's taking the data, it's finding

00:04:34.410 --> 00:04:37.650
a different set of axis, that are just like regular axis in that

00:04:37.650 --> 00:04:42.690
they're mutually orthogonal. But it lines up the varience of the data with those

00:04:42.690 --> 00:04:46.370
axis, so that we can drop the least significant ones, and that gives us a

00:04:46.370 --> 00:04:48.730
way to do. Feature selection. But the

00:04:48.730 --> 00:04:50.640
whole thing is a feature transformation algorithm, in

00:04:50.640 --> 00:04:54.010
the sense that it first moved the data around, to be able to do that.

00:04:54.010 --> 00:04:57.370
&gt;&gt; That's exactly right. So, you do, transformation into

00:04:57.370 --> 00:04:59.390
a new space, where you know how to do filtering.

00:04:59.390 --> 00:05:00.160
&gt;&gt; Got it.

00:05:00.160 --> 00:05:00.900
&gt;&gt; Excellent.

