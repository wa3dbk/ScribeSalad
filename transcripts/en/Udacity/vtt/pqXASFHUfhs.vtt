WEBVTT
Kind: captions
Language: en

00:00:00.210 --> 00:00:03.060
Well, what you're doing in order to make
that work and what you end up doing in

00:00:03.060 --> 00:00:05.160
supervised learning and
functions approximation in general,

00:00:05.160 --> 00:00:07.920
is you make some fundamental
assumptions about the work, right?

00:00:07.920 --> 00:00:11.560
You decide that you have a well-behaved
function that is consistent with

00:00:11.560 --> 00:00:15.480
the data that you're given, and with
that, you're able to generalize, and

00:00:15.480 --> 00:00:18.120
in fact that is the fundamental
problem in machine learning.

00:00:18.120 --> 00:00:19.760
It is generalization.

00:00:19.760 --> 00:00:23.055
Now what's behind all of is
I'm going to claim, Michael,

00:00:23.055 --> 00:00:27.030
you jump in whenever you disagree is
&gt;&gt; I disagree sorry to soon, go ahead.

00:00:27.030 --> 00:00:28.520
&gt;&gt; is bias and in particular

00:00:28.520 --> 00:00:29.652
&gt;&gt; bias
&gt;&gt; inductive bias.

00:00:29.652 --> 00:00:30.496
&gt;&gt; Inductive bias.

00:00:30.496 --> 00:00:32.595
&gt;&gt; Right, so all of machine learning or

00:00:32.595 --> 00:00:37.360
certainly supervised learning is about
induction, as opposed to deduction.

00:00:37.360 --> 00:00:39.070
&gt;&gt; I see.
Induction of course being a problem of

00:00:39.070 --> 00:00:41.820
going from examples to
a more general rule.

00:00:41.820 --> 00:00:44.700
&gt;&gt; Right, specifics to generalities.

00:00:44.700 --> 00:00:46.160
By contrast, deduction is?

00:00:47.730 --> 00:00:48.250
&gt;&gt; Be the opposite.

00:00:48.250 --> 00:00:51.300
It would be going from a general
rule to specific instances,

00:00:51.300 --> 00:00:53.290
basically like reasoning.

00:00:53.290 --> 00:00:54.190
&gt;&gt; Right, and in fact,

00:00:54.190 --> 00:00:56.900
a lot of AI in the beginning
was about deductive reasoning,

00:00:56.900 --> 00:01:00.200
about logic programming, those sorts of
things, where you have certain rules,

00:01:00.200 --> 00:01:03.400
and you deduce only those things that
follow immediately from those rules.

00:01:03.400 --> 00:01:06.030
So for example,
you'd have something like A implies B.

00:01:06.030 --> 00:01:08.200
That's a rule in the universe.

00:01:08.200 --> 00:01:09.650
And then I tell you A.

00:01:09.650 --> 00:01:12.540
So if you A implies B is a rule in
the universe, and I tell you A,

00:01:12.540 --> 00:01:14.540
then you also know?

00:01:14.540 --> 00:01:16.310
&gt;&gt; That A implies B.

00:01:16.310 --> 00:01:17.930
&gt;&gt; And therefore you can infer that?

00:01:17.930 --> 00:01:18.480
&gt;&gt; And A. B. &gt;&gt; B.

00:01:18.480 --> 00:01:23.270
You have A implies B,
you have A, that implies B.

00:01:23.270 --> 00:01:26.890
&gt;&gt; Okay.
&gt;&gt; That's what we just said.

00:01:28.230 --> 00:01:29.040
But what,
&gt;&gt; that's deduction.

00:01:29.040 --> 00:01:31.680
&gt;&gt; That's deduction, but
we just did was not deduction.

00:01:31.680 --> 00:01:35.480
Before then when I asked you one, one,
two, four, three, nine, four sixteen and

00:01:35.480 --> 00:01:36.230
so forth.

00:01:36.230 --> 00:01:37.310
&gt;&gt; Right,
&gt;&gt; we did induction.

00:01:37.310 --> 00:01:38.090
&gt;&gt; That was induction.

00:01:38.090 --> 00:01:41.900
&gt;&gt; Induction is more about
did the sun rise yesterday?

00:01:41.900 --> 00:01:43.610
&gt;&gt; yes.
&gt;&gt; Did the sun rise the day before that?

00:01:43.610 --> 00:01:45.090
&gt;&gt; yes.
&gt;&gt; Did the sun rise the day before that?

00:01:46.880 --> 00:01:48.080
&gt;&gt; I slept in.

00:01:48.080 --> 00:01:49.250
Did the sun rise the day before that?

00:01:49.250 --> 00:01:49.860
&gt;&gt; Yes.

00:01:49.860 --> 00:01:50.360
&gt;&gt; Yes.

00:01:50.360 --> 00:01:51.520
So the sun has risen every day.

00:01:51.520 --> 00:01:52.810
Is the sun going to rise tomorrow?

00:01:53.870 --> 00:01:54.740
&gt;&gt; I sure hope so.

00:01:54.740 --> 00:01:57.030
&gt;&gt; We all hope so, and
we all act like it does,

00:01:57.030 --> 00:02:00.010
because if it doesn't, then there are a
whole bunch of other things we ought to

00:02:00.010 --> 00:02:02.290
be doing besides sitting in this
studio and having this interview.

00:02:02.290 --> 00:02:03.481
&gt;&gt; I think we should warn the plants.

00:02:03.481 --> 00:02:05.701
&gt;&gt; [LAUGH] I don't think
the plants are going to care.

00:02:05.701 --> 00:02:07.050
&gt;&gt; They are.
They really need sun.

00:02:07.050 --> 00:02:09.389
I think we all need sun, Mike.

00:02:09.389 --> 00:02:11.200
&gt;&gt; Eh.
&gt;&gt; So, the idea there is

00:02:11.200 --> 00:02:14.310
induction is crucial, and
the inductive bias is crucial.

00:02:14.310 --> 00:02:16.650
We'll talk about all of this in,
in the course.

00:02:16.650 --> 00:02:17.680
&gt;&gt; Kay.
&gt;&gt; But that's kind of a fundamental

00:02:17.680 --> 00:02:19.930
notion behind supervised learning and
machine learning in general.

00:02:19.930 --> 00:02:20.500
&gt;&gt; I agree with that.

00:02:20.500 --> 00:02:21.110
&gt;&gt; Agreed?
&gt;&gt; Yeah.

00:02:21.110 --> 00:02:22.080
&gt;&gt; Al lright, so we're on the same page.

00:02:22.080 --> 00:02:23.260
So that's supervised learning.

00:02:23.260 --> 00:02:25.820
Supervised learning, you can talk about
it in these high muckity muck ways, but

00:02:25.820 --> 00:02:27.990
at the end of the day,
it's function approximation.

00:02:27.990 --> 00:02:30.090
It's figuring out how to take
a bunch of training examples and

00:02:30.090 --> 00:02:33.050
coming up with some function
that generalizes beyond

00:02:33.050 --> 00:02:33.660
the data that you see.

00:02:33.660 --> 00:02:35.590
&gt;&gt; So, why wouldn't you call
it function induction, then?

00:02:37.175 --> 00:02:38.770
&gt;&gt; because someone said
supervised learning first.

00:02:38.770 --> 00:02:39.280
Well, there is a-

00:02:39.280 --> 00:02:39.850
&gt;&gt; No, no, no, no, no.
Sorry.

00:02:39.850 --> 00:02:43.297
You said supervised learning is function
approximation and I want to say,

00:02:43.297 --> 00:02:46.680
why don't you say supervised
learning is function induction.

00:02:46.680 --> 00:02:48.210
&gt;&gt; As opposed to function approximation?

00:02:48.210 --> 00:02:48.960
&gt;&gt; Yeah.

00:02:48.960 --> 00:02:49.470
&gt;&gt; Okay.

00:02:49.470 --> 00:02:51.840
It's a
&gt;&gt; Approximate function induction.

00:02:53.230 --> 00:02:55.353
&gt;&gt; Or induction of approximate, of.

00:02:55.353 --> 00:02:56.061
&gt;&gt; Approximate functions?

00:02:56.061 --> 00:02:56.960
&gt;&gt; Approximate functions,
something like that, yeah.

00:02:56.960 --> 00:02:59.160
&gt;&gt; You don't want to induce
an approximate function,

00:02:59.160 --> 00:03:00.600
you want to induce the actual function.

00:03:00.600 --> 00:03:01.970
&gt;&gt; Yeah, but sometimes you can't,
&gt;&gt; Yeah.

00:03:01.970 --> 00:03:06.254
&gt;&gt; Because sometimes you think
it's quadratic, but it's not.

00:03:06.254 --> 00:03:07.900
&gt;&gt; I have that as a plaque on my wall.

00:03:07.900 --> 00:03:09.370
&gt;&gt; You do?

00:03:09.370 --> 00:03:09.962
&gt;&gt; No.

00:03:09.962 --> 00:03:10.682
&gt;&gt; Yeah I didn't think so.

00:03:10.682 --> 00:03:11.970
Okay so that's supervised learning

