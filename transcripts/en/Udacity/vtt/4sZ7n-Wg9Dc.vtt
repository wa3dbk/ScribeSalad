WEBVTT
Kind: captions
Language: en

00:00:00.110 --> 00:00:03.840
We've been talking about core Hadoop. Which consists of HDFS

00:00:03.840 --> 00:00:06.970
and map reduce but since the project was first started, an

00:00:06.970 --> 00:00:09.530
awful lot of other software has grown up around it. And

00:00:09.530 --> 00:00:12.760
that's what we call the Hadoop Ecosystem. Some of the software

00:00:12.760 --> 00:00:14.910
is intended to make it easier to load data into

00:00:14.910 --> 00:00:17.790
the Hadoop cluster. Well lots of it designed to make Hadoop

00:00:17.790 --> 00:00:20.920
easier to use. For example as you'll see in the next

00:00:20.920 --> 00:00:25.140
lesson. Writing Map reduced code isn't completely simple. You need to

00:00:25.140 --> 00:00:28.930
know a programming language such as Java, Python, Ruby or

00:00:28.930 --> 00:00:31.195
Perl. But there are lots of folks out there who

00:00:31.195 --> 00:00:34.410
aren't programmers, but can write SQL queries to access data

00:00:34.410 --> 00:00:37.540
in a traditional relational database system, like SQL Server. And,

00:00:37.540 --> 00:00:40.400
of course, lots of business intelligence tools one way to

00:00:40.400 --> 00:00:44.670
hook into Hadoop. For that reason, Other open source projects

00:00:44.670 --> 00:00:46.580
have been created to make it easier for people to

00:00:46.580 --> 00:00:50.550
query their data without knowing how to code. Two key ones

00:00:50.550 --> 00:00:53.860
are Hive and Pig. Instead of having to write

00:00:53.860 --> 00:00:57.180
macros and reducers, in Hive you just write statements

00:00:57.180 --> 00:01:00.670
like this. Which looks very much like standard SQL.

00:01:00.670 --> 00:01:04.890
The Hive interpreter turns the SQL into map produced code,

00:01:04.890 --> 00:01:07.080
which then runs on the cluster. And an alternative

00:01:07.080 --> 00:01:09.735
is Pig, which allows you to write code to

00:01:09.735 --> 00:01:12.690
analyse your data in a fairly simple scripting language,

00:01:12.690 --> 00:01:15.660
rather than map reduce. Again the code is just turned

00:01:15.660 --> 00:01:20.774
into map reduce and run on a cluster. Hive and Pig are great, but they're still

00:01:20.774 --> 00:01:23.140
running map reduce jobs. Which as you'll see

00:01:23.140 --> 00:01:26.090
can take a reasonable around of time to run.

00:01:26.090 --> 00:01:29.420
Especially over large amounts of data. So another

00:01:29.420 --> 00:01:33.290
open source project, is called Impala. Impala was developed

00:01:33.290 --> 00:01:35.520
as a way to query your data with

00:01:35.520 --> 00:01:40.350
SQL, but which directly accesses the data in HDFS.

00:01:40.350 --> 00:01:43.960
Rather than needing map reduce. Impala is

00:01:43.960 --> 00:01:46.650
optimized for low latency queries. In other

00:01:46.650 --> 00:01:49.410
words Impala queries run very quickly, typically

00:01:49.410 --> 00:01:52.350
many times faster than Hive, while Hive

00:01:52.350 --> 00:01:55.520
is optimized for running long batch processing

00:01:55.520 --> 00:01:58.580
jobs. Another project used by many people

00:01:58.580 --> 00:02:01.290
is Sqoop. Sqoop takes data from a

00:02:01.290 --> 00:02:05.570
traditional relational database, such as Microsoft SQL Server.

00:02:05.570 --> 00:02:08.020
And, puts it in HDFS, as the limited

00:02:08.020 --> 00:02:10.330
files. So, it can be processed along with other

00:02:10.330 --> 00:02:13.930
data on the cluster. Then, there's Flume. Which

00:02:13.930 --> 00:02:17.600
injests data as it's generated by external systems. And,

00:02:17.600 --> 00:02:20.470
again, puts it into the cluster. HBase is

00:02:20.470 --> 00:02:23.780
a real time database, built on top of HDFS.

00:02:23.780 --> 00:02:26.750
And there's more. Hue is a graphical front end

00:02:26.750 --> 00:02:30.990
to the questor. Oozie is a workflow management tool.

00:02:30.990 --> 00:02:34.020
Mahout is a machine learning library. In fact

00:02:34.020 --> 00:02:36.630
there are so many ecosystem projects that making

00:02:36.630 --> 00:02:38.470
them all talk to one another, and work

00:02:38.470 --> 00:02:42.490
well, can be tricky. To make installing and maintaining

00:02:42.490 --> 00:02:46.530
a cluster like this easier, Cloudera, the company

00:02:46.530 --> 00:02:49.200
we work for, has put together a distribution

00:02:49.200 --> 00:02:52.670
of HADOOP called CDH. CDH or the Cloudera

00:02:52.670 --> 00:02:56.310
distribution including a patchy HADOOP, takes all the key

00:02:56.310 --> 00:03:00.340
ecosystem projects, along with HADOOP itself, and packages them

00:03:00.340 --> 00:03:03.820
together so that installation is a really easy process. And

00:03:03.820 --> 00:03:05.500
the components are all tested together, so you can

00:03:05.500 --> 00:03:08.800
be sure there's no incompatibilities between them. Of course, it's

00:03:08.800 --> 00:03:11.740
free and open source, just like Hadoop itself. While

00:03:11.740 --> 00:03:15.630
you could install everything from scratch, it's far easier to

00:03:15.630 --> 00:03:19.040
use CDH, and that's certainly what we'd recommend. In

00:03:19.040 --> 00:03:21.520
the next lesson, in fact, you'll be downloading and running

00:03:21.520 --> 00:03:26.350
a virtual machine, which has CDH installed. For more information on the

00:03:26.350 --> 00:03:28.250
Hadoop ecosystem and how each of

00:03:28.250 --> 00:03:30.840
these components works, see the instructor notes.

