WEBVTT
Kind: captions
Language: en

00:00:00.380 --> 00:00:03.590
So far you've covered supervised and
unsupervised learning.

00:00:03.590 --> 00:00:05.478
There's a third major school of
thought in machine learning,

00:00:05.478 --> 00:00:06.740
called reinforcement learning.

00:00:06.740 --> 00:00:10.150
Reinforcement learning is inspired
in part by the behaviorist school of

00:00:10.150 --> 00:00:11.080
thought in psychology.

00:00:11.080 --> 00:00:14.110
And it's a way of tweaking our
system to prefer being right or

00:00:14.110 --> 00:00:15.840
wrong in certain ways.

00:00:15.840 --> 00:00:19.180
And so a good way to think about this is
with an example, think of a smart cab,

00:00:19.180 --> 00:00:20.530
a self driving car.

00:00:20.530 --> 00:00:23.230
It's learning how to navigate roads,
it's learning how to transport its

00:00:23.230 --> 00:00:26.410
passengers and it knows that there's
different ways it can get optimal.

00:00:26.410 --> 00:00:29.070
It can be optimal if it gets there
in the shortest amount of time.

00:00:29.070 --> 00:00:32.130
It can also be optimal if it
gets there without crashing.

00:00:32.130 --> 00:00:34.842
But really we would want it to
prefer one of those definitions of

00:00:34.842 --> 00:00:37.140
the optimality way above the other one.

00:00:37.140 --> 00:00:40.215
We would much prefer it get there
much later than it would otherwise,

00:00:40.215 --> 00:00:42.294
than to actually get in
an accident on the way.

00:00:42.294 --> 00:00:44.881
We really want it to prefer
not getting in an accident,

00:00:44.881 --> 00:00:47.640
to just getting there
as fast as possible.

00:00:47.640 --> 00:00:50.980
So, reinforcement learning would
be a way of teaching the system to

00:00:50.980 --> 00:00:52.550
prioritize in that way.

00:00:52.550 --> 00:00:55.623
We attach rewards and
punishments to different outcomes.

00:00:55.623 --> 00:00:58.181
And by weighting those rewards and
punishments in the right way,

00:00:58.181 --> 00:01:01.040
we can teach at the right level of
priority to assign to different goals.

00:01:01.040 --> 00:01:01.700
So in this case,

00:01:01.700 --> 00:01:05.150
we would assign a much bigger
punishment to getting an accident,

00:01:05.150 --> 00:01:08.070
to just being a couple of minutes later
than they could have been otherwise.

00:01:08.070 --> 00:01:10.020
So in this unit, we'll talk about
reinforcement learning, and

00:01:10.020 --> 00:01:12.730
how augmenting the machine learning
process with these rewards and

00:01:12.730 --> 00:01:15.570
punishments, can really
fundamentally change the system's

00:01:15.570 --> 00:01:16.620
understanding that we generate.

