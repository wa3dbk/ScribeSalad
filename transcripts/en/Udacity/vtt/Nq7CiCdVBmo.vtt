WEBVTT
Kind: captions
Language: en

00:00:00.330 --> 00:00:04.760
That all we're going to say about Model
Based Reinforcement Learning in POMDPs

00:00:04.760 --> 00:00:07.866
and now we're going to talk a little bit
about Model Free Reinforcement Learning

00:00:07.866 --> 00:00:08.758
in POMDPS.

00:00:08.758 --> 00:00:09.540
&gt;&gt; Okay.
&gt;&gt; And

00:00:09.540 --> 00:00:11.560
particular we're going to focus
on memory loss policies and

00:00:11.560 --> 00:00:14.120
what I'd like to show actually is not so
much about learning, but

00:00:14.120 --> 00:00:15.450
more about planning.

00:00:15.450 --> 00:00:16.530
So here's a little POMDP.

00:00:17.690 --> 00:00:20.570
This is a POMDP that has
four underlying states.

00:00:20.570 --> 00:00:23.160
Two observations there's this
sort of clear blue thing and

00:00:23.160 --> 00:00:24.780
there's a green thing.

00:00:24.780 --> 00:00:27.610
Two actions, black and red.

00:00:27.610 --> 00:00:30.060
Black takes you to the right and

00:00:30.060 --> 00:00:32.470
red takes you to the left
back around the other way.

00:00:32.470 --> 00:00:36.010
And the way that this is set up, I think
of it as a one dimensional hallway.

00:00:37.170 --> 00:00:39.140
And we don't know exactly
where we are in the hallway.

00:00:39.140 --> 00:00:44.170
But we're trying to get to you know
say my office along the hallway.

00:00:44.170 --> 00:00:48.180
If we're to the left of my hallway,
we should go right to go to my office.

00:00:48.180 --> 00:00:50.180
But if we just keep going left and
left and left and left and

00:00:50.180 --> 00:00:53.600
left we're just going to go and get
stuck and It's infinitely long hallway,

00:00:53.600 --> 00:00:54.800
it feels like an infinitely long.

00:00:54.800 --> 00:00:56.340
We just going to get stuck at the end.

00:00:56.340 --> 00:00:59.770
Whereas if we're to the right of
my office and we go to the right,

00:00:59.770 --> 00:01:02.740
we also kind of get stuck, we need to go
to the left to get where we're going.

00:01:02.740 --> 00:01:05.765
Once we actually get to the office,
we have a reset action, the black action

00:01:05.765 --> 00:01:08.515
resets us with equal probability
to one of those three states.

00:01:08.515 --> 00:01:10.495
because I don't know we're drunk and

00:01:10.495 --> 00:01:13.495
then we get to go again try
to get back to the office.

00:01:13.495 --> 00:01:14.355
&gt;&gt; Okay, good.

00:01:14.355 --> 00:01:15.385
&gt;&gt; First, I want to point out.

00:01:15.385 --> 00:01:17.645
I think we talked about an example
just like this before and

00:01:17.645 --> 00:01:22.022
we said that probably the right thing
to do is to go like left, right,

00:01:22.022 --> 00:01:23.932
right, that sort of thing.

00:01:23.932 --> 00:01:28.022
Because each time we reset we're in one
of these states which we go left hoping

00:01:28.022 --> 00:01:30.572
that it's going to take us out of here,
but that doesn't seem to work so

00:01:30.572 --> 00:01:33.742
we go right to take us
from this side back out.

00:01:33.742 --> 00:01:35.552
Actually right, right,
left is probably even better.

00:01:35.552 --> 00:01:37.620
Just keep doing this over and
over and over again.

00:01:37.620 --> 00:01:38.720
All right.
But now what we're talking about

00:01:38.720 --> 00:01:39.690
are memoryless policies.

00:01:39.690 --> 00:01:41.760
So memoryless policy has the form.

00:01:41.760 --> 00:01:43.150
When you're in the green state.

00:01:43.150 --> 00:01:45.250
Well, there's only one action choice so
you take it.

00:01:45.250 --> 00:01:46.710
When you're in the clear blue state.

00:01:46.710 --> 00:01:48.430
You have to go either left or right.

00:01:48.430 --> 00:01:51.320
Or maybe you can choose
probabilistically among left and right.

00:01:51.320 --> 00:01:53.975
But you can't do these sequences
because it's memoryless.

00:01:53.975 --> 00:01:55.751
[SOUND] You need memory
to do a sequence.

00:01:55.751 --> 00:01:57.095
&gt;&gt; Right, that's a problem.

00:01:57.095 --> 00:02:00.535
Okay I see, I see why this is a problem
or you could just invent memory.

00:02:00.535 --> 00:02:01.890
&gt;&gt; POMPDP's do invent memory.

00:02:01.890 --> 00:02:03.920
If you're going to learn
the POMDP model of this,

00:02:03.920 --> 00:02:07.570
it captures the memory that you need
to actually make optimal decisions.

00:02:07.570 --> 00:02:08.210
&gt;&gt; All right, fair enough.

