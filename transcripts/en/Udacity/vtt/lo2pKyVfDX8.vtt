WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.923
So the analyze stage is really all about profiling the whole application,

00:00:02.923 --> 00:00:05.552
looking not just at the kernels that you intend to parallelize

00:00:05.552 --> 00:00:07.742
but looking at the whole thing and trying to figure out,

00:00:07.742 --> 00:00:10.576
where can this application benefit from parallelization,

00:00:10.576 --> 00:00:13.866
and how much can you expect to benefit?

00:00:13.866 --> 00:00:17.233
So once you've decided that there's a region of the code that you need to parallelize,

00:00:17.233 --> 00:00:19.998
of course there's different approaches to doing this.

00:00:19.998 --> 00:00:22.766
Certainly no code is better than the code you don't have to write at all.

00:00:22.766 --> 00:00:25.241
So if you can find that there's a parallel library for the GPU

00:00:25.241 --> 00:00:30.272
that already does exactly what you want, then great, you're done. You can simply call out to that library.

00:00:30.272 --> 00:00:33.881
Sometimes you have a lot of code and you want to be able to instrument it,

00:00:33.881 --> 00:00:37.451
you want to do a minimal amount of work to get a little bit of parallelization,

00:00:37.451 --> 00:00:41.194
and there's an approach for this called Directives.

00:00:41.194 --> 00:00:43.862
The most well known one on the CPU is called OpenMP.

00:00:43.862 --> 00:00:48.294
There's another cross-platform standard called OpenACC which has emerged,

00:00:48.294 --> 00:00:50.962
which ACC stands for accelerator.

00:00:50.962 --> 00:00:56.555
This is sort of an extension of OpenMP to encompass the ideas of accelerators like the GPU.

00:00:56.555 --> 00:00:59.102
And so if you're looking at GPU programming, OpenACC

00:00:59.102 --> 00:01:02.467
is a really lightweight way to experiment with it.

00:01:02.467 --> 00:01:06.841
But of course sometimes what you want to do is actually go in and write a parallel routine,

00:01:06.841 --> 00:01:11.602
and naturally that's what we've been focusing on throughout this course using CUDA C++.

00:01:11.602 --> 00:01:15.110
So of course we're going to focus here.

00:01:15.110 --> 00:01:18.410
So assuming that you're going to be coding something up from scratch for this purpose,

00:01:18.410 --> 00:01:22.364
the next choice is how to pick an algorithm. And this is a huge deal.

00:01:22.364 --> 00:01:26.185
This is your real chance to make a huge improvement. Pick the right algorithm.

00:01:26.185 --> 00:01:30.542
So all the bit twiddling optimization in the world won't gain you nearly as much

00:01:30.542 --> 00:01:34.373
as choosing the right sort of fundamentally parallel-friendly algorithm.

00:01:34.373 --> 00:01:36.474
I'll give a couple of simple examples in this unit,

00:01:36.474 --> 00:01:38.643
and you'll see more examples in Unit 6.

00:01:38.643 --> 00:01:40.552
There's no recipe for picking the right algorithm.

00:01:40.552 --> 00:01:44.457
What you need to do is sort of think deeply about what is the parallelism in your problem.

00:01:44.457 --> 00:01:47.693
We'll try to give you a few examples of that so that you have a little practice.

00:01:47.693 --> 00:01:50.397
So once you've decided how to parallelize your algorithm,

00:01:50.397 --> 00:01:55.336
then you want to optimize the code, and there will be sort of a cycle between these.

00:01:55.336 --> 00:01:58.296
As you'll see in the example, you try a parallelization,

00:01:58.296 --> 00:02:02.539
study how well it does, suggest some changes that might suggest a way

00:02:02.539 --> 00:02:05.912
that you approach the parallelization differently.

00:02:05.912 --> 00:02:10.447
So we're really talking about profile-driven optimization, by which I mean measure it.

00:02:10.447 --> 00:02:16.380
Measure, measure, measure, measure how fast things are going and use that to base your decisions.

00:02:16.380 --> 00:02:19.651
Don't just sort of take a guess at what's going to work well and what doesn't.

00:02:19.651 --> 00:02:21.591
And finally, the deploy step.

00:02:21.591 --> 00:02:24.169
So look, in this class we do these little homeworks,

00:02:24.169 --> 00:02:28.201
and they're fairly self-contained so that you can get them done in a reasonable amount of time.

00:02:28.201 --> 00:02:32.729
So the process of actually deploying a GPU-accelerated code into real use

00:02:32.729 --> 00:02:34.771
is not going to come up a lot in this class,

00:02:34.771 --> 00:02:37.617
so consider this just sort of free software engineering advice.

00:02:37.617 --> 00:02:42.242
When you're working on a real code, it's a really, really bad idea to optimize in a vacuum.

00:02:42.242 --> 00:02:46.180
And what I mean by this is that you can easily spend tons of time

00:02:46.180 --> 00:02:49.272
adding tons of unnecessary complexity to your code,

00:02:49.272 --> 00:02:53.177
speeding up a kernel way past the point where it's no longer a bottleneck.

00:02:53.177 --> 00:02:58.378
And it's just fundamentally a good idea to push any improvements through to the end code.

00:02:58.378 --> 00:03:01.296
You're making it real by doing that.

00:03:01.296 --> 00:03:04.701
And making it real as soon as possible ensures that you're running and profiling

00:03:04.701 --> 00:03:07.181
and parallelizing and optimizing real workloads.

00:03:07.181 --> 00:03:11.617
And the other thing to remember is that even small improvements are useful.

00:03:11.617 --> 00:03:16.473
If you make your code 50% faster or 2 times faster or 4 times faster,

00:03:16.473 --> 00:03:19.677
that's useful and you can push it out to the users of your code;

00:03:19.677 --> 00:03:21.990
you can start employing it and making it real,

00:03:21.990 --> 00:03:26.572
even if you think that there's a factor of 20 times speedup in the wings.

00:03:26.572 --> 00:03:31.695
The advice here again is be disciplined, analyze what you're doing,

00:03:31.695 --> 00:03:34.298
decide how you're going to parallelize it,

00:03:34.298 --> 00:03:38.429
decide how you're going to optimize it by studying that code

00:03:38.429 --> 00:03:41.639
and measuring it using profile-driven optimization,

00:03:41.639 --> 00:03:46.936
and finally, be sure to deploy frequently. So deploy early and often.

00:03:46.936 --> 00:03:50.386
Remember that this whole thing is intended to be a cycle.

