WEBVTT
Kind: captions
Language: en

00:00:00.240 --> 00:00:01.819
&gt;&gt; This is where LSTMs come in.

00:00:02.910 --> 00:00:06.309
LSTM stands for long short-term memory.

00:00:06.309 --> 00:00:10.220
Now, conceptually, a recurrent neural
network consists of a repetition of

00:00:10.220 --> 00:00:14.325
simple little units like this,
which take as an input the past,

00:00:14.325 --> 00:00:20.560
a new inputs, and produces a new
prediction and connects to the future.

00:00:20.560 --> 00:00:24.140
Now, what's in the middle of that is
typically a simple set of layers.

00:00:24.140 --> 00:00:27.550
Some weights and some linearities.A
typical neural network.

00:00:27.550 --> 00:00:28.750
With LSTMs,

00:00:28.750 --> 00:00:32.870
we're going to replace this little
module with a strange little machine.

00:00:32.870 --> 00:00:36.030
It will look complicated, but
it's still functionally a neural net.

00:00:36.030 --> 00:00:39.200
And you can drop it into your RNN and
not worry about it.

00:00:39.200 --> 00:00:42.010
Everything else about the architecture
remains the same, and

00:00:42.010 --> 00:00:44.710
it greatly reduces
the vanishing gradient problem.

