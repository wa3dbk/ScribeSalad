WEBVTT
Kind: captions
Language: en

00:00:00.012 --> 00:00:04.582
So in short, CUDA makes few guarantees about when and where thread blocks will run,

00:00:04.582 --> 00:00:06.788
and this is actually a huge advantage for CUDA.

00:00:06.788 --> 00:00:10.368
This is one of the big reasons why GPU programs can go so fast.

00:00:10.368 --> 00:00:11.789
Why is that?

00:00:11.789 --> 00:00:14.132
So among the advantages that the GPU gains from this programing model

00:00:14.132 --> 00:00:17.938
is that the hardware can run things really efficiently, because it has so much flexibility.

00:00:17.938 --> 00:00:22.774
For example, if 1 thread block completes quickly, the SM can immediately schedule another thread block

00:00:22.774 --> 00:00:25.477
without waiting for any others to complete.

00:00:25.477 --> 00:00:27.919
But the biggest advantage is scalability.

00:00:27.919 --> 00:00:30.750
Because you've made no guarantees about

00:00:30.750 --> 00:00:32.747
where the thread blocks will run

00:00:32.747 --> 00:00:35.219
or how many thread blocks might be running at a time,

00:00:35.219 --> 00:00:39.424
that means that you can scale all the way down to a GPU that would be running with a single SM,

00:00:39.424 --> 00:00:41.792
something that you might find in a tablet or a cell phone,

00:00:41.792 --> 00:00:45.409
all the way up to the massive GPUs that are used in supercomputers.

00:00:45.409 --> 00:00:48.667
Importantly, the scalability also applies to future GPUs,

00:00:48.667 --> 00:00:52.137
so you can be sure that GPUs will get more and more SMs

00:00:52.137 --> 00:00:55.317
as Moore's Law gives us more and more transistors on a chip,

00:00:55.317 --> 00:01:01.385
and by writing the code in such a way that it can run on an arbitrary number of SMs

00:01:01.385 --> 00:01:03.415
and doesn't depend on a certain number of SM's,

00:01:03.415 --> 00:01:05.817
a certain number of thread blocks being resident at a time,

00:01:05.817 --> 00:01:10.305
you can be sure that your CUDA code will scale forward to larger and larger GPUs.

00:01:10.305 --> 00:01:14.273
So this scalability applies from cell phones to supercomputers,

00:01:14.273 --> 00:01:18.231
from current to future GPUs, and that's a really huge advantage.

00:01:18.231 --> 00:01:20.631
And there are also consequences to this programming model,

00:01:20.631 --> 00:01:22.840
which are the things we've been talking about.

00:01:22.840 --> 00:01:26.305
You can make no assumptions about what blocks will run on what SM,

00:01:26.305 --> 00:01:29.341
and you can't have any explicit communication between blocks.

00:01:29.341 --> 00:01:34.861
For example, if block x is waiting for block y to give it some result before it can proceed,

00:01:34.861 --> 00:01:38.584
but block y has already run to completion and exited,

00:01:38.584 --> 00:01:40.783
then you're going to be in a bad place.

00:01:40.783 --> 00:01:46.005
That, by the way, is an example of something called dead lock in parallel computing.

00:01:46.005 --> 00:01:48.827
This really means that threads and blocks must complete, okay?

00:01:48.827 --> 00:01:51.099
You can't simply have a thread that hangs around forever,

00:01:51.099 --> 00:01:54.098
processing input or doing something,

00:01:54.098 --> 00:01:56.301
because that thread must complete

00:01:56.301 --> 00:01:59.956
in order for the block that it's in to complete so that other threads and blocks

00:01:59.956 --> 00:02:02.988
can get scheduled onto that SM.

