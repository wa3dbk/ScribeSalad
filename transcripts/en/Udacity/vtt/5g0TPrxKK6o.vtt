WEBVTT
Kind: captions
Language: en

00:00:00.560 --> 00:00:03.080
Alright. So in the examples up to this point, we've be

00:00:03.080 --> 00:00:07.689
setting the weights by hand to make various functions happen. And that's

00:00:07.689 --> 00:00:10.470
not really that useful in the context of machine learning. We'd really

00:00:10.470 --> 00:00:14.330
like a system, that given examples, finds weights that map the inputs

00:00:14.330 --> 00:00:16.940
to the outputs. And we're going to actually look at two different

00:00:16.940 --> 00:00:20.300
rules that have been developed for doing exactly that, to figuring out

00:00:20.300 --> 00:00:23.300
what the weights ought to be from training examples. One is called

00:00:23.300 --> 00:00:25.880
the the Perceptron Rule, and the other is called gradient descent or

00:00:25.880 --> 00:00:29.840
the Delta Rule. And the difference between them is the

00:00:29.840 --> 00:00:32.689
perception rule is going to make use of the threshold

00:00:32.689 --> 00:00:36.500
outputs, and the, the other mechanism is going to use

00:00:36.500 --> 00:00:39.400
unthreshold values. Alright so what we need to talk about

00:00:39.400 --> 00:00:41.530
now is the perception rule for, which is, how to

00:00:41.530 --> 00:00:45.690
set the weights of a single unit. So that it

00:00:45.690 --> 00:00:48.080
matches some training set. So we've got a training set,

00:00:48.080 --> 00:00:51.410
which is a bunch of examples of x, these are vectors,

00:00:51.410 --> 00:00:54.270
and we have y's which are zeros and ones which are the,

00:00:54.270 --> 00:00:57.150
the output that we want to hit. And what we want to do is

00:00:57.150 --> 00:01:02.300
set the, set the weights so that we capture this, this same data

00:01:02.300 --> 00:01:07.970
set. And we're going to do that by, modifying the weights over time.

00:01:07.970 --> 00:01:11.930
&gt;&gt; Oh, Michiel, what's the series of dashes over on the left.

00:01:11.930 --> 00:01:14.800
&gt;&gt; Oh, sorry, right. I should mention that, so

00:01:14.800 --> 00:01:16.430
one of the things that we're going to do here is

00:01:16.430 --> 00:01:18.910
were going to give a learning rate for the weights W,

00:01:18.910 --> 00:01:22.260
and not give a learning rule for Theta But we do

00:01:22.260 --> 00:01:24.500
need to learn the theta. So there's a, there's a very

00:01:24.500 --> 00:01:29.040
convenient trick for actually learning them by just treating it as

00:01:29.040 --> 00:01:32.310
a, as another kind of weight. So if you think about

00:01:32.310 --> 00:01:34.790
the way that the, the thresholding function works. We're taking a

00:01:34.790 --> 00:01:37.960
linear combination of the W's and X's, then we're comparing it

00:01:37.960 --> 00:01:41.440
to theta,but if you think about just subtracting theta from both

00:01:41.440 --> 00:01:45.820
sides, then, in some sense theta just becomes another

00:01:45.820 --> 00:01:48.250
one of the weights, and we're just comparing to

00:01:48.250 --> 00:01:50.540
zero. So what, what I did here was took

00:01:50.540 --> 00:01:53.400
the actual data, the x's, and I added what is

00:01:53.400 --> 00:01:55.920
sometimes called a, a bias unit to it. So

00:01:55.920 --> 00:02:00.550
basically, the input is one always to that. And the

00:02:00.550 --> 00:02:03.120
weight corresponding to it is going to correspond to

00:02:03.120 --> 00:02:06.890
negative theta ultimately. So, just, just again, this just simplifies

00:02:06.890 --> 00:02:09.660
things so that the threshold can be treated the same

00:02:09.660 --> 00:02:11.550
as the weights. So from now on, we don't have

00:02:11.550 --> 00:02:13.370
to worry about the threshold. It just gets folded into

00:02:13.370 --> 00:02:16.480
the weights, and all our comparisons are going to be just

00:02:16.480 --> 00:02:20.170
to zero instead of some, instead of theta. Centric, yeah.

00:02:21.550 --> 00:02:24.720
It certainly makes the math shorter. So okay, so this

00:02:24.720 --> 00:02:27.230
is what we're going to do. We're going to iterate over this

00:02:27.230 --> 00:02:31.900
training set, grabbing an x, which includes the bias piece,

00:02:31.900 --> 00:02:35.550
and the y. Where y is our target X is our

00:02:35.550 --> 00:02:38.460
input. And what we're going to do is we're going to

00:02:38.460 --> 00:02:43.070
change weight i, the, the, the weight corresponding to the ith

00:02:43.070 --> 00:02:46.580
unit, by the amount that we're changing the weight by. So

00:02:46.580 --> 00:02:49.580
this is sort of a tautology, right. This is truly just

00:02:49.580 --> 00:02:52.270
saying the amount we've changed the weight by is exactly delta

00:02:52.270 --> 00:02:54.716
W - in other words the amount we've changed the weight

00:02:54.716 --> 00:02:56.796
by. So we need to define that what that weight change is.

00:02:56.796 --> 00:02:58.972
The weight change is going to be find as

00:02:58.972 --> 00:03:03.520
falls. We're going to take the target, the thing that

00:03:03.520 --> 00:03:05.860
we want the output to be. And compare it

00:03:05.860 --> 00:03:09.780
to, what the network with the current weight actually spits

00:03:09.780 --> 00:03:12.580
out. So we compute this, this y hat. This

00:03:12.580 --> 00:03:17.240
approximate output y. By again summing up the inputs according

00:03:17.240 --> 00:03:18.880
to the weights and comparing it to zero. That gets

00:03:18.880 --> 00:03:22.050
us a zero one value.So we're now comparing that to

00:03:22.050 --> 00:03:24.810
what the actual value is. So what's going to happen here, if

00:03:24.810 --> 00:03:28.600
they are both zero so let's, let's look at this. Each of

00:03:28.600 --> 00:03:30.840
y and y hat can only be zero and one. If they

00:03:30.840 --> 00:03:34.350
are both zeros then this y minus y hat is zero. If

00:03:34.350 --> 00:03:37.010
they're both ones and what does that mean? It means the output

00:03:37.010 --> 00:03:39.970
should have been zero and the output of our current. Network really

00:03:39.970 --> 00:03:44.850
was zero, so that's, that's kind of good. If they are both ones,

00:03:44.850 --> 00:03:47.770
it means the output was supposed to be one and our network outputted

00:03:47.770 --> 00:03:50.570
one, and the difference between them is going to be zero. But in

00:03:50.570 --> 00:03:53.770
this other case, y minus y hat, if the output was supposed to

00:03:53.770 --> 00:03:56.140
be zero, but we said one, our network says one, then we

00:03:56.140 --> 00:03:59.490
get a negative one. If the output was supposed to be one and

00:03:59.490 --> 00:04:02.030
we said zero, then we get a positive one. Okay, so those

00:04:02.030 --> 00:04:06.150
are the four cases for what's happening here. We're going to take that value

00:04:06.150 --> 00:04:10.430
multiply it by the current input to that unit i, scale it

00:04:10.430 --> 00:04:12.790
down by the sort of thing that is going to be cut the learning

00:04:12.790 --> 00:04:15.660
rate and use that as the the weight update

00:04:15.660 --> 00:04:18.390
change. So essentially what we are saying is if the

00:04:18.390 --> 00:04:21.290
output is already correct either both on or both

00:04:21.290 --> 00:04:23.490
off. Then there's gong to be no change to the

00:04:23.490 --> 00:04:28.800
weights. But, if our output is wrong. Let's say

00:04:28.800 --> 00:04:32.540
that we are giving a one when we should have

00:04:32.540 --> 00:04:35.120
been giving a zero. That means our, the total

00:04:35.120 --> 00:04:37.820
here is too large. And so we need to make

00:04:37.820 --> 00:04:38.920
it smaller. How are we going to make it

00:04:38.920 --> 00:04:45.530
smaller? Which ever input XI's correspond too, very large

00:04:45.530 --> 00:04:48.550
values, we're going to move those weights very far in

00:04:48.550 --> 00:04:51.080
a negative direction. We're taking this negative one times that

00:04:51.080 --> 00:04:54.520
value times this, this little learning rate. Alright, the

00:04:54.520 --> 00:04:56.950
other case is if the output was supposed to one

00:04:56.950 --> 00:04:59.160
but we're outputting a zero, that means our total

00:04:59.160 --> 00:05:03.110
is too small. And what this rule says is increase

00:05:03.110 --> 00:05:06.320
the weights essentially to try to make the sum bigger. Now, we

00:05:06.320 --> 00:05:08.590
don't want to kind of overdo it, and that's what this learning rate

00:05:08.590 --> 00:05:11.610
is about. Learning rate basically says we'll figure out the direction that

00:05:11.610 --> 00:05:13.810
we want to move things and just take a little step in that

00:05:13.810 --> 00:05:18.950
direction. We'll keep repeating over all of the, the input output pairs.

00:05:18.950 --> 00:05:21.330
So, we'll have a chance to get in to really build things up,

00:05:21.330 --> 00:05:23.000
but we're going to do it a little bit at a time so

00:05:23.000 --> 00:05:26.190
we don't overshoot. And that's the

00:05:26.190 --> 00:05:28.050
rule. It's actually extremely simple. Like, you,

00:05:28.050 --> 00:05:31.790
actually writing this in code is, is quite trivial. And and

00:05:31.790 --> 00:05:35.630
yet, it does some remarkable things. So let's imagine for a

00:05:35.630 --> 00:05:37.500
second that we have a training set that looks like this.

00:05:37.500 --> 00:05:40.600
It's in two dimensions, again, so that it's easy to visualize.

00:05:40.600 --> 00:05:43.960
That we've got. A bunch of positive examples, these green x's

00:05:43.960 --> 00:05:46.620
and we've got a bunch of negative examples these red x's,

00:05:46.620 --> 00:05:50.410
and were trying to learn basically a half plane right? Were

00:05:50.410 --> 00:05:53.070
trying to learn a half plane that separates the positive from the

00:05:53.070 --> 00:05:57.500
negative examples. So Charles do you see a, a, half plane

00:05:57.500 --> 00:05:58.900
that we could put in here that would do the trick?

00:05:58.900 --> 00:05:59.870
&gt;&gt; I do.

00:06:00.980 --> 00:06:01.630
&gt;&gt; What would it look like?

00:06:01.630 --> 00:06:02.470
&gt;&gt; It's that one.

00:06:02.470 --> 00:06:06.115
&gt;&gt; By that one do you mean, this one?

00:06:06.115 --> 00:06:08.880
&gt;&gt; Yeah. That's exactly what I was thinking, Michael.

00:06:08.880 --> 00:06:11.600
&gt;&gt; That's awesome! Yeah, there are isn't, isn't a whole lot

00:06:11.600 --> 00:06:13.940
of flexibility in what the answer is in this case, if

00:06:13.940 --> 00:06:16.180
we really want to get all greens on one side and all

00:06:16.180 --> 00:06:18.300
the reds on the other. If there is such a half

00:06:18.300 --> 00:06:20.870
plane that separates the positive from the negative examples, then

00:06:20.870 --> 00:06:24.750
we say that the data set is linearly separable, right? That

00:06:24.750 --> 00:06:27.590
there is a way of separating the positives and negatives with

00:06:27.590 --> 00:06:31.710
a line. And what's cool about the perception rule, is that

00:06:31.710 --> 00:06:35.530
if we have data that is linearly separable. The Perceptron Rule

00:06:35.530 --> 00:06:39.080
will find it. It only needs a finite number of iterations

00:06:39.080 --> 00:06:41.200
to find it. In fact, which I guess is really the

00:06:41.200 --> 00:06:43.770
same as saying that it will actually find it. It won't

00:06:43.770 --> 00:06:47.070
eventually get around to getting to something close to it.

00:06:47.070 --> 00:06:49.080
It will actually find a line, and it will stop

00:06:49.080 --> 00:06:52.010
saying okay I now have a set of weights that,

00:06:52.010 --> 00:06:55.520
that do the trick. So that's happens if the data set

00:06:55.520 --> 00:06:59.150
is in fact linearly separable and that's pretty cool. It's

00:06:59.150 --> 00:07:01.130
pretty amazing that it can do that, it's a very

00:07:01.130 --> 00:07:03.440
simple rule and it just goes through and iterates and,

00:07:03.440 --> 00:07:07.380
and solves the problem. So. Charles Sened solves the problem. So.

00:07:07.380 --> 00:07:09.340
&gt;&gt; I can think of one.

00:07:09.340 --> 00:07:10.830
What if it is not linearly separable?

00:07:10.830 --> 00:07:15.060
&gt;&gt; Hmm, I see. So, if the data is

00:07:15.060 --> 00:07:17.530
linearlly separable, then the algorithm works, so the algorithm

00:07:17.530 --> 00:07:20.300
simply needs to only be run when the data

00:07:20.300 --> 00:07:23.810
is linearlly separable. It's generally not that easy tell actually,

00:07:23.810 --> 00:07:25.770
when your data is linearly separable especially, here we

00:07:25.770 --> 00:07:28.710
have it in two dimensions, if it's in 50 dimensions,

00:07:28.710 --> 00:07:30.720
know whether or not there is a setting of

00:07:30.720 --> 00:07:34.740
those perimeters that makes it linearly separable, not so clear.

00:07:34.740 --> 00:07:37.820
&gt;&gt; Well there is one way you could do it.

00:07:37.820 --> 00:07:38.110
&gt;&gt; Whats that?

00:07:38.110 --> 00:07:44.170
&gt;&gt; You could run this algorithm, and see if it ever stops. I see,

00:07:44.170 --> 00:07:49.280
yes of course, there's a problem with that particular scheme, right, which says,

00:07:49.280 --> 00:07:52.140
well for one thing this algorithm never stops, so wait, we need to, we

00:07:52.140 --> 00:08:00.414
need to address that. But, but really we should be running this loop here,

00:08:00.414 --> 00:08:05.210
while, there's some error so I neglected to say that

00:08:05.210 --> 00:08:07.350
before. But what you'll notice is if you continue to run

00:08:07.350 --> 00:08:10.470
this after the point where it's getting all the answers right.

00:08:10.470 --> 00:08:13.110
It found a set of weights that lineally separate the positive

00:08:13.110 --> 00:08:15.640
and negative instances what will happen is when it gets

00:08:15.640 --> 00:08:19.290
to this delta w line that y minus y hat will

00:08:19.290 --> 00:08:22.370
always be zero the weights will never change we'll go back

00:08:22.370 --> 00:08:25.510
and update them by adding zero to them repeatedly over and

00:08:25.510 --> 00:08:29.300
over again. So. If it ever does reach zero

00:08:29.300 --> 00:08:31.850
error, if it ever does separate the data set

00:08:31.850 --> 00:08:33.250
then we can just put a little condition in

00:08:33.250 --> 00:08:35.020
there and tell it to stop filtering So what you

00:08:35.020 --> 00:08:39.190
are suggesting is that we could run this algorithm

00:08:39.190 --> 00:08:40.429
and if it stops then we know that it is

00:08:40.429 --> 00:08:43.950
linearly separable and if it doesn't stop Then we

00:08:43.950 --> 00:08:46.760
know that it's not linearly separable, right? By this guarantee.

00:08:46.760 --> 00:08:46.980
&gt;&gt; Sure.

00:08:46.980 --> 00:08:50.420
&gt;&gt; The problem is we, we don't know when finite is done, right?

00:08:50.420 --> 00:08:54.890
If, if this were like 1,000 iterations, we could run it for 1,000 if it wasn't

00:08:54.890 --> 00:08:58.550
done. It's not done, but all we know at this point is that it's a finite number

00:08:58.550 --> 00:09:00.350
of iterations, and so that could be a

00:09:00.350 --> 00:09:02.440
thousand, 10 thousand, a million, ten million, we

00:09:02.440 --> 00:09:04.100
don't know, so we never know when to

00:09:04.100 --> 00:09:06.980
stop and declare the data set not linearly separable.

00:09:06.980 --> 00:09:08.200
&gt;&gt; Hmm, so if we could do that,

00:09:08.200 --> 00:09:10.460
then we would have solved the halting problem, and

00:09:10.460 --> 00:09:13.190
we would all have nobel prizes Well, that's

00:09:13.190 --> 00:09:15.490
not necessarily the case. But it's certainly the other

00:09:15.490 --> 00:09:17.260
direction is true. That if we could solve

00:09:17.260 --> 00:09:18.800
the halting problem, then we could solve this.

00:09:18.800 --> 00:09:19.540
&gt;&gt; Hm.

00:09:19.540 --> 00:09:20.990
&gt;&gt; But it could be that this problem

00:09:20.990 --> 00:09:23.160
might be solvable even without solving the halting problem.

00:09:23.160 --> 00:09:24.640
&gt;&gt; Fair enough. Okay.

