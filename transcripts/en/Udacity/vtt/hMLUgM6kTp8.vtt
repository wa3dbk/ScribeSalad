WEBVTT
Kind: captions
Language: en

00:00:00.450 --> 00:00:03.460
The problem with scaling
gradient descent is simple.

00:00:03.460 --> 00:00:05.500
You need to compute this gradient.

00:00:05.500 --> 00:00:07.480
Here is another rule of thumb.

00:00:07.480 --> 00:00:11.230
If computing your loss takes
n floating point operations,

00:00:11.230 --> 00:00:14.990
computing its gradient takes
about three times that compute.

00:00:14.990 --> 00:00:18.150
As we saw earlier,
this last function is huge.

00:00:18.150 --> 00:00:21.880
It depends on every single
element in your training set.

00:00:21.880 --> 00:00:24.350
That can be a lot of compute
if your data set is big.

00:00:24.350 --> 00:00:28.400
And we want to be able to train
lots of data because in practice

00:00:28.400 --> 00:00:33.100
on real problems you will always get
more gains the more data you use.

00:00:33.100 --> 00:00:34.900
And because gradient
descent is intuitive,

00:00:34.900 --> 00:00:37.270
you have to do that for many steps.

00:00:37.270 --> 00:00:41.000
That means going through your
data tens or hundreds of times.

00:00:41.000 --> 00:00:43.650
That's not good, so instead,
we're going to cheat.

00:00:43.650 --> 00:00:47.190
Instead of computing the loss, we're
going to compute an estimate of it,

00:00:47.190 --> 00:00:50.640
a very bad estimate,
a terrible estimate in fact.

00:00:50.640 --> 00:00:54.280
That estimate is going to be so bad,
you might wonder why it works at all.

00:00:54.280 --> 00:00:55.480
And you would be right,

00:00:55.480 --> 00:00:59.630
because we're going to also have to
spend some time making it less terrible.

00:00:59.630 --> 00:01:03.490
The estimate we're going to use is
simply computing the average loss for

00:01:03.490 --> 00:01:06.360
a very small random fraction
of the training data.

00:01:06.360 --> 00:01:09.790
Think between 1 and
1000 training samples each time.

00:01:10.790 --> 00:01:13.240
I say random because
it's very important.

00:01:13.240 --> 00:01:16.150
If the way you pick your
samples isn't random enough,

00:01:16.150 --> 00:01:18.070
It no longer works at all.

00:01:18.070 --> 00:01:22.060
So, we're going to take a very small
sliver of the training data, compute

00:01:22.060 --> 00:01:26.890
the loss for that sample, compute the
derivative for that sample and pretend

00:01:26.890 --> 00:01:30.530
that that derivative is the right
direction to use to do gradient descent.

00:01:31.630 --> 00:01:33.800
It is not at all the right direction,
and

00:01:33.800 --> 00:01:37.910
in fact at times it might increase
the real loss, not reduce it.

00:01:37.910 --> 00:01:41.090
But we're going to compensate
by doing this many, many times,

00:01:41.090 --> 00:01:44.740
taking very, very small steps each time.

00:01:44.740 --> 00:01:47.308
So each step is a lot
cheaper to compute.

00:01:47.308 --> 00:01:48.900
But we pay a price.

00:01:48.900 --> 00:01:52.910
We have to take many more smaller steps,
instead of one large step.

00:01:52.910 --> 00:01:55.710
On balance, though, we win by a lot.

00:01:55.710 --> 00:01:57.580
In fact,
as you'll see in the assignments,

00:01:57.580 --> 00:02:01.470
doing this is vastly more efficient
than doing gradient descent.

00:02:01.470 --> 00:02:04.170
This technique is called
stochastic gradient descent and

00:02:04.170 --> 00:02:06.100
is at the core of deep learning.

00:02:06.100 --> 00:02:09.440
That's because stochastic gradient
descent scales well with both data and

00:02:09.440 --> 00:02:13.570
model size, and
we want both big data and big models.

00:02:13.570 --> 00:02:17.910
Stochastic gradient descent, SGD for
short, is nice and scalable.

00:02:17.910 --> 00:02:21.420
But because it is fundamentally a pretty
bad optimizer, that happens to be

00:02:21.420 --> 00:02:25.420
the only one that's fast enough, it
comes with a lot of issues in practice.

