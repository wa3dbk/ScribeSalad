WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.000
There are a couple of things to observe here.

00:00:03.000 --> 00:00:10.000
One is in general the fake data pulls everything towards 0.5.

00:00:10.000 --> 00:00:15.000
Where you go to extremes over here, we are less extreme in this case.

00:00:15.000 --> 00:00:20.000
0.33 is further away from 0.5 than 0.4.

00:00:20.000 --> 00:00:24.000
So, all these numbers get moved towards 0.5.

00:00:24.000 --> 00:00:27.000
This is somewhat smoother.

00:00:27.000 --> 00:00:32.000
We also see that these two outcomes--the first and the last--

00:00:32.000 --> 00:00:36.000
on the division model gives us the same extreme estimate,

00:00:36.000 --> 00:00:43.000
but the more data we get in our new estimator, the more we are willing to move away from 0.5.

00:00:43.000 --> 00:00:49.000
One observation of heads gave us 0.667, two of them 0. 75.

00:00:49.000 --> 00:00:54.000
I can promise you in the limit, as you only see heads for infinitely many,

00:00:54.000 --> 00:00:57.000
we will finally approach 1. Now, this is really cool.

00:00:57.000 --> 00:01:05.000
We added fake data, and I will tell you that I generally think these are better estimates in practice.

00:01:05.000 --> 00:01:13.000
The reason why is it's really reckless after a single coin flip to assume that all coins come up positive.

00:01:13.000 --> 00:01:17.000
I think it's much more moderate to say, well, we already have some evidence

00:01:17.000 --> 00:01:21.000
that heads might be more likely, but we're not quite convinced yet.

00:01:21.000 --> 00:01:26.000
The not quite convinced is the same as having a prior.

00:01:26.000 --> 00:01:29.000
There's an entire literature that talks about these priors.

00:01:29.000 --> 00:01:31.000
They have a very cryptic name.

00:01:31.000 --> 00:01:33.000
They're called Dirichlet priors.

00:01:33.000 --> 00:01:40.000
But, more importantly, the method of adding fake data is called a Laplacian estimator.

00:01:40.000 --> 00:01:44.000
When there is plenty data, Laplacian estimator gives about the same results

00:01:44.000 --> 00:01:46.000
as the maximum likelihood estimator.

00:01:46.000 --> 00:01:52.000
But when data is scarce, this works usually much, much, much better

00:01:52.000 --> 00:01:54.000
than the maximum likelihood estimator.

00:01:54.000 --> 99:59:59.000
It's a really important lesson in statistics.

