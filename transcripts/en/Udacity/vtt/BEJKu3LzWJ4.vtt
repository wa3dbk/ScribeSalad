WEBVTT
Kind: captions
Language: en

00:00:00.200 --> 00:00:02.510
&gt;&gt; So this is a remarkable fact about this Q-learning

00:00:02.510 --> 00:00:05.030
rule, and that is if we start Q hat off

00:00:05.030 --> 00:00:07.880
pretty much anywhere, and then we update it according to

00:00:07.880 --> 00:00:11.580
the rule that we talked about. Q for, for when

00:00:11.580 --> 00:00:15.910
we see a transition s,a, r, s prime, then we

00:00:15.910 --> 00:00:19.870
update (s,a), the Q value for (s,a), move it alpha

00:00:19.870 --> 00:00:22.290
of the way towards r plus gamma, max a of

00:00:22.290 --> 00:00:25.740
the, well basically the Q value of the state S prime.

00:00:25.740 --> 00:00:29.590
Then as long as we do that, then this estimate, this q hat S A

00:00:29.590 --> 00:00:33.080
goes to Q S A. The actual solution to the Bellman equation. And I write

00:00:33.080 --> 00:00:34.390
this with an exclamation mark, because it's

00:00:34.390 --> 00:00:36.350
like, it's one line of code! It's one

00:00:36.350 --> 00:00:39.900
line of code, like, how could you not just go out and write this right now?

00:00:39.900 --> 00:00:40.680
&gt;&gt; Hm.

00:00:40.680 --> 00:00:45.636
&gt;&gt; But the, the, the, let me just, to finish is, this is only true if we

00:00:45.636 --> 00:00:51.470
actually visit SA infinitely often. So you know, that's an important caveat.

00:00:51.470 --> 00:00:53.940
That for this to, to actually hold true, for you to really converge

00:00:53.940 --> 00:00:56.870
to the, the solution, it has to run for a long time. It

00:00:56.870 --> 00:00:59.760
has to visit all state action pairs. The learning rates have to be

00:00:59.760 --> 00:01:02.770
updated the way that we talked about before. The next states need to

00:01:02.770 --> 00:01:05.950
be drawn from the actual transition probabilities but that's, that's cool, if we

00:01:05.950 --> 00:01:10.320
actually are learning in some actual environment and the rewards need to be

00:01:10.320 --> 00:01:13.010
drawn from the rewards function. So, this isn't so problematic. This is a

00:01:13.010 --> 00:01:16.620
little bit problematic, but it is still very reassuring, this idea that we have

00:01:16.620 --> 00:01:19.320
the right form of an update rule, so that the thing

00:01:19.320 --> 00:01:22.330
that we converge to is the actual optimal solution to the MDP.

00:01:22.330 --> 00:01:23.640
&gt;&gt; Cool. And we just have to wait til the

00:01:23.640 --> 00:01:26.040
heat death of the universe, or infinity, and then we're done.

00:01:26.040 --> 00:01:27.080
&gt;&gt; Yeah.

