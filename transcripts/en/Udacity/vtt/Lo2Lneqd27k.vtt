WEBVTT
Kind: captions
Language: en

00:00:00.930 --> 00:00:03.980
Now let's look at how we can exploit the caches

00:00:03.980 --> 00:00:07.610
available. Now, it is a fact that a tested set

00:00:07.610 --> 00:00:11.910
instruction has to necessarily go to memory when we want

00:00:11.910 --> 00:00:13.790
to acquire the lock, we have to execute a tested

00:00:13.790 --> 00:00:16.570
set instructions so that we can atomically make sure that

00:00:16.570 --> 00:00:19.590
exactly one processor gets the lock. But on the other

00:00:19.590 --> 00:00:23.320
hand, the guys that don't have the lock. Could, in

00:00:23.320 --> 00:00:26.310
fact, exploit the caches in order to wait for the lock.

00:00:26.310 --> 00:00:28.480
And that's why this particular algorithm that I'm going to

00:00:28.480 --> 00:00:31.010
describe to you is what is called spin on read,

00:00:31.010 --> 00:00:33.660
and the assumption here is that you have a

00:00:33.660 --> 00:00:37.330
shared memory machine in which the architecture is providing cache

00:00:37.330 --> 00:00:40.170
coherence, or in other words, through the system bus

00:00:40.170 --> 00:00:44.320
or interconnection network, The hardware is ensuring that the caches

00:00:44.320 --> 00:00:47.780
are kept coherent. Well that gives us an idea

00:00:47.780 --> 00:00:51.350
as to how we can exploit the caches. The waiters,

00:00:51.350 --> 00:00:53.860
instead of executive a test [INAUDIBLE] instruction that has

00:00:53.860 --> 00:00:56.760
to go to memory, they can spin locally on the

00:00:56.760 --> 00:00:59.410
cached value of the lock. because when you are

00:00:59.410 --> 00:01:02.400
spinning on the local cached value of the lock. If

00:01:02.400 --> 00:01:04.900
that value changes in memory, these guys are going

00:01:04.900 --> 00:01:09.170
to notice that. That's the principle behind the cache coherence

00:01:09.170 --> 00:01:11.890
that is implemented in hardware. And so we can exploit

00:01:11.890 --> 00:01:16.790
that fact in implementing a more efficient way of spinning.

00:01:16.790 --> 00:01:19.700
Which is called spin on read. The idea is that the

00:01:19.700 --> 00:01:23.220
lock algorithm, the first thing it's going to do is go and

00:01:23.220 --> 00:01:26.090
do a check on the, the memory location to see if

00:01:26.090 --> 00:01:29.630
it is locked. So this is a, a normal atomic read operation

00:01:29.630 --> 00:01:32.540
that is being done, not a test and spin operation, so

00:01:32.540 --> 00:01:34.900
if it is not in the cache, you're going to go to memory

00:01:34.900 --> 00:01:37.770
and bring it in, and once you bring it in, so

00:01:37.770 --> 00:01:41.860
long as this value doesn't change, we're going to basically looking at the

00:01:41.860 --> 00:01:44.210
value that is in that cache in order to do

00:01:44.210 --> 00:01:46.650
the checking And I'm not going to go to the

00:01:46.650 --> 00:01:50.790
bus and therefore, I'm not producing any contention on the

00:01:50.790 --> 00:01:54.410
network. And there could be any number of processes waiting on

00:01:54.410 --> 00:01:56.960
the lock simultaneously. No problem with that because all of

00:01:56.960 --> 00:02:00.290
them are going to be spinning on the local value of

00:02:00.290 --> 00:02:03.790
L in their respective caches. And so if there is

00:02:03.790 --> 00:02:06.880
one processor that's actually doing useful work and it has to

00:02:06.880 --> 00:02:08.880
go to memory, it's not going to find that

00:02:08.880 --> 00:02:11.710
to be a problem. No contention on the network

00:02:11.710 --> 00:02:15.600
from the waiting processors because of this. Now,

00:02:15.600 --> 00:02:18.890
if the one processor that was having the lock

00:02:18.890 --> 00:02:22.260
eventually releases it, they go, everybody's going to notice

00:02:22.260 --> 00:02:24.990
that. And so if I'm waiting for the lock,

00:02:24.990 --> 00:02:26.610
and I've been spinning here locally in my

00:02:26.610 --> 00:02:31.990
cache, when the, the lock is released, I'll notice

00:02:31.990 --> 00:02:34.920
that through the cache coherence mechanism as I'll, I'll

00:02:34.920 --> 00:02:37.820
break out of this spin loop. But immediately, I

00:02:37.820 --> 00:02:40.260
want to check, I want check if the lock

00:02:40.260 --> 00:02:42.560
is available by doing this test and set and

00:02:42.560 --> 00:02:46.520
get it uniquely for myself. So multiple processors are

00:02:46.520 --> 00:02:51.030
trying to execute this testing set simultaneously. It's possible

00:02:51.030 --> 00:02:53.480
somebody else is going to beat me to the punch

00:02:53.480 --> 00:02:57.000
and if that happens, I simply go back and, and,

00:02:57.000 --> 00:03:04.630
and do the grouping on my private copy of L and wait for the guy who beat me

00:03:04.630 --> 00:03:07.760
to the punch to release a lock eventually. So

00:03:07.760 --> 00:03:10.530
that I can get it. So that's the idea.

00:03:10.530 --> 00:03:13.810
The idea is you spin locally. When you notice

00:03:13.810 --> 00:03:15.820
that the lock has been released you try and

00:03:15.820 --> 00:03:22.020
do a test and set. If you get lucky you win, if you lose you go back and spin

00:03:22.020 --> 00:03:24.870
again locally. So that's the idea behind spinning on

00:03:24.870 --> 00:03:29.230
read. The unlock operation of course is pretty straightforward. The

00:03:29.230 --> 00:03:32.420
guy that wants to unlock is simply going to change the

00:03:32.420 --> 00:03:35.880
memory location to indicate that L is no longer locked.

00:03:35.880 --> 00:03:37.960
So that's all it has to do. And then, and

00:03:37.960 --> 00:03:41.280
then the other processes can observe it through the cache

00:03:41.280 --> 00:03:44.280
coherence mechanism, and be able to acquire the lock. But

00:03:44.280 --> 00:03:47.070
note what happens when the lock is released. When the

00:03:47.070 --> 00:03:50.430
lock is released, all the processes that are stuck here

00:03:50.430 --> 00:03:52.880
in the spin loop, they are going to go and

00:03:52.880 --> 00:03:55.470
try to do this test and operation at the same

00:03:55.470 --> 00:03:59.110
time, and we know that test and set has to

00:03:59.110 --> 00:04:01.740
bypass the cache, everyone is hitting on the bus, right?

00:04:01.740 --> 00:04:03.610
Everybody is hitting on the bus, trying to go to

00:04:03.610 --> 00:04:07.210
memory, in order to do this test and operation. And

00:04:07.210 --> 00:04:12.278
so that essentially means that, in a right invalidate this

00:04:12.278 --> 00:04:16.630
cache coherence mechanism is going to result

00:04:16.630 --> 00:04:21.130
in order of n squared bus transactions.

00:04:21.130 --> 00:04:25.920
For all of these guys to stop chattering on the bus, because everyone of

00:04:25.920 --> 00:04:29.070
these test and set instructions is going to

00:04:29.070 --> 00:04:32.880
result in invalidating the caches, and as

00:04:32.880 --> 00:04:38.380
a result, you have an order of n squared operation that is going to result

00:04:38.380 --> 00:04:40.510
when a lock is released, where n is the number

00:04:40.510 --> 00:04:44.560
of processors that are simultaneously trying to get the lock.

00:04:44.560 --> 00:04:48.060
And, obviously, this is impeding that one guy that got

00:04:48.060 --> 00:04:51.290
the lock and can actually get some useful work done.

00:04:51.290 --> 00:04:54.820
And this is clearly disruptive. And earlier one of the

00:04:54.820 --> 00:04:57.670
things that we said is that we want to avoid

00:04:57.670 --> 00:05:00.830
or limit the amount of disruption to useful work that

00:05:00.830 --> 00:05:03.580
can be done by the process that acquired the lock.

