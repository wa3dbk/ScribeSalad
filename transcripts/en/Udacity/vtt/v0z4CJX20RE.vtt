WEBVTT
Kind: captions
Language: en

00:00:00.240 --> 00:00:03.510
Here's another algebraic way
of thinking about this, okay?

00:00:04.600 --> 00:00:08.610
Using our old definition
of b transpose b, okay,

00:00:08.610 --> 00:00:14.365
you'll notice that what it is
is actually this matrix okay?

00:00:14.365 --> 00:00:19.565
Sum of x squared, sum of xys,
sum of xys, sum of y squared.

00:00:19.565 --> 00:00:23.115
Well that's just the covariance matrix

00:00:23.115 --> 00:00:26.405
of the points assuming that
there are about the origin.

00:00:26.405 --> 00:00:29.365
So I don't have to track that the mean,
that would be the covariance.

00:00:29.365 --> 00:00:33.910
Speaking of which, yet one more
way of thinking about this is that

00:00:33.910 --> 00:00:38.100
B transpose B here is
the sum of this xx transpose.

00:00:38.100 --> 00:00:40.720
That's what's referred
to as the outer product.

00:00:40.720 --> 00:00:44.150
And if it's about the origin, I don't
have to subtract off the mean, but

00:00:44.150 --> 00:00:47.770
the, here it is written just like that.

00:00:47.770 --> 00:00:52.220
That again is the definition
of the covariance matrix.

00:00:52.220 --> 00:00:55.670
This makes some sense, for those of you
thinking about it, the covariance matrix

00:00:55.670 --> 00:00:58.400
is a relationship between
the expected value of the squares,

00:00:58.400 --> 00:01:03.120
the sum of the squares of points
distributed about the mean.

00:01:03.120 --> 00:01:04.750
So that's actually,

00:01:04.750 --> 00:01:08.060
if you think about it,
the saying what we wanted to, to know.

00:01:08.060 --> 00:01:10.130
We wanted to know the greatest variance.

00:01:10.130 --> 00:01:12.900
For those of you that care, or
are thinking about, if you think about

00:01:12.900 --> 00:01:17.220
a covariance matrix as an ellipsoid in
space your eigenvectors are going to be,

00:01:17.220 --> 00:01:19.920
end up being,
the major axis of these ellipsoids.

