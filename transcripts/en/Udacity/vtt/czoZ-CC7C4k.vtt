WEBVTT
Kind: captions
Language: en

00:00:00.230 --> 00:00:06.130
So a distributed loss level cache
is logically a single cache in that

00:00:06.130 --> 00:00:11.500
a data block will not be replicated
like it would be in private caches,

00:00:11.500 --> 00:00:14.310
where if two or three or four cores, for

00:00:14.310 --> 00:00:18.040
example, read a block, each of
their caches would have that block.

00:00:18.040 --> 00:00:19.100
And thus,

00:00:19.100 --> 00:00:23.910
really the total capacity of the caches
is not how much memory we are covering,

00:00:23.910 --> 00:00:28.410
because some memory appears many times,
possibly in these caches.

00:00:28.410 --> 00:00:32.710
A last level cache that is distributed
is logically still one cache,

00:00:32.710 --> 00:00:35.610
meaning if it's capacity is
let's say four megabytes,

00:00:35.610 --> 00:00:40.350
then we really cover four megabytes
of memory without any replication.

00:00:40.350 --> 00:00:45.250
But this distributed cache is sliced
up so that each tile, which contains

00:00:45.250 --> 00:00:50.580
a core and possibly local caches,
now also contains a part of this cache.

00:00:50.580 --> 00:00:57.670
So now what we have is in our mash,
we have a tile that contains a core,

00:00:57.670 --> 00:01:03.570
a level one cache, a level two cache,
and a slice of the L3 cache.

00:01:03.570 --> 00:01:08.123
So if this slice here that we
use per core is let's say 1MB,

00:01:08.123 --> 00:01:12.771
is really N x 1MB where N
is the number of cores, so

00:01:12.771 --> 00:01:16.360
now as you can see,
if we use four cores,

00:01:16.360 --> 00:01:21.087
we get 4MB, with 16 cores we have 16MB,
so we get what we want.

00:01:21.087 --> 00:01:26.120
The big cache that is there
to prevent memory accesses

00:01:26.120 --> 00:01:31.290
is growing with the number of cores, but
it doesn't have a single entry point.

00:01:31.290 --> 00:01:34.920
In fact,
each slice has its own entry point.

00:01:34.920 --> 00:01:37.840
So if we want the data
that is in this slice,

00:01:37.840 --> 00:01:40.630
we have to route our
message to that slice.

00:01:40.630 --> 00:01:43.110
If we want the data that
resides in another slice,

00:01:43.110 --> 00:01:45.210
we send the message there.

00:01:45.210 --> 00:01:50.420
If the data is spread among
the slices so that all

00:01:50.420 --> 00:01:54.850
of them get equal demands for the data,
then the natural traffic will be nicely

00:01:54.850 --> 00:01:59.690
distributed across our chip, so that
none of the links get over utilized.

00:01:59.690 --> 00:02:03.980
But if we have a level two
miss in a particular core,

00:02:03.980 --> 00:02:09.460
how do we know which slice of the level
three cache to ask for the data?

00:02:09.460 --> 00:02:12.390
One simple solution is
to spread the cache,

00:02:12.390 --> 00:02:16.440
a round robin among slices,
by cache index.

00:02:16.440 --> 00:02:19.677
So if we have our level
three cache with let's say,

00:02:19.677 --> 00:02:24.420
1,024 sets slice zero gets set zero.

00:02:24.420 --> 00:02:31.170
Slice one gets set one, etc., and
then let's say that we have eight cores.

00:02:31.170 --> 00:02:33.790
Then slice seven gets set seven.

00:02:33.790 --> 00:02:36.390
Slice zero gets set eight.

00:02:36.390 --> 00:02:39.090
Set nine is in slice one again and
so on.

00:02:39.090 --> 00:02:43.020
So now we can relatively easily,
after we break down the address to

00:02:43.020 --> 00:02:47.490
access the cache,
we know the set number, and

00:02:47.490 --> 00:02:52.970
the least significant tree bits in
this case tell us which slice to ask.

00:02:52.970 --> 00:02:58.010
If we have even more cores, then the
number of sets in the cache will grow,

00:02:58.010 --> 00:03:02.300
and as a result the number of bits
that we need to tell us which slice we

00:03:02.300 --> 00:03:06.850
have is growing and that means that the
number of bits in the index are growing,

00:03:06.850 --> 00:03:10.690
and we will need additional bits
to tell us in which set to look.

00:03:10.690 --> 00:03:14.420
Know that if we sequentially access
data, then we would be really be looking

00:03:14.420 --> 00:03:18.290
at one set and then the next and
then the next set, and

00:03:18.290 --> 00:03:22.120
this nicely spreads the load among
the slices of the level three cache.

00:03:22.120 --> 00:03:25.110
Because sequential axises
are basically going to be spread

00:03:25.110 --> 00:03:26.520
across the entire shape.

00:03:26.520 --> 00:03:31.710
However, the spreading of blocks
round robin across the entire chip

00:03:31.710 --> 00:03:36.689
may not be good for locality,
because a core is just as likely to

00:03:36.689 --> 00:03:40.740
access something that is in the opposite
corner of the chip, as it is to access

00:03:40.740 --> 00:03:45.320
its own slice in which case there is
no on chip network traffic whatsoever.

00:03:45.320 --> 00:03:49.949
So another way that can help with that
is that we spread the data around, but

00:03:49.949 --> 00:03:52.313
this time we spread it by page number.

00:03:52.313 --> 00:03:57.605
So all of the blocks that belong to the
same page would end up in the same slice

00:03:57.605 --> 00:04:03.233
of the level three, that helps because
now the operating system can map pages,

00:04:03.233 --> 00:04:06.840
so that it makes the accesses
to L3 more local.

00:04:06.840 --> 00:04:11.290
For example here, this core is
running a thread that has a stack.

00:04:11.290 --> 00:04:15.100
The pages that belong to that stack
should be those that end up in

00:04:15.100 --> 00:04:15.610
this slice.

00:04:15.610 --> 00:04:17.390
So that if you have a level two miss,

00:04:17.390 --> 00:04:22.540
we are very likely to access our
own local slice of the L3 cache.

00:04:22.540 --> 00:04:26.260
Of course, some of the shared
data will be elsewhere, but

00:04:26.260 --> 00:04:31.440
this dramatically improves the locality
in terms of how far do we need to go

00:04:31.440 --> 00:04:33.210
to get to the correct slice of L3.

00:04:34.320 --> 00:04:37.360
Any data that tends to
be accessed by one core

00:04:37.360 --> 00:04:39.860
can be now mapped by
the operating system so

00:04:39.860 --> 00:04:44.670
that it's usually found in the local
slice of the last level cache.

