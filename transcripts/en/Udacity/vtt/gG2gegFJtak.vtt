WEBVTT
Kind: captions
Language: en

00:00:00.920 --> 00:00:04.360
We would like of course to do better in terms of prediction accuracy.

00:00:05.720 --> 00:00:08.330
Unfortunately there's quite a lot of variance in this data.

00:00:09.830 --> 00:00:15.590
So, we've already been looking at intertweet time.

00:00:15.590 --> 00:00:16.810
Meaning if there's a tweet,

00:00:16.810 --> 00:00:22.360
t1 and t2, intertweet time is the number of seconds between them.

00:00:22.360 --> 00:00:25.980
But what we're trying to predict is more granular than this right?

00:00:25.980 --> 00:00:31.090
Specifically, if it's been delta t seconds since the tweet occurred,

00:00:31.090 --> 00:00:33.800
we want to try to predict the amount of time,

00:00:33.800 --> 00:00:37.820
labeled p here, until the person tweets again.

00:00:37.820 --> 00:00:43.300
So in order to do this, we have collect a bunch of training examples.

00:00:43.300 --> 00:00:46.450
So our training examples, are going to look like this.

00:00:46.450 --> 00:00:48.340
We want lots of pairs.

00:00:48.340 --> 00:00:51.300
Of elapsed time since the last tweet.

00:00:51.300 --> 00:00:54.030
And how far we have to go, until the next tweet.

00:00:54.030 --> 00:00:57.230
To get an understanding of the variance in our data.

00:00:57.230 --> 00:01:01.270
Let's create this training data from the given raw data.

00:01:01.270 --> 00:01:03.090
And then make some graphs.

00:01:03.090 --> 00:01:07.040
You can see here, we're just producing these very same points.

00:01:07.040 --> 00:01:10.740
&gt;From the given raw data and timeUntilNext, which is

00:01:10.740 --> 00:01:16.800
just the time between tweets, we produce a series of more granular points.

00:01:16.800 --> 00:01:20.190
How much time has elapsed, and how much to go until the next tweet.

00:01:20.190 --> 00:01:25.390
And they collect them together in the list data_points.

00:01:25.390 --> 00:01:30.020
So, to get an idea of variance let's take a look.

00:01:30.020 --> 00:01:32.020
At a few fixed points.

00:01:32.020 --> 00:01:35.930
Specifically, let's look at, what the data looks like when ten

00:01:35.930 --> 00:01:42.020
seconds has elapsed, 150 seconds has elapsed, and 100 seconds has elapsed.

00:01:42.020 --> 00:01:46.970
To get an initial idea about what the variance looks like in our data,

00:01:46.970 --> 00:01:49.210
let's fix it at a couple of points.

00:01:49.210 --> 00:01:50.720
Specifically.

00:01:50.720 --> 00:01:54.420
Let's look at what the data looks like after 10 seconds has elapsed,

00:01:54.420 --> 00:01:57.070
and 150 seconds has elapsed.

00:01:57.070 --> 00:02:00.370
Let's zero in on the 10 second graph first.

00:02:00.370 --> 00:02:02.500
So let's look at what this graph is saying.

00:02:02.500 --> 00:02:07.830
This is just a histogram, saying that if 10 seconds has elapsed since

00:02:07.830 --> 00:02:12.190
my last tweet, how long did I have to wait until the next tweet occurred.

00:02:12.190 --> 00:02:16.080
So here, zero seconds had to elapse until the next tweet occurred.

00:02:16.080 --> 00:02:18.365
And at the other end of the graph,

00:02:18.365 --> 00:02:21.710
90,000 seconds have elapsed until the next tweet occurs.

00:02:21.710 --> 00:02:25.490
And as you can see, it's skewed strongly towards zero seconds.

00:02:25.490 --> 00:02:28.220
Again, it looks like an exponential distribution.

00:02:28.220 --> 00:02:31.100
Now, let's look at what this looks like for 150 seconds.

00:02:32.320 --> 00:02:34.080
Again, this is a graph of.

00:02:34.080 --> 00:02:38.460
After 150 seconds has elapsed since the last tweet,

00:02:38.460 --> 00:02:41.560
how long did we have to wait until the next tweet.

00:02:41.560 --> 00:02:44.070
At the left end, we had to wait zero seconds.

00:02:44.070 --> 00:02:46.500
At the right end, 90 thousand seconds.

00:02:46.500 --> 00:02:50.200
As you can see again, it looks just like an exponential distribution.

00:02:50.200 --> 00:02:53.830
In fact, let's look at a graph of the standard deviation.

00:02:53.830 --> 00:02:56.870
Again, the x axis here is the number of seconds that

00:02:56.870 --> 00:02:59.150
has elapsed since the last tweet.

00:02:59.150 --> 00:03:02.860
So zero seconds, 10 seconds, 20 and so on.

00:03:02.860 --> 00:03:08.540
The y axis describes the number of seconds until the next tweet occurs.

00:03:08.540 --> 00:03:12.340
And this graph describes the standard deviation in that quantity.

00:03:13.400 --> 00:03:15.200
So as you can see.

00:03:15.200 --> 00:03:18.640
The spread of the times that we have to wait until the next tweet

00:03:18.640 --> 00:03:23.570
occurs increases as the time elapses since the last tweet.

00:03:23.570 --> 00:03:24.903
Let's visualize this in another way.

00:03:24.903 --> 00:03:30.308
This graph shows a 95% confidence interval of the time we have to wait,

00:03:30.308 --> 00:03:33.980
until the next tweet as a function of lapsed time.

00:03:35.040 --> 00:03:37.270
So after 20 seconds has elapsed.

00:03:37.270 --> 00:03:39.360
Here's the spread of how long we have to wait.

00:03:39.360 --> 00:03:44.560
As you can see it spreads all the way from zero seconds to 30,000 seconds.

00:03:44.560 --> 00:03:47.750
After 60 seconds has elapsed the spread goes all the way

00:03:47.750 --> 00:03:50.540
from zero seconds to 35 thousand seconds.

00:03:50.540 --> 00:03:55.050
So as you can see, this is just another way

00:03:55.050 --> 00:03:59.970
to describe the graph we just saw of the increasing standard deviation.

00:03:59.970 --> 00:04:03.040
The confidence band just increases as time goes on.

00:04:03.040 --> 00:04:07.860
This is just another way of saying that as time goes on, we're more and

00:04:07.860 --> 00:04:12.690
more uncertain of how long it's going to be until they tweet next.

00:04:12.690 --> 00:04:16.589
Any estimator that we create, which is a function of delta t.

00:04:16.589 --> 00:04:20.050
Will do so by essentially averaging over this spread.

00:04:20.050 --> 00:04:22.140
At any given point along the graph.

00:04:22.140 --> 00:04:26.480
Again as you can see the spread gets wider as the elapsed time goes on.

00:04:26.480 --> 00:04:29.660
Reflecting greater and greater uncertainty in the data.

00:04:29.660 --> 00:04:32.240
As the time since the last tweet grows.

00:04:32.240 --> 00:04:35.120
Elapsed time becomes less and less good.

00:04:35.120 --> 00:04:37.880
At determining what the inter-tweet time is.

00:04:37.880 --> 00:04:42.370
And any predictor we make based on delta t will suffer from this.

00:04:42.370 --> 00:04:46.280
This problem is called inherent variance.

00:04:46.280 --> 00:04:49.790
Note, this is not the same thing as model variance,

00:04:49.790 --> 00:04:54.320
which refers to how the output of your model is sensitive to training data.

00:04:54.320 --> 00:04:57.870
Inherent variance refers to the following concepts.

00:04:57.870 --> 00:05:00.500
Let's suppose, that our real model,

00:05:00.500 --> 00:05:05.730
meaning the actual real life phenomenon was in fact a linear one,

00:05:05.730 --> 00:05:11.000
meaning you had some function f and it was a function of the single variable x.

00:05:11.000 --> 00:05:16.140
As data scientists we collect records of the form x, f.

00:05:16.140 --> 00:05:19.560
&gt;From these records, we will try to build an estimator and

00:05:19.560 --> 00:05:22.270
we'll probably have a pretty easy time of it, right?

00:05:22.270 --> 00:05:24.180
For a given fixed value, x1,

00:05:24.180 --> 00:05:28.217
we will always see the very same value of f beside it.

00:05:28.217 --> 00:05:31.050
A times x1.

00:05:31.050 --> 00:05:34.090
Over and over as we collect records.

00:05:34.090 --> 00:05:39.540
Every time we see x1, we will always see a times x1 beside it.

00:05:39.540 --> 00:05:44.620
Now, let's suppose we introduce some noise into the situation.

00:05:44.620 --> 00:05:50.910
The way to think about this is, x occurs but before we can measure it as f.

00:05:50.910 --> 00:05:56.400
Some noise process occurs to x, and the result is the thing that we measure.

00:05:56.400 --> 00:06:02.130
The consequence is, now we no longer see the same f for a given x.

00:06:02.130 --> 00:06:05.730
The first time we see x1, we might see value 1 beside it.

00:06:06.850 --> 00:06:10.890
The next time value 2 and so on.

00:06:10.890 --> 00:06:12.810
This spread of values, v1,

00:06:12.810 --> 00:06:18.660
v2 and so on, produces the confidence band effect that we saw earlier.

00:06:18.660 --> 00:06:22.150
That of course makes finding the right value to guess for

00:06:22.150 --> 00:06:24.900
a given x1 much more difficult.

00:06:24.900 --> 00:06:26.250
In the next video.

00:06:26.250 --> 00:06:29.350
We'll talk about sources of this variance, and what we can do about it.

