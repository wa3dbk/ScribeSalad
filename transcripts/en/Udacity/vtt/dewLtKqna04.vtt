WEBVTT
Kind: captions
Language: en

00:00:00.750 --> 00:00:02.910
And that's the correct answer. All right. So let's

00:00:02.910 --> 00:00:06.080
actually go through that exercise and derive how we

00:00:06.080 --> 00:00:07.530
do that. because it's not so bad in two

00:00:07.530 --> 00:00:09.900
dimensions and it generalizes to higher dimensions as well.

00:00:09.900 --> 00:00:10.730
&gt;&gt; Okay.

00:00:10.730 --> 00:00:13.000
&gt;&gt; So it turns out that we can use calculus to do this, I

00:00:13.000 --> 00:00:17.790
am not going to step through the two-variable example for reasons that I am

00:00:17.790 --> 00:00:22.290
embarrassed to say. But I am going to show you a different example. So

00:00:22.290 --> 00:00:26.420
imagine that what we're trying to do is that we've got a bunch of data

00:00:26.420 --> 00:00:30.770
points, and we're trying to find the best constant function, right? So the best

00:00:30.770 --> 00:00:36.420
function that has the form, the value of the function for

00:00:36.420 --> 00:00:41.550
any given X is always the same constant, C. So if our data looks like this, we

00:00:41.550 --> 00:00:43.960
got a bunch of X's and a bunch of Y's,

00:00:43.960 --> 00:00:45.530
then what we're going to do, we're going to say for

00:00:45.530 --> 00:00:49.530
any given value of C, any given constant, we can have

00:00:49.530 --> 00:00:51.480
an error. What's the error going to be? The error is

00:00:51.480 --> 00:00:54.410
going to be the sum over all of the data points. Speaker 1: The square

00:00:54.410 --> 00:00:58.460
difference between that constant we chose and what

00:00:58.460 --> 00:01:00.660
the actual yi value is. So that's why.

00:01:00.660 --> 00:01:01.290
&gt;&gt; Michael.

00:01:01.290 --> 00:01:03.300
&gt;&gt; These differences here. Yes, Charles.

00:01:04.379 --> 00:01:05.665
&gt;&gt; Can I ask you a question?

00:01:05.665 --> 00:01:07.200
&gt;&gt; Sure.

00:01:07.200 --> 00:01:08.739
&gt;&gt; Why are we doing sum of squares?

00:01:11.010 --> 00:01:15.130
&gt;&gt; There is many different error functions and sometimes called

00:01:15.130 --> 00:01:17.430
a, a relative concept called the loss function. There is

00:01:17.430 --> 00:01:19.250
lots of difference once that could work, you can do

00:01:19.250 --> 00:01:22.410
the absolute error, you can do the squared error, you can

00:01:22.410 --> 00:01:26.060
do various kinds of squashed errors where you know. The

00:01:26.060 --> 00:01:28.880
errors count different depending on how, how much deviation there is.

00:01:30.320 --> 00:01:32.790
It turns out that this one is particularly well behaved

00:01:32.790 --> 00:01:36.550
because of this reason that I'm explaining now that that because

00:01:36.550 --> 00:01:40.240
this error function is smooth as a function of the constant

00:01:40.240 --> 00:01:44.450
C, we can use calculus to actually find the minimum error

00:01:44.450 --> 00:01:47.360
value. But there's lots of other things that could work and

00:01:47.360 --> 00:01:51.540
they actually do find utility in various different machine learning settings.

00:01:51.540 --> 00:01:52.400
&gt;&gt; Okay.

00:01:52.400 --> 00:01:53.970
&gt;&gt; So just now using the chain rule, if

00:01:53.970 --> 00:01:56.560
you want to find how do this error function

00:01:56.560 --> 00:01:59.320
output change as a function of input c. We

00:01:59.320 --> 00:02:02.300
can take the derivative of this sum you know,

00:02:02.300 --> 00:02:06.950
bring the two over. Times this, times the derivative of the inside, which

00:02:06.950 --> 00:02:09.460
is negative one in this case. And now this gives us a nice, smooth

00:02:09.460 --> 00:02:13.480
function saying what the error is as a function of c. And if we

00:02:13.480 --> 00:02:15.880
want to find the minimum, what do we want to do to this quantity?

00:02:15.880 --> 00:02:18.859
&gt;&gt; Set it equal to zero, because that's what I remember from Calculus.

00:02:20.510 --> 00:02:23.220
&gt;&gt; That's right. So in particular if the error you know, the error

00:02:23.220 --> 00:02:27.420
function is a nice smooth thing the derivative is negative and then zero and

00:02:27.420 --> 00:02:29.450
then positive. When it hits zero that's when the thing

00:02:29.450 --> 00:02:32.170
has bottomed-out. Alright. So now we just need to solve

00:02:32.170 --> 00:02:35.690
this, this equation for c. So we have one equation

00:02:35.690 --> 00:02:40.000
and one unknown. Alright, so that gets us this. But,

00:02:40.000 --> 00:02:43.150
this quantity, it's just the constant added to itself n

00:02:43.150 --> 00:02:46.590
times. So it's n times c. We move that to

00:02:46.590 --> 00:02:49.700
the other side. We get n times c. N is

00:02:49.700 --> 00:02:52.460
the number of data points as you recall. Is the

00:02:52.460 --> 00:02:57.660
sum of the yi's. We divide

00:02:57.660 --> 00:03:02.580
two by n and what do we see? So what is it Charles?

00:03:02.580 --> 00:03:05.080
&gt;&gt; The best constant is the average of all your y's.

00:03:05.080 --> 00:03:09.750
&gt;&gt; Great, it's the mean. The mean comes back. Right, so

00:03:09.750 --> 00:03:12.170
in the case of finding the best constant here, we just have

00:03:12.170 --> 00:03:15.070
to average the y, the y's together and that catches thing that

00:03:15.070 --> 00:03:17.560
minimizes the squared air. So squared air is this really nice thing

00:03:17.560 --> 00:03:20.820
because it tends to bring things like mean back into the picture. It's really

00:03:20.820 --> 00:03:23.310
very convenient. And, it generalizes to higher,

00:03:23.310 --> 00:03:26.950
higher order of function tier, not higher

00:03:26.950 --> 00:03:29.790
functions, but more variables like, like lines.

00:03:29.790 --> 00:03:31.740
Sorry. Lines that have some, some non

00:03:31.740 --> 00:03:35.770
constant slope. By doing the same kind

00:03:35.770 --> 00:03:37.840
of process and things actually work really nicely.

