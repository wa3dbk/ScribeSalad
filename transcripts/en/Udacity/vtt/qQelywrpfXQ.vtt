WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.939
Today we're going to talk about optimizing GPU programs.

00:00:02.939 --> 00:00:08.645
Now the whole reason we want to use a parallel platform like the GPU is to solve problems faster,

00:00:08.645 --> 00:00:13.483
and in turn the reason we might want to solve problems faster could be simply because

00:00:13.483 --> 00:00:17.821
we want to a solve problems faster. Or more often it's because we want to solve bigger problems,

00:00:17.821 --> 00:00:24.092
or solve more problems. So the good news is that it's often the case that the first initial port

00:00:24.092 --> 00:00:28.832
of a problem gets a speed up, assuming that you got a parallel problem to begin with,

00:00:28.832 --> 00:00:33.303
and in my experience, this is actually when a lot of GPU programmers get hooked.

00:00:33.303 --> 00:00:35.508
You know, over the weekend they go home and out of curiosity

00:00:35.508 --> 00:00:39.129
they try porting some piece of their existing CPU code to the GPU,

00:00:39.129 --> 00:00:41.810
and they get a nice speed up, a 5x or 8x speed up.

00:00:41.810 --> 00:00:44.347
And that's, that's what sort of gets them hooked and makes them realize they

00:00:44.347 --> 00:00:46.916
could put some more effort into this and get a bigger speed up.

00:00:46.916 --> 00:00:51.628
So if you have a naturally parallel problem, it's often the case that that first initial

00:00:51.628 --> 00:00:55.825
Cuda port will get you good speed up, and that's cool.

00:00:55.825 --> 00:01:01.560
But, you know, by definition GPU programmers care about performance, that's why they're using the GPU.

00:01:01.560 --> 00:01:06.268
That means they often want to spend additional effort to maximize the speed up beyond that first initial try.

00:01:06.268 --> 00:01:09.238
So in this unit we're going to talk about how to optimize GPU programs.

00:01:09.238 --> 00:01:12.407
So cast your mind back to unit 2,

00:01:12.407 --> 00:01:17.122
when we talked for a little bit about some basic principles of efficient GPU programming.

00:01:17.122 --> 00:01:19.844
So check which of these principles accurately correspond to things

00:01:19.844 --> 00:01:22.511
we talked about in unit 2 about efficient GPU programming.

00:01:22.511 --> 00:01:25.817
Do we want to decrease arithmetic intensity?

00:01:25.817 --> 00:01:30.221
Do we want to decrease the time spent on memory operations per thread?

00:01:30.221 --> 00:01:33.198
Do we want to coalesce global memory accesses?

00:01:33.198 --> 00:01:39.672
Do we want to do fewer memory operations per thread? Do we want to avoid thread divergence?

00:01:39.672 --> 00:01:42.467
Do we want to move all data to shared memory?

