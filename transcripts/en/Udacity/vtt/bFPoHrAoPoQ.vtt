WEBVTT
Kind: captions
Language: en

00:00:00.230 --> 00:00:02.220
Alright, we're going to drill down and talk about a

00:00:02.220 --> 00:00:04.510
specific reinforcement learning algorithm in just a moment. But

00:00:04.510 --> 00:00:07.200
I wanted to remind everybody about, some of the

00:00:07.200 --> 00:00:10.090
quantities that Charles introduced in the lesson last time

00:00:10.090 --> 00:00:12.890
when he was talking about MVP's, and use them

00:00:12.890 --> 00:00:15.750
to describe three different approaches for solving reinforcement learning

00:00:15.750 --> 00:00:19.430
problems. So this first box here pi, maps states

00:00:19.430 --> 00:00:22.720
to actions, and what did you call this Charles?

00:00:22.720 --> 00:00:23.650
&gt;&gt; A policy.

00:00:23.650 --> 00:00:25.295
&gt;&gt; That's right. And reinforcement learning

00:00:25.295 --> 00:00:28.750
algorithms that work directly on trying to find the policy,

00:00:28.750 --> 00:00:32.119
are called policy search algorithms. So the good thing about policy

00:00:32.119 --> 00:00:36.420
search is, you're learning the quantity that you directly need

00:00:36.420 --> 00:00:38.530
to use. Right? You're learning the policy. That's supposed to be

00:00:38.530 --> 00:00:40.980
the output. So that seems like a really good thing.

00:00:40.980 --> 00:00:45.570
Unfortunately, learning this, this function is very indirect. We don't get

00:00:45.570 --> 00:00:48.570
direct access to, I was in this state, what actions should

00:00:48.570 --> 00:00:50.330
I have chosen? Right? So this is, what did you call

00:00:50.330 --> 00:00:54.140
this, Charles, the the temporal credit assignment problem, right?

00:00:54.140 --> 00:00:54.230
&gt;&gt; Right.

00:00:54.230 --> 00:00:57.340
&gt;&gt; So, the data doesn't tell you what action to actually choose.

00:00:57.340 --> 00:00:59.840
&gt;&gt; Right. And this is why it's not exactly

00:00:59.840 --> 00:01:02.560
like the way we've formulated supervised learning in the past.

00:01:02.560 --> 00:01:05.080
&gt;&gt; Well, at least if we're trying to do policy certs,

00:01:05.080 --> 00:01:08.450
that's right. But what we're going to do is now consider, well, maybe

00:01:08.450 --> 00:01:11.862
that's not the quantity that we want to learn. Let's, let's think about

00:01:11.862 --> 00:01:15.720
learning this function U, which you had said maps states to values.

00:01:15.720 --> 00:01:18.160
So, what was this guy?

00:01:18.160 --> 00:01:19.150
&gt;&gt; U is a utility.

00:01:19.150 --> 00:01:20.510
&gt;&gt; Yeah.

00:01:20.510 --> 00:01:23.080
&gt;&gt; Yeah the true utility of the state, sometimes I,

00:01:23.080 --> 00:01:25.010
I think I called it the, the value of the state.

00:01:25.010 --> 00:01:28.290
&gt;&gt; Yeah so, so, so sometimes it's referred to

00:01:28.290 --> 00:01:31.630
as a value function, and learning methods, reinforcement learning

00:01:31.630 --> 00:01:33.970
methods that target that as, as what they're trying

00:01:33.970 --> 00:01:36.630
to learn directly, are called value function based approaches.

00:01:36.630 --> 00:01:37.390
&gt;&gt; Mm-hm.

00:01:37.390 --> 00:01:40.860
&gt;&gt; And, let's say that we, that we try to learn this, we're trying to learn

00:01:40.860 --> 00:01:43.400
to map states to values, well the good news is, at

00:01:43.400 --> 00:01:47.260
least if we're acting in the world, we're getting to see, okay

00:01:47.260 --> 00:01:49.190
I was in some state, I took some action, duh, duh,

00:01:49.190 --> 00:01:51.890
duh, and I can, I can observe the values that actually result

00:01:51.890 --> 00:01:53.900
from that, and maybe use that to, to make updates. So

00:01:53.900 --> 00:01:57.680
you can kind of imagine learning this, so that the, the learning's

00:01:57.680 --> 00:02:00.450
not quite as indirect. How do we use this to decide how

00:02:00.450 --> 00:02:03.500
to behave? So we, we need to turn it into a policy.

00:02:03.500 --> 00:02:04.050
&gt;&gt; Mm hm.

00:02:04.050 --> 00:02:05.900
&gt;&gt; And we didn't quite, we sort of talked

00:02:05.900 --> 00:02:08.570
about how to do that, in terms of, of

00:02:08.570 --> 00:02:10.979
looking at the Bellman Equations. We're going to, we're going to, it

00:02:10.979 --> 00:02:12.600
turns out it's actually kind of hard to use

00:02:12.600 --> 00:02:16.130
U directly to do that. But we're going to talk about

00:02:16.130 --> 00:02:17.890
a different form of the value function that's going to

00:02:17.890 --> 00:02:20.000
make that easy. So this is actually, it turns out

00:02:20.000 --> 00:02:22.080
it'll be a relatively easy kind of argmax

00:02:22.080 --> 00:02:25.020
operation, once we have the right kind of value function.

00:02:25.020 --> 00:02:25.160
&gt;&gt; Okay.

00:02:25.160 --> 00:02:28.120
&gt;&gt; So, there's some computation we have to do, and

00:02:28.120 --> 00:02:31.240
there's some indirectness to the learning, but, you know, it's

00:02:31.240 --> 00:02:34.410
okay. Alright, so the other quantities that you

00:02:34.410 --> 00:02:36.880
told us about were T, which is, I

00:02:36.880 --> 00:02:39.920
think of it as the transition function, I think you had a different name for it.

00:02:39.920 --> 00:02:41.700
&gt;&gt; I just call it the transition model.

00:02:41.700 --> 00:02:44.210
&gt;&gt; The, the model, right, yeah. So this is the model,

00:02:44.210 --> 00:02:47.710
and and the, the R is the reinforcement function, the reward function.

00:02:47.710 --> 00:02:48.365
&gt;&gt; Mm-hm.

00:02:48.365 --> 00:02:50.730
&gt;&gt; And what do these things do, they take states and actions

00:02:50.730 --> 00:02:52.180
and the transition function, or the

00:02:52.180 --> 00:02:56.040
transition model, predicts next states, maybe probabilistically,

00:02:56.040 --> 00:03:01.370
and the reward function tells you, next rewards, but just rewards. And

00:03:01.370 --> 00:03:06.470
so this is a model, jointly, and I already mentioned this, but

00:03:06.470 --> 00:03:11.440
we call methods that learn

00:03:11.440 --> 00:03:13.880
this quantity, learn these, these functions and

00:03:13.880 --> 00:03:15.960
then use them to do decisions, model based

00:03:15.960 --> 00:03:20.260
reinforcement learners, so how do we go from T and R to something like U?

00:03:20.260 --> 00:03:21.070
&gt;&gt; Well

00:03:21.070 --> 00:03:25.640
if we had those, if we had T and we had R, and we could see the S's, and we,

00:03:25.640 --> 00:03:29.500
and the actions we took, then we could do you

00:03:29.500 --> 00:03:31.930
know, something like value iteration we did before the learned values.

00:03:31.930 --> 00:03:36.100
&gt;&gt; Right, which value iteration was used to solve the Bellman Equations.

00:03:36.100 --> 00:03:36.770
&gt;&gt; Right.

00:03:36.770 --> 00:03:42.460
&gt;&gt; So that is somewhat heavy weight computation to do, but it's doable.

00:03:42.460 --> 00:03:46.700
So, what would happens over here, is we actually have fairly direct learning.

00:03:46.700 --> 00:03:48.320
Why is that? Because when we're trying to learn the

00:03:48.320 --> 00:03:53.240
transition and reward function, we get state action pairs as input.

00:03:53.240 --> 00:03:56.060
And then when we receive next state reward pairs as

00:03:56.060 --> 00:03:58.955
output, so we can solve this as a supervised learning problem.

00:03:58.955 --> 00:03:59.955
&gt;&gt; Hm.

00:03:59.955 --> 00:04:03.350
&gt;&gt; So, so learning is rather direct, the usage of this

00:04:03.350 --> 00:04:06.080
is a little bit computationally complex because you actually have to do

00:04:06.080 --> 00:04:09.630
the planning and then the optimizing to actually develop what you're,

00:04:09.630 --> 00:04:12.150
you know the policy doesn't come directly out of that. But but

00:04:12.150 --> 00:04:13.590
this kind of gives you a sense of

00:04:13.590 --> 00:04:15.880
three different ways, three different ways, places that

00:04:15.880 --> 00:04:20.339
you can target the pieces of the MVP so that you can do reinforcement learning.

00:04:20.339 --> 00:04:21.519
&gt;&gt; I like that.

00:04:21.519 --> 00:04:25.320
&gt;&gt; Now, we're going to focus on this, this middle piece. Partly

00:04:25.320 --> 00:04:29.670
because, you know, it's kind of the Goldilocks situation. It's you know, the

00:04:29.670 --> 00:04:33.840
learning's not so indirect and the usage is not so indirect. There's

00:04:33.840 --> 00:04:37.220
just been a tremendous amount of work focused on, on value function

00:04:37.220 --> 00:04:41.700
based approaches. And it's, you know, remarkably simple it turns out,

00:04:41.700 --> 00:04:44.760
if you do it right. And also very powerful. That there's

00:04:44.760 --> 00:04:46.990
lots of ways to use these simple ideas to, to learn

00:04:46.990 --> 00:04:49.230
hard problems. So, I think this is a good place to focus.

00:04:49.230 --> 00:04:50.370
&gt;&gt; Okay. I buy that.

