WEBVTT
Kind: captions
Language: en

00:00:00.260 --> 00:00:03.320
So we've determined that we want to
figure out which set of parameters

00:00:03.320 --> 00:00:06.700
provide the best predictions for our
output variable, but how can we do that?

00:00:07.900 --> 00:00:10.590
We'll use an algorithm
called gradient descent.

00:00:10.590 --> 00:00:13.390
First, we need to define
some cost function

00:00:13.390 --> 00:00:14.970
which we'll call J of big theta.

00:00:16.290 --> 00:00:20.340
I'm going to use big theta here to
represent our entire set of theta's and

00:00:20.340 --> 00:00:22.860
I'll use this notation throughout
the rest of this lesson.

00:00:22.860 --> 00:00:25.530
The cost function is meant to
provide a measure of how well our

00:00:25.530 --> 00:00:28.880
current set of thetas does at
modeling the observed data.

00:00:28.880 --> 00:00:31.060
So we want to minimize
the cost functions value.

00:00:32.340 --> 00:00:35.500
As we discussed just a moment ago,
when we're doing linear regression,

00:00:35.500 --> 00:00:39.390
our cost function J of theta can simply
measure the sum of the squares of

00:00:39.390 --> 00:00:42.050
the differences between our
predicted and observed values.

