WEBVTT
Kind: captions
Language: en

00:00:00.350 --> 00:00:02.600
To conclude our discussion of information theory, we will

00:00:02.600 --> 00:00:07.330
also discuss something called Kullback-Leibler divergence. It is also famously

00:00:07.330 --> 00:00:10.270
called the KL divergence. And you must have heard this

00:00:10.270 --> 00:00:12.860
term in our previous lectures. So it is useful to

00:00:12.860 --> 00:00:16.070
realize that mutual information is also a particular case

00:00:16.070 --> 00:00:19.300
of KL divergence. So KL divergence actually measures the difference

00:00:19.300 --> 00:00:22.560
between any two distributions. It is used as a distance

00:00:22.560 --> 00:00:25.980
measure. For this particular lesson, it is sufficient to understand

00:00:25.980 --> 00:00:29.810
how KL divergence is used to measure the distance between

00:00:29.810 --> 00:00:33.270
two distributions. The KL divergence is given by this particular

00:00:33.270 --> 00:00:37.070
formula. And it is always non-negative and zero only when

00:00:37.070 --> 00:00:39.790
P is equal to Q. When P is equal to Q,

00:00:39.790 --> 00:00:41.940
the log of 1 is zero, and that's why the

00:00:41.940 --> 00:00:45.850
distance is zero. Otherwise, it is always some non-negative quantity. So

00:00:45.850 --> 00:00:48.650
it serves as a distance measure. But it is not

00:00:48.650 --> 00:00:51.890
completely a distance measure because it doesn't follow the triangle law.

00:00:51.890 --> 00:00:53.650
But then you should ask yourself why you need to

00:00:53.650 --> 00:00:58.100
know KL divergence, or where it is used. Usually, and usually

00:00:58.100 --> 00:01:01.060
in supervised learning you are always trying to model our

00:01:01.060 --> 00:01:05.489
data to a particular distribution. So in that case our distrib,

00:01:05.489 --> 00:01:09.010
one of our distributions can be of unknown distribution. And

00:01:09.010 --> 00:01:11.460
we can denote that as P of X. And then can

00:01:11.460 --> 00:01:14.310
sample our data set to find out Q of X.

00:01:14.310 --> 00:01:17.510
While doing that, we can use KL divergence as a substitute

00:01:17.510 --> 00:01:20.498
to the least square formula that we used for

00:01:20.498 --> 00:01:23.820
fitting. So it's just a different way of trying to

00:01:23.820 --> 00:01:26.810
fit your data to your existing model. And we'll come

00:01:26.810 --> 00:01:29.290
back to KL divergence in some of our problem sets.

