WEBVTT
Kind: captions
Language: en

00:00:00.190 --> 00:00:02.800
Now suppose the starting point is a sequential

00:00:02.800 --> 00:00:05.920
program. How can we exploit the cluster? We

00:00:05.920 --> 00:00:08.790
have multiple processors, how do we exploit the

00:00:08.790 --> 00:00:11.450
cluster if the starting point is a sequential

00:00:11.450 --> 00:00:13.882
program? One possibility is to do what is

00:00:13.882 --> 00:00:18.130
called automatic parallelization. That is, instead of writing

00:00:18.130 --> 00:00:21.480
an explicitly parallel program, we write a sequential

00:00:21.480 --> 00:00:25.380
program. And let somebody else do the heavy lifting

00:00:25.380 --> 00:00:29.270
in terms of identifying opportunities for parallelism that

00:00:29.270 --> 00:00:31.730
exist in the program and map it to

00:00:31.730 --> 00:00:34.360
the underlying cluster. And this is what is

00:00:34.360 --> 00:00:38.230
called an implicitly parallel program. There are opportunities for

00:00:38.230 --> 00:00:40.970
parallelism, but the program itself is not written

00:00:40.970 --> 00:00:43.250
as a parallel program. And, now it is

00:00:43.250 --> 00:00:46.540
the onus of the tool, in this case

00:00:46.540 --> 00:00:50.510
an automatic parallelizing compiler, to look at the sequential

00:00:50.510 --> 00:00:54.670
program and identify opportunities for parallelism

00:00:54.670 --> 00:00:58.210
and exploit that by using the resources

00:00:58.210 --> 00:01:00.295
that are available in the cluster. So

00:01:00.295 --> 00:01:03.070
high-performance FORTRAN is an example of a

00:01:03.070 --> 00:01:07.320
programming language that does automatic parallelization,

00:01:07.320 --> 00:01:09.760
but it is user-assisted parallelization in the

00:01:09.760 --> 00:01:12.370
sense that the user who is writing

00:01:12.370 --> 00:01:15.880
the sequential program is using directives for

00:01:15.880 --> 00:01:18.830
distribution of data and computation. And those

00:01:18.830 --> 00:01:23.020
directives are then used by this parallelizing

00:01:23.020 --> 00:01:26.560
compiler to say, oh, these are opportunities

00:01:26.560 --> 00:01:30.280
for mapping these computations onto the resources of

00:01:30.280 --> 00:01:35.320
a cluster. So it puts it on different nodes on the cluster and that

00:01:35.320 --> 00:01:38.150
way, it exploits the parallelism that is

00:01:38.150 --> 00:01:41.175
there in the hardware, starting from the sequential

00:01:41.175 --> 00:01:43.290
program and doing the heavy lifting in

00:01:43.290 --> 00:01:45.740
terms of converting the sequential program to a

00:01:45.740 --> 00:01:49.420
parallel program to extract performance for this

00:01:49.420 --> 00:01:54.310
application. This kind of automatic parallelization, or implicitly

00:01:54.310 --> 00:01:57.690
parallel programming, works really well for certain

00:01:57.690 --> 00:02:00.840
classes of program called data parallel programs. In

00:02:00.840 --> 00:02:06.400
such programs, for the most part, the data accesses are fairly static, and it is

00:02:06.400 --> 00:02:11.710
determinable at compile time. So in other words, there is limited potential for

00:02:11.710 --> 00:02:16.050
exploiting the available parallelism in the cluster

00:02:16.050 --> 00:02:18.885
if we resort to implicitly parallel programming.

