WEBVTT
Kind: captions
Language: en

00:00:01.347 --> 00:00:02.716
So, if we set things up this way, we actually

00:00:02.716 --> 00:00:04.969
get some wonderful properties coming out. So here are some

00:00:04.969 --> 00:00:08.185
things we know about this set up for zero-sum stochastic

00:00:08.185 --> 00:00:12.037
games. Value iteration works, so we can actually solve this

00:00:12.037 --> 00:00:16.012
system of equations by using the value iteration trick, which

00:00:16.012 --> 00:00:19.237
is to say, we initialize these Q values to whatever

00:00:19.237 --> 00:00:22.762
and then we just iterate this as an assignment, right,

00:00:22.762 --> 00:00:26.910
we just say, you know, equals. So value iteration works.

00:00:26.910 --> 00:00:31.130
This minimax Q algorithm converges under the same kinds

00:00:31.130 --> 00:00:33.520
of conditions that Q learning converges, so we get this

00:00:33.520 --> 00:00:36.590
nice, you know, Q learning analogue in this multi-agent

00:00:36.590 --> 00:00:40.340
setting. The Q star that's defined by these equations is

00:00:40.340 --> 00:00:43.490
unique, so we iterate it and we find it

00:00:43.490 --> 00:00:46.400
and it's just, there's just that one answer. The policies

00:00:46.400 --> 00:00:49.310
for the two players can be computed independently, that

00:00:49.310 --> 00:00:52.050
is to say, if two different players are running minimax

00:00:52.050 --> 00:00:55.760
Q on their own and not really coordinating with each other except for by

00:00:55.760 --> 00:00:57.790
playing the game, that the policies that

00:00:57.790 --> 00:01:00.400
they get out will actually converge to minimax

00:01:00.400 --> 00:01:05.040
optimal policies. So it really does solve the zero sum game, which is maybe not

00:01:05.040 --> 00:01:06.870
so surprising because, you know, they are

00:01:06.870 --> 00:01:08.533
trying to kill each other after all. [LAUGH]

00:01:08.533 --> 00:01:08.690
&gt;&gt; Yeah.

00:01:08.690 --> 00:01:10.180
&gt;&gt; So, the idea that they'd have to

00:01:10.180 --> 00:01:12.415
collaborate to do that efficiently would be weird.

00:01:12.415 --> 00:01:14.200
&gt;&gt; [LAUGH] I never thought about it

00:01:14.200 --> 00:01:15.480
like that, but, yeah, that would be weird.

00:01:15.480 --> 00:01:17.090
&gt;&gt; The, this update that

00:01:17.090 --> 00:01:20.430
I've written here can be computed efficiently, which is to say

00:01:20.430 --> 00:01:22.710
in polynomial time. Because this minimax

00:01:22.710 --> 00:01:26.170
can be computed using linear programming.

00:01:26.170 --> 00:01:26.810
&gt;&gt; Yes of course.

00:01:28.140 --> 00:01:31.720
&gt;&gt; And, finally, if we actually iterate this Q

00:01:31.720 --> 00:01:35.730
equation and, and it's converging to Q star, knowing Q

00:01:35.730 --> 00:01:39.590
star is enough to figure out how to behave optimally.

00:01:39.590 --> 00:01:42.130
So we can convert these Q values into an actual

00:01:42.130 --> 00:01:46.170
behavior, again, by using the, the solution in the linear program.

00:01:46.170 --> 00:01:48.994
&gt;&gt; So it's just like MDPs of value iteration with Q-learning?

00:01:48.994 --> 00:01:50.630
&gt;&gt; Exactly. It's like we've gone to, to

00:01:50.630 --> 00:01:52.840
a second agent and it really hasn't impacted things

00:01:52.840 --> 00:01:54.980
negatively at all. This is, this is all

00:01:54.980 --> 00:01:57.810
the, pretty much all the things that we want,

00:01:57.810 --> 00:01:59.300
come out. There, there are some things that

00:01:59.300 --> 00:02:01.466
don't come out. For example, in the case of

00:02:01.466 --> 00:02:03.914
an MDP, we can solve these, this system

00:02:03.914 --> 00:02:07.320
of linear equations in polynomial time. Not just by

00:02:07.320 --> 00:02:10.150
value iteration, but we can actually set up it as a single linear

00:02:10.150 --> 00:02:13.250
program and solve it and be done in linear time or, sorry, not linear

00:02:13.250 --> 00:02:16.280
time, polynomial time. This is not known to be true in the zero-sum

00:02:16.280 --> 00:02:18.440
stochastic game case, it's not known whether

00:02:18.440 --> 00:02:19.780
it can be solved in polynomial time.

00:02:19.780 --> 00:02:20.500
&gt;&gt; Hm.

00:02:20.500 --> 00:02:23.630
&gt;&gt; So there, it is a little harder as a problem, but it's, you know, not

00:02:23.630 --> 00:02:25.340
harder, not deeply harder and not harder in

00:02:25.340 --> 00:02:27.010
a way that matters in a machine learning setting.

00:02:27.010 --> 00:02:27.720
&gt;&gt; Cool.

00:02:27.720 --> 00:02:30.170
&gt;&gt; So this is really great. So let's,

00:02:30.170 --> 00:02:32.400
let's try to take this same approach and

00:02:32.400 --> 00:02:34.060
see if we can deal with general sum games.

00:02:34.060 --> 00:02:34.690
&gt;&gt; Okay.

