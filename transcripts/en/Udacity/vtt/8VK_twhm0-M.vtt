WEBVTT
Kind: captions
Language: en

00:00:00.110 --> 00:00:02.344
So let's keep track of what we're doing.

00:00:02.344 --> 00:00:05.173
Version 1 is serial in a single thread,

00:00:05.173 --> 00:00:08.280
and it took about 466 milliseconds.

00:00:08.280 --> 00:00:10.811
So 466 milliseconds is pretty slow,

00:00:10.811 --> 00:00:13.106
but sometimes that's okay.

00:00:13.106 --> 00:00:15.034
For code that's only going to be executed once,

00:00:15.034 --> 00:00:18.534
code that's not performance critical at all, or code that's going to run on a really small data set,

00:00:18.534 --> 00:00:23.793
like that 8 by 8 matrix that we started with, it's just not worthwhile to optimize the heck out of this.

00:00:23.793 --> 00:00:27.230
So even though this simple serial kernel may seem very naive,

00:00:27.230 --> 00:00:30.166
that's really sometimes the right thing to do,

00:00:30.166 --> 00:00:32.269
so keep this in mind when you're optimizing.

00:00:32.269 --> 00:00:35.238
Think about what you need to optimize, whether it's important.

00:00:35.238 --> 00:00:41.574
Now let's assume that in fact, performance is critical on this section, and that's why we're optimizing it,

00:00:41.574 --> 00:00:44.196
and let's go back to the code and see what we can do.

00:00:44.196 --> 00:00:48.718
Now one easy step would be to launch 1 thread for each row of the input, okay?

00:00:48.718 --> 00:00:51.388
So now here's the code that does that.

00:00:51.388 --> 00:00:54.958
In this code, we're going to launch 1 thread per row of the output matrix.

00:00:54.958 --> 00:00:59.603
So the value of i is going to be fixed by the thread ID,

00:00:59.603 --> 00:01:01.973
and every thread is going to execute

00:01:01.973 --> 00:01:06.009
just the outer loop of this code we saw before,

00:01:06.009 --> 00:01:11.182
and the inner loop we're essentially handing off to be run across many different threads.

00:01:11.182 --> 00:01:13.674
So these 2 codes are almost identical,

00:01:13.674 --> 00:01:18.747
and the only difference is that we're launching threads instead of looping over values of i.

00:01:18.747 --> 00:01:21.325
Let's time this.

00:01:21.325 --> 00:01:23.687
Okay so here's the code for calling our new function.

00:01:23.687 --> 00:01:27.086
We're going to launch the function transpose parallel per row as a kernel.

00:01:27.086 --> 00:01:30.094
We're going to launch a single thread block consisting of n threads.

00:01:30.094 --> 00:01:33.563
Remember, n is the size of our matrix, 1,024, currently.

00:01:33.563 --> 00:01:37.831
We're going to pass in the input matrix, pull out the output matrix, copy it,

00:01:37.831 --> 00:01:40.702
and then we're going to print out the timing and verify it.

00:01:40.702 --> 00:01:42.772
Let's compile and run this code.

00:01:42.772 --> 00:01:44.841
Okay.

00:01:44.841 --> 00:01:48.478
Transpose serial ran 484 milliseconds again, roughly what we saw before.

00:01:48.478 --> 00:01:52.148
Transpose parallel per row is running in 4.7 milliseconds.

00:01:52.148 --> 00:01:54.919
So obviously we're making a huge improvement

00:01:54.919 --> 00:01:58.869
by parallelizing this just across the threads of a single thread block.

00:01:58.869 --> 00:02:00.955
So let's note that down:

00:02:00.955 --> 00:02:04.955
4.7 milliseconds, roughly a 100x improvement.

