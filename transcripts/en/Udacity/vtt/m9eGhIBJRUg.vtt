WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.214
Finally, let's talk about optimizations at the whole system level,

00:00:03.214 --> 00:00:06.986
including the interactions that happen between the host and the GPU.

00:00:06.986 --> 00:00:11.357
So on most systems the CPU and the GPU are on different chips,

00:00:11.357 --> 00:00:17.127
and they communicate through something called a PCI Express bus, or PCIe for short.

00:00:17.127 --> 00:00:20.867
Today, for example, most systems are PCI Express Gen2,

00:00:20.867 --> 00:00:26.263
which can, in practice, get about 6 gigabytes per second maximum in either direction.

00:00:26.263 --> 00:00:30.946
PCI can transfer memory that has been page locked, or pinned,

00:00:30.946 --> 00:00:35.547
and it keeps a special chunk of pinned host memory set aside for this purpose.

00:00:35.547 --> 00:00:40.183
So when you have a chunk of memory that you want to copy over to the device memory on the GPU,

00:00:40.183 --> 00:00:42.687
CUDA first has to copy it into the staging area

00:00:42.687 --> 00:00:46.597
before it can then transfer it over the PCI Express bus onto the GPU.

00:00:46.597 --> 00:00:50.329
This extra copy here increases the total time taken by the memory transfer.

00:00:50.329 --> 00:00:53.703
Instead, you can use the cudaHostMalloc function

00:00:53.703 --> 00:00:56.462
to allocate pinned host memory in the first place,

00:00:56.462 --> 00:01:00.505
such that the memory you've got sitting on the host is already ready to copy directly over

00:01:00.505 --> 00:01:03.006
without this additional staging step.

00:01:03.006 --> 00:01:05.809
Or you can use the cudaHostRegister command

00:01:05.809 --> 00:01:09.115
to take some data you've already allocated and pin it,

00:01:09.115 --> 00:01:12.606
again avoiding the extra copy to take it into GPU memory.

00:01:12.606 --> 00:01:14.698
So that's why you care.

00:01:14.698 --> 00:01:18.946
Memory transfers from the host to device can go faster if you pin the memory first.

00:01:18.946 --> 00:01:23.195
And if you're transferring pinned memory, you can use the asynchronous memory transfer,

00:01:23.195 --> 00:01:29.096
cudaMemcpyAsync, which let's the CPU keep working while the memory transfer completes.

00:01:29.096 --> 00:01:31.072
How does that work?

00:01:31.072 --> 00:01:33.572
Well with cudaMemcpy, which you have been using until now,

00:01:33.572 --> 00:01:37.027
you make a call to cudaMemcpy, you pass it some information,

00:01:37.027 --> 00:01:39.138
like the pointers and the size and bytes

00:01:39.138 --> 00:01:42.732
and whether this is a transfer from device to host or host to device.

00:01:42.732 --> 00:01:45.122
And then there's a semicolon at the end of that statement.

00:01:45.122 --> 00:01:49.122
And with cudaMemcpy the CPU blocks until the transfer completes.

00:01:49.122 --> 00:01:53.559
With cudaMemcpyAsync, when it hits the semicolon it just keeps going.

00:01:53.559 --> 00:01:58.305
This kicks off an asynchronous memory transfer that continues to go on

00:01:58.305 --> 00:02:03.375
while control returns to the CPU and the CUDA program continues executing.

00:02:03.375 --> 00:02:07.643
In other words, the CPU can keep getting worked on while this memory transfers.

00:02:07.643 --> 00:02:10.074
So to control that asynchronous interaction,

00:02:10.074 --> 00:02:14.849
we have to introduce the next topic in host GPU interaction, and that's streams.

