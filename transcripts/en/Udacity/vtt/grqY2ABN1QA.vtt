WEBVTT
Kind: captions
Language: en

00:00:00.411 --> 00:00:03.873
So one of the really exciting new things that you guys have recently put into

00:00:03.873 --> 00:00:09.479
CUDA is this idea of dynamic parallelism, and so we've had some material on that in the course.

00:00:09.479 --> 00:00:14.089
So before we talk about what it actually is, can you talk about what problem you're trying to solve?

00:00:14.089 --> 00:00:18.396
&amp;gt;&amp;gt; The thing we really want to do, at least, the way that I think about it,

00:00:18.396 --> 00:00:21.824
the thing that I want to be able to do with the GPU is to make it easier to program

00:00:21.824 --> 00:00:24.994
and to broaden the number of problems you can do on the GPU.

00:00:24.994 --> 00:00:31.135
so the GPU starts out being really, really good at very broad bulk parallelism.

00:00:31.135 --> 00:00:38.273
You can throw a lot of threads at a problem. It handles them very efficiently, but for less regular problems,

00:00:38.273 --> 00:00:41.752
for more sort of diverse or fine grain parallelism,

00:00:41.752 --> 00:00:45.648
it's hard to express that in CUDA today.

00:00:45.648 --> 00:00:52.727
So the dynamic parallelism effort was to try and aim at an easier way

00:00:52.727 --> 00:00:55.197
to extract more parallelism from your problem.

00:00:55.197 --> 00:01:02.032
So we wanted to solve the kinds of problems, things like recursion, things like task parallelism,

00:01:02.032 --> 00:01:06.636
those types of problems that are difficult to express in a single big grid of threads,

00:01:06.636 --> 00:01:13.007
sort of paradigm, we might want to enable new types of programming problems to be solved.

