WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.555
So let's wrap up and, and summarize some of the things that we've learned.

00:00:03.555 --> 00:00:07.390
We've learned about parallel communication patterns going beyond the simple map

00:00:07.390 --> 00:00:11.387
operation that you saw in Unit 1 to encompass other important communication patterns,

00:00:11.387 --> 00:00:14.572
like gather, scatter, stencil, and transpose.

00:00:14.572 --> 00:00:18.116
We've learned more about the GPU programming model and the underlying hardware

00:00:18.116 --> 00:00:21.724
such as how thread blocks run on streaming multi-processors, or SMs,

00:00:21.724 --> 00:00:23.934
and what assumptions you can make about ordering,

00:00:23.934 --> 00:00:27.827
and how threads and thread blocks can synchronize to safely share data and memory.

00:00:27.827 --> 00:00:33.014
We've learned about that GPU memory model, topics like local, and global, and shared memory,

00:00:33.014 --> 00:00:37.970
and how atomic operations can simplify memory writes by concurrent threads.

00:00:37.970 --> 00:00:42.479
Finally we got a quick preview of strategies for efficient GPU programming.

00:00:42.479 --> 00:00:46.788
The first principle is to minimize the time spent on memory accesses

00:00:46.788 --> 00:00:49.737
by doing things like coalescing global memory access.

00:00:49.737 --> 00:00:53.470
We saw that the extremely fast global memory on the GPU operates best

00:00:53.470 --> 00:00:57.183
when adjacent threads access contiguous chunks of global memory,

00:00:57.183 --> 00:00:59.627
and this is called a coalesced memory access.

00:00:59.627 --> 00:01:03.225
We also learned to move frequently accessed data to faster memory.

00:01:03.225 --> 00:01:05.725
So, for example, promoting data to shared memory.

00:01:05.725 --> 00:01:09.941
And we learned that atomic memory operations have a cost, but they're great and they're

00:01:09.941 --> 00:01:12.754
useful and you shouldn't necessarily freak out about the cost.

00:01:12.754 --> 00:01:15.543
And often the cost is negligible, but it's something to be aware of.

00:01:15.543 --> 00:01:18.609
Along the same lines, we learned about avoiding thread divergence

00:01:18.609 --> 00:01:23.161
that comes with branches and loops and, once again, thread divergence comes at a cost.

00:01:23.161 --> 00:01:27.000
You should be aware of that, but it isn't necessarily something that you should be freaked out about.

00:01:27.000 --> 00:01:32.557
We're going to revisit these topics and talk much more about optimizing GPU programs in Unit 5. So that's it.

