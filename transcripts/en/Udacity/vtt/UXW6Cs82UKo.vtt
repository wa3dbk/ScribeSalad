WEBVTT
Kind: captions
Language: en

00:00:00.600 --> 00:00:02.420
What can you do with an?

00:00:02.420 --> 00:00:03.550
Lots of things.

00:00:03.550 --> 00:00:05.500
Imagine that you have a model
that predicts the next step

00:00:05.500 --> 00:00:06.580
of your sequence, for example.

00:00:06.580 --> 00:00:10.090
You can use that to generate sequences.

00:00:10.090 --> 00:00:14.940
For example, text, either one word at
a time or one character at a time.

00:00:14.940 --> 00:00:18.780
For that, you take your sequence at
time, t, you generate the predictions

00:00:18.780 --> 00:00:22.120
from your RNN, and then you sample
from the predicted distribution.

00:00:22.120 --> 00:00:25.530
And then you pick one element
based on its probability.

00:00:25.530 --> 00:00:28.830
And feed that sample to
the next step and go on.

00:00:28.830 --> 00:00:33.700
Do this repeatedly, predict,
sample, predict, sample, and

00:00:33.700 --> 00:00:37.580
you can generate a pretty good
sequence of whatever your RNN models.

00:00:37.580 --> 00:00:40.910
There is a more sophisticated way to
do this, that gives better results.

00:00:40.910 --> 00:00:45.300
Because just sampling the next
prediction every time, is very greedy.

00:00:45.300 --> 00:00:47.730
Instead of just sampling
once at each step,

00:00:47.730 --> 00:00:49.415
you could imagine
sampling multiple times.

00:00:49.415 --> 00:00:52.425
Here we pick O but also A for example.

00:00:53.455 --> 00:00:56.795
When we have multiple sequences,
often call that hypotheses,

00:00:56.795 --> 00:00:59.915
that you could continue
predicting from at every step.

00:00:59.915 --> 00:01:03.175
You can choose the best sequence out
of those, by computing the total

00:01:03.175 --> 00:01:07.025
probability of all the characters
that you generated so far.

00:01:07.025 --> 00:01:10.800
By looking at the total probability
of the multiple time steps at a time.

00:01:10.800 --> 00:01:14.690
You prevent the sampling from
accidentally making one by choice, and

00:01:14.690 --> 00:01:17.950
being stuck with that one
bad decision forever.

00:01:17.950 --> 00:01:20.950
For example, we could have just
picked A by chance here, and

00:01:20.950 --> 00:01:26.170
never explored the hypothesis that O
could lead to a very sensible next word.

00:01:26.170 --> 00:01:29.890
Of course, if you do this the number
of sequences that you need to consider

00:01:29.890 --> 00:01:30.950
grows exponentially.

00:01:32.000 --> 00:01:35.620
There is a smarter way to do that, which
is to do what's called a Beam Search.

00:01:35.620 --> 00:01:39.760
You only keep, say, the most likely few
candidate sequences at every time step,

00:01:39.760 --> 00:01:41.600
and then simply prune the rest.

00:01:41.600 --> 00:01:43.620
In practice, this works very well for

00:01:43.620 --> 00:01:45.610
generating very good
sequences from your model.

