WEBVTT
Kind: captions
Language: en

00:00:00.150 --> 00:00:03.380
So it's a little bit easier to
understand what these eigenvectors do,

00:00:03.380 --> 00:00:05.320
if we look at it this way.

00:00:05.320 --> 00:00:09.980
Here are the principle component
eigenvectors, one through whatever.

00:00:09.980 --> 00:00:12.980
Here's two different reconstructions,
all right.

00:00:12.980 --> 00:00:18.490
If I take the average face, and
I just add three times, and sigma

00:00:18.490 --> 00:00:22.390
k is just the standard deviation of
the coefficient, don't worry about that.

00:00:22.390 --> 00:00:23.980
If I take the average face and

00:00:23.980 --> 00:00:28.420
I add in some of that component,
I get this kind of thing.

00:00:28.420 --> 00:00:32.200
And if I subtract off that kind of
component, I get that thing.

00:00:32.200 --> 00:00:36.450
Notice, that that just moved
the lighting from the left to the right.

00:00:36.450 --> 00:00:39.170
So one way of thinking about
it is that these components,

00:00:39.170 --> 00:00:42.410
the first components just
change with the direction,

00:00:42.410 --> 00:00:44.670
lighting direction was from
the left to the right.

00:00:44.670 --> 00:00:47.770
Whereas, let's see,
let's take another one.

00:00:47.770 --> 00:00:51.340
Here, this component here,
if I add positive, I don't know,

00:00:51.340 --> 00:00:53.850
it tends to look a lot more feminine.

00:00:53.850 --> 00:00:57.120
When I go negative,
it tends to look a lot more masculine.

00:00:57.120 --> 00:01:00.260
Okay.
So each of these, different eigenvectors

00:01:00.260 --> 00:01:04.800
are adding in some other different
variation, some other different element.

00:01:04.800 --> 00:01:05.850
So, how do you use these?

00:01:05.850 --> 00:01:07.890
Well, a couple of things
we have to talk about.

00:01:07.890 --> 00:01:12.680
First is, I have to be able
to get the coefficients.

00:01:12.680 --> 00:01:15.230
Well, that's real easy,
just like I said before.

00:01:15.230 --> 00:01:18.350
I take in some image, x, here it is, and

00:01:18.350 --> 00:01:22.000
I just subtract off the mean,
the mean face.

00:01:22.000 --> 00:01:25.570
And I take the dot product with,
in this case, the first eigenvector,

00:01:25.570 --> 00:01:28.240
second, third,
all the way up to the kth eigenvector.

00:01:28.240 --> 00:01:31.790
Those dot products will give
me these coefficients, or

00:01:31.790 --> 00:01:34.660
weights, w1 through w k.

00:01:34.660 --> 00:01:38.710
What's cool is that vector of numbers,
just k of them, maybe it's 20,

00:01:38.710 --> 00:01:43.540
maybe it's 200, that's my entire
representation of this face.

00:01:43.540 --> 00:01:47.264
So I've taken this 10,000-dimensional
vector, 10,000 elements,

00:01:47.264 --> 00:01:51.560
10,000 pixels, and
I've reduced it to 20 numbers.

00:01:51.560 --> 00:01:57.450
So how do we make use of this vector,
these w's that are the coefficients?

00:01:57.450 --> 00:02:02.090
Well, the first thing is, I can actually
use them to reconstruct the face.

00:02:02.090 --> 00:02:02.620
Right?

00:02:02.620 --> 00:02:08.300
Each one of these w's tells me how much
of each eigenvector to add back in.

00:02:08.300 --> 00:02:11.740
And in general,
what I can do is I take a picture and

00:02:11.740 --> 00:02:16.325
I say this picture is represented
by the mean plus the w's,

00:02:16.325 --> 00:02:19.795
each of the w's times the eigenvector,
because that's what this is.

00:02:19.795 --> 00:02:25.025
And if I were to keep a lot of them, I
would get a really great reconstruction.

00:02:25.025 --> 00:02:28.305
If I keep some of them,
I get an okay reconstruction.

00:02:28.305 --> 00:02:30.825
Well, how many of them
do I want to keep?

00:02:30.825 --> 00:02:33.885
Well remember, I wasn't doing
this to do a reconstruction,

00:02:33.885 --> 00:02:35.145
I was trying to do recognition.

