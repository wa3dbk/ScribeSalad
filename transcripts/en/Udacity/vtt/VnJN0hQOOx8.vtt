WEBVTT
Kind: captions
Language: en

00:00:00.340 --> 00:00:04.129
Before jumping into the code let's
quickly review the components

00:00:04.129 --> 00:00:05.887
of a Markov decision process,

00:00:05.887 --> 00:00:09.754
or MDP, and see how they are implemented
using the BURLAP library.

00:00:09.754 --> 00:00:14.860
The first component of an MDP is a set
of states S that the system can be in.

00:00:14.860 --> 00:00:19.650
Associated to each state S,
in S, is a set of actions A(s),

00:00:19.650 --> 00:00:22.470
that can be taken at the state,
little s.

00:00:22.470 --> 00:00:27.360
Additionally, there is
a transition function T(s,a,s'),

00:00:27.360 --> 00:00:31.440
that gives the probability
that if we are in state s, and

00:00:31.440 --> 00:00:35.650
take action a,
that we end up in state s prime.

00:00:35.650 --> 00:00:37.930
Finally, there is a reward function.

00:00:37.930 --> 00:00:44.090
R(s,a,s'), which gives
the reward we attain by taking

00:00:44.090 --> 00:00:50.450
action a at state s and
ending up in state s prime.

00:00:50.450 --> 00:00:54.790
Recall that it is sometimes more natural
to express the reward as a function of

00:00:54.790 --> 00:01:00.300
just s and a, or even as a function
of just the initial state, s.

00:01:01.420 --> 00:01:05.921
We will use the most general form,
R(s,a,s') in this

00:01:05.921 --> 00:01:11.220
tutorial since reward functions are
implemented in BURLAP in this manner.

00:01:11.220 --> 00:01:16.280
A terminal state is a state in which
no further agent action is possible.

00:01:16.280 --> 00:01:20.220
After entering the terminal state
an agent receives no more reward for

00:01:20.220 --> 00:01:21.880
the rest of time.

00:01:21.880 --> 00:01:26.750
In lecture, we did not define terminal
states as a component of an MDP,

00:01:26.750 --> 00:01:31.210
because they can be considered states
without any associated actions.

00:01:31.210 --> 00:01:32.392
However, in BURLAP,

00:01:32.392 --> 00:01:36.500
we will have the capability to
specially denote terminal states.

00:01:36.500 --> 00:01:39.790
So it is good to keep this concept
in mind as we build our MDP.

