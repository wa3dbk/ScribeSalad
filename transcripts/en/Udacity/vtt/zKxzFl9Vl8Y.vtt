WEBVTT
Kind: captions
Language: en

00:00:00.310 --> 00:00:02.460
So, let's unpack a little bit about what you said Michael. You,

00:00:02.460 --> 00:00:05.990
you're exactly right in everything that you said and we can sort of

00:00:05.990 --> 00:00:09.720
write down these things I think more generally be simply noting a few

00:00:09.720 --> 00:00:12.530
of the pluses and the minuses, the pros and the cons. So in

00:00:12.530 --> 00:00:16.230
the filtering case one of the things that you, you didn't say explicity

00:00:16.230 --> 00:00:18.340
but I think was built into what you said. Is a question of

00:00:18.340 --> 00:00:23.170
speed, so filtering is faster than wrapping because you don't have to worry

00:00:23.170 --> 00:00:25.310
about what the learner wants, you don't have to worry about paying the

00:00:25.310 --> 00:00:28.160
cost of what the learner is going to do. You can basically just

00:00:28.160 --> 00:00:31.690
apply any fast algorithm you might imagine that takes a look at the

00:00:31.690 --> 00:00:36.030
features and does some sort of filtering. It is of course, as we

00:00:36.030 --> 00:00:39.320
pointed out before an exponential problem but you can do some kind of

00:00:39.320 --> 00:00:42.390
an approximation here and you can make a. Basically arbitrarily as fast as

00:00:42.390 --> 00:00:44.300
you want it to be. Which I think is what you were thinking

00:00:44.300 --> 00:00:47.540
about originally when we started the quiz. So you do get speed and

00:00:47.540 --> 00:00:50.560
the price that you're paying for that speed it that you tend to look

00:00:50.560 --> 00:00:54.130
at the features in isolation. So what do I mean by that.

00:00:54.130 --> 00:00:56.240
Well you may look at a feature and say this feature is

00:00:56.240 --> 00:01:00.510
unimportant. For whatever reason. But maybe that feature is important if you

00:01:00.510 --> 00:01:03.990
combined it with another feature. Say maybe we're in a kind of of parity

00:01:03.990 --> 00:01:07.620
problem again. And so some feature might look unimportant but actually, in

00:01:07.620 --> 00:01:10.040
fact, it is important. And the only way you're going to get the kind

00:01:10.040 --> 00:01:12.350
of speed up that you need is typically by looking at features

00:01:12.350 --> 00:01:15.570
in isolation. Either adding them or removing them and of course, the reason

00:01:15.570 --> 00:01:17.420
it gets this speed, is that it ignores the

00:01:17.420 --> 00:01:20.720
learning problem itself. At least the learner itself, if not

00:01:20.720 --> 00:01:23.680
the learning problem. Now, in contrast wrapping actually takes into

00:01:23.680 --> 00:01:27.450
account, model bias, it takes into account whatever the learning

00:01:27.450 --> 00:01:30.670
bias of the learning alogarithm is, in order to determine

00:01:30.670 --> 00:01:33.480
what the best sub features are. Which means it's actually

00:01:33.480 --> 00:01:36.470
worried about the problem of learning, itself. Which, I don't

00:01:36.470 --> 00:01:40.260
know how to spell, apparently. [LAUGH] But in the process,

00:01:40.260 --> 00:01:44.760
it's much much slower. Because every time it tries to look

00:01:44.760 --> 00:01:47.760
at a particular subset, a candidate subset, it has to run

00:01:47.760 --> 00:01:51.460
the learning algorithm. Imagine this learning algorithm is something like neural

00:01:51.460 --> 00:01:53.830
network learning. This could take thousands

00:01:53.830 --> 00:01:55.320
and thousands of iterations, you might

00:01:55.320 --> 00:01:58.020
have to do cross validation, there's all kinds of things you

00:01:58.020 --> 00:02:00.170
might have to do before you even get to look at

00:02:00.170 --> 00:02:02.950
the next thing and and get a score. And so wrapping

00:02:02.950 --> 00:02:05.410
makes a lot more sense because it takes into account the learning

00:02:05.410 --> 00:02:09.990
problem and im more importantly the learning bias or

00:02:09.990 --> 00:02:13.390
the inductive bias of the learning algorithm. But, it's going to

00:02:13.390 --> 00:02:14.960
take a very, very long time to do it

00:02:14.960 --> 00:02:17.950
while filtering tends to go pretty well. You buy that?

00:02:17.950 --> 00:02:18.990
&gt;&gt; Yeah. That makes sense.

00:02:18.990 --> 00:02:20.460
&gt;&gt; Okay. So I'm going to ask you a question

00:02:20.460 --> 00:02:24.440
before we, sort of move on. Can you think

00:02:24.440 --> 00:02:27.270
of any algorithms that we've looked at in the,

00:02:27.270 --> 00:02:30.930
the last mini course, on supervised learning? It basically

00:02:30.930 --> 00:02:32.940
was already doing filtering.

00:02:32.940 --> 00:02:35.660
&gt;&gt; So, it's not quite the same,

00:02:35.660 --> 00:02:37.130
because that's supervised, and now we're talking

00:02:37.130 --> 00:02:39.070
about unsupervised. But we did look at

00:02:39.070 --> 00:02:41.910
some algorithms that explicitly decided which features to

00:02:41.910 --> 00:02:43.590
include, and which ones not. I'm thinking

00:02:43.590 --> 00:02:46.770
of decision trees, maybe. And, I think boosting

00:02:46.770 --> 00:02:49.630
kind of has a little bit of this depending on what the weak learner set is.

00:02:49.630 --> 00:02:53.500
&gt;&gt; Right, that's true. I mean, we, the problem with, boosting is

00:02:53.500 --> 00:02:55.980
that it's sort of not. It's not picking the features as much as

00:02:55.980 --> 00:02:59.820
it's picking examples. But certainly with decision trees, you're exactly right.

00:02:59.820 --> 00:03:03.240
Decision trees are in it's own way a kind of filtering algorithm.

00:03:03.240 --> 00:03:05.950
So, this might not be obvious, and maybe I could have been

00:03:05.950 --> 00:03:09.770
clearer about it, you get these features and I think you actually

00:03:09.770 --> 00:03:12.460
asked me this question. But you get these features, but you do

00:03:12.460 --> 00:03:15.230
know what the labels are if there's a supervised learner built it.

00:03:15.230 --> 00:03:17.320
If you're going to have a learner at the end of the day,

00:03:17.320 --> 00:03:21.290
you, it's not considered cheating in the filtering case. To look inside

00:03:21.290 --> 00:03:24.020
the learner, but its not consider cheating to understand

00:03:24.020 --> 00:03:26.450
what the labels are so you can take advantage

00:03:26.450 --> 00:03:28.840
of the actual labels of your examples to do

00:03:28.840 --> 00:03:32.870
your filtering. So in fact if you take the main

00:03:32.870 --> 00:03:36.200
part of the decision trees which is the information

00:03:36.200 --> 00:03:39.260
gain thats actually a way of doing filtering on the

00:03:39.260 --> 00:03:42.630
features. So the criterion is information gain. Find me

00:03:42.630 --> 00:03:46.680
the features that provide you with the most information given

00:03:46.680 --> 00:03:49.120
the class label and that gets exactly the subset.

00:03:49.120 --> 00:03:51.980
&gt;&gt; Cool. Now I don't, I don't quite see how to iterate that

00:03:51.980 --> 00:03:54.540
though. So I can see how you can get the first feature that way,

00:03:54.540 --> 00:03:56.730
then are we talking after that we're

00:03:56.730 --> 00:03:59.250
going to select features that. Add information

00:03:59.250 --> 00:04:00.910
above and beyond what we've already gotten

00:04:00.910 --> 00:04:02.650
out of the other features we've selected?

00:04:02.650 --> 00:04:04.680
&gt;&gt; Sure. So in fact, imagine that

00:04:04.680 --> 00:04:08.170
my search algorithm here is actual decision trees.

00:04:08.170 --> 00:04:11.730
So I take a bunch of features. I take a bunch of labeled data. I run

00:04:11.730 --> 00:04:15.130
my decisino tree algorithim. It picks a subset of the features. Or maybe

00:04:15.130 --> 00:04:17.170
I do pruning or cross. Whatever.

00:04:17.170 --> 00:04:18.920
Assisted arbitrarty search algorithm. I just drew

00:04:18.920 --> 00:04:22.079
a box here. And then what I do is I take all the

00:04:22.079 --> 00:04:25.400
features that were used by the decision And I pass that to my learner.

00:04:25.400 --> 00:04:25.940
&gt;&gt; Hm.

00:04:25.940 --> 00:04:28.420
&gt;&gt; Now the learner is a decision tree learner. I presume

00:04:28.420 --> 00:04:30.990
we get back the same decision tree. But maybe that's a

00:04:30.990 --> 00:04:33.100
way of picking a subset of features that are useful for,

00:04:33.100 --> 00:04:37.540
for example a new network or perceptron or KNN. In which case,

00:04:37.540 --> 00:04:39.820
I don't quite see how to do the fed, the filtering.

00:04:39.820 --> 00:04:43.573
&gt;&gt; Well, you run a decision tree algorithm here. [CROSSTALK]

00:04:43.573 --> 00:04:44.180
&gt;&gt; Oh.

00:04:44.180 --> 00:04:45.370
&gt;&gt; That gives you the set of

00:04:45.370 --> 00:04:47.660
features that the decision tree thinks is important.

00:04:47.660 --> 00:04:48.590
&gt;&gt; Oh.

00:04:48.590 --> 00:04:51.920
&gt;&gt; And you return that set of features. Basically, the entire, not the

00:04:51.920 --> 00:04:54.100
tree, but you flatten the tree and then just turn that into a

00:04:54.100 --> 00:04:58.250
set of features. So imagine, for example, that we actually put inside this

00:04:58.250 --> 00:05:02.670
box a decision-tree learner. That uses the set of data as input with all

00:05:02.670 --> 00:05:04.520
of its features and all of the labels

00:05:04.520 --> 00:05:07.090
that it knows about. It builds a decision tree

00:05:07.090 --> 00:05:09.530
which gives you typically a subset, well in fact

00:05:09.530 --> 00:05:11.510
by definition, gives you a subset of all the

00:05:11.510 --> 00:05:13.760
features. And then the features that come out of

00:05:13.760 --> 00:05:15.770
the decision tree are passed on to another learned

00:05:15.770 --> 00:05:18.140
like for a example a perceptron or a nueral

00:05:18.140 --> 00:05:20.319
network or even something like K and N. [UNKNOWN]

00:05:20.319 --> 00:05:22.270
&gt;&gt; It always comes back to K and N.

00:05:22.270 --> 00:05:23.601
&gt;&gt; Everything comes back to K and N. [LAUGH]

00:05:23.601 --> 00:05:25.890
&gt;&gt; So let me make sure i understand we

00:05:25.890 --> 00:05:27.700
run a decisoin tree learner but we dont want to

00:05:27.700 --> 00:05:30.880
use what it actually determined? Instead, we say let's

00:05:30.880 --> 00:05:32.660
just look at all the things that got split

00:05:32.660 --> 00:05:34.850
on. And the union of all those now become

00:05:34.850 --> 00:05:36.800
features to hand off to some other learning algorithm.

00:05:36.800 --> 00:05:37.510
&gt;&gt; Right.

00:05:37.510 --> 00:05:38.840
&gt;&gt; But, why would we want to do that

00:05:38.840 --> 00:05:40.330
if we already trained up the decision tree?

00:05:40.330 --> 00:05:43.060
&gt;&gt; Because maybe we're worried that the decision tree

00:05:43.060 --> 00:05:44.630
doesn't do as good a job as you would

00:05:44.630 --> 00:05:49.660
like. On noisy data for example. Or you're just

00:05:49.660 --> 00:05:52.460
going to run the decision tree to some depth. Or,

00:05:52.460 --> 00:05:53.820
even, what you want to do is, you're going to

00:05:53.820 --> 00:05:56.490
run the decision tree until it actually overfits. And

00:05:56.490 --> 00:05:58.040
now you know all the features that you could

00:05:58.040 --> 00:05:59.880
use, and you pass that on to some other

00:05:59.880 --> 00:06:02.410
learner, which has a different bias. So basically, you

00:06:02.410 --> 00:06:04.270
use the inductive bias of the decision tree to

00:06:04.270 --> 00:06:06.830
choose features, but then you use the inductive bias

00:06:06.830 --> 00:06:09.130
of your other learner in order to do learning. Neat.

00:06:09.130 --> 00:06:10.270
&gt;&gt; Right, and if you think about the

00:06:10.270 --> 00:06:13.040
big difference between for example KNN and decision trees,

00:06:13.040 --> 00:06:14.840
we know that KNN suffers from the curse of

00:06:14.840 --> 00:06:17.820
dimensionality because it doesn't know which features are important.

00:06:17.820 --> 00:06:20.000
So this, and but the decisions trees are very good at figuring out

00:06:20.000 --> 00:06:23.260
which features are important, at least for predicting the label. And so you

00:06:23.260 --> 00:06:26.230
can ask you're decision tree to give some hint to KNN which has

00:06:26.230 --> 00:06:27.770
other nice things going for it, which

00:06:27.770 --> 00:06:29.120
features it should actually pay attention to.

00:06:29.120 --> 00:06:29.880
&gt;&gt; Cool.

00:06:29.880 --> 00:06:30.890
&gt;&gt; Alright so there you go.

