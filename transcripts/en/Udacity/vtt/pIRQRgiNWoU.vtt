WEBVTT
Kind: captions
Language: en

00:00:00.260 --> 00:00:05.060
Okay Michael, so
I have brought back yet another slide,

00:00:05.060 --> 00:00:09.150
slightly modified, from the discussion
on options and constraints.

00:00:09.150 --> 00:00:11.260
Do you recognize it, you remember this?

00:00:11.260 --> 00:00:14.530
&gt;&gt; Well, it's got the Pac-Man thing
on it, and it has the four options or

00:00:14.530 --> 00:00:16.250
whatever that we just talked about.

00:00:16.250 --> 00:00:18.490
And it has the beginnings
of a stoplight.

00:00:18.490 --> 00:00:22.310
&gt;&gt; Right, exactly right, so in fact I
told you a little about this experiment

00:00:22.310 --> 00:00:24.330
that we did, where we asked people
to come up with these options.

00:00:24.330 --> 00:00:25.880
But there was a second
part to the experiment.

00:00:25.880 --> 00:00:27.047
There are two buttons,

00:00:27.047 --> 00:00:29.737
this is me attempting to draw
what a button looks like.

00:00:29.737 --> 00:00:31.837
There's a green button and
there's a red button.

00:00:31.837 --> 00:00:35.695
And what you're being asked to do is
to watch Pac-Man over here run around

00:00:35.695 --> 00:00:38.944
executing actions including
these options you came up with.

00:00:38.944 --> 00:00:42.470
And then occasionally you're supposed
to hit a green or red button.

00:00:42.470 --> 00:00:44.430
And what do you think that the green and
the red stand for?

00:00:44.430 --> 00:00:47.330
&gt;&gt; Probably green is Pac-Man
did something good, and

00:00:47.330 --> 00:00:49.310
the red is the Pac-Man did
something not so good.

00:00:49.310 --> 00:00:50.070
&gt;&gt; Right, that's exactly right.

00:00:50.070 --> 00:00:51.350
So this is supposed to mean good.

00:00:51.350 --> 00:00:53.990
And this is supposed to mean bad,
just like you would expect.

00:00:53.990 --> 00:00:55.500
We had people do this, and they did it.

00:00:55.500 --> 00:00:57.910
They would say that's good,
that's bad, that's bad, that's bad.

00:00:57.910 --> 00:00:59.820
That's good, that's good, that's good,
that's good, that's good, that's good,

00:00:59.820 --> 00:01:02.020
that's good, that's bad,
and so on and so forth.

00:01:02.020 --> 00:01:04.510
Now, here's my question to you.

00:01:04.510 --> 00:01:06.535
Given that people have done all of this,
and

00:01:06.535 --> 00:01:09.947
they've given us lots of data on
whether Pac-Man did well or did poorly.

00:01:09.947 --> 00:01:12.887
What would be the obvious thing
to do with that information?

00:01:12.887 --> 00:01:15.680
&gt;&gt; The obvious thing would be,
it seems like the right thing.

00:01:15.680 --> 00:01:19.390
Which is to,
if you treat the good as a plus one and

00:01:19.390 --> 00:01:23.130
the bad as a minus one, and
have the agent just maximize for reward.

00:01:23.130 --> 00:01:25.500
That should basically be
reinforcement learning.

00:01:25.500 --> 00:01:26.335
&gt;&gt; Right, and
that makes a lot of sense, right?

00:01:26.335 --> 00:01:27.933
So since we do reinforcement learning,

00:01:27.933 --> 00:01:30.427
we'd want to turn this into
a reinforcement learning problem.

00:01:30.427 --> 00:01:32.889
And we would say well,
every time you said something was good,

00:01:32.889 --> 00:01:35.737
I'm going to treat that as a reward
of say one, it doesn't really matter.

00:01:35.737 --> 00:01:39.297
And bad say is something like that,
and you get a reward for

00:01:39.297 --> 00:01:42.577
say, being in a state in
taking a particular action.

00:01:42.577 --> 00:01:44.177
And that's a perfectly
reasonable thing to do.

00:01:44.177 --> 00:01:48.207
And in fact this ends up being a way
to get human beings to help us to

00:01:48.207 --> 00:01:50.457
automatically do reward shaping.

00:01:50.457 --> 00:01:52.757
And in fact you can show
this works pretty well.

00:01:52.757 --> 00:01:55.523
And again,
there's been a lot of work on this,

00:01:55.523 --> 00:01:59.810
we have provided some papers on this for
the students to read about.

00:01:59.810 --> 00:02:02.290
And you do better, as you might imagine,

00:02:02.290 --> 00:02:05.220
by treating these things as reward
shaping than if you didn't.

00:02:05.220 --> 00:02:07.440
Because really they're hints,
as you move along,

00:02:07.440 --> 00:02:10.139
about where you ought to be and
the kinds of things you have to do.

00:02:10.139 --> 00:02:11.830
But remember what I said before,

00:02:11.830 --> 00:02:16.010
that human beings actually
are trying to tell you something.

00:02:16.010 --> 00:02:18.474
And they might be trying to tell you
something different from what you

00:02:18.474 --> 00:02:19.077
want them to be.

00:02:19.077 --> 00:02:21.746
So, if I think back to what you just
said, what you kind of said is,

00:02:21.746 --> 00:02:23.857
well look it's a reinforcement
learning problem.

00:02:23.857 --> 00:02:26.057
Reinforcement learning
problems need rewards, and so

00:02:26.057 --> 00:02:27.950
we should convert these
things into rewards.

00:02:27.950 --> 00:02:31.440
Well, it turns out that in practice,
when you actually talk to humans,

00:02:31.440 --> 00:02:34.380
and work out what they do,
they're not actually doing this.

00:02:34.380 --> 00:02:36.140
When they say good and they say bad,

00:02:36.140 --> 00:02:39.901
they're not thinking in terms of
actual rewards, plus one, minus one.

00:02:39.901 --> 00:02:43.850
Or plus ten and minus nine,
and these kinds of things.

00:02:43.850 --> 00:02:46.550
Because of course they're not
reinforcement learning experts,

00:02:46.550 --> 00:02:47.770
they're not thinking about rewards.

00:02:47.770 --> 00:02:50.400
They're actually giving you
a different kind of information.

00:02:50.400 --> 00:02:52.330
So here, let me just ask you.

00:02:52.330 --> 00:02:54.390
If I told you in English,

00:02:54.390 --> 00:03:00.520
that you should hit the green button
whenever the agent does something right.

00:03:00.520 --> 00:03:03.640
And the red button when
the agent does something wrong,

00:03:03.640 --> 00:03:08.280
then what kind of information am I
conveying, other than a reward value?

00:03:08.280 --> 00:03:08.950
&gt;&gt; Well, one way to put it,

00:03:08.950 --> 00:03:12.620
is your you're giving commentary
on the behavior or the policy.

00:03:12.620 --> 00:03:15.550
&gt;&gt; That's exactly right,
that really in fact, it turns out,

00:03:15.550 --> 00:03:18.090
you could convert these
things into rewards.

00:03:18.090 --> 00:03:20.350
&gt;&gt; But you could also convert them into
something that's a bit more direct.

00:03:20.350 --> 00:03:23.170
You can just say well you're
actually telling me that in this

00:03:23.170 --> 00:03:26.210
particular state,
I should in fact take this action.

00:03:26.210 --> 00:03:29.703
Or that in this particular state,
I should not take this action.

00:03:29.703 --> 00:03:32.950
And this is direct policy advice,
as opposed to reward advice.

00:03:32.950 --> 00:03:36.770
And so maybe, what we're really doing,
is not reward shaping, or

00:03:36.770 --> 00:03:40.460
what humans are doing is not reward
shaping, it's actually a policy shaping.

