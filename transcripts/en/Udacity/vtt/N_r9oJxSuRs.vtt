WEBVTT
Kind: captions
Language: en

00:00:00.350 --> 00:00:05.400
Now comes the cool thing and
it's so cool we call it a trick.

00:00:05.400 --> 00:00:08.119
I mean, not too many things in
science are called a trick, and

00:00:08.119 --> 00:00:10.885
this is called the kernel trick,
all right?

00:00:10.885 --> 00:00:14.111
We saw that a,

00:00:14.111 --> 00:00:20.560
the linear classifier only depends
on dot products, remember that?

00:00:20.560 --> 00:00:21.360
I circled it.

00:00:21.360 --> 00:00:22.540
I jumped up and down about it.

00:00:22.540 --> 00:00:25.220
So let's define some function K.

00:00:25.220 --> 00:00:27.190
Of two points xi and

00:00:27.190 --> 00:00:32.850
xj as just being the dot product,
okay, between them, all right.

00:00:32.850 --> 00:00:36.680
If I map a data point to some
high dimensional space, so

00:00:36.680 --> 00:00:41.510
my function phi here maps x to phi of x,
all right?

00:00:41.510 --> 00:00:42.800
Then the dot product.

00:00:42.800 --> 00:00:45.770
Whoops, I actually should
write it this way, right?

00:00:45.770 --> 00:00:50.040
The dot product k of xi, xj would become

00:00:51.910 --> 00:00:54.630
phi of x, transpose phi of xj.

00:00:54.630 --> 00:00:56.220
That's what's written right there.

00:00:56.220 --> 00:00:57.280
Okay?

00:00:57.280 --> 00:01:00.980
And in fact, what I'm actually going to
do is, I'm just going to redefine k,

00:01:00.980 --> 00:01:05.910
instead of just being the dot product of
xi and xj, I'm going to have a new k,

00:01:05.910 --> 00:01:11.030
right, which is just this dot product
of the, the higher dimensional space.

00:01:12.500 --> 00:01:16.840
K stands for kernel, okay, and it's a,

00:01:16.840 --> 00:01:20.690
it's, it's a kernel function and it can
be thought of as a similarity function.

00:01:20.690 --> 00:01:24.040
Similarity functions the idea
is that they get bigger,

00:01:24.040 --> 00:01:26.230
the more similar things are and
you know,

00:01:26.230 --> 00:01:30.600
dot products can go from, you can even
think of it as minus 1 to 1, it's true.

00:01:30.600 --> 00:01:35.680
So the idea being the most similar is 1,
the least similar is minus 1, okay?

00:01:35.680 --> 00:01:40.730
And a kernel function is a similarity
function that is the dot product,

00:01:40.730 --> 00:01:45.066
sometimes called the inner product,
of the higher dimensional space.

00:01:45.066 --> 00:01:48.061
Okay.
So, I'm going to show you an example in

00:01:48.061 --> 00:01:53.528
a minute, but basically the idea is, if
I give you a function of x1, of xi and

00:01:53.528 --> 00:01:59.253
xj, some function K, as long as there's
some higher dimensional space in which

00:01:59.253 --> 00:02:04.423
that function is just the dot product
of that higher dimensional space.

00:02:04.423 --> 00:02:08.560
That kernel is a dot product,
and therefore,

00:02:08.560 --> 00:02:12.176
it can be used in our linear classifier.

00:02:12.176 --> 00:02:15.565
There's a whole bunch of theory there
about what are called Mercer kernels,

00:02:15.565 --> 00:02:19.020
which have to do with what are,
what types of kernels are acceptable.

00:02:19.020 --> 00:02:22.620
You basically have to end up with these
positive definite matrices between

00:02:22.620 --> 00:02:25.070
your points a couple
of other constraints.

00:02:25.070 --> 00:02:27.100
What, most of that doesn't matter.

00:02:27.100 --> 00:02:30.920
What matters is that you have to is,
is you, is that you can show that

00:02:30.920 --> 00:02:35.900
essentially it's a dot product and
it turns out a lot of functions are.

