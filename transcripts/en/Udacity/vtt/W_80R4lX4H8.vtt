WEBVTT
Kind: captions
Language: en

00:00:00.640 --> 00:00:05.840
So the question then becomes, how might
an error agent learn from its mistakes?

00:00:05.840 --> 00:00:07.290
Learning by correcting mistakes, or

00:00:07.290 --> 00:00:11.950
learning from failures, really entails
answering three separate questions.

00:00:13.140 --> 00:00:17.740
The first question is, how can the agent
isolate the error in its former model?

00:00:17.740 --> 00:00:20.220
So the agent had some
model of the world.

00:00:20.220 --> 00:00:22.260
It made a mistake based on its model,

00:00:22.260 --> 00:00:25.490
how can it identify
the error in its model?

00:00:25.490 --> 00:00:28.410
Note that this particular problem
is very closely connected

00:00:28.410 --> 00:00:31.550
to the equation of diagnosis
that we discussed earlier.

00:00:31.550 --> 00:00:36.220
The second question is how can the agent
explain to itself that default it has

00:00:36.220 --> 00:00:39.840
identified the error in fact led
it to the problem to the failure.

00:00:40.910 --> 00:00:43.070
Having identified the fault and

00:00:43.070 --> 00:00:45.550
explained how the fault
led to the failure.

00:00:45.550 --> 00:00:48.400
The third question is how can
the agent repair the fault

00:00:48.400 --> 00:00:50.940
in order to prevent the error,
the failure from recurring.

00:00:51.950 --> 00:00:53.145
You may have noticed that earlier,

00:00:53.145 --> 00:00:56.775
we had related learning by correcting
mistakes with matter cognition.

00:00:56.775 --> 00:00:59.605
You can see how that
relationship occurs.

00:00:59.605 --> 00:01:01.685
The agent has some
knowledge of the world.

00:01:01.685 --> 00:01:03.515
That knowledge leads it to failure.

00:01:03.515 --> 00:01:07.215
The agent is using that failure
to repair it's own knowledge.

00:01:07.215 --> 00:01:13.065
It is as if the agent is looking into
itself, looking into it's own reasoning,

00:01:13.065 --> 00:01:15.085
into it's own knowledge and
correcting itself.

00:01:16.250 --> 00:01:19.300
Note once again that the learning
here is incremental.

00:01:19.300 --> 00:01:22.260
We are learning from
one example at a time.

00:01:22.260 --> 00:01:25.010
However instead of simply
learning from an example

00:01:25.010 --> 00:01:27.405
we are also using
explanation-based learning.

00:01:27.405 --> 00:01:32.215
We're trying to explain why a particular
fault led to a particular failure.

00:01:32.215 --> 00:01:35.300
An explanation connects them with
explanation that's learning,

00:01:35.300 --> 00:01:38.080
not just with the notion
of incremental learning.

00:01:38.080 --> 00:01:42.190
Let us see how these three questions
occur in the example of the pail.

00:01:42.190 --> 00:01:45.440
The first question is how
can the agent identify that

00:01:45.440 --> 00:01:48.870
the fact that this particular
pail has a moveable handle,

00:01:48.870 --> 00:01:52.830
not a fixed handle, is why this
is not a good example of a cup.

00:01:52.830 --> 00:01:57.610
The second question is how can the agent
with an explanation that proves why

00:01:57.610 --> 00:02:01.240
having that moveable handle makes
this the poor example of a cup.

00:02:01.240 --> 00:02:03.260
Why does that lead to a failure?

00:02:03.260 --> 00:02:06.435
The third question is how can
the agent change its model of a cup so

00:02:06.435 --> 00:02:11.935
that it never again picks an object with
a movable handle as an example of a cup.

00:02:11.935 --> 00:02:13.935
&gt;&gt; So when we talked about
explanation-based learning,

00:02:13.935 --> 00:02:15.445
we used another example of this as well.

00:02:15.445 --> 00:02:18.645
We imagined a desktop assistant
that I can just say hey,

00:02:18.645 --> 00:02:21.230
fetch me that important
file from last Tuesday.

00:02:21.230 --> 00:02:23.830
And it can construct it's own
understanding of what file I might

00:02:23.830 --> 00:02:25.080
be talking about.

00:02:25.080 --> 00:02:27.380
It has a notion of what files have
been important in the past and

00:02:27.380 --> 00:02:29.360
certain criteria of those files.

00:02:29.360 --> 00:02:31.810
So constructs an understanding
of what important is and

00:02:31.810 --> 00:02:34.690
tries to transfer that onto
files from last Tuesday.

00:02:34.690 --> 00:02:36.430
Now imagine that I told this agent hey,

00:02:36.430 --> 00:02:38.100
fetch me that important
file from last Tuesday.

00:02:38.100 --> 00:02:41.230
And it returns me a file that
actually wasn't important.

00:02:41.230 --> 00:02:43.700
And I say hey, that file isn't
actually important at all.

00:02:43.700 --> 00:02:47.350
The agent would first try to isolate
what error it made in diagnosing

00:02:47.350 --> 00:02:49.940
that particular document as important.

00:02:49.940 --> 00:02:52.900
You might, for example, notice that
every other document I ever labeled as

00:02:52.900 --> 00:02:56.360
important was very recent
whereas this one was really old.

00:02:56.360 --> 00:02:59.040
So even though it met all the criteria
for an important document

00:02:59.040 --> 00:03:01.460
there might be more criteria
that it didn't consider yet, and

00:03:01.460 --> 00:03:04.090
one of those might be that only
new documents are very important.

00:03:04.090 --> 00:03:07.320
It would then explain that the problem
came from the assumption that

00:03:07.320 --> 00:03:08.830
an old document could be important, and

00:03:08.830 --> 00:03:12.995
it would It would then repair its model
to say that in the future old documents

00:03:12.995 --> 00:03:16.815
can't be important, even if they meet
the other criteria for importance.

00:03:16.815 --> 00:03:20.951
&gt;&gt; This problem identifying the error in
one's knowledge that led to a failure

00:03:20.951 --> 00:03:23.435
was called credit assignment.

00:03:23.435 --> 00:03:25.315
Blame assignment might be a better term.

00:03:25.315 --> 00:03:26.725
A failure has occurred.

00:03:26.725 --> 00:03:31.205
What fault of gap in one's knowledge
was responsible for the failure?

00:03:31.205 --> 00:03:32.700
That's blame assignment.

00:03:32.700 --> 00:03:37.410
In this lesson, we'll be focusing on
gaps or errors in one's knowledge.

00:03:37.410 --> 00:03:41.960
In general, the error could be in one's
reasoning or in one's architecture.

00:03:41.960 --> 00:03:45.120
Credit assignment applies to all
of those different kind of errors.

00:03:45.120 --> 00:03:47.380
Several Herculists, Marvin Minsky for

00:03:47.380 --> 00:03:52.660
example, consider credit assignment to
be the central problem in learning.

00:03:52.660 --> 00:03:56.630
This is because error agents
live in dynamic worlds.

00:03:56.630 --> 00:04:00.390
Therefore, we'll never be able to
create an error agent which is perfect.

00:04:00.390 --> 00:04:04.057
Even if you were to create an error
agent which had complete knowledge and

00:04:04.057 --> 00:04:06.342
perfect reasoning that
lives in some world,

00:04:06.342 --> 00:04:09.190
the world around it
would changed over time.

00:04:09.190 --> 00:04:11.720
As it changes,
the agent will start failing.

00:04:11.720 --> 00:04:15.590
Once it starts failing, it must have
the ability of correcting itself,

00:04:15.590 --> 00:04:18.350
of correcting its known knowledge,
correcting its own reasoning,

00:04:18.350 --> 00:04:20.050
correcting its own architecture.

00:04:20.050 --> 00:04:23.545
You can see again how this
a record of meta cognition.

00:04:23.545 --> 00:04:27.005
The agent is not diagnosing
some electrical circuit or

00:04:27.005 --> 00:04:30.115
a car or software program outside.

00:04:30.115 --> 00:04:33.145
Instead, it is self diagnosing,
self repairing.

