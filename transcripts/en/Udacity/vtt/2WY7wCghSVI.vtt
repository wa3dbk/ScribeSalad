WEBVTT
Kind: captions
Language: en

00:00:00.080 --> 00:00:02.580
Okay Michael, so the second algorithm that we're going to

00:00:02.580 --> 00:00:05.270
look at it just like principle components analysis, except it's

00:00:05.270 --> 00:00:08.780
called independent components analysis. Okay, and the major difference is

00:00:08.780 --> 00:00:12.240
really the difference between the first word principle and independent.

00:00:12.240 --> 00:00:15.810
So the main idea here is that PCA is about

00:00:15.810 --> 00:00:18.500
finding correlation. And the way it does that is by

00:00:18.500 --> 00:00:21.990
maximizing variance. And what that gives you, is the ability

00:00:21.990 --> 00:00:25.330
to do reconstruction. What independent components analysis is doing, or

00:00:25.330 --> 00:00:31.090
often called ICA by those in the know, is it's trying to maximize independence.

00:00:31.090 --> 00:00:33.330
Very simply put, it tries to find

00:00:33.330 --> 00:00:36.690
a linear transformation of your feature space, into

00:00:36.690 --> 00:00:42.810
a new feature space, such that each of the individual new features are mutually

00:00:42.810 --> 00:00:44.470
independent and I mean that in a

00:00:44.470 --> 00:00:49.290
statistical sense. So, you are converting your XI,

00:00:49.290 --> 00:00:55.850
your X1, X2, XI... And there's some new features space let's call it I don't

00:00:55.850 --> 00:01:04.019
know let's call it a Y, Y1,Y2 YI... Such that each

00:01:04.019 --> 00:01:09.280
one of the new features are statistically independent of one another, that is

00:01:09.280 --> 00:01:13.510
to say their mutual information is equal to zero. Does that make sense?

00:01:13.510 --> 00:01:14.390
&gt;&gt; And

00:01:14.390 --> 00:01:16.320
this is going to be a linear transformation?

00:01:16.320 --> 00:01:18.360
&gt;&gt; It's going to be a linear transformation.

00:01:18.360 --> 00:01:21.930
&gt;&gt; T, to make them statistically independent.

00:01:21.930 --> 00:01:26.010
&gt;&gt; So, I find some linear transformation here, which is going to take my

00:01:26.010 --> 00:01:28.640
original feature space, which I'm representing with

00:01:28.640 --> 00:01:30.760
these X's, and transform it into a

00:01:30.760 --> 00:01:34.400
new feature space, such that, if I were to treat each of these new

00:01:34.400 --> 00:01:36.800
features as random variables and compute their

00:01:36.800 --> 00:01:39.590
mutual information, I would get for all

00:01:39.590 --> 00:01:43.730
pairs a mutual information of zero. That's part one and

00:01:43.730 --> 00:01:45.330
the second thing that it's trying to do is that

00:01:45.330 --> 00:01:49.240
it's trying to make certain that the mutual information between

00:01:49.240 --> 00:01:54.280
all of the features, y and the original feature space x

00:01:54.280 --> 00:01:56.540
is as high as possible. So in other words, we

00:01:56.540 --> 00:01:58.910
want to be able to reconstruct the data. We want

00:01:58.910 --> 00:02:04.840
to be able to predict an X from a Y and a Y from a X. While at the same time

00:02:04.840 --> 00:02:07.200
making sure that each of the new dimensions is in

00:02:07.200 --> 00:02:10.620
&gt;&gt; fact mutually independent. In a statistical sense.

00:02:10.620 --> 00:02:12.490
&gt;&gt; I think I'm going to need an example.

