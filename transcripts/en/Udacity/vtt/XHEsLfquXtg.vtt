WEBVTT
Kind: captions
Language: en

00:00:00.510 --> 00:00:04.370
You can also write at those gates as
multiplying the inputs either by 0,

00:00:04.370 --> 00:00:08.990
when the gate is closed, or
by 1, when the gate is open.

00:00:08.990 --> 00:00:12.400
That's all fine, but what does it
have to do with neural networks?

00:00:12.400 --> 00:00:15.750
Imagine that instead of having
binary decisions at each gate,

00:00:15.750 --> 00:00:17.990
you had continuous decisions.

00:00:17.990 --> 00:00:22.820
For example, instead of yes-no gate
at the input, you take the input and

00:00:22.820 --> 00:00:25.700
multiply it by a value
that's between 0 and 1.

00:00:25.700 --> 00:00:28.880
If it's exactly 0, no input comes in.

00:00:28.880 --> 00:00:32.060
If it's exactly 1,
you write it entirely to memory.

00:00:32.060 --> 00:00:35.610
Anything in between can be
added partially to the memory.

00:00:37.028 --> 00:00:38.770
Now that becomes very interesting for

00:00:38.770 --> 00:00:43.190
us, because if that multiplicative
factor is a continuous function

00:00:43.190 --> 00:00:47.180
that's also differentiable, that
means we can take derivatives of it.

00:00:47.180 --> 00:00:49.280
And that also means we can
back propagate through it.

00:00:50.530 --> 00:00:52.930
That's exactly what an LSTM is.

00:00:52.930 --> 00:00:54.950
We take this simple model of memory,

00:00:54.950 --> 00:00:57.820
we replace everything with
continuous functions.

00:00:57.820 --> 00:01:00.550
And make that the new Lowell
machine that's at the heart

00:01:00.550 --> 00:01:01.850
of a recurrent neural network.

