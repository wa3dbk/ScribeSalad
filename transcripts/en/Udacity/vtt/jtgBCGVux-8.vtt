WEBVTT
Kind: captions
Language: en

00:00:00.310 --> 00:00:04.010
Maximal matchings are a useful tool for
getting a coarsened graph.

00:00:04.010 --> 00:00:06.160
All we need now is
an algorithm to compute one.

00:00:06.160 --> 00:00:10.670
Let me describe a simple scheme
to you based on randomization.

00:00:10.670 --> 00:00:15.170
At each step of this scheme, you'll
choose any unmatched vertex at random.

00:00:15.170 --> 00:00:19.600
So initially, no edges are matched,
so let's pick any vertex.

00:00:19.600 --> 00:00:21.090
How about this one?

00:00:21.090 --> 00:00:25.190
Next, you'll match this vertex to
one of it's unmatched neighbors.

00:00:25.190 --> 00:00:29.060
Again, since we're just starting,
all of the neighbors are unmatched.

00:00:29.060 --> 00:00:30.430
So how do we pick one?

00:00:30.430 --> 00:00:32.630
Well, there are lots of strategies.

00:00:32.630 --> 00:00:34.960
For example,
you could just pick one at random.

00:00:34.960 --> 00:00:39.020
A different approach is something
called a heavy edge matching strategy.

00:00:39.020 --> 00:00:40.270
So heavy.

00:00:40.270 --> 00:00:42.250
So heavy.

00:00:42.250 --> 00:00:45.900
The idea is to look at the unmatched
edge with the highest weight and

00:00:45.900 --> 00:00:47.120
then choose it.

00:00:47.120 --> 00:00:49.085
For instance,
suppose these are the edge weights.

00:00:49.085 --> 00:00:53.358
Then among the two unmatched neighbors,
this edge has the higher edge weight,

00:00:53.358 --> 00:00:55.020
two versus one.

00:00:55.020 --> 00:00:56.830
So you'd choose it.

00:00:56.830 --> 00:01:00.280
But why is the heavy edge
matching strategy a good one?

00:01:00.280 --> 00:01:04.060
In fact, there's not a whole lot of
definitive rigorous analysis out there.

00:01:04.060 --> 00:01:06.760
But there is a lot of
experimental evidence.

00:01:06.760 --> 00:01:10.260
What I want to do now is just
give you some informal intuition.

00:01:10.260 --> 00:01:12.790
Consider some graph Gi.

00:01:12.790 --> 00:01:15.310
Suppose we were to
match these two edges.

00:01:15.310 --> 00:01:18.020
Contracting them would
yield a course in graph.

00:01:18.020 --> 00:01:19.920
Let's call it Gi + 1.

00:01:19.920 --> 00:01:23.410
Note the edge weight of two that
connects the two super vertices.

00:01:24.460 --> 00:01:28.710
Now, if you were to sum the edge weights
in the original graph, you'd get 12.

00:01:28.710 --> 00:01:31.690
That's because there are 12
edges each of unit weight.

00:01:31.690 --> 00:01:34.570
Here let me use the notation X of some

00:01:34.570 --> 00:01:37.790
object to count the number
of edges in that object.

00:01:37.790 --> 00:01:39.978
Now the sum of the weights
on the final graph is 10.

00:01:39.978 --> 00:01:44.520
This is just the total edge weight of
the original graph minus the edge weight

00:01:44.520 --> 00:01:46.550
of the matched edges.

00:01:46.550 --> 00:01:49.340
Here, let Mi denote the matching.

00:01:49.340 --> 00:01:52.110
Now, the strategy of
selecting heavy edges will

00:01:52.110 --> 00:01:55.315
tend to try to increase this factor.

00:01:55.315 --> 00:01:59.655
And if this factor is bigger,
then this factor will be smaller.

00:01:59.655 --> 00:02:03.325
In other words, the heavy edge
heuristic is really about increasing

00:02:03.325 --> 00:02:07.550
this term in order to decrease the
overall edge weight in the next graph.

00:02:07.550 --> 00:02:10.650
The heuristic idea is that
the smaller this term is,

00:02:10.650 --> 00:02:14.470
the more likely it is that all
the edges you'll cut will be small.

00:02:14.470 --> 00:02:17.510
Again, this is clearly
not a rigorous proof.

00:02:17.510 --> 00:02:19.900
In fact,
there are a ton of other heuristics.

00:02:19.900 --> 00:02:22.190
But most of the evidence is empirical.

00:02:22.190 --> 00:02:25.380
Anyway, if this is a topic of interest
to you, head to the instructors notes or

00:02:25.380 --> 00:02:26.440
the class forum for

00:02:26.440 --> 00:02:29.690
pointers to papers that will discuss
all these issues in a lot of detail.

