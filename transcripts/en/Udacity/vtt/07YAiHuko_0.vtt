WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.292
You replace memcachedb with Cassandra
for the precompute stuff?

00:00:03.292 --> 00:00:06.000
That solved the right contention issue.

00:00:06.646 --> 00:00:08.000
At that point,

00:00:11.000 --> 00:00:15.000
Every listing has a lock --you have to lock around.

00:00:15.000 --> 00:00:18.000
So you have this listing, which is a set of IDs

00:00:18.000 --> 00:00:21.000
of all of the links in a subreddit, right?

00:00:21.000 --> 00:00:23.000
And you have to modify that.

00:00:23.000 --> 00:00:25.000
You have to lock around that item,

00:00:25.000 --> 00:00:27.000
and that's at the subreddit level.

00:00:27.000 --> 00:00:30.000
If a lot of people are hanging out in one subreddit,

00:00:30.000 --> 00:00:32.000
that thing will have a lot of locking going on.

00:00:32.000 --> 00:00:39.000
We talked a little bit just in the
office hours I think for Unit 4,

00:00:39.000 --> 00:00:41.353
somebody had asked about

00:00:41.353 --> 00:00:46.000
in datastore, if two users tried to register
the same username at the same time,

00:00:46.000 --> 00:00:48.815
and datastore doesn't enforce
any uniqueness constraints

00:00:48.815 --> 00:00:53.000
on a field in the database.
How do you manage that?

00:00:53.000 --> 00:00:55.492
And my answer was

00:00:55.492 --> 00:00:59.000
you can either use transactions in the
datastore, which I don't fully understand,

00:00:59.000 --> 00:01:02.000
because I'm new to a datastore,
or you can use memcache,

00:01:02.000 --> 00:01:04.000
which is one of the 101 uses of memcache,

00:01:04.000 --> 00:01:07.000
it has this global lock.

00:01:07.000 --> 00:01:11.000
So you guys are moving away from that?
&gt;&gt;Yes.

00:01:11.000 --> 00:01:13.000
When you use memcache as a locking service,

00:01:13.000 --> 00:01:15.000
the problem is

00:01:15.000 --> 00:01:18.000
if you lose a single memcache node,

00:01:18.000 --> 00:01:20.000
then you lose the site,

00:01:20.000 --> 00:01:22.000
because you can't throw away that node

00:01:22.000 --> 00:01:25.000
without potentially,

00:01:25.000 --> 00:01:28.000
say half the apps can't see a memcache node,

00:01:28.000 --> 00:01:30.000
and the others can.

00:01:30.000 --> 00:01:32.000
So the ones that don't see
it decide that they're not

00:01:32.000 --> 00:01:34.000
gonna talk to that guy anymore,
and they try

00:01:34.000 --> 00:01:35.000
to lock on a different set of servers.

00:01:35.000 --> 00:01:38.400
When we store data on multiple
nodes of Cassandra.

00:01:38.415 --> 00:01:40.000
It's different from how we store data

00:01:40.000 --> 00:01:42.000
across different nodes of memcache,

00:01:42.000 --> 00:01:46.000
which is--correct me if I'm wrong--when
you distribute across memcache,

00:01:46.000 --> 00:01:49.000
you basically hash your key to
a particular memcache box,

00:01:49.000 --> 00:01:51.000
and there's no notion of replication.

00:01:51.000 --> 00:01:54.000
Yeah, it's similar in Cassandra.

00:01:54.000 --> 00:01:57.000
This node has a key space,

00:01:57.000 --> 00:02:01.000
and it just happens to go +1 and -1.

00:02:01.000 --> 00:02:04.000
And you can do the same. You can do
replication like that in memcache, as well.

00:02:04.000 --> 00:02:07.000
Now, were you guys using--

00:02:07.000 --> 00:02:10.000
when I left, we were using a
naive memcache library

00:02:10.000 --> 00:02:13.000
that basically took a key and said,

00:02:13.000 --> 00:02:18.000
which box is this hashed to, and
would store it on that box.

00:02:18.000 --> 00:02:21.000
If you lost that box, it would effect
the hashing of every other key.

00:02:21.000 --> 00:02:24.000
Losing a memcache box was really painful.

00:02:24.000 --> 00:02:26.000
So we'd replicate memcaches for mostly space.

00:02:26.000 --> 00:02:30.000
We had more space--things wouldn't
expire out of the cache fast enough

00:02:30.000 --> 00:02:33.000
or would expire out of the cache too fast.

00:02:33.000 --> 00:02:35.000
But that had the big balance setup that

00:02:35.000 --> 00:02:37.230
we lost a single memcache box

00:02:37.276 --> 00:02:40.000
all of the keys would get rearranged.

00:02:40.000 --> 00:02:43.323
Right. Module hashing.

00:02:43.753 --> 00:02:47.000
We are using consistent hashing now.

00:02:47.000 --> 00:02:49.000
The way that works is

00:02:49.000 --> 00:02:53.000
it basically builds up this ring similar to this,

00:02:53.000 --> 00:02:56.000
and instead of mapping

00:02:56.000 --> 00:02:59.000
keys 1 through 1,000 to this box,

00:02:59.000 --> 00:03:02.000
it actually sort of assigns them
to a place on a circle

00:03:02.000 --> 00:03:06.000
and finds the nearest
server on that circle.

00:03:06.000 --> 00:03:09.000
When this node fails, it will
just go to the next one.

00:03:09.000 --> 00:03:12.000
We have 10 nodes, we lose 1.

00:03:12.000 --> 00:03:15.000
Only 1/10th of your keys get redistributed.

00:03:15.000 --> 00:03:18.000
If you're using consistent hashing,

00:03:18.000 --> 00:03:21.000
a key might hash to this space on a circle.

00:03:21.000 --> 00:03:24.000
And if you lose this guy, the key
still goes in that space,

00:03:24.000 --> 00:03:28.000
but all of the keys over here
stay in the same spot.

00:03:28.000 --> 00:03:30.000
When you're using modular hashing--

00:03:30.000 --> 00:03:32.723
remember we talked about modular in unit 6--

00:03:32.723 --> 00:03:36.261
that's a really naive way to distribute keys.

00:03:36.261 --> 00:03:41.723
But all of a sudden you go from modular 10,
because you have 10 servers, to modular 9.

00:03:41.723 --> 00:03:45.000
Instead of losing 1/10th of your keys,

00:03:45.000 --> 00:03:47.600
you lose 9/10th of your keys;
that's a big problem.

00:03:47.600 --> 00:03:51.000
We actually had this happen a couple
of times while I was there.

00:03:51.000 --> 00:03:53.000
You lose one memcache box,

00:03:53.000 --> 00:03:55.000
or more likely, you misconfigure it,

00:03:55.000 --> 00:03:58.000
and the app service doesn't see it anymore--

00:03:58.000 --> 00:04:01.000
All of the sudden, your cache is not warm anymore,

00:04:01.000 --> 00:04:03.000
and you've got users who are just like

00:04:03.000 --> 00:04:08.000
hitting the databases, pummeling the
hard cache or the memchache.

00:04:08.000 --> 00:04:10.000
We call it the hard cache.

00:04:10.000 --> 00:04:13.861
And all of a sudden, your load profile completely
changes, and everything explodes.

00:04:14.430 --> 00:04:17.000
That was a really nice improvement.

