WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.220
And if I can only run 1 block on that SM,

00:00:04.220 --> 00:00:08.170
then I'm getting a maximum of 1024 threads on that SM.

00:00:08.170 --> 00:00:12.992
So my occupancy is not as high as the SM could handle.

00:00:12.992 --> 00:00:17.098
You can get these vital statistics for the particular GPU you're programming to

00:00:17.098 --> 00:00:22.192
by looking it up in a CUDA reference manual or by making various CUDA calls on the GPU.

00:00:22.192 --> 00:00:26.611
And the CUDA SDK ships with a sample called deviceQuery—we saw it earlier—

00:00:26.611 --> 00:00:30.933
that conveniently collects all of this information into a single useful utility.

00:00:30.933 --> 00:00:37.250
Earlier we did a print out of the device query, the result of deviceQuery on my GPU.

00:00:37.250 --> 00:00:39.088
Let's look at that again.

00:00:39.088 --> 00:00:41.976
It prints out a lot of information.

00:00:41.976 --> 00:00:45.251
Earlier we were using this to estimate the total peak bandwidth

00:00:45.251 --> 00:00:48.223
by looking at the memory clock rate and the memory bus width.

00:00:48.223 --> 00:00:52.953
Now I want to draw your attention to the total amount of shared memory per block,

00:00:52.953 --> 00:00:55.842
the total number of registers per block,

00:00:55.842 --> 00:00:59.213
the maximum number of threads per multiprocessor or SM,

00:00:59.213 --> 00:01:01.702
the maximum number of threads per block.

00:01:01.702 --> 00:01:05.235
And here's the register and shared memory usage

00:01:05.235 --> 00:01:07.573
for our tiled transpose kernel.

00:01:07.573 --> 00:01:10.362
Going back into NVPP, we can see that we're launching

00:01:10.362 --> 00:01:14.763
a grid size of 32 x 32, a block size of 32 x 32,

00:01:14.763 --> 00:01:16.874
we're using 7 registers per thread,

00:01:16.874 --> 00:01:19.778
and we're requesting 4 kilobytes of shared memory per block.

00:01:19.778 --> 00:01:23.503
Given the register and shared memory usage for the that kernel we just saw

00:01:23.503 --> 00:01:28.655
and given the GPU statistics from deviceQuery running on my laptop that we just saw,

00:01:28.655 --> 00:01:31.800
how many thread blocks per SM can we run?

00:01:31.800 --> 00:01:34.546
And how many threads per SM can we run?

00:01:34.546 --> 00:01:37.797
And which resources are preventing us from running more?

00:01:37.797 --> 00:01:41.120
Are we limited by the maximum number of threads per SM

00:01:41.120 --> 00:01:43.902
or the maximum number of registers per SM,

00:01:43.902 --> 00:01:46.773
or the maximum amount of shared memory per SM,

00:01:46.773 --> 00:01:50.154
or by the maximum number of thread blocks per SM?

