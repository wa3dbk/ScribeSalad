WEBVTT
Kind: captions
Language: en

00:00:00.130 --> 00:00:01.783
Okay, so let's quickly recap.

00:00:01.783 --> 00:00:06.250
We had a serial version of the code that did everything in a single thread--

00:00:06.250 --> 00:00:10.892
trivial to write that code, 0 parallelism, and pretty terrible performance.

00:00:10.892 --> 00:00:17.467
Then we had a parallel per row version of the code that was also pretty simple to write

00:00:17.467 --> 00:00:21.895
and assigned 1 thread per row and a single thread block--

00:00:21.895 --> 00:00:24.333
again did not get great performance,

00:00:24.333 --> 00:00:27.575
although a huge improvement factor of about a hundred.

00:00:27.575 --> 00:00:32.197
Then in the third version, we extracted pretty much all the parallelism that the problem has to

00:00:32.197 --> 00:00:34.796
offer and assigned one thread per element to the matrix.

00:00:34.796 --> 00:00:37.953
So we did this transposed operation now and sort of massive parallel.

00:00:37.953 --> 00:00:41.796
And now we got quite a bit better performance.

00:00:41.796 --> 00:00:49.823
Remember that all this is happening in the context of APOD, Analyze, Parallelize, Optimize and Deploy.

00:00:49.823 --> 00:00:54.707
And this actually might be fast enough that you're ready to deploy it.

00:00:54.707 --> 00:01:00.712
So we started by looking at analyzing the code and deciding that this function of

00:01:00.712 --> 00:01:02.565
transposing matrix need to be sped up.

00:01:02.565 --> 00:01:07.537
We started exploring ways to parallelize it. And this might be fast enough.

00:01:07.537 --> 00:01:11.707
This is already a place where you ought to look at this and say is this the bottleneck anymore?

00:01:11.707 --> 00:01:16.079
Is speeding this up going to make a big difference to my application? If not, then deploy.

00:01:16.079 --> 00:01:19.163
If so, then we can start thinking about optimizing it.

00:01:19.163 --> 00:01:21.268
And that was the next thing we did.

00:01:21.268 --> 00:01:26.163
We looked at the performance here we saw that we were getting pretty poor DRAM utilization.

00:01:26.163 --> 00:01:32.139
And we diagnosed that the problem must be that our global stores were getting bad coalescing.

00:01:32.139 --> 00:01:37.829
In other words, when we were writing the output matrix we were getting bad coalescing in those accesses to global memory.

00:01:37.829 --> 00:01:41.709
So to fix that we came up with the tiled version of our code, where each thread

00:01:41.709 --> 00:01:45.620
block is responsible for reading a tile of the input matrix, transposing it

00:01:45.620 --> 00:01:48.908
and writing that transposed matrix to its location in the output matrix.

00:01:48.915 --> 00:01:52.668
And it can perform these reads and writes in a coalesced fashion.

00:01:52.668 --> 00:01:58.207
We did this in blocks of 32 by 32 tiles and got excellent coalescing.

00:01:58.207 --> 00:02:03.340
This improved our speed a little bit. But we're still not getting great DRAM utilization.

00:02:03.340 --> 00:02:06.012
So, we looked into why.

00:02:06.012 --> 00:02:11.622
We thought a little bit more about why we wouldn't have taken great improvement in bandwidth.

00:02:11.622 --> 00:02:15.156
We concluded we probably had too many threads waiting at barriers.

00:02:15.187 --> 00:02:18.171
So we improved that by shrinking the tile size.

00:02:18.171 --> 00:02:21.765
Experimenting with different tile sizes led us to conclude that a 16 by 16

00:02:21.765 --> 00:02:27.285
thread block writing into a 16 by 16 tile ensured memory, struck a good balance

00:02:27.285 --> 00:02:31.404
between achieving coalesced writes, reads and writes to memory, and not spending

00:02:31.404 --> 00:02:34.000
too much time waiting at barriers.

