WEBVTT
Kind: captions
Language: en

00:00:00.140 --> 00:00:01.850
So at this point, I should stop and talk a

00:00:01.850 --> 00:00:04.000
little bit about the type of data that we'll be working

00:00:04.000 --> 00:00:06.210
with, in a number of places in the rest of

00:00:06.210 --> 00:00:09.790
this course. So I'm sure you're aware that Wikipedia, in addition

00:00:09.790 --> 00:00:12.450
to the article text for many types of data, also

00:00:12.450 --> 00:00:16.660
includes info box data. Now, info box data is essentially structure

00:00:16.660 --> 00:00:20.800
data that complements the subject of the article. In this

00:00:20.800 --> 00:00:25.030
case, this is an article for the city of Hyderabad and,

00:00:25.030 --> 00:00:26.860
in addition to a description of the city

00:00:26.860 --> 00:00:30.020
here and other interesting information about the city,

00:00:30.020 --> 00:00:33.010
we have data in the form of, population,

00:00:34.010 --> 00:00:37.170
land area in square kilometers, as well as a

00:00:37.170 --> 00:00:39.100
number of other useful pieces of data about

00:00:39.100 --> 00:00:42.820
this city. Now, Wikipedia has info boxes for a

00:00:42.820 --> 00:00:46.090
lot of different types of data. People, animals,

00:00:46.090 --> 00:00:50.160
automobiles and so on. An organization called DBpedia has

00:00:50.160 --> 00:00:52.980
taken the info box data from Wikipedia and produced it as

00:00:52.980 --> 00:00:55.990
a collection of data sets that we can download. Here's a

00:00:55.990 --> 00:00:58.960
description of some of the data sets themselves and some of

00:00:58.960 --> 00:01:02.010
the details about them. Now, in our case, we're going to

00:01:02.010 --> 00:01:06.080
be working with the city's data set. This data is distributed

00:01:06.080 --> 00:01:09.200
as a CSV file that we can download. It contains all

00:01:09.200 --> 00:01:11.960
of the info box data for a particular snapshot in time

00:01:11.960 --> 00:01:15.614
for all cities described on Wikipedia, for which there is info

00:01:15.614 --> 00:01:18.940
box data. Now, in order to prepare this data as

00:01:18.940 --> 00:01:22.120
CSV, the DBPedia folks had to do a little bit

00:01:22.120 --> 00:01:24.670
of work in encoding it, to get certain types of

00:01:24.670 --> 00:01:27.910
data values into individual cells. Let's take a look at

00:01:27.910 --> 00:01:30.760
the city's data set. We're going to look here, in

00:01:30.760 --> 00:01:34.030
Excel, because this data is so dense, we really do

00:01:34.030 --> 00:01:37.160
need the added structure that the spreadsheet app provides. So

00:01:37.160 --> 00:01:40.730
here's a row for Hyderabad. And, there are about 20,000

00:01:40.730 --> 00:01:43.930
rows in this CSV file; so, lots of cities

00:01:43.930 --> 00:01:46.260
are represented here. And, we can see that there are

00:01:46.260 --> 00:01:49.670
lots of null cell values. This is coding in this

00:01:49.670 --> 00:01:52.710
data set which indicates that there is no value supplied

00:01:52.710 --> 00:01:55.732
for that particular piece of data. Now, something that's

00:01:55.732 --> 00:01:58.212
also important to bear in mind is that while this

00:01:58.212 --> 00:02:01.374
data is actually in pretty good shape, it's entirely human

00:02:01.374 --> 00:02:06.329
generated. You can edit it by editing the Wikipedia page,

00:02:06.329 --> 00:02:09.823
as usual. There is some of that data here. So

00:02:09.823 --> 00:02:12.274
what this means of course, is that there's a lot of

00:02:12.274 --> 00:02:15.130
dirty data here. Data that's going to need to be cleaned

00:02:15.130 --> 00:02:18.230
up. So this data set provides a nice example of wrangling

00:02:18.230 --> 00:02:20.930
data, because we're going to have to decode some of the

00:02:20.930 --> 00:02:23.120
fields in order to get them into the form that we

00:02:23.120 --> 00:02:26.080
want. We're going to be looking at transforming these lines of

00:02:26.080 --> 00:02:31.170
data into JSON documents, and then, eventually, storing them into MongoDB.

00:02:31.170 --> 00:02:32.720
Let's take a look at a couple of examples

00:02:32.720 --> 00:02:36.510
of the task that we have ahead of us. Okay,

00:02:36.510 --> 00:02:39.500
this is the postal code field, and we can see

00:02:39.500 --> 00:02:43.200
that there are lots of different formats represented here. Something

00:02:43.200 --> 00:02:45.560
else that's interesting about this data, is the way

00:02:45.560 --> 00:02:50.250
they've encoded arrays. Some fields have multiple values stored within

00:02:50.250 --> 00:02:54.150
them. Here's the utcOffset field. And in this case, we

00:02:54.150 --> 00:02:56.590
can see that there are two values for the utcOffset

00:02:56.590 --> 00:02:59.790
for this city and a couple of others here. There's two

00:02:59.790 --> 00:03:03.310
different values that represent the utcOffset for when the city is on

00:03:03.310 --> 00:03:07.350
Daylight Savings Time versus when it's not. And you can see that

00:03:07.350 --> 00:03:11.470
we have some HTML entities represented here, and some Unicode characters, and

00:03:11.470 --> 00:03:13.302
some other characters that we're going to need to a little

00:03:13.302 --> 00:03:15.670
work with. So we've got our work cut out for us with

00:03:15.670 --> 00:03:18.700
this data in terms of pulling it out of the CSV form,

00:03:18.700 --> 00:03:21.920
parsing it into individuals fields, and cleaning up some of the problematic

00:03:21.920 --> 00:03:25.080
data that's here. After that, then we'll take a look

00:03:25.080 --> 00:03:28.182
at getting into MangoDB. So as you can see this

00:03:28.182 --> 00:03:30.500
data set provides a nice opportunity for us to explore

00:03:30.500 --> 00:03:33.492
things like auditing a data set, to see where the

00:03:33.492 --> 00:03:36.850
problems lie. And then thinking about exactly what we should

00:03:36.850 --> 00:03:39.530
do with fields like this and other structures that we're

00:03:39.530 --> 00:03:42.140
going to find in this data set. And then finally,

00:03:42.140 --> 00:03:44.170
we'll take a look at putting this data into MangoDB.

