WEBVTT
Kind: captions
Language: en

00:00:00.400 --> 00:00:04.553
So Raid 0 uses a technique called
striping to improve performance.

00:00:04.553 --> 00:00:08.610
So if we have one disk,
it will have a Track 0,

00:00:08.610 --> 00:00:13.490
Track 1, Track 2, et cetera,
and the problem is that

00:00:13.490 --> 00:00:18.280
while the head is positioned to read
Track 0, it cannot be reading Track 1,

00:00:18.280 --> 00:00:22.150
and also its very far away from
all of these other tracks.

00:00:22.150 --> 00:00:26.530
So what happens is really, we have to
serialize accesses to all of the tracks.

00:00:26.530 --> 00:00:29.380
If we need to read Tracks 0,

00:00:29.380 --> 00:00:32.229
17 and 27, we will have to decide
in what order we access them.

00:00:32.229 --> 00:00:35.880
But for each of these accesses,
we move the we read the track.

00:00:35.880 --> 00:00:37.640
We move on to something else.

00:00:37.640 --> 00:00:42.630
So we cannot get more performance
than just reading one track at a time

00:00:42.630 --> 00:00:44.970
at the current rotational
speed of the disk.

00:00:44.970 --> 00:00:48.670
What rate 0 does is it
takes two disks and

00:00:48.670 --> 00:00:54.360
makes them look like this one disk,
by putting the original Track 0 here,

00:00:54.360 --> 00:00:59.770
Track 1 here, Track 2 here,
Track 3 here, and so on.

00:00:59.770 --> 00:01:05.180
So the idea is that now each of
these tracks is called a stripe now,

00:01:05.180 --> 00:01:09.680
and we're going to put stripe 0 really
here, stripe 1 here, and so on.

00:01:09.680 --> 00:01:15.520
So Track 0 of the first disk gets
Track 0 from the overall large disk.

00:01:15.520 --> 00:01:18.810
Track 0 here, however,
gets the stripe zero.

00:01:18.810 --> 00:01:24.870
So instead of calling this tracks,
we call them stripes to avoid confusion

00:01:24.870 --> 00:01:30.080
between what's a track on a physical
disk and these things that

00:01:30.080 --> 00:01:35.070
really are tracks that would belong
to a fake disk that we don't really

00:01:35.070 --> 00:01:38.999
have because really these two disks
are pretending to be this one disk.

00:01:40.010 --> 00:01:42.050
As far as performance is concerned,

00:01:42.050 --> 00:01:45.260
we get twice the data throughput
if we have two discs.

00:01:45.260 --> 00:01:45.910
Why?

00:01:45.910 --> 00:01:48.640
Well, because, while the first
disk is reading some data,

00:01:48.640 --> 00:01:50.730
the second could be reading data, too.

00:01:50.730 --> 00:01:52.330
And because controllers and

00:01:52.330 --> 00:01:55.630
buses today are faster than
what the disk can get to.

00:01:55.630 --> 00:01:58.960
Effectively we can transfer
twice the amount of data

00:01:58.960 --> 00:02:01.740
assuming that those disks
are actually transferring the data.

00:02:01.740 --> 00:02:03.040
You can be unlucky and

00:02:03.040 --> 00:02:07.110
all the data you want is on the stripes
that are placed in this disk in which

00:02:07.110 --> 00:02:10.840
case you're going to get only
the data throughput of a single disk.

00:02:10.840 --> 00:02:15.740
But on average we tend to get almost
twice the throughput of a single disk

00:02:15.740 --> 00:02:18.730
by splitting the data
across two disks like this.

00:02:18.730 --> 00:02:20.780
As far as latency is concerned,

00:02:20.780 --> 00:02:24.070
because we have more throughput
we get less curing delay.

00:02:24.070 --> 00:02:27.260
Normally we would be able to handle
requests one at a time here.

00:02:27.260 --> 00:02:30.270
Now we can actually in
parallel do two requests.

00:02:30.270 --> 00:02:34.150
So in the queue,
we get faster to the requests

00:02:34.150 --> 00:02:38.560
by basically handling
previous requests faster.

00:02:38.560 --> 00:02:40.850
But what about reliability?

00:02:40.850 --> 00:02:44.000
Unfortunately, it gets
worse than one disk.

