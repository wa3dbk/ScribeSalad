WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.000
[Sebastian:] Let me talk about dynamic programming with stochastic actions.

00:00:04.000 --> 00:00:08.000
At the end of this assignment, you'll be able to program this.

00:00:08.000 --> 00:00:10.000
The motivation to study this is as follows:

00:00:10.000 --> 00:00:13.000
suppose we have a robot, and we have an obstacle and a goal state.

00:00:13.000 --> 00:00:19.000
Then this robot might be tempted to scratch the wall over here and head towards the goal.

00:00:19.000 --> 00:00:24.000
In practice this is a bad idea, because robots that get really close to obstacles are frightening.

00:00:24.000 --> 00:00:30.000
In fact, if their actions aren't quite perfect they might actually hit the obstacle, which is no good.

00:00:30.000 --> 00:00:33.000
In practice, what we tend to do is we go more like this.

00:00:33.000 --> 00:00:36.000
We maintain a certain clearance to obstacles.

00:00:36.000 --> 00:00:40.000
Now we're going to modify our dynamic programming planner to do just that--

00:00:40.000 --> 00:00:45.000
to find paths that are a little bit safer and lead a little bit away from obstacles.

00:00:45.000 --> 00:00:49.000
The way we will do this is we will model actions as stochastic,

00:00:49.000 --> 00:00:53.000
and we have not done this before, but you'll learn how to do it.

00:00:53.000 --> 00:00:56.000
Take the action of going north from the from the cell over here.

00:00:56.000 --> 00:01:01.000
In a deterministic action, it obviously succeeds, unless of course we run into a wall.

00:01:01.000 --> 00:01:05.000
In the stochastic case, it succeeds with a certain probability--say 50%--

00:01:05.000 --> 00:01:11.000
whereas with 25% chance, it might accidentally go left or right

00:01:11.000 --> 00:01:14.000
even though we commanded it to go up.

00:01:14.000 --> 00:01:19.000
This is a stochastic action, and it's exactly the type of action I want you to program later on.

00:01:19.000 --> 00:01:23.000
If, for example, there was a wall over here and the robot decided to go up,

00:01:23.000 --> 00:01:27.000
there'd be a 25% chance it would hit the wall, which is not good.

00:01:27.000 --> 00:01:32.000
So staying away from the wall is a good idea in a stochastic situation.

00:01:32.000 --> 00:01:37.000
I will now show you how the backup works without asking or quizzes.

00:01:37.000 --> 00:01:42.000
Suppose we study the action of going up over here, and we have a 50% of success.

00:01:42.000 --> 00:01:46.000
Suppose at the time we do the backup, the value over here is 10,

00:01:46.000 --> 00:01:50.000
20 over here, and 40 on the right.

00:01:50.000 --> 00:01:56.000
Then the value of the state for the action "go up" would be obtained as follows.

00:01:56.000 --> 00:02:06.000
It's a 50% chance times 10, 25% times 20, and 25% chance times 40 plus 1,

00:02:06.000 --> 00:02:16.000
which is our motion cost, which together gives me 5 plus 5 plus 10 plus 1 equals 21.

00:02:16.000 --> 00:02:19.000
That is the backed up value over here for the action go up,

00:02:19.000 --> 00:02:23.000
and you can do the same with go right, go left, go down.

00:02:23.000 --> 00:02:27.000
The interesting case in your implementation is the one for an action

00:02:27.000 --> 00:02:30.000
that either runs off the grid or into a wall.

00:02:30.000 --> 00:02:34.000
Let's study the case of going north from this cell over here

00:02:34.000 --> 00:02:39.000
where there is a 25% chance of falling off the grid.

00:02:39.000 --> 00:02:43.000
Then the value for the cell and the action of going north

00:02:43.000 --> 00:02:52.000
is 50% times 20, which is 10, 25% chance times 40, which is also 10,

00:02:52.000 --> 00:02:54.000
and then we have this case where we fall off the wall.

00:02:54.000 --> 00:03:00.000
Let's define the penalty for falling off the grid or for hitting an obstacle as 100.

00:03:00.000 --> 00:03:04.000
So we add 25% chance times 100 plus 1, 1,

00:03:04.000 --> 00:03:10.000
hence the new value is 10 plus 10 plus 25 plus, which is 46.

00:03:10.000 --> 00:03:16.000
I want your update to produce for the cell over here 46 for this specific action of going up.

00:03:16.000 --> 00:03:19.000
Of course, the value might be smaller because there might be a better action

00:03:19.000 --> 00:03:21.000
like the action of going right.

00:03:21.000 --> 00:03:25.000
But for this specific calculation you should receive 46.

00:03:25.000 --> 00:03:30.000
Your programming assignment looks as follows. I will give you a grid.

00:03:30.000 --> 00:03:33.000
In this case it's a grid that's entirely unoccupied.

00:03:33.000 --> 00:03:37.000
As before, I give you a goal state and a set of actions.

00:03:37.000 --> 00:03:41.000
I want you to program dynamic programming all the way to the end,

00:03:41.000 --> 00:03:45.000
so when I run it, the values I get are 0 for the goal state,

00:03:45.000 --> 00:03:49.000
37 for the 2 adjacent states, and 44, 16, and 63.

00:03:49.000 --> 00:03:53.000
If I go right to the goal, there is a 50% chance of hitting the goal,

00:03:53.000 --> 00:03:56.000
in which case my cost is just one, which is the cost of motion.

00:03:56.000 --> 00:04:00.000
There is 25% chance of running into the wall,

00:04:00.000 --> 00:04:04.000
which costs me 100 plus, of course, 1 for the robot motion,

00:04:04.000 --> 00:04:09.000
and there's a 25% chance I find myself down here, which costs us 44.

00:04:09.000 --> 00:04:14.000
If I put this into mathematics, we get 50% chance for reaching the goal,

00:04:14.000 --> 00:04:18.000
25% for hitting a wall, which costs me 100,

00:04:18.000 --> 00:04:25.000
25% for going south, and the value here is 44.77 plus the cost of motion is 1.

00:04:25.000 --> 00:04:33.000
If you add this all up, you get exactly the 37.193 that this value over here constitutes.

00:04:33.000 --> 00:04:38.000
After conversions for this grid, I want the value function to look just like this.

00:04:38.000 --> 00:04:42.000
Here is the corresponding policy.

00:04:42.000 --> 00:04:45.000
If I modify the grid to insert an obstacle over here,

00:04:45.000 --> 00:04:50.000
then I get a value function like this. One thousand is my initial value for each cell.

00:04:50.000 --> 00:04:54.000
It's unmodified for the obstacle cell, and these are the values I receive--

00:04:54.000 --> 00:04:58.000
94, 86, 73, and 44.

00:04:58.000 --> 00:05:00.000
You can check whether these values are correct.

00:05:00.000 --> 00:05:03.000
Let me pick a slightly bigger grid

00:05:03.000 --> 00:05:09.000
with a goal position in the top right corner and two obstacles down here--size 4 x 4.

00:05:09.000 --> 00:05:13.000
Here is my output for the value function. I can read those values.

00:05:13.000 --> 00:05:16.000
They happen to be 1000 for the two obstacles.

00:05:16.000 --> 00:05:18.000
What's remarkable is the policy.

00:05:18.000 --> 00:05:21.000
When I'm close to the wall, you can see that I'm being pulled away

00:05:21.000 --> 00:05:25.000
from those wall elements to stay in free space.

00:05:25.000 --> 00:05:28.000
I will eventually reach the goal, but I never am willing to drive in a way

00:05:28.000 --> 00:05:32.000
that crashes me into a wall if it's not avoidable.

00:05:32.000 --> 00:05:36.000
What you have to do with the code we provide is to modify

00:05:36.000 --> 00:05:39.000
the dynamic programming routine that assumes a deterministic action

00:05:39.000 --> 00:05:41.000
with the one for a stochastic action.

00:05:41.000 --> 00:05:46.000
Specifically, what this routine should do is it should incorporate

00:05:46.000 --> 00:05:49.000
the probability of successful action and the collision costs.

00:05:49.000 --> 00:05:54.000
For example, if I modify the probability of success into a deterministic function,

00:05:54.000 --> 00:05:56.000
then my value function will look as follows.

00:05:56.000 --> 00:06:00.000
It's just a number of steps in the usual way--1, 2, 3--as before,

00:06:00.000 --> 00:06:03.000
and the policy drives me straight to the goal as shown over here.

00:06:03.000 --> 00:06:06.000
In your final submission, please exclude any printouts

00:06:06.000 --> 00:06:11.000
so we can modify the grid and these values for success probabiilty and collision cost,

00:06:11.000 --> 00:06:13.000
and we can see what your code generates.

00:06:13.000 --> 09:59:59.000
In the initial value function, please initialize your value function with 1000.

