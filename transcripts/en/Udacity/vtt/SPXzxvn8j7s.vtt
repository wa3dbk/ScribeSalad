WEBVTT
Kind: captions
Language: en

00:00:00.300 --> 00:00:03.130
Next, we'll talk with Dr.
Juergen Fritsch.

00:00:03.130 --> 00:00:06.965
After earning a Ph.D degree at
Carnegie-Mellon University,

00:00:06.965 --> 00:00:09.040
he was a co-founder of M*Modal.

00:00:09.040 --> 00:00:11.860
Today, he's the company's
chief scientist.

00:00:11.860 --> 00:00:14.520
Juergen, thank you very much for
being with us today.

00:00:14.520 --> 00:00:16.970
&gt;&gt; Thanks for having me.

00:00:16.970 --> 00:00:20.190
&gt;&gt; You are the chief technology
officer of M*Modal, I believe.

00:00:20.190 --> 00:00:21.670
&gt;&gt; Chief scientist, actually.

00:00:21.670 --> 00:00:25.460
&gt;&gt; And the company which, as you know,

00:00:25.460 --> 00:00:30.060
I've been interested in for
years, was founded in 2001.

00:00:30.060 --> 00:00:34.030
But, it was quite a number of years
before the technology was built into

00:00:34.030 --> 00:00:38.220
electronic medical records systems for
direct use by physicians.

00:00:38.220 --> 00:00:42.710
Could you explain why
that happened that way?

00:00:42.710 --> 00:00:47.100
And more specifically,
how you went about training the system?

00:00:47.100 --> 00:00:47.920
&gt;&gt; Yes.
So

00:00:47.920 --> 00:00:51.120
this was actually very
intentional on our part.

00:00:51.120 --> 00:00:54.260
When we founded the company,
we were looking for

00:00:54.260 --> 00:00:59.000
an application of speech technologies
that would tolerate the error rate

00:00:59.000 --> 00:01:02.080
that's still present
in today's technology.

00:01:02.080 --> 00:01:07.470
If you're familiar with telephony
systems and other speech enabled

00:01:07.470 --> 00:01:11.540
dialogue systems, you're familiar with
how annoying it can be to an end user to

00:01:11.540 --> 00:01:15.160
have to deal with misinterpretations and
errors by the technology.

00:01:15.160 --> 00:01:16.590
So instead of doing that and

00:01:16.590 --> 00:01:20.060
going straight to the end users,
we decided to go a different path.

00:01:20.060 --> 00:01:24.170
And we used the technology
on the back end first.

00:01:24.170 --> 00:01:28.520
And we did this by enabling
an existing workflow,

00:01:28.520 --> 00:01:29.940
based on dictation and transcription.

00:01:29.940 --> 00:01:34.580
So in the US, there is billions of lines
of documentation created in healthcare

00:01:34.580 --> 00:01:38.660
every day or every year through
dictation and transcription workflows.

00:01:38.660 --> 00:01:42.770
And we enable those dictation and
transcription workflows, specifically

00:01:42.770 --> 00:01:46.260
the transcriptionists that were doing
that, to be much more productive.

00:01:46.260 --> 00:01:49.770
In that we produced a draft
document that they can review and

00:01:49.770 --> 00:01:55.210
correct, and thereby be twice as fast as
if they had to type it all from scratch.

00:01:55.210 --> 00:01:59.628
So that was an immediate use case for
the technology.

00:01:59.628 --> 00:02:01.400
In other words,
an ROI attached to it and

00:02:01.400 --> 00:02:03.220
it allowed us to build up a business.

00:02:03.220 --> 00:02:05.420
And at the same time,
collect huge amounts of data.

00:02:05.420 --> 00:02:10.380
We collected billions of audio
recordings and got them transcribed,

00:02:10.380 --> 00:02:13.970
corrected if you will, by these
transcriptionist as part of the process.

00:02:13.970 --> 00:02:20.690
So it was a fairly genius way to
basically collect the data, including

00:02:20.690 --> 00:02:24.960
correct the transcripts, to then train
the system to get better and better.

00:02:24.960 --> 00:02:27.470
And after doing this for
five, six, seven years,

00:02:27.470 --> 00:02:28.850
we got it to the point where it was so

00:02:28.850 --> 00:02:32.810
accurate that we could enable end users
to actually use it in the front end.

00:02:32.810 --> 00:02:35.160
And use it for
the biggest paycheck commission.

00:02:35.160 --> 00:02:38.160
So, when I first became
aware of M*Modal and

00:02:38.160 --> 00:02:40.670
traveled to your office
in Pittsburgh to see it.

00:02:41.830 --> 00:02:46.900
One of the most striking aspects of
the system to me was it's ability to

00:02:46.900 --> 00:02:55.200
go into the text that it had created
from the verbal transcriptions.

00:02:55.200 --> 00:02:58.580
Find clinical concepts, and
encode them into SNOMED.

00:02:58.580 --> 00:03:03.080
Now, the students are familiar with
SNOMED as we have this interview.

00:03:03.080 --> 00:03:06.990
They know how complicated it is, or
at least they have an idea of that.

00:03:06.990 --> 00:03:09.660
So how hard was it to actually do that?

00:03:09.660 --> 00:03:11.010
How technically challenging?

00:03:12.140 --> 00:03:13.250
&gt;&gt; It was fairly challenging.

00:03:14.770 --> 00:03:20.200
Apart from the usual challenges with
any natural language processing system.

00:03:20.200 --> 00:03:24.880
We had to deal with the complexities of
the ontology, of the SNOMED ontology,

00:03:24.880 --> 00:03:27.340
and with the medical
terminology to begin with.

00:03:27.340 --> 00:03:30.800
So, in addition to dealing with
things like syntax, semantics,

00:03:30.800 --> 00:03:35.340
pragmatics, and just the context
of the entire dialog happening.

00:03:35.340 --> 00:03:42.350
We had to also deal with similar words,
words that have different meanings.

00:03:42.350 --> 00:03:44.720
Or, it can have different
meanings in different contexts,

00:03:44.720 --> 00:03:45.840
things like that in healthcare.

00:03:46.910 --> 00:03:50.500
Just a simple term like cold, it can
have like three different meanings.

00:03:50.500 --> 00:03:52.880
It can be abbreviated COLD and
it's a disease.

00:03:52.880 --> 00:03:55.830
It could be the common cold, or
it just could be feeling cold.

00:03:55.830 --> 00:03:59.370
So there is all sorts of
disambiguation that needed to happen.

00:03:59.370 --> 00:04:01.970
And needed to be accurate
in order to codify

00:04:01.970 --> 00:04:04.240
these terms correctly
to their SNOMED codes.

00:04:04.240 --> 00:04:07.560
&gt;&gt; So how accurate is it at this point?

00:04:07.560 --> 00:04:10.304
&gt;&gt; That's a tough question to answer
because it really depends a lot on

00:04:10.304 --> 00:04:11.280
the use case.

00:04:11.280 --> 00:04:15.580
We are not attempting to codify
to the entirety of SNOMED,

00:04:15.580 --> 00:04:18.640
to the hundreds of thousands or
300,000 concepts in SNOMED.

00:04:18.640 --> 00:04:21.519
But, we're focusing on specific
subsets based on a use case.

00:04:21.519 --> 00:04:26.510
So one of the use cases we're pursuing
is chronic medical conditions.

00:04:26.510 --> 00:04:29.990
Things like diabetes or heart diseases.

00:04:29.990 --> 00:04:35.600
And in that realm, if you focus on
a particular subset of diseases and

00:04:35.600 --> 00:04:39.040
remodeling those with a data model
that's not just including the disease.

00:04:39.040 --> 00:04:42.410
But, also the temporality and
the specificity,

00:04:42.410 --> 00:04:45.500
and who is concerned with that disease.

00:04:45.500 --> 00:04:46.920
You can be extremely accurate.

00:04:46.920 --> 00:04:49.281
You can be up in the high
90s in discovering and

00:04:49.281 --> 00:04:52.190
detecting these things in
all sorts of contexts.

00:04:52.190 --> 00:04:57.070
But if you would attempt to codify each
and every concept in the SNOMED corpus

00:04:57.070 --> 00:05:02.600
and the SNOMED ontology, you would be
ending up in a much lower accuracy rate.

00:05:02.600 --> 00:05:05.720
And it's just not happening today,

00:05:05.720 --> 00:05:09.360
that people would automatically
codify to the entirety of SNOMED.

00:05:09.360 --> 00:05:11.720
I have not seen anybody
doing that accurately.

00:05:11.720 --> 00:05:14.550
And I think you really don't need to,
because at the end,

00:05:14.550 --> 00:05:16.410
you're always pursuing
a particular use case.

00:05:16.410 --> 00:05:18.160
You're always trying to
solve a particular problem.

00:05:18.160 --> 00:05:21.390
And for that, you really only
need a subset of that corpus.

00:05:21.390 --> 00:05:27.530
&gt;&gt; So what can you tell us about
the technology that's under the surface?

00:05:27.530 --> 00:05:33.740
What languages and databases,
and approaches are you using?

00:05:33.740 --> 00:05:35.490
&gt;&gt; So let's start on the speech side.

00:05:35.490 --> 00:05:41.400
As your students might know, all the
speech recognition, speech understanding

00:05:41.400 --> 00:05:45.710
systems that are out there today
are based on statistical algorithms.

00:05:45.710 --> 00:05:47.685
So based on machine learning,

00:05:47.685 --> 00:05:50.990
statistically-trained machine
learning algorithms.

00:05:50.990 --> 00:05:53.560
And there's no difference
here in our company.

00:05:53.560 --> 00:05:56.350
We do use those, a variety of those,

00:05:56.350 --> 00:06:00.420
starting with hidden Markov models,
neural networks, regression models.

00:06:01.810 --> 00:06:05.560
Even the more complex machine learning
algorithms that are on the market today,

00:06:05.560 --> 00:06:06.510
or that have been used.

00:06:06.510 --> 00:06:10.350
But we combine them in a fairly unique
way, in that we're not just seeing it

00:06:10.350 --> 00:06:14.250
as a pattern matching problem as
most speech recognition systems do.

00:06:14.250 --> 00:06:17.390
But, we combine speech technologies
with natural language processing

00:06:17.390 --> 00:06:18.450
technologies.

00:06:18.450 --> 00:06:21.650
The theory is that if you get
a little bit of an understanding of

00:06:21.650 --> 00:06:22.970
what you're trying to recognize,

00:06:22.970 --> 00:06:26.430
you can do better at the acoustic
identification of the sounds.

00:06:26.430 --> 00:06:27.330
And vice versa, right?

00:06:27.330 --> 00:06:30.880
So it's a constraint satisfaction
problem that you're solving.

00:06:30.880 --> 00:06:34.410
And the more knowledge sources you
bring in, the more accurate you can be.

00:06:34.410 --> 00:06:38.762
So we do combine syntactic power servers
and annotators, and cross taggers with

00:06:38.762 --> 00:06:43.200
the actual hidden Markov models new and
networks that do the speech recognition.

00:06:43.200 --> 00:06:48.060
And altogether by combing them,
achieve better results that way.

00:06:48.060 --> 00:06:51.320
When it comes to
identifying concepts and

00:06:51.320 --> 00:06:56.610
tagging them in a medical corpus such
as SNOMED, we use a lot of classifiers.

00:06:56.610 --> 00:07:00.880
We use anything from a statistical
classifier such as even the hidden

00:07:00.880 --> 00:07:05.180
Markov model, all the way to maximum
entropy models and things like that.

00:07:06.880 --> 00:07:12.100
&gt;&gt; I know you've begun to incorporate
M*Modal into commercial EMRs,

00:07:12.100 --> 00:07:15.280
I believe Greenway here
locally was the first.

00:07:16.720 --> 00:07:17.840
How's that going?

00:07:17.840 --> 00:07:19.510
&gt;&gt; It's going really well.

00:07:19.510 --> 00:07:24.000
Initially, there was a little bit of
a reluctance by the EMR community,

00:07:24.000 --> 00:07:27.720
I would say, to even incorporate speech
technologies into their systems.

00:07:27.720 --> 00:07:33.010
There was this perception that
physicians will eventually converge

00:07:33.010 --> 00:07:37.900
on structural data entry, and
will accept the drop down menus and

00:07:37.900 --> 00:07:40.130
the choice lists that they
have in their systems.

00:07:40.130 --> 00:07:41.210
But, the opposite did happen.

00:07:41.210 --> 00:07:43.280
The physicians were pushing
back harder and harder, and

00:07:43.280 --> 00:07:45.260
saying we need to
preserve the narrative.

00:07:45.260 --> 00:07:48.160
Not just from a productivity
perspective, they

00:07:48.160 --> 00:07:51.190
don't just want to be more productive
and faster in entering the data.

00:07:51.190 --> 00:07:52.750
But, they also want to tell a story.

00:07:52.750 --> 00:07:55.990
They want to be able to explain
what they thought about,

00:07:55.990 --> 00:07:58.470
what the thought process
was in the diagnosis.

00:07:58.470 --> 00:08:01.910
Why they tried certain things, and
why they worked or didn't work.

00:08:01.910 --> 00:08:05.620
And that, you just simply cannot
do by picking from a choice list.

00:08:05.620 --> 00:08:09.220
You have to be able to
narrate a paragraph or two.

00:08:09.220 --> 00:08:12.990
And so, this need for, not just speech
technologies, but just a narrative.

00:08:12.990 --> 00:08:16.410
A way to answer a narrative, whether
it's typing, speech recognition, or

00:08:16.410 --> 00:08:19.650
other ways, was really something
that they pushed hard for.

00:08:19.650 --> 00:08:21.820
The EMR community has come around.

00:08:23.000 --> 00:08:27.100
We're not aware of a single big vendor,
whether it's Epic,

00:08:27.100 --> 00:08:31.420
Cerner, Greenway Athena, or Allscripts.

00:08:31.420 --> 00:08:35.610
All these companies have come around,
and are now heavily investing into

00:08:35.610 --> 00:08:40.200
preserving a narrative part of the chart
of the physician's documentation.

00:08:40.200 --> 00:08:42.880
And so as part of that,
they are all including

00:08:42.880 --> 00:08:45.680
some sort of a speech technology for
authoring these things.

00:08:45.680 --> 00:08:46.690
But also, they are more and

00:08:46.690 --> 00:08:50.760
more incorporating natural language
processing technologies in order

00:08:50.760 --> 00:08:53.030
to search the narrative and
find the relevant information.

00:08:53.030 --> 00:08:57.170
And combine it with the structure
data that they have.

00:08:57.170 --> 00:09:00.679
&gt;&gt; When you look forward,
beyond where we are today,

00:09:00.679 --> 00:09:04.516
at the future of healthcare,
and the potential role and

00:09:04.516 --> 00:09:09.010
future of voice recognition
technologies such as yours.

00:09:09.010 --> 00:09:10.530
What do you see?

00:09:10.530 --> 00:09:12.580
&gt;&gt; I wouldn't limit it to
just the speech technologies.

00:09:12.580 --> 00:09:14.990
I think for me, again,
it's the narrative.

00:09:14.990 --> 00:09:17.680
And what I find exciting in
healthcare is that there is this

00:09:17.680 --> 00:09:22.330
renaissance almost, of the narrative
in physician documentation.

00:09:22.330 --> 00:09:23.510
It started out with that.

00:09:23.510 --> 00:09:27.740
Since 100 years ago, physicians
have been writing and scribbling.

00:09:27.740 --> 00:09:29.980
And really telling a story
about their patients.

00:09:29.980 --> 00:09:34.620
And then, it was kind of on the verge
of being abandoned with all these

00:09:34.620 --> 00:09:39.150
initial EMR systems,
using drop down menus, pick lists.

00:09:39.150 --> 00:09:40.470
And now, you see the opposite happening.

00:09:40.470 --> 00:09:44.260
Again, physicians are creating
much richer storyline and

00:09:44.260 --> 00:09:46.170
narrative around their patients.

00:09:46.170 --> 00:09:48.770
&gt;&gt; And the more these
technologies mature, the more,

00:09:48.770 --> 00:09:50.130
the better the speech recognition gets,

00:09:50.130 --> 00:09:53.530
the better the natural language
processing taggers and identifiers get.

00:09:53.530 --> 00:09:57.000
The more use cases you see appearing.

00:09:57.000 --> 00:10:00.890
And to me, the exciting thing is that
we can get to real evidence-based

00:10:00.890 --> 00:10:02.350
medicine in the next few years.

00:10:02.350 --> 00:10:06.000
Where you can use these millions and
millions of

00:10:06.000 --> 00:10:09.650
clinical documents that the physicians
are creating in any given hospital.

00:10:09.650 --> 00:10:10.530
That we can use those.

00:10:10.530 --> 00:10:12.160
That we can scan those.

00:10:12.160 --> 00:10:13.230
That we can find trends.

00:10:13.230 --> 00:10:17.150
That we can tell exactly for this
patient of that gender, of that age,

00:10:17.150 --> 00:10:19.650
with that disease and that outcome.

00:10:19.650 --> 00:10:21.880
This medication worked or
this medication didn't work, and

00:10:21.880 --> 00:10:23.050
in what context.

00:10:23.050 --> 00:10:26.160
And we can use that information
to derive better treatments,

00:10:26.160 --> 00:10:30.460
and more accurate diagnoses for
other patients.

00:10:30.460 --> 00:10:35.440
And I think that's when you really have
reached that evidence-based medicine

00:10:35.440 --> 00:10:39.130
realm that we're all talking about,
and really haven't achieved yet.

00:10:39.130 --> 00:10:42.990
&gt;&gt; Yeah, I think there's a very
exciting landscape that you portray.

00:10:42.990 --> 00:10:47.040
I hope the students remember
these comments when in Lesson 1.

00:10:47.040 --> 00:10:51.020
We look at some of the contemporary
research efforts in things like clinical

00:10:51.020 --> 00:10:52.550
student support.

00:10:52.550 --> 00:10:53.840
&gt;&gt; So thank you very much.

00:10:53.840 --> 00:10:54.470
&gt;&gt; Pleasure.

00:10:54.470 --> 00:10:58.110
&gt;&gt; I'm thrilled to see how far you guys
have come since I first visited you,

00:10:58.110 --> 00:11:00.330
I don't remember how many years ago.

00:11:00.330 --> 00:11:03.150
And perhaps,
we can do this interview again in two or

00:11:03.150 --> 00:11:06.100
three years, and
see what you've done then.

00:11:06.100 --> 00:11:07.030
&gt;&gt; Any time, my pleasure.

00:11:07.030 --> 00:11:08.080
&gt;&gt; Thank you.

00:11:08.080 --> 00:11:08.580
&gt;&gt; Thank you, Mark.

