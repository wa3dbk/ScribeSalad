WEBVTT
Kind: captions
Language: en

00:00:00.260 --> 00:00:02.860
To try to motivate this idea
let's look at a little example.

00:00:02.860 --> 00:00:05.566
So let's imagine we've got two agents,
agent 1 and agent 2.

00:00:05.566 --> 00:00:07.886
And agent 1 has kind of a goal or

00:00:07.886 --> 00:00:11.648
some kind of reward
function that it's running.

00:00:11.648 --> 00:00:16.810
And to make that reward function have
high reward, agent 1 needs an apple.

00:00:16.810 --> 00:00:20.160
Now the apple is not far away but
it is very close to agent two.

00:00:20.160 --> 00:00:25.600
So what are some things that agent
one could do to satisfy its goal.

00:00:25.600 --> 00:00:27.350
&gt;&gt; It could walk over to the apple.

00:00:27.350 --> 00:00:28.277
&gt;&gt; Good, anything else?

00:00:28.277 --> 00:00:32.119
Well, once it gets there can say I
thought you were giving me an iPad and

00:00:32.119 --> 00:00:33.655
then kick agent 2 and run.

00:00:33.655 --> 00:00:36.710
&gt;&gt; [LAUGH]
I see, wrong apple.

00:00:36.710 --> 00:00:38.030
This is the food kind of apple.

00:00:38.030 --> 00:00:40.530
But, you're right that probably
this particular agent 1 would

00:00:40.530 --> 00:00:42.230
would like an Apple watch.

00:00:42.230 --> 00:00:45.490
&gt;&gt; Yes, well actually this particular
agent 1 has an Apple watch.

00:00:45.490 --> 00:00:48.620
Then let's see,
the other thing that could happen is

00:00:48.620 --> 00:00:52.310
agent 1 could ask agent 2
To bring him the apple.

00:00:52.310 --> 00:00:56.240
&gt;&gt; Good and I guess presumably each one
could just also just do nothing and

00:00:56.240 --> 00:01:00.410
just wait for agent two to for
the apple to just show up using artwork.

00:01:00.410 --> 00:01:04.959
Like maybe agent two would bring it by-
&gt;&gt; I guess it maybe not apples but like

00:01:04.959 --> 00:01:09.390
all men's there's certainly agents that
bring each one home mints and ice cream.

00:01:09.390 --> 00:01:09.990
&gt;&gt; Yeah.

00:01:09.990 --> 00:01:11.270
Have you got ice cream in while?

00:01:12.340 --> 00:01:14.130
&gt;&gt; Yeah.
Okay, and then yes.

00:01:14.130 --> 00:01:18.410
That's too bad looks like your looks
like your Apple Watch exploded.

00:01:18.410 --> 00:01:20.270
&gt;&gt; That's okay,
I'll just get another one.

00:01:20.270 --> 00:01:21.720
&gt;&gt; Yeah, because this one's broken.

00:01:21.720 --> 00:01:23.310
Anyway, when we're doing planning or

00:01:23.310 --> 00:01:26.390
sequential decision making we need
to decide amongst these different

00:01:26.390 --> 00:01:30.580
options if you will or different
courses of action that we could take.

00:01:30.580 --> 00:01:33.660
How do we usually do that in the
framework that we've been talking about?

00:01:33.660 --> 00:01:37.360
&gt;&gt; Well, if we're living in an MDP like
world we actually define the MDP and

00:01:37.360 --> 00:01:40.470
I think the secret sauce there is to
figure out what the right rewards are.

00:01:40.470 --> 00:01:42.300
&gt;&gt; Right.
&gt;&gt; And the reason I said it that way is

00:01:42.300 --> 00:01:46.410
because in the game theory case we
usually call those utilities and

00:01:46.410 --> 00:01:49.410
not rewards, and I just want to
differentiate between the two of them.

00:01:49.410 --> 00:01:50.960
Okay and then what we do with those?

00:01:50.960 --> 00:01:52.290
&gt;&gt; Well once we have rewards or

00:01:52.290 --> 00:01:55.870
utilities we then find
a policy that maximizes them.

00:01:55.870 --> 00:01:59.380
We maximize our reward actually
as we maximize our utility.

00:01:59.380 --> 00:02:03.080
&gt;&gt; It seems like we could do this sort
of thing in the scenario where we've got

00:02:03.080 --> 00:02:04.650
multiple agents and
they need to coordinate and

00:02:04.650 --> 00:02:05.820
communicate with each other.

00:02:05.820 --> 00:02:09.030
But it turns out it starts to get
a little bit hard to define exactly what

00:02:09.030 --> 00:02:13.960
the costs are, and was the likelihood
that this is actually going to work and

00:02:13.960 --> 00:02:14.680
so forth.

00:02:14.680 --> 00:02:17.990
So what we're going to do
is actually think about

00:02:17.990 --> 00:02:20.170
just like we did when
we talked about POMDPs.

00:02:20.170 --> 00:02:26.120
We said that they allow us to strike a
balance between actions to gain reward.

00:02:26.120 --> 00:02:30.100
And actions to gain information and we
don't have special like a special other

00:02:30.100 --> 00:02:33.320
way of reasoning about that all of
that being folded into one model.

00:02:33.320 --> 00:02:34.820
Where actions are being taken and

00:02:34.820 --> 00:02:38.540
they might have informational impact and
they might have reward impact.

00:02:38.540 --> 00:02:42.780
The informational impact is subservient
to the reward in a sense right so

00:02:42.780 --> 00:02:44.440
the agent just tries to figure out.

00:02:44.440 --> 00:02:47.350
I will take an action to gain
information if it helps me get more

00:02:47.350 --> 00:02:48.760
reward in the long run.

00:02:48.760 --> 00:02:52.640
And so,
the DEC-POMDPs is a similar sort of idea

00:02:52.640 --> 00:02:56.340
that's going to let us put coordinating
and communicating along with

00:02:56.340 --> 00:02:59.630
actions that they may become actions
just like everything else like this.

00:02:59.630 --> 00:03:03.600
Ask for it becomes a kind of action and
walk to Apple becomes a kind of action,

00:03:03.600 --> 00:03:06.060
and the agents can reason about
whether that's a good idea or

00:03:06.060 --> 00:03:09.151
not depending on how to
maximize their utility.

00:03:09.151 --> 00:03:10.391
&gt;&gt; Sounds promising.

