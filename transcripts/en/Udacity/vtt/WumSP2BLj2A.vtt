WEBVTT
Kind: captions
Language: en

00:00:00.240 --> 00:00:04.220
The next thing I want to talk about
is what happens in the system on

00:00:04.220 --> 00:00:05.930
Page Evictions.

00:00:05.930 --> 00:00:11.310
Remember that on Page Eviction, when a
Node decides that it wants to throw out

00:00:11.310 --> 00:00:17.040
a page, it sends it in the algorithm
that I described to the Node.

00:00:17.040 --> 00:00:20.330
Which is candidly ignored for
hosting that page, and

00:00:20.330 --> 00:00:23.280
it might use the weight
information to make the decision.

00:00:23.280 --> 00:00:27.580
Of which node to send that page that
it is discarding from it's own node.

00:00:27.580 --> 00:00:30.480
When a page fault happens, the key thing

00:00:30.480 --> 00:00:35.030
is to make sure that we service
the page fault so that we get the page.

00:00:35.030 --> 00:00:38.490
And restart the execution that
has been stalled on this node.

00:00:38.490 --> 00:00:40.130
That's the important thing to do.

00:00:40.130 --> 00:00:43.730
The less important thing, but
something that needs to happen

00:00:43.730 --> 00:00:47.230
in the universe of things
that is being managed by GMS,

00:00:47.230 --> 00:00:53.350
is to also send the victim page
from this node to the target node.

00:00:53.350 --> 00:00:56.660
That's part of the algorithm for
eviction on page four.

00:00:56.660 --> 00:01:00.470
However, we don't want to do
it on every page fault, but

00:01:00.470 --> 00:01:03.320
we want to do it in
an aggregated manner.

00:01:03.320 --> 00:01:07.060
And for that reason every
node has a paging daemon.

00:01:07.060 --> 00:01:12.430
And this is typical of virtual memory
systems that when a page fault happens.

00:01:12.430 --> 00:01:15.640
That's not the time the virtual memory
manager is running around trying to

00:01:15.640 --> 00:01:17.430
find the page free.

00:01:17.430 --> 00:01:21.490
It always has a stash of free
page frames to allocate.

00:01:21.490 --> 00:01:23.990
To service this particular page fault.

00:01:23.990 --> 00:01:28.260
As I mentioned earlier, the paging
daemon in the virtual memory manager

00:01:28.260 --> 00:01:30.410
is integrated with the GMS system.

00:01:30.410 --> 00:01:32.880
And what the paging
daemon is going to do is,

00:01:32.880 --> 00:01:37.890
when the free list falls below
a threshold, then the paging

00:01:37.890 --> 00:01:43.280
daemon is going to do put page of
the oldest pages on this node.

00:01:43.280 --> 00:01:46.160
And remember that in
the integration of GMS with

00:01:46.160 --> 00:01:48.700
the virtual memory
manager in the put page,

00:01:48.700 --> 00:01:54.230
I said that the Paging Daemon is
also modified to work with the GMS.

00:01:54.230 --> 00:01:56.300
This is where the modification comes in.

00:01:56.300 --> 00:02:00.710
Normally what the Paging Daemon
would've done is when the free list

00:02:00.710 --> 00:02:05.000
goes below threshold,
it would take a bunch of pages,

00:02:05.000 --> 00:02:08.820
LRU candidate pages, and
put it all onto the disc.

00:02:08.820 --> 00:02:13.160
But with the integration with GMS, what
the paging demon is going to do is, for

00:02:13.160 --> 00:02:17.160
the same condition when the freelist
falls below threshold, it's going to

00:02:17.160 --> 00:02:23.650
basically do putpage of the oldest
pages that it has on this node.

00:02:23.650 --> 00:02:28.430
And when we do the put page, we're
going to choose the candidate node.

00:02:28.430 --> 00:02:34.000
Where we going to do the putpage based
on the weight information that we got

00:02:34.000 --> 00:02:39.380
as part of the geriatric management
that we talked about in the beginning.

00:02:39.380 --> 00:02:41.800
And so we do a putpage of UID.

00:02:41.800 --> 00:02:43.790
Into this PFD of this node so

00:02:43.790 --> 00:02:48.470
that this guy will be the one that
will be backing this particular UID.

00:02:48.470 --> 00:02:54.010
And once I do this,
I also have to update the GCD

00:02:54.010 --> 00:03:01.090
to indicate that the new PFD for
this particular UID is C.

00:03:01.090 --> 00:03:05.620
So this update message is being
sent to the owner of this UID.

00:03:05.620 --> 00:03:10.590
That is this node, and
the owner the guy that has the portion

00:03:10.590 --> 00:03:15.510
of the UID space that is
managed by this node.

00:03:15.510 --> 00:03:19.760
So the GCD data structure
contains the mapping of the UID

00:03:19.760 --> 00:03:24.300
to the node that contains the PFD for
that particular UID.

00:03:24.300 --> 00:03:29.270
And so I send this update message saying
please update the GCD to indicate that

00:03:29.270 --> 00:03:36.230
this particular UID is now backed
by PFD that sites on note C.

00:03:36.230 --> 00:03:38.500
So that's the information
I'm sending to this guy.

00:03:38.500 --> 00:03:42.240
And this is not being done
on every page eviction, but

00:03:42.240 --> 00:03:47.600
it is done by the paging demon in
a aggregated, coordinated manner.

00:03:47.600 --> 00:03:50.710
When the free list falls
below a threshold.

00:03:50.710 --> 00:03:53.120
So we've covered a lot of ground from

00:03:53.120 --> 00:03:56.970
just sort of the principal
behind the thought experiment.

00:03:56.970 --> 00:04:03.235
That is, using network memory as
a paging device, rather than disc.

00:04:03.235 --> 00:04:05.395
Because the networks have gotten faster.

00:04:05.395 --> 00:04:09.425
And we came up with an algorithm for

00:04:09.425 --> 00:04:14.415
age management globally in the entire
cluster, and how to have that

00:04:14.415 --> 00:04:20.610
age management done in a manner
that doesn't burden any one node.

00:04:20.610 --> 00:04:25.420
But it picks the node that is lightest
in terms of load at any point of time.

00:04:25.420 --> 00:04:29.150
And we also saw given the solution for

00:04:29.150 --> 00:04:33.680
cluster white memory management for
paging, how to go about implementing it.

00:04:33.680 --> 00:04:38.500
And all of the heavy lifting that
needs to be done in order to take

00:04:38.500 --> 00:04:40.880
an idea and put it into practice.

