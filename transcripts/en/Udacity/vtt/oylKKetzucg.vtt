WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.007
So, here's a bunch of threads executing instructions over time.

00:00:03.007 --> 00:00:07.342
Now these threads all belong to the same warp, and in all Nvidia GPU's today,

00:00:07.342 --> 00:00:12.117
a warp has 32 threads. Here's another warp where all the threads take the else clause,

00:00:12.117 --> 00:00:13.735
so there's no problems with this.

00:00:13.735 --> 00:00:16.754
All of these threads execute all of their instructions in lock step,

00:00:16.754 --> 00:00:21.466
but here's a warp where some threads take the if the branch and some take the else branch.

00:00:21.466 --> 00:00:25.147
And because the threads in a warp can only execute a single instruction at a time,

00:00:25.147 --> 00:00:29.295
the hardware automatically deactivates some of the threads, executes the red instructions,

00:00:29.295 --> 00:00:33.055
then flips which threads are activated and executes the blue instructions.

00:00:33.055 --> 00:00:37.465
And as you can see, if this is time, the code is going to take longer to execute overall.

00:00:37.465 --> 00:00:41.364
Some of our threads are wasting time sitting idle while their workmates execute.

00:00:41.364 --> 00:00:47.100
We say that the threads have diverged during this section of code and would refer to this as 2-way branch divergence.

00:00:47.100 --> 00:00:49.844
If I'd written a nested if-else statement,

00:00:49.844 --> 00:00:53.036
I could have 4-way branch divergence and could take even longer to execute.

00:00:53.036 --> 00:00:58.146
So, as a quiz, what is the maximum branch divergence penalty that a CUDA thread block with

00:00:58.146 --> 00:01:04.786
1024 threads could experience when executing a kernel? Is it a 2x slow down, a 4x slowdown, or so forth?

