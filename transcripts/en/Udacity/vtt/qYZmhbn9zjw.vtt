WEBVTT
Kind: captions
Language: en

00:00:00.460 --> 00:00:04.930
We can build queries of knowledge based AI at many levels of instructions.

00:00:04.930 --> 00:00:09.010
This is the scale here, low level to high level. At one level,

00:00:09.010 --> 00:00:13.070
we can build queries at hardware level. So we can talk about a brain or

00:00:13.070 --> 00:00:19.050
transistor sort of microchip. At the next level, we can talk about the kinds

00:00:19.050 --> 00:00:22.925
of methods and the kinds of representations we have been talking about,

00:00:22.925 --> 00:00:26.360
means-ends analysis that has an algorithm associated with it, or

00:00:26.360 --> 00:00:30.590
semantic network that's a knowledge representation in some symbolic form.

00:00:30.590 --> 00:00:34.740
At a yet higher level, we can talk about knowledge and

00:00:34.740 --> 00:00:39.990
tasks. So, the question here becomes what exactly is the task the decision

00:00:39.990 --> 00:00:44.420
maker has to make? What exactly is the knowledge the decision maker has?

00:00:44.420 --> 00:00:47.440
So, when David was giving his answer about what the pitcher might do in

00:00:47.440 --> 00:00:51.970
the situation I showed you earlier, David was clearly using a lot of knowledge,

00:00:51.970 --> 00:00:55.580
and he was trying to use this knowledge toward to a particular task. Now,

00:00:55.580 --> 00:01:00.840
in the history of AI, David Mark talked about three levels, the level of tasks,

00:01:00.840 --> 00:01:04.080
which he called the [UNKNOWN] theory, the level of algorithms and

00:01:04.080 --> 00:01:09.480
the level of implementation. And [INAUDIBLE] talked also about multiple levels.

00:01:09.480 --> 00:01:13.370
He was talking about the knowledge level, the symbol level and lower levels like

00:01:13.370 --> 00:01:18.010
the hardware. The various levels are connected with each other. So I might think

00:01:18.010 --> 00:01:23.280
that, the hardware level is a level for implementing what is happening

00:01:23.280 --> 00:01:27.620
at the algorithm level. And the algorithm level provides and architecture for

00:01:27.620 --> 00:01:32.370
implementing what is happening at the task level. In the opposite direction,

00:01:32.370 --> 00:01:37.690
I might think that the task level provides the content of what needs to be

00:01:37.690 --> 00:01:42.650
represented or manipulated at the algorithm level. And the algorithm and

00:01:42.650 --> 00:01:46.940
symbol level provide the content for what needs to be manipulated at processor,

00:01:46.940 --> 00:01:51.990
the hardware level. So as an example, one might say, we're representing this

00:01:51.990 --> 00:01:56.560
in a form of a semantic network, fair enough. But, what exactly are you going to

00:01:56.560 --> 00:02:00.010
represent in a semantic network? That's going to come from the knowledge level.

00:02:00.010 --> 00:02:04.040
It is the knowledge level that tells us, what is the content of the knowledge

00:02:04.040 --> 00:02:08.680
that is required to play baseball? Once you have the content of knowledge,

00:02:08.680 --> 00:02:12.740
you can perhaps implement it in many different ways. One way is through semantic

00:02:12.740 --> 00:02:17.000
network. Similarly, once you know what kind of decision you have to make,

00:02:17.000 --> 00:02:20.660
and what a decision-making process might look like overall,

00:02:20.660 --> 00:02:25.130
there might be many different methods of making that particular decision. Just

00:02:25.130 --> 00:02:28.540
like we can build a relationship between the task level and the algorithm level,

00:02:28.540 --> 00:02:31.010
a similar relationship exists between the algorithm level and

00:02:31.010 --> 00:02:36.520
the hardware level. This is an important point, so let me take another example.

00:02:36.520 --> 00:02:42.200
All of you are familiar with your standard smartphone. Let us suppose that I

00:02:42.200 --> 00:02:46.820
was coming from Mars, I was a Martian. And I did not know how your mobile phone

00:02:46.820 --> 00:02:51.620
works. So I would ask you, well, how, exactly, does your mobile phone work? And

00:02:51.620 --> 00:02:54.950
you might give an account of how the phone works at that level of distraction.

00:02:54.950 --> 00:02:58.190
There will be a legitimate account, you have somewhere else for

00:02:58.190 --> 00:03:01.730
give an account of how the smartphone works at a, at a level of tasks and

00:03:01.730 --> 00:03:05.930
knowledge. This person might say, well, a phone allows you to communicate with

00:03:05.930 --> 00:03:10.760
other people at long distances. How that is implemented is a different matter.

00:03:10.760 --> 00:03:14.780
Now you will see, I'm sure, that all three of these interpretations,

00:03:14.780 --> 00:03:18.160
all three of these descriptions are legitimate and valid.

00:03:18.160 --> 00:03:23.220
You will see also, that we really need all three of these levels of description.

00:03:23.220 --> 00:03:26.622
We do need to understand what this smartphone does, and that kind of

00:03:26.622 --> 00:03:31.366
knowledge it uses to do it. We do need to understand what kind of algorithm and

00:03:31.366 --> 00:03:35.431
knowledge presentations it uses, and what kind of hardware implements all of

00:03:35.431 --> 00:03:40.320
this. Now, you can do a similar kind of analysis for other kinds of devices.

00:03:40.320 --> 00:03:45.250
Let's say, like your calculator. Could we do a similar kind of analysis for

00:03:45.250 --> 00:03:49.090
intelligent agents? Are these different layers also meaningful for

00:03:49.090 --> 00:03:53.330
analyzing what happens in cognitive systems, whether they are natural or

00:03:53.330 --> 00:03:57.740
artificial? And at what layer should we be building a theory?

00:03:57.740 --> 00:04:02.660
Our hypothesis is, that these three layers are also useful for trying to

00:04:02.660 --> 00:04:07.000
analyze how cognitive systems might work, but natural cognitive systems and

00:04:07.000 --> 00:04:12.060
artificial cognitive systems. Further, our hypothesis is that we want to

00:04:12.060 --> 00:04:16.089
build theories at all three of these different level, levels of abstraction,

00:04:16.089 --> 00:04:20.380
not at any one of them. In fact, their constraint's flowing in both directions.

00:04:20.380 --> 00:04:23.610
If we know about what kind of tasks we want to do and what kind of knowledge we

00:04:23.610 --> 00:04:28.580
want to use, then that tells us something about what kind of algorithms we

00:04:28.580 --> 00:04:31.280
need to and what kind of knowledge representations we need.

00:04:31.280 --> 00:04:34.853
And that tells us something about what kind of hardware we need. In the other

00:04:34.853 --> 00:04:38.750
direction, if we know what kind of hardware we have that imposes constraints and

00:04:38.750 --> 00:04:42.595
provide [UNKNOWN] for what kind algorithms and knowledge representations can be

00:04:42.595 --> 00:04:45.780
there, which then provides accordance within constraints. Well,

00:04:45.780 --> 00:04:49.600
what kinds of tasks can be done and what kind of knowledge can be used.

00:04:49.600 --> 00:04:53.660
In this class, we'll be concerned mostly with the top two layers,

00:04:53.660 --> 00:04:57.050
although I allude occasionally to the third layer as well.

00:04:57.050 --> 00:05:01.290
A lot of work in AI is at the top two layers of abstraction.

