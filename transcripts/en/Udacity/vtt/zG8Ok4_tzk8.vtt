WEBVTT
Kind: captions
Language: en

00:00:01.035 --> 00:00:04.040
There're a number of advanced and open issues in analogical reasoning,

00:00:04.040 --> 00:00:08.760
that are the subject for current research. First, because analogical reasoning

00:00:08.760 --> 00:00:12.930
entails cross-domain transfer, does it mean that we necessarily need a common

00:00:12.930 --> 00:00:17.350
vocabulary across all the domains? Consider the example of the atomic structure

00:00:17.350 --> 00:00:22.050
and the solar system once again. Suppose I were to use this term revolve,

00:00:22.050 --> 00:00:26.870
to say the electron revolves around the nucleus. But use the term rotate to say

00:00:26.870 --> 00:00:31.460
the planet rotates in an orbit around the sun. I have used two different terms.

00:00:31.460 --> 00:00:35.970
How then can I do alignment between these two situations? Should I use the same

00:00:35.970 --> 00:00:39.679
vocabulary? If I don't use the same vocabulary, what alternative is there?

00:00:41.020 --> 00:00:46.293
Second, analogical reasoning entails problem abstraction and transformation. So

00:00:46.293 --> 00:00:49.627
far we have talked as if the problem remain fixed, it's source case is

00:00:49.627 --> 00:00:54.440
retrieved and transferred across. But often, the agent needs to abstract and

00:00:54.440 --> 00:00:58.140
transfer the problem, in order to be able to retrieve the source case.

00:00:58.140 --> 00:01:01.570
A third issue in analogical reasoning concerns compound and

00:01:01.570 --> 00:01:06.700
compositional analogies. So far we have talked that given a problem, we

00:01:06.700 --> 00:01:10.640
can retrieve a case and transfer some knowledge from that case to the problem.

00:01:11.930 --> 00:01:16.470
But often we retrieve not one case, and we transfer knowledge from not one case,

00:01:16.470 --> 00:01:20.951
but from several cases. If you're designing a car, you might design the engine

00:01:20.951 --> 00:01:25.580
binology to one vehicle and the chassis binology to some other vehicle. This

00:01:26.770 --> 00:01:30.930
is an example of compound analogy. But how can we make compound analogy work?

00:01:33.440 --> 00:01:38.270
In compositional analogy, analogy works at several levels of abstraction.

00:01:38.270 --> 00:01:41.900
Supposing we were to make an analogy between your business organisation and

00:01:41.900 --> 00:01:46.005
some other business organisation. We might make this compositional analogy,

00:01:46.005 --> 00:01:51.260
first at the level of people. Next to the level of processes. Third of level of

00:01:51.260 --> 00:01:55.550
the organisation as a whole. This is another example of compositional analogy,

00:01:55.550 --> 00:02:00.750
where mapping at one level supports transfer to the next level. How do

00:02:00.750 --> 00:02:06.080
we do compositional analogy in AI agents? Fourth, visuospatial analogies. So

00:02:06.080 --> 00:02:10.330
far we have talked about analogies in which it transferred necessarily engages

00:02:10.330 --> 00:02:14.780
causal knowledge. But a large number of analogies in which causality is at

00:02:14.780 --> 00:02:19.579
most implicit. We'll consider these visuospatial analogies later in the class.

00:02:20.880 --> 00:02:25.980
Fifth, conceptual combination. A powerful learning mechanism is learning a new

00:02:25.980 --> 00:02:31.360
concept by combining parts of familiar concepts. Analogical reasoning is one

00:02:31.360 --> 00:02:36.440
mechanism for conceptual combination. I have a one concept, [UNKNOWN] concept,

00:02:36.440 --> 00:02:39.970
that of the atomic structure, another concept, the solution concept.

00:02:39.970 --> 00:02:43.970
The concept of the solar system. I take some part of the solar system knowledge,

00:02:43.970 --> 00:02:47.860
combine it with my concept of the atom to get a new concept of the atom.

00:02:50.192 --> 00:02:52.130
If you're interested in any of these issues,

00:02:52.130 --> 00:02:54.340
I invite you to join the PhD program in Computer Science.

