WEBVTT
Kind: captions
Language: en

00:00:00.260 --> 00:00:04.160
So in particular, once we've
supplied a set of actions in a state

00:00:04.160 --> 00:00:08.500
representation, we would like
agents to do as well as they can,

00:00:08.500 --> 00:00:11.690
without necessarily
questioning what we gave them.

00:00:11.690 --> 00:00:13.810
So in this case of
temporal abstractions,

00:00:13.810 --> 00:00:19.190
again, hierarchical optimality is
the notion that the agent should

00:00:19.190 --> 00:00:24.120
do the best that it can,
respecting the high level,

00:00:24.120 --> 00:00:28.780
their temporal extract actions that
they're given without having to question

00:00:28.780 --> 00:00:32.729
whether it could do better by not
listening to that whole concept.

00:00:32.729 --> 00:00:37.300
&gt;&gt; Right and so maybe, it's the case
that we should include in our options,

00:00:37.300 --> 00:00:40.540
every single primitive action,
every single atomic action.

00:00:40.540 --> 00:00:42.170
Maybe we don't have to do that.

00:00:42.170 --> 00:00:45.200
It really just sort of depends upon
what you're trying to accomplish and

00:00:45.200 --> 00:00:46.000
what the trade-offs are.

00:00:46.000 --> 00:00:48.030
But we're making those
trade-offs all the time,

00:00:48.030 --> 00:00:50.130
even when we define
the problem in the first place.

00:00:50.130 --> 00:00:51.910
&gt;&gt; I guess that's fair.

00:00:51.910 --> 00:00:55.495
&gt;&gt; Right, but in the same way that
once we have all the machinery of

00:00:55.495 --> 00:01:00.370
SMDPs in place, we might as well just
pretend that we're dealing with MDPs for

00:01:00.370 --> 00:01:02.782
this kind of purposes of planning and
learning.

00:01:02.782 --> 00:01:05.440
We sort of should do that
with the set of options

00:01:05.440 --> 00:01:07.570
that we have in the set of
state features that we have.

00:01:07.570 --> 00:01:10.140
So that's a big win that we're
going to still get optimality.

00:01:10.140 --> 00:01:13.690
It'll be limited by the set of
options that we've constructed, but

00:01:13.690 --> 00:01:16.310
it was limited by the set of actions
that we had in the first place.

00:01:16.310 --> 00:01:18.354
There's other things that
sort of fall out from that.

00:01:18.354 --> 00:01:20.525
One, is state abstraction,
which we talked about before,

00:01:20.525 --> 00:01:24.740
that because we have these different
options, if we are lucky or

00:01:24.740 --> 00:01:29.280
clever we might be able to define
options that don't require that we pay

00:01:29.280 --> 00:01:33.440
attention to the entire state and pay
attention to everything that's going on.

00:01:33.440 --> 00:01:34.770
And if so, that can be a big win.

00:01:34.770 --> 00:01:39.941
But there's also something else and
I said this on the first slide,

00:01:39.941 --> 00:01:45.384
because we're able to jump ahead
from one doorway to another doorway,

00:01:45.384 --> 00:01:51.304
it allows us to avoid wandering around
in say, this northeast room, right?

00:01:51.304 --> 00:01:55.468
Which means, we get to avoid what I'll
call the boring parts of the mark off

00:01:55.468 --> 00:01:56.900
decision process.

00:01:56.900 --> 00:01:59.380
The parts of the state
space that don't matter.

00:01:59.380 --> 00:02:02.630
Because remember, fundamentally,
in order to do reinforcement learning,

00:02:02.630 --> 00:02:05.300
we have to worry about
exploration versus exploitation.

00:02:05.300 --> 00:02:08.120
But the options, because they
have these policies built in, and

00:02:08.120 --> 00:02:11.390
because they're trying to accomplish
very specific things, allow us to

00:02:11.390 --> 00:02:14.760
avoid having to pay attention to
large parts of the state space.

00:02:14.760 --> 00:02:17.760
So, they actually help
us with exploration.

00:02:17.760 --> 00:02:21.970
So, with the idea that we can focus our
decision making energy on the places

00:02:21.970 --> 00:02:26.210
where those decisions actually have an
impact, as opposed to stuff that really

00:02:26.210 --> 00:02:28.640
there's only one right answer,
we shouldn't be thinking about it.

00:02:28.640 --> 00:02:31.610
&gt;&gt; Right, so in fact, the word that you
that you mentioned as a phrase used

00:02:31.610 --> 00:02:32.680
there that I think is just perfect,

00:02:32.680 --> 00:02:37.790
which is, we need to focus on the places
where we can actually make decisions.

00:02:37.790 --> 00:02:41.580
And because options allow us to kind
of jump forward in time, it means,

00:02:41.580 --> 00:02:44.180
in some very important sense
from planning at that level,

00:02:44.180 --> 00:02:46.010
there are no decisions to happen here.

00:02:46.010 --> 00:02:49.030
We made the decision when we
decided to execute the option,

00:02:49.030 --> 00:02:50.360
go to the southern door.

00:02:50.360 --> 00:02:52.310
How it gets there, is it's business.

00:02:52.310 --> 00:02:53.429
We're just assuming that it works.

00:02:53.429 --> 00:02:56.992
In the same way, that when we
decided to execute the up action or

00:02:56.992 --> 00:02:58.001
the left action,

00:02:58.001 --> 00:03:02.990
we didn't worry about what was happening
while that action was being executed.

00:03:02.990 --> 00:03:04.430
We didn't think about it at all.

00:03:04.430 --> 00:03:06.190
Those are the decision points.

00:03:06.190 --> 00:03:08.010
That's what makes
the decision process and

00:03:08.010 --> 00:03:09.881
that's where we should be building
our computational energy.

00:03:09.881 --> 00:03:13.910
So, insofar as we can jump ahead in the
future, we can avoid making decisions,

00:03:13.910 --> 00:03:17.230
which means we avoid doing
exploration and that is a huge win.

