WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.421
So when I run all of these, this one is slightly the fastest of all of these on the diagonal--

00:00:04.421 --> 00:00:09.965
64 threads per block operating on 16 items each.

00:00:09.965 --> 00:00:13.889
And so the thing to notice is that throughput is generally increasing

00:00:13.889 --> 00:00:17.159
as the granularity per thread increases, right?

00:00:17.159 --> 00:00:21.758
So having threads do more and more serial work is generally a good thing.

00:00:21.758 --> 00:00:24.168
On the other hand, if you've got a fixed tile size--

00:00:24.168 --> 00:00:28.763
in other words, the tile here is the total number of items that we're scanning over--

00:00:28.763 --> 00:00:33.366
for a fixed tile size there's diminishing returns.

00:00:33.366 --> 00:00:37.780
The improvements get smaller and smaller as you approach this sweet spot

00:00:37.780 --> 00:00:43.226
because you're trading off increased granularity per thread for decreased parallelilsm.

00:00:43.226 --> 00:00:46.104
And then it starts to go up again, and in my measurements on Fermi,

00:00:46.104 --> 00:00:50.151
which is the same GPU that you'll be using on the Udacity IDE,

00:00:50.151 --> 00:00:56.349
I get 32x32 being slightly slower than 64 threads with 16 items each.

00:00:56.349 --> 00:01:02.821
And at some point, you get to the point where you can no longer fit a problem size

00:01:02.821 --> 00:01:07.235
in a single thread's registers, and then performance falls of a cliff again.

00:01:07.235 --> 00:01:12.749
And for me on Fermi, 64 items per thread running at only 16 threads

00:01:12.749 --> 00:01:16.390
really starts to get pretty bad performance again.

00:01:16.390 --> 00:01:19.545
I encourage you to play around, experiment a little bit more

00:01:19.545 --> 00:01:22.222
to see what the rest of this matrix looks like.

00:01:22.222 --> 00:01:29.634
And if you're interested, go check out the CUB homepage and see what else you can do.

00:01:29.634 --> 00:01:32.320
There's different varieties of block scan, for example.

00:01:32.320 --> 00:01:36.507
There's work-efficient and step-efficient varieties of block scan,

00:01:36.507 --> 00:01:41.525
and so experiment a little bit and see how fast you can get this scan throughput,

00:01:41.525 --> 00:01:45.187
how few clocks you can spend per element scanned.

00:01:45.187 --> 00:01:47.220
And what I've found is that with the right balance,

00:01:47.220 --> 00:01:51.056
you can have a computational overhead of less than 1 clock cycle per item scanned.

00:01:51.056 --> 00:01:53.118
That is really pretty remarkable.

00:01:53.118 --> 00:01:55.568
Think about running this code on a CPU.

00:01:55.568 --> 00:01:59.228
You couldn't really achieve less than 1 clock per item scanned.

