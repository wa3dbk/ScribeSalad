WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.850
Okay. Well, this first code snippet doesn't assign streams at all.

00:00:02.850 --> 00:00:07.715
So none of these operations declare a stream, and therefore they're all in the default stream.

00:00:07.715 --> 00:00:10.578
And since they are in the same stream, each one must complete

00:00:10.578 --> 00:00:12.950
before the next one in that stream is allowed to run.

00:00:12.950 --> 00:00:17.697
So cudaMemcpy will run for 1 second, kernel A will launch and run for 1 second,

00:00:17.697 --> 00:00:21.590
and cudaMemcpy will run from another second, will copy the memory down,

00:00:21.590 --> 00:00:24.095
do some operation on it, copy the memory back up.

00:00:24.095 --> 00:00:27.539
And after 3 seconds the results will be ready for the host.

00:00:27.539 --> 00:00:32.435
This next code snippet does declare a stream. It puts them all in s1.

00:00:32.435 --> 00:00:34.849
And so since all 3 operations are in the same stream,

00:00:34.849 --> 00:00:38.517
the previous operation must complete before the next one can start.

00:00:38.517 --> 00:00:40.919
So once again, this will take a second, this will take a second,

00:00:40.919 --> 00:00:44.777
this will take a second, and the final result will be ready in 3 seconds.

00:00:44.777 --> 00:00:46.561
This one's a bit more complicated.

00:00:46.561 --> 00:00:49.223
Here I'm doing 2 operations on different chunks of memory,

00:00:49.223 --> 00:00:56.607
so I'm copying h array 1 into d array 1, calling kernel A on d array 1,

00:00:56.607 --> 00:00:59.978
and copying d array 1 back to h array 1.

00:00:59.978 --> 00:01:07.531
And then I'm copying h array 2 to d array 2, doing some operation on d array 2 by launching kernel B,

00:01:07.531 --> 00:01:10.524
and copying d array 2 back to h array 2.

00:01:10.524 --> 00:01:13.218
So the first 3 operations are in stream 1.

00:01:13.218 --> 00:01:15.820
So cudaMemcpyAsync will be called,

00:01:15.820 --> 00:01:18.907
and it'll start a memory copy to happen on the GPU

00:01:18.907 --> 00:01:21.960
and then CPU execution will continue; the next statement will happen.

00:01:21.960 --> 00:01:25.872
We don't stop and wait for Memcpy to finish before we execute the next statement,

00:01:25.872 --> 00:01:27.895
which is the launch of kernel A.

00:01:27.895 --> 00:01:30.001
Kernel launches are always asynchronous.

00:01:30.001 --> 00:01:33.653
So, in effect, this queues up the launch of kernel A,

00:01:33.653 --> 00:01:36.610
which will be run on the GPU after this Memcpy has finished,

00:01:36.610 --> 00:01:40.454
and it proceeds to the next statement, which is another cudaMemcpyAsync,

00:01:40.454 --> 00:01:44.199
to copy the results of kernel A back onto the host.

00:01:44.199 --> 00:01:48.188
All 3 of these are in stream 1, so they will all wait for each other on the GPU,

00:01:48.188 --> 00:01:51.959
but the host just issues a command, issues a command, issues a command.

00:01:51.959 --> 00:01:57.396
It doesn't stop. It proceeds to cudaMemcpyAsync in s2.

00:01:57.396 --> 00:02:00.840
And so now it starts another asynchronous memory transfer.

00:02:00.840 --> 00:02:05.708
This one, because it's in s2, doesn't have to wait for any of these operations to complete,

00:02:05.708 --> 00:02:08.685
so it gets started right away. It'll complete in 1 second.

00:02:08.685 --> 00:02:13.645
It calls kernel B in 1 block of 192 threads, also in s2,

00:02:13.645 --> 00:02:19.265
and then calls cudaMemcpyAsync, again in s2, to copy the results back.

00:02:19.265 --> 00:02:25.013
So the first memory copy will happen, these next 2 operations in s1 will get queued up,

00:02:25.013 --> 00:02:28.954
and the next memory copy will happen essentially immediately.

00:02:28.954 --> 00:02:32.161
So in the first second this Memcpy and this Memcpy will be running,

00:02:32.161 --> 00:02:35.441
in the next second kernel A will be running and kernel B will be running,

00:02:35.441 --> 00:02:39.170
and in the final second the return Memcpy will be running in s1

00:02:39.170 --> 00:02:41.986
and the return Memcpy will be running in s2.

00:02:41.986 --> 00:02:44.043
And so the minimum amount of time this can take,

00:02:44.043 --> 00:02:49.384
if all of the operations that can run concurrently do run concurrently, will again be 3 seconds.

00:02:49.384 --> 00:02:52.956
But in this 3 seconds we'll have actually gotten twice as much work done.

00:02:52.956 --> 00:02:54.927
We've overlapped the execution here.

00:02:54.927 --> 00:02:59.591
And I phrased this very carefully when I talked about the minimum time this is going to take.

00:02:59.591 --> 00:03:05.496
It's worth noting that there's a lot of reasons why it might not be able to overlap all of these operations.

00:03:05.496 --> 00:03:10.035
Some older GPUs can't do this many asynchronous Memcpys at the same time.

00:03:10.035 --> 00:03:16.721
And I was pretty careful to launch a single block, so this probably will not fill the GPU.

00:03:16.721 --> 00:03:20.952
On the other hand, if kernel A had run tens of thousands of blocks,

00:03:20.952 --> 00:03:24.014
it might well fill up the GPU with those blocks

00:03:24.014 --> 00:03:29.026
before kernel B got a chance to start filling the GPU with any of its blocks.

00:03:29.026 --> 00:03:33.959
And so kernel B might end up waiting on kernel A simply because resources are not available to run it.

00:03:33.959 --> 00:03:38.195
The final example is the same operations just reorganized.

00:03:38.195 --> 00:03:41.765
So in this case we issue both Memcpys in stream 1 and 2,

00:03:41.765 --> 00:03:47.364
we launch both kernels in stream 1 and 2, and we issue both Memcpys back in stream 1 and 2.

00:03:47.364 --> 00:03:49.645
So rearranging these things shouldn't make a difference.

00:03:49.645 --> 00:03:53.110
The minimum time that this could take is still 3 seconds.

