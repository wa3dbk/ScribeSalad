WEBVTT
Kind: captions
Language: en

00:00:00.490 --> 00:00:03.250
So now we've seen conceptually how MapReduce works. In the

00:00:03.250 --> 00:00:05.990
next lesson, we'll talk about how to write the code

00:00:05.990 --> 00:00:09.510
to perform these MapReduce jobs on your cluster. But before

00:00:09.510 --> 00:00:12.010
that, let's go back to the cluster. Just as we

00:00:12.010 --> 00:00:15.720
was with HDFS, there are a set of daemons, which

00:00:15.720 --> 00:00:18.710
is just a piece of code, running all the time,

00:00:18.710 --> 00:00:21.570
running on each of these machines. There were the data

00:00:21.570 --> 00:00:25.750
nodes and the name node. When you run a MapReduce

00:00:25.750 --> 00:00:27.950
job, you submit the job to what's called

00:00:27.950 --> 00:00:31.394
the Job Tracker. That splits the work into mappers

00:00:31.394 --> 00:00:34.740
and reducers. Those mappers and reducers will run

00:00:34.740 --> 00:00:38.240
on the other cluster nodes. Running the actual map

00:00:38.240 --> 00:00:41.140
and reduce tasks is handled by a daemon

00:00:41.140 --> 00:00:44.880
called the TaskTracker. The TaskTracker software will run on

00:00:44.880 --> 00:00:47.445
each of these nodes. Note that since the

00:00:47.445 --> 00:00:50.860
TaskTracker runs on the same machine as the data

00:00:50.860 --> 00:00:53.560
nodes, the Hadoop framework will be able to have

00:00:53.560 --> 00:00:56.980
the map tasks work directly on the pieces of data

00:00:56.980 --> 00:00:59.240
that are stored on that machine. This will save

00:00:59.240 --> 00:01:02.590
a lot of network traffic. As we saw, each Mapper

00:01:02.590 --> 00:01:06.010
processes a portion of the input data. That's known

00:01:06.010 --> 00:01:09.920
as the input split. And by default, Hadoop use an

00:01:09.920 --> 00:01:13.860
HDFS block as the input split for each Mapper.

00:01:13.860 --> 00:01:16.220
It will try to make sure that a Mapper works

00:01:16.220 --> 00:01:19.600
on data on the same machine. If this green

00:01:19.600 --> 00:01:22.990
black, for example, needs processing, then the TaskTracker on this

00:01:22.990 --> 00:01:25.540
machine will likely be the one chosen to process

00:01:25.540 --> 00:01:29.880
that block. That won't always be possible, because the TaskTrackers

00:01:29.880 --> 00:01:32.350
on these three machines that have the green block

00:01:32.350 --> 00:01:36.010
could already be busy. In which case, a different node

00:01:36.010 --> 00:01:38.400
will be chosen to process the green block, and

00:01:38.400 --> 00:01:41.390
it will be streamed over the network. This actually happens

00:01:41.390 --> 00:01:43.750
rather rarely. So the Mappers will

00:01:43.750 --> 00:01:46.860
read their input data. They'll produce intermediate

00:01:46.860 --> 00:01:50.900
data, which the Hadoop framework will pass to the reducers, remember that's the

00:01:50.900 --> 00:01:55.390
shuffle and sort. Then the reducers process that data and write their final

00:01:55.390 --> 00:02:00.370
output back to HDFS. So let's have Ian run a job on our cluster.

