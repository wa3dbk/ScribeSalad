WEBVTT
Kind: captions
Language: en

00:00:00.230 --> 00:00:02.420
Alright, so this is now the Q in the equation, again.

00:00:02.420 --> 00:00:06.130
Which again is one of these alphabased things. And, just to

00:00:06.130 --> 00:00:08.860
be clear, I really do mean, you know, alpha sub t.

00:00:08.860 --> 00:00:10.750
That we're, that were doing this over time. We're updating our

00:00:10.750 --> 00:00:13.160
learning rates as we go. It's just, sometimes it's a little

00:00:13.160 --> 00:00:17.370
irritating to put that, that there. But but yeah. So let's

00:00:17.370 --> 00:00:19.880
imagine that we're doing that. We're, we're, we're using that same

00:00:19.880 --> 00:00:25.770
kind of weighted process. Bumping the values around. So, what would,

00:00:25.770 --> 00:00:26.750
based on what we just talked about,

00:00:26.750 --> 00:00:28.610
Charles, what would this actually be computing?

00:00:28.610 --> 00:00:36.460
&gt;&gt; Well, it would be computing the average value that you would get for

00:00:36.460 --> 00:00:39.930
following, you know, kind of optimal policy

00:00:39.930 --> 00:00:42.260
after you take this particular action. Yeah, that

00:00:42.260 --> 00:00:46.340
it's, it's trying to go to this expected value and, and what is that

00:00:46.340 --> 00:00:48.980
expected value. So the linearity of expectation

00:00:48.980 --> 00:00:51.740
says that we can actually move the expectation

00:00:51.740 --> 00:00:55.110
to break up, break up the sum using the expectation. So this,

00:00:55.110 --> 00:00:59.503
the expected value of the reward is actually r of s. Mm-hm.

00:00:59.503 --> 00:01:02.570
&gt;&gt; The gamma's going to come out because of linearity of expectation. ANd

00:01:02.570 --> 00:01:07.110
then what we're left with is the expectation over all next dates of

00:01:07.110 --> 00:01:12.180
the maximum estimated value of the estimated Q value of the next state.

00:01:12.180 --> 00:01:16.760
But what is this distribution over S prime? It's the distribution that is

00:01:16.760 --> 00:01:22.900
determeined by the transtiion function. SO it's this. Which is you know this.

00:01:22.900 --> 00:01:27.310
So that's good. It's I'm kind of cheating though. Do you see how I'm cheating.

00:01:27.310 --> 00:01:29.020
&gt;&gt; I think I know how you are cheating but tell me.

00:01:29.020 --> 00:01:34.400
&gt;&gt; Well so when I told the story about this alpa arrow updating thing. I

00:01:34.400 --> 00:01:37.860
said that it convergence to the expected value

00:01:37.860 --> 00:01:41.670
of this quantity here. But this quantity here...

00:01:41.670 --> 00:01:44.590
Since it's "Q hat", it's actually changing over time.

00:01:44.590 --> 00:01:45.400
&gt;&gt; Right.

00:01:45.400 --> 00:01:48.640
&gt;&gt; So this target is actually a moving target. It's changing over time. So we

00:01:48.640 --> 00:01:52.440
can't quite do this analysis because the

00:01:52.440 --> 00:01:54.770
first step is a little bit questionable. But

00:01:54.770 --> 00:01:59.915
it turns out that there really is a theorem that this simple update rule, this

00:01:59.915 --> 00:02:02.420
Q-learning update rule, this tiny little one

00:02:02.420 --> 00:02:05.540
line of code, actually solves Markov decision processes.

00:02:05.540 --> 00:02:06.770
&gt;&gt; Yeah.

