WEBVTT
Kind: captions
Language: en

00:00:00.036 --> 00:00:02.935
Alright, let's get through these. So picking good algorithms,

00:00:02.935 --> 00:00:06.176
obviously this is the most important level of optimization,

00:00:06.176 --> 00:00:13.381
and when sorting a large random list, an order N log N algorithm like mergesort is just intrinsically going to be

00:00:13.381 --> 00:00:19.028
faster than an order N squared algorithm like insertion sort, and clearly this is a case of choosing the right algorithm.

00:00:19.028 --> 00:00:24.427
You know much of performance CPU programming relies on making efficient use of the cache.

00:00:24.427 --> 00:00:29.052
This is sort of the number one thing to keep in mind when you're trying to write efficient CPU code,

00:00:29.052 --> 00:00:32.713
is that there is this big cache hierarchy and you want to write code that's going to make good use of it.

00:00:32.713 --> 00:00:38.074
And so, I would consider this an example of optimization level 2,

00:00:38.074 --> 00:00:41.078
right, it's a basic principle of efficiency on a CPU.

00:00:41.078 --> 00:00:43.714
On the other hand, blocking for the L1 cache,

00:00:43.714 --> 00:00:48.383
which means carefully sizing your working set to fit exactly in the per-core cache on the CPU.

00:00:48.383 --> 00:00:53.389
This is a detailed optimization. It's going to depend on the exact CPU model you're using

00:00:53.389 --> 00:00:56.031
because every CPU core has different size L1 cache.

00:00:56.031 --> 00:01:01.563
I would also put the use of vector registers like SSE or AVX intrinsics into this category.

00:01:01.563 --> 00:01:09.738
Not every CPU has SSE; some have AVX. Some don't have either. So, the use of vector registers,

00:01:09.738 --> 00:01:10.402
the choice of blocking for the L1 cache, cache blocking,

00:01:12.807 --> 00:01:15.561
I would consider these architecture-specific detailed optimizations.

00:01:15.561 --> 00:01:22.502
So these kind of architectural-specific optimizations tend to be the domain of what I call ninja programmers.

00:01:22.502 --> 00:01:27.505
And we'll also touch on a few ninja-style GPU topics like shared-memory bank conflicts.

00:01:27.505 --> 00:01:31.679
I'll try to highlight these ninja topics with this little icon here.

00:01:31.679 --> 00:01:38.126
The idea is that these are really not needed or necessarily accessible to every programmer.

00:01:38.126 --> 00:01:42.794
They tend to be when you're sort of squabbling over the last few percent of optimizations,

00:01:42.794 --> 00:01:45.290
the last few percent of speedup.

00:01:45.290 --> 00:01:48.760
I think this is one of the big differences between CPU and GPU programming.

00:01:48.760 --> 00:01:54.074
On a CPU, these sort of ninja-level optimizations can make a really big difference.

00:01:54.074 --> 00:02:01.013
For example, if you ignore the existence of SSE registers or AVX registers on really modern processors,

00:02:01.013 --> 00:02:05.352
then you're only getting a fourth or an eighth of the power of each core on your CPU,

00:02:05.352 --> 00:02:07.890
so you're taking a huge hit in potential performance.

00:02:07.890 --> 00:02:14.628
On the GPU, for the most part as a rule of thumb, the speedups to be gained by these sort of ninja optimizations

00:02:14.628 --> 00:02:21.665
are usually comparatively minor. Okay, so we're talking more like a 30% or 80% speedup

00:02:21.665 --> 00:02:25.119
by using some of the techniques that we'll talk about with this little ninja icon

00:02:25.119 --> 00:02:29.151
versus the speedup that you might hope to get from just

00:02:29.151 --> 00:02:32.520
picking the right algorithm in the first place or just obeying the

00:02:32.520 --> 00:02:38.325
basic principles of efficiency on a GPU such as coalescing your global memory accesses.

00:02:38.325 --> 00:02:44.028
These can also often make a difference of 3 times, 10 times, sometimes more.

00:02:44.028 --> 00:02:53.008
So these, just doing a good job, the sort of higher level principles of optimization matter more on the GPU,

00:02:53.008 --> 00:02:57.541
and the ninja level optimizations definitely help, sometimes you can get a speedup here,

00:02:57.541 --> 00:03:02.784
but it's not a sort of vital step to extract maximum performance the way that it is on a CPU

00:03:02.784 --> 00:03:07.021
where if you don't start doing something to make sure you're using the vector registers,

00:03:07.021 --> 00:03:11.691
you're going to take a huge hit in terms of the efficiency that you get out of your modern CPU.

00:03:11.691 --> 00:03:15.497
Finally, this last one is, I really just threw that in there for fun.

00:03:15.497 --> 00:03:19.419
Clearly, shifting a number, casting it, shifting a floating point number,

00:03:19.419 --> 00:03:22.786
casting it to a float and subtracting it from this magic constant,

00:03:22.786 --> 00:03:27.634
is firmly in the category of micro optimization at this sort of bit-twiddling instruction level.

00:03:27.634 --> 00:03:32.866
This is an infamous hack that's been used in many places, most famously in the video game Quake 3,

00:03:32.882 --> 00:03:38.114
and if you're curious go to Wikipedia and look up fast inverse square root.

00:03:38.114 --> 00:03:43.754
Of course these are firmly in the ninja category, and we won't be talking about them at all today or in rest of the course.

