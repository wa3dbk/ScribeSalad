WEBVTT
Kind: captions
Language: en

00:00:00.170 --> 00:00:03.469
Now we will look at the Vapnik Chervonenkis dimension.

00:00:04.860 --> 00:00:08.690
This gives us a way to measure the complexity of the model.

00:00:08.690 --> 00:00:12.910
Remember the complexity of the model determines the performance of

00:00:12.910 --> 00:00:16.950
the cost on both the training and the test sets.

00:00:16.950 --> 00:00:21.870
There are various techniques to determine bounds on the generalization error,

00:00:21.870 --> 00:00:22.800
on the test set.

00:00:22.800 --> 00:00:27.520
A good treatment of this is given in the reference in the instructor's notes.

00:00:27.520 --> 00:00:30.140
Let us now look at the following expression.

00:00:30.140 --> 00:00:35.640
This shows that the error calculated on the test set has an upper bound.

00:00:35.640 --> 00:00:41.280
We can quantify a degree of belief in this upper bound using a parameter eta.

00:00:41.280 --> 00:00:47.945
Such that given eta for example 0.01 we have a probability of

00:00:47.945 --> 00:00:54.040
91% that the test error is bounded above by the expression inside the radical.

00:00:54.040 --> 00:00:59.250
Also notice the expression inside the radical depends on the quantity h.

00:01:00.350 --> 00:01:04.489
This quantity h quantifies the complexity of the model.

00:01:04.489 --> 00:01:06.800
Let's look at the following diagram.

00:01:06.800 --> 00:01:07.850
In this diagram,

00:01:07.850 --> 00:01:13.060
we're increasing the complexity of the model as we go along the horizontal axis.

00:01:13.060 --> 00:01:17.520
The cost calculated on each of the data set is on the vertical axis.

00:01:17.520 --> 00:01:19.230
Now if you look at the training term,

00:01:19.230 --> 00:01:24.050
which is the training error here, you will have curve like this.

00:01:24.050 --> 00:01:26.480
Looking at the term under the radical sign,

00:01:26.480 --> 00:01:29.580
here, is the complexity term that increases like that.

00:01:30.770 --> 00:01:34.980
So you see, the test error is upper bounded by this red curve.

00:01:36.230 --> 00:01:39.750
At some point, we can select the minimum of the test error,

00:01:39.750 --> 00:01:41.370
which is something over here.

00:01:41.370 --> 00:01:46.860
If we fix the sample size N, and the training set error is given,

00:01:46.860 --> 00:01:49.360
then we see that the expression here,

00:01:49.360 --> 00:01:55.180
depends on the quantity h which we have already introduced as the VC dimension.

00:01:55.180 --> 00:01:58.840
There are various ways one can calculate the VC dimension and

00:01:58.840 --> 00:02:02.960
the topic is covered in detail in the references and the instructor's notes.

00:02:04.510 --> 00:02:09.160
For general models, calculating the VC dimension is a challenging problem.

00:02:09.160 --> 00:02:12.920
However, for linear classifier models with m dimensions or

00:02:12.920 --> 00:02:17.590
variables, or features, we have a very simple expressions.

00:02:17.590 --> 00:02:23.204
The VC dimension is given as h which is simply m plus 1.

