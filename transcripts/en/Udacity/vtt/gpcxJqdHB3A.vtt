WEBVTT
Kind: captions
Language: en

00:00:00.540 --> 00:00:03.500
Talk about HMMs just in general,
and then we'll move onto vision.

00:00:03.500 --> 00:00:08.600
In general, HMMs are generative
models of time series,

00:00:08.600 --> 00:00:11.290
with some notion of a hidden state, that
is they can talk about the probability

00:00:11.290 --> 00:00:13.730
of generating a particular output.

00:00:13.730 --> 00:00:17.950
The forward-backward algorithms, those
recursive ones allow you for computing

00:00:17.950 --> 00:00:24.000
over the states and you can also use
that computing to train up your models.

00:00:24.000 --> 00:00:28.870
And I will say that HMMs were their
big win was in originally in speech

00:00:28.870 --> 00:00:33.690
recognition dealing with low level
auditory features and using that to

00:00:33.690 --> 00:00:38.550
recover phonemes and then phonemes into
parts of words and things like that.

00:00:38.550 --> 00:00:42.180
And then in Computer Vision, although I
will say there's a whole new area these

00:00:42.180 --> 00:00:46.590
days called conditional random fields,
which is really better as I mentioned.

00:00:46.590 --> 00:00:50.550
If you've got a long sequence and
you want to do segmentation,

00:00:50.550 --> 00:00:53.290
conditional random fields are a slightly
more complicated version.

00:00:53.290 --> 00:00:55.140
Actually, they're
significantly more complicated

00:00:56.450 --> 00:00:59.400
graphical models of
representing time series.

00:00:59.400 --> 00:01:02.980
And it tends to do better than HMMs for
the actual segmentation.

00:01:02.980 --> 00:01:06.150
But from the notion of a generative
model of saying, was it an A, or a B, or

00:01:06.150 --> 00:01:08.830
a C HMMs are still used extensively.

