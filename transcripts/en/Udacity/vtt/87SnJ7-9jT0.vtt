WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.000
Welcome back. We're about to start the final unit in this course.

00:00:04.000 --> 00:00:08.000
This time we're mostly going to focus on review and some practice problems

00:00:08.000 --> 00:00:11.000
to get you ready for the exam or at least to get you in the mood for the exam.

00:00:11.000 --> 00:00:13.000
But there may also be just a little bit of fun.

00:00:13.000 --> 00:00:16.000
You may get the chance to hear someone who is not Wes Weimer talking.

00:00:16.000 --> 00:00:18.000
I know, I know.

00:00:18.000 --> 00:00:21.000
Someone with real voice inflection? Be still, your beating heart.

00:00:21.000 --> 00:00:24.000
But before we get going with practice problems, let us make The List,

00:00:24.000 --> 00:00:27.000
a high level summary of everything you've learned thus far.

00:00:27.000 --> 00:00:32.000
We started off by introducing the concept of a language as a set of strings.

00:00:32.000 --> 00:00:35.000
Regular expressions, finite state machines, formal grammars--

00:00:35.000 --> 00:00:39.000
these all denote or accept or correspond to sets of strings.

00:00:39.000 --> 00:00:44.000
In fact, the set of all valid JavaScript programs is just a set of really big strings.

00:00:44.000 --> 00:00:48.000
One of the first tools we introduced was regular expressions,

00:00:48.000 --> 00:00:52.000
which are just a concise notation for specifying some sets of strings.

00:00:52.000 --> 00:00:55.000
Those sets of strings are called regular languages.

00:00:55.000 --> 00:00:59.000
An incredible surprise move: regular expressions denote regular languages.

00:00:59.000 --> 00:01:05.000
And we learned a bunch of regular expressions--+, *, disjunctive choice,

00:01:05.000 --> 00:01:10.000
ranges of letters, 0 or 1 copies--and we ended up using these to specify tokens.

00:01:10.000 --> 00:01:12.000
More on that in just a bit.

00:01:12.000 --> 00:01:15.000
Then we learned about finite state machines, which are a cool way

00:01:15.000 --> 00:01:19.000
to draw regular expressions and also, it turns out,

00:01:19.000 --> 00:01:21.000
a way that we implement them under the hood.

00:01:21.000 --> 00:01:23.000
That's how Python actually does it.

00:01:23.000 --> 00:01:26.000
Here I've shown a finite state machine for ab*.

00:01:26.000 --> 00:01:30.000
Possible to have ambiguity or epsilon transitions in a finite state machine.

00:01:30.000 --> 00:01:33.000
That makes a finite state machine nondeterministic,

00:01:33.000 --> 00:01:37.000
because if you're trying to simulate it, you don't know exactly where to go at any given point.

00:01:37.000 --> 00:01:40.000
It turns out, however, that is not a problem at all.

00:01:40.000 --> 00:01:43.000
We can convert nondeterministic finite state machines

00:01:43.000 --> 00:01:45.000
down into deterministic finite state machines.

00:01:45.000 --> 00:01:47.000
They may get a little bit bigger, but it will totally work.

00:01:47.000 --> 00:01:51.000
Then we moved on to the more powerful context-free grammars,

00:01:51.000 --> 00:01:54.000
which are a concise notation for specifying some sets of strings.

00:01:54.000 --> 00:01:56.000
Wait! I thought that's what regular expressions were.

00:01:56.000 --> 00:02:02.000
Actually, they're both just concise notations for specifying possibly infinite sets of strings.

00:02:02.000 --> 00:02:07.000
And your typical context-free grammar is just a set of rewrite rules

00:02:07.000 --> 00:02:10.000
with a nonterminal symbol on the left, an arrow,

00:02:10.000 --> 00:02:13.000
and then some terminals and nonterminals on the right.

00:02:13.000 --> 00:02:16.000
Terminals are the same thing as tokens.

00:02:16.000 --> 00:02:18.000
They're the actual input that we're trying to match.

00:02:18.000 --> 00:02:21.000
There are some cool things that we can do with context-free grammars,

00:02:21.000 --> 00:02:24.000
like matching balanced parentheses, that we could not do--

00:02:24.000 --> 00:02:29.000
we're certain we cannot do it, it is impossible to do correctly--with regular expressions.

00:02:29.000 --> 00:02:34.000
We often want to check and see if a string is in the language of a context-free grammar

00:02:34.000 --> 00:02:36.000
or matches that context-free grammar,

00:02:36.000 --> 00:02:39.000
can be derived or generated by that context-free grammar--

00:02:39.000 --> 00:02:43.000
these are all the same question--and one way to do that was memoization,

00:02:43.000 --> 00:02:48.000
which for many years I always wanted to call "memorization," but it's just not.

00:02:48.000 --> 00:02:52.000
It's also called dynamic programming, which sounds really exciting,

00:02:52.000 --> 00:02:57.000
but in practice basically builds charts where we write down previously computed results

00:02:57.000 --> 00:02:59.000
so that we don't have to compute them again.

00:02:59.000 --> 00:03:04.000
This is called being lazy, and it's a phenomenal virtue when you're writing programs.

00:03:04.000 --> 00:03:08.000
We can combine context-free grammars and, potentially, memoization together

00:03:08.000 --> 00:03:13.000
to get parsing, which is the process of determining if a list of tokens

00:03:13.000 --> 00:03:16.000
is in the language of a context-free grammar.

00:03:16.000 --> 00:03:18.000
If so, we produce a parse tree.

00:03:18.000 --> 00:03:20.000
Where did we get that list of tokens, you ask?

00:03:20.000 --> 00:03:25.000
The process of lexing breaks a big string, like a web page, up into a list of tokens

00:03:25.000 --> 00:03:27.000
or important words.

00:03:27.000 --> 00:03:30.000
The tokens are specified using regular expressions,

00:03:30.000 --> 00:03:33.000
which means that a lexer is implemented using finite state machines.

00:03:33.000 --> 00:03:35.000
We do lexing first and then parsing.

00:03:35.000 --> 00:03:39.000
I have written them out of order to shake things up.

00:03:39.000 --> 00:03:44.000
Once we have our parse tree, we're getting closer and closer to the meaning of a program.

00:03:44.000 --> 00:03:47.000
One aspect of program semantics or program meanings

00:03:47.000 --> 00:03:54.000
is the notion of types--that we can classify objects or values like 1, 2, and 3 into groups

00:03:54.000 --> 00:03:56.000
and say, "Those are all numbers."

00:03:56.000 --> 00:04:01.000
So a type is just a set of values and some associated operations that you can apply.

00:04:01.000 --> 00:04:05.000
So the values might be things like all numbers, all strings, or all lists,

00:04:05.000 --> 00:04:09.000
and the operations might be things like + - / or length.

00:04:09.000 --> 00:04:11.000
I can apply length to a string or a list but not a number.

00:04:11.000 --> 00:04:15.000
I can add numbers, strings, and lists, but it means something different every time.

00:04:15.000 --> 00:04:19.000
I can divide numbers, but I can't really divide strings or lists,

00:04:19.000 --> 00:04:21.000
at least not using this division operator.

00:04:21.000 --> 00:04:24.000
Types are our first step along the road to meaning,

00:04:24.000 --> 00:04:27.000
and in computer science we formally call that semantics.

00:04:27.000 --> 00:04:29.000
By the way, if you've been wondering the whole time,

00:04:29.000 --> 00:04:32.000
semantics is a tricky word that essentially always ends in an S,

00:04:32.000 --> 00:04:36.000
even when we're using it in sort of a singular fashion.

00:04:36.000 --> 00:04:39.000
Semantics of a program: its meaning, what does it compute.

00:04:39.000 --> 00:04:44.000
A program may have type errors, like if you try to divide a string by an integer,

00:04:44.000 --> 00:04:46.000
or it may have any number of other exceptions.

00:04:46.000 --> 00:04:49.000
But in the general case, it produces a value.

00:04:49.000 --> 00:04:52.000
This means that we have computed something. That was the result.

00:04:52.000 --> 00:04:57.000
That's the meaning of our program, just like a sentence in English or French

00:04:57.000 --> 00:04:59.000
or Cantonese might have a meaning.

00:04:59.000 --> 00:05:02.000
Once we have a grip on semantics, we can introduce optimization,

00:05:02.000 --> 00:05:07.000
where we replace one program with another or, conceptually, one part of a program

00:05:07.000 --> 00:05:11.000
with another as long as the whole thing has the same semantics.

00:05:11.000 --> 00:05:13.000
This is the critical rule of optimization.

00:05:13.000 --> 00:05:15.000
You can't change the meaning of the program.

00:05:15.000 --> 00:05:17.000
If you can't change the meaning, what can you change?

00:05:17.000 --> 00:05:22.000
Typically, the new code you've brought in uses fewer resources--

00:05:22.000 --> 00:05:26.000
less time, less memory, consumes less power--

00:05:26.000 --> 00:05:28.000
and we've seen a bunch of examples of these.

00:05:28.000 --> 00:05:32.000
x * 1 could be replaced with just x,

00:05:32.000 --> 00:05:36.000
but x / x cannot be replaced with 1

00:05:36.000 --> 00:05:39.000
because in the single case where x is 0, this changes the meaning of the program.

00:05:39.000 --> 00:05:44.000
After optimizing, or not--you never have to optimize--we can move on to interpretation.

00:05:44.000 --> 00:05:46.000
This is the fun part.

00:05:46.000 --> 00:05:49.000
We recursively walk over the parse tree,

00:05:49.000 --> 00:05:54.000
and the meaning of a program, the final result, the picture we should display for a web page,

00:05:54.000 --> 00:05:57.000
the result of a computation in a financial program,

00:05:57.000 --> 00:06:01.000
is computed from the meanings of its subexpressions.

00:06:01.000 --> 00:06:04.000
So if I'm in a state or environment where a maps to 5,

00:06:04.000 --> 00:06:08.000
I can compute the meaning of this abstract syntax tree expression.

00:06:08.000 --> 00:06:12.000
Well, we have times and plus. I'll go down here and figure out what a is.

00:06:12.000 --> 00:06:16.000
a is 5, 3 is 3. I multiply them together and I get 15.

00:06:16.000 --> 00:06:18.000
Over here, 1 and 2. I add them together and I get 3.

00:06:18.000 --> 00:06:20.000
The whole thing I get 18.

00:06:20.000 --> 00:06:24.000
Walk down the tree on both sides, and only as I'm coming back up do I compute the values.

00:06:24.000 --> 00:06:27.000
Typically, to perform interpretation we have to track state,

00:06:27.000 --> 00:06:32.000
like the values of variables which may change, in environments.

00:06:32.000 --> 00:06:36.000
And environments are often chained together, especially as you make function calls.

00:06:36.000 --> 00:06:40.000
Finally, we put all of that together to build our web browser.

00:06:40.000 --> 00:06:42.000
We followed a particular architecture.

00:06:42.000 --> 00:06:45.000
You could imagine doing another one, but this is the one we used for this class.

00:06:45.000 --> 00:06:50.000
We start by lexing and parsing HTML, treating any embedded JavaScript

00:06:50.000 --> 00:06:52.000
as a special token.

00:06:52.000 --> 00:06:55.000
Our HTML interpreter walks over the HTML parse tree,

00:06:55.000 --> 00:07:00.000
and whenever it gets to this special JavaScript token, it calls the JavaScript interpreter,

00:07:00.000 --> 00:07:02.000
which just returns a string.

00:07:02.000 --> 00:07:04.000
We got the string from a bunch of calls to document.write().

00:07:04.000 --> 00:07:08.000
The HTML interpreter gathers up all the words--words in HTML

00:07:08.000 --> 00:07:12.000
or words computed by JavaScript--and just calls the graphics library to display them.

00:07:12.000 --> 09:59:59.000
Wow! And then we're done.

