WEBVTT
Kind: captions
Language: en

00:00:00.320 --> 00:00:03.469
There are two finer points that I
wanted to mention about Q-learning.

00:00:03.469 --> 00:00:09.380
First is that the success of
Q-learning depends to some extent,

00:00:09.380 --> 00:00:12.190
well, to a large extent on exploration.

00:00:12.190 --> 00:00:18.760
So we need to explore as much of
the state and action space as possible.

00:00:18.760 --> 00:00:21.350
One way to accomplish
this is with randomnes.

00:00:21.350 --> 00:00:26.070
And the way we can interject that fairly
easily in the step of Q-learning,

00:00:26.070 --> 00:00:30.360
where we are selecting an action,
we flip a coin and

00:00:30.360 --> 00:00:35.080
randomly decide if we're going to
randomly choose an action.

00:00:35.080 --> 00:00:38.360
So that means really
two flips of the coin.

00:00:38.360 --> 00:00:41.490
The first is are we going to
choose a random action or

00:00:41.490 --> 00:00:44.910
are we just going to pick the action
with the highest Q value?

00:00:44.910 --> 00:00:49.300
Then if we decide okay, we're going to
choose an action randomly, now we need

00:00:49.300 --> 00:00:52.770
to flip the coin again to choose which
of those actions we're going to select.

00:00:54.010 --> 00:00:59.138
Typical way to implement this is
to set this probability at about

00:00:59.138 --> 00:01:03.490
0.3 or
something at the beginning of learning.

00:01:04.840 --> 00:01:08.820
And then over each iteration to slowly
make it smaller and smaller and

00:01:08.820 --> 00:01:13.660
smaller until eventually we essentially
don't choose random actions at all.

00:01:13.660 --> 00:01:18.740
What we gain here is when we're choosing
these random actions fairly frequently

00:01:18.740 --> 00:01:21.730
is we're forcing
the system to explore and

00:01:21.730 --> 00:01:24.590
try different actions
in different states.

00:01:24.590 --> 00:01:28.070
And it also causes us to arrive at
different states that we might not

00:01:28.070 --> 00:01:31.340
otherwise arrive at if we didn't
try those random actions.

