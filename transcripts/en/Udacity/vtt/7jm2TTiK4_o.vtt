WEBVTT
Kind: captions
Language: en

00:00:00.630 --> 00:00:03.980
Most of the time we don't have
this transition function,

00:00:03.980 --> 00:00:07.280
and we don't have
the reward function either.

00:00:07.280 --> 00:00:08.990
So the robot, or

00:00:08.990 --> 00:00:14.430
the trader, whatever environment we're
in, has to interact with the world,

00:00:14.430 --> 00:00:19.260
observe what happens, and work with
that data to try to build a policy.

00:00:19.260 --> 00:00:21.490
So let me give you an example here.

00:00:21.490 --> 00:00:24.030
Let's say we were in state S1.

00:00:24.030 --> 00:00:25.710
So, that's what we observed there.

00:00:25.710 --> 00:00:29.579
Our robot took action, A1.

00:00:29.579 --> 00:00:34.430
I'm making this little subscript
to indicate which step in

00:00:34.430 --> 00:00:37.500
this series of steps it's at.

00:00:37.500 --> 00:00:40.900
We then find our self in S'.

00:00:40.900 --> 00:00:43.500
And we get reward R.

00:00:43.500 --> 00:00:47.560
Now this is an experience tubal.

00:00:47.560 --> 00:00:51.720
This is very similar to experience
tubal in regression learning

00:00:51.720 --> 00:00:55.780
where we have an X and
a Y paired together.

00:00:55.780 --> 00:01:00.520
That's an experience tubal of you know,
when you observe this X you see this Y.

00:01:00.520 --> 00:01:05.340
Here we're saying when you observe
the state, S1, you take action, A1,

00:01:05.340 --> 00:01:09.180
you end up in this new state, at least
it's an example of you ending up in this

00:01:09.180 --> 00:01:13.730
new state S1', and reward, R1.

00:01:13.730 --> 00:01:15.870
Now we find ourselves in a new state S2,

00:01:15.870 --> 00:01:20.490
but that's really,
this state is where we found our self.

00:01:20.490 --> 00:01:25.180
We take some new action,
A2, we end up in some

00:01:25.180 --> 00:01:30.050
new state, S2', and
we get a new reward, R2.

00:01:30.050 --> 00:01:33.260
When we do this over and
over and over and over and

00:01:33.260 --> 00:01:37.000
over again, gathering experience
tuples all along the way.

00:01:38.390 --> 00:01:42.960
Now, if we have this trail
of experience tuples,

00:01:42.960 --> 00:01:46.800
there's two things we can do with
them in order to find that policy pi.

00:01:48.070 --> 00:01:52.540
The first set of approaches is called
model based reinforcement learning.

00:01:52.540 --> 00:01:55.660
What we do is we look at
this data over time and

00:01:55.660 --> 00:02:01.320
we build a model of T just by looking
statistically at these transitions.

00:02:01.320 --> 00:02:05.380
In other words we can look a every
time we were in a particular state and

00:02:05.380 --> 00:02:07.180
took a particular action.

00:02:07.180 --> 00:02:09.900
And see which new states we ended up in.

00:02:09.900 --> 00:02:12.810
And just build a tabular
representation of that.

00:02:12.810 --> 00:02:14.190
It's not hard.

00:02:14.190 --> 00:02:17.930
Similarly, we can build a model of R.

00:02:17.930 --> 00:02:21.620
We just look statistically when
we're in a particular state, and

00:02:21.620 --> 00:02:24.500
we take an action, what's the reward?

00:02:24.500 --> 00:02:28.600
We can just average that
over all these instances.

00:02:28.600 --> 00:02:33.340
Once we have these models,
we can then use value iteration or

00:02:33.340 --> 00:02:36.080
policy iteration to solve the problem.

00:02:36.080 --> 00:02:38.750
There's another set of
approaches called model-free.

00:02:38.750 --> 00:02:41.720
And that's the type
we're going to focus on.

00:02:41.720 --> 00:02:45.510
In particular we're going to
learn about Q-learning.

00:02:45.510 --> 00:02:52.690
And model-free methods develop a policy
just directly by looking at the data.

00:02:52.690 --> 00:02:54.600
And of course we'll
talk about those soon.

