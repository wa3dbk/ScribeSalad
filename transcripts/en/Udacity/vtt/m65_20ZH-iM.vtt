WEBVTT
Kind: captions
Language: en

00:00:00.070 --> 00:00:01.460
&gt;&gt; So, let's look at the last quantity that

00:00:01.460 --> 00:00:03.930
we haven't talked about so far. And that is the

00:00:03.930 --> 00:00:08.109
probability of the hypothesis. Well, just like the probability of

00:00:08.109 --> 00:00:10.810
D is the prior on the data, this is in

00:00:10.810 --> 00:00:15.120
fact your prior on the hypothesis. So, just like

00:00:15.120 --> 00:00:18.420
the probability of D is a prior on the data.

00:00:18.420 --> 00:00:21.840
The probability of H is a prior on a particular

00:00:21.840 --> 00:00:25.430
hypothesis drawn from the hypothesis space. So in other words,

00:00:25.430 --> 00:00:30.320
in encapsulates our prior belief that one hypothesis is likely

00:00:30.320 --> 00:00:33.400
or unlikely compared to other hypotheses. So in fact what's

00:00:33.400 --> 00:00:35.720
really neat about this from a sort of AI point

00:00:35.720 --> 00:00:38.690
of view is that the prior ,as its called, is in

00:00:38.690 --> 00:00:42.032
fact our domain knowledge. So if every angle that we've

00:00:42.032 --> 00:00:45.106
seen so far, everything that we've said there's always some

00:00:45.106 --> 00:00:47.960
place where we stick in out domain knowledge. Are prior

00:00:47.960 --> 00:00:50.800
belief about the way the world works. Whether that's a similarity

00:00:50.800 --> 00:00:56.660
metric for Knn It, it's something about which

00:00:56.660 --> 00:00:58.710
features might be important, so we care about high

00:00:58.710 --> 00:01:01.540
information gain and decision trees, or our belief

00:01:01.540 --> 00:01:04.819
about the, the structure of a neural network. Those

00:01:04.819 --> 00:01:07.410
are prior beliefs, those are, that represents the

00:01:07.410 --> 00:01:10.180
main knowledge. And here in Bayesian Learning, here in

00:01:10.180 --> 00:01:12.580
this notion of, of Bayes' Rule, all of our

00:01:12.580 --> 00:01:16.280
prior knowledge sits here in the probability or prior

00:01:16.280 --> 00:01:19.320
probability over the hypotheses. Does that all make sense?

00:01:19.320 --> 00:01:22.630
&gt;&gt; Yeah its really interesting I guess. So we talked about things like

00:01:22.630 --> 00:01:24.880
kernels and similarity functions as ways

00:01:24.880 --> 00:01:26.850
of capturing this kind of domain knowledge.

00:01:26.850 --> 00:01:29.710
And I guess, I guess what its saying is that its maybe tending

00:01:29.710 --> 00:01:32.540
to prefer or assign higher probability to

00:01:32.540 --> 00:01:35.460
hypothesis that group things a certain way.

00:01:35.460 --> 00:01:37.760
&gt;&gt; exactly right. So, in fact, when you use something

00:01:37.760 --> 00:01:41.240
like Euclidian distance in KNN, what you're saying is,'Well,

00:01:41.240 --> 00:01:45.430
points that are closer together ought to have, similar labels, and so,

00:01:45.430 --> 00:01:49.010
we would believe any hypothesis that puts points that are physically close to

00:01:49.010 --> 00:01:52.950
one another to have similar outputs, we would say, are more likely than

00:01:52.950 --> 00:01:56.230
ones that put points that are very close together to have different outputs.

00:01:56.230 --> 00:01:56.590
&gt;&gt; Neat.

00:01:56.590 --> 00:01:58.880
&gt;&gt; So let me just mention one last thing

00:01:58.880 --> 00:02:01.590
before I give you a quiz, okay? So, see

00:02:01.590 --> 00:02:02.590
if this makes sense, I'm a see if you

00:02:02.590 --> 00:02:06.460
really understand Bayes' rule. So let's imagine that I wanted

00:02:06.460 --> 00:02:11.460
to know under what circumstances the, probability of a hypothesis, given the

00:02:11.460 --> 00:02:15.050
data, goes up. What on the right side of the equation would

00:02:15.050 --> 00:02:17.430
you expect to change, go up or go down, or stay the

00:02:17.430 --> 00:02:21.690
same, that would influence whether the probability of a hypothesis goes up.

00:02:21.690 --> 00:02:24.220
&gt;&gt; So the probability of the hypothesis given

00:02:24.220 --> 00:02:26.560
the data, what could make that combined quantity

00:02:26.560 --> 00:02:29.080
go up, so one is looking at the

00:02:29.080 --> 00:02:31.330
right hand side, the probability of the hypothesis,

00:02:31.330 --> 00:02:32.900
so, so if you have a hypothesis that has

00:02:32.900 --> 00:02:35.600
a higher prior, has, is more likely to be

00:02:35.600 --> 00:02:37.520
a good one. Before you see the data then

00:02:37.520 --> 00:02:39.940
that would raise it after you see the data too.

00:02:39.940 --> 00:02:40.180
&gt;&gt; Right.

00:02:40.180 --> 00:02:46.230
&gt;&gt; And I guess the probability of the data given the hypothesis should

00:02:46.230 --> 00:02:49.700
go up. Oh, which is kind of like accuracy. It's kind of like

00:02:49.700 --> 00:02:52.480
saying that if you pick a hypothesis that does a better job of

00:02:52.480 --> 00:02:56.980
labeling the data, then also your probability of the hypothesis will go up.

00:02:56.980 --> 00:02:58.580
&gt;&gt; Right. Anything else?

00:02:58.580 --> 00:03:01.000
&gt;&gt; I guess the probability of the data going

00:03:01.000 --> 00:03:03.810
down. But that's not really a change from the hypothesis.

00:03:03.810 --> 00:03:05.630
&gt;&gt; Right. But it is true that if those

00:03:05.630 --> 00:03:08.680
goes down, then the probability in the hypothesis can and

00:03:08.680 --> 00:03:09.860
the data will go up. But as you point

00:03:09.860 --> 00:03:13.870
out, it's not connected to the hypothesis directly. And I'll

00:03:13.870 --> 00:03:15.620
write in equation for you in, in just a

00:03:15.620 --> 00:03:17.500
moment that'll kind of make that, I think, a little

00:03:17.500 --> 00:03:19.650
bit clearer. Okay, but you got all this, right? So

00:03:19.650 --> 00:03:21.780
I think you understand it. So we got Bayes' Rule.

00:03:21.780 --> 00:03:23.880
And, notice what we've done. We've gone from this sort

00:03:23.880 --> 00:03:25.710
of general notion of saying we need to find the best

00:03:25.710 --> 00:03:29.120
hypothesis, to actually coming up with an equation, that sort

00:03:29.120 --> 00:03:32.010
of makes explicit what we mean by that. That what we

00:03:32.010 --> 00:03:34.730
care about is the probability of some hypothesis given the

00:03:34.730 --> 00:03:38.040
data. That's what we mean by best. And that, that can

00:03:38.040 --> 00:03:41.700
be further thought as, the probablity of us seeing, some labels

00:03:41.700 --> 00:03:46.820
on some data, given hypothesis. Times the probability of the hypothesis,

00:03:46.820 --> 00:03:49.880
even without any data whatsoever, normalized by the

00:03:49.880 --> 00:03:52.675
probability of the data. So let's play around with

00:03:52.675 --> 00:03:54.130
Bayes' rules a little bit and make certain that

00:03:54.130 --> 00:03:55.580
we all, we all kind of get it. Okay?

00:03:55.580 --> 00:03:56.090
&gt;&gt; Sure.

00:03:56.090 --> 00:03:56.510
&gt;&gt; Okay.

