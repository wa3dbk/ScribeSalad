WEBVTT
Kind: captions
Language: en

00:00:00.180 --> 00:00:03.440
A couple more points about generative
models, they are pretty straightforward.

00:00:03.440 --> 00:00:06.510
First of all,
we've been doing the two class case.

00:00:06.510 --> 00:00:07.040
Right?

00:00:07.040 --> 00:00:09.050
You're either a skin or not skin.

00:00:09.050 --> 00:00:11.770
Well, suppose I've got a bunch
of classes, c sub i, and

00:00:11.770 --> 00:00:13.520
I've got a measurement, x.

00:00:13.520 --> 00:00:14.920
Which c should I choose?

00:00:14.920 --> 00:00:17.130
It's written here in green, c star.

00:00:17.130 --> 00:00:24.200
So, very simply, I'm going to take the
category that maximizes p of c given x.

00:00:24.200 --> 00:00:28.090
That is,
which category is most likely given x?

00:00:28.090 --> 00:00:31.740
Okay, that makes some sense, well,
but I don't have that directly, so

00:00:31.740 --> 00:00:35.670
in order to do that, I use
the likelihood times the, what's p of c?

00:00:35.670 --> 00:00:38.310
That's our prior.

00:00:38.310 --> 00:00:39.680
Okay, so I have to have my prior.

00:00:39.680 --> 00:00:44.840
So, basically in generative models,
as you add a new, category, you

00:00:44.840 --> 00:00:47.920
just need to build a likelihood model
and you need a prior model and that will

00:00:47.920 --> 00:00:52.980
allow you to compare, one category to
the next in, in deciding a new label.

00:00:52.980 --> 00:00:56.410
The other thing is, you remember before,
we were drawing these,

00:00:56.410 --> 00:00:59.800
histograms to represent those,
those probability distributions?

00:00:59.800 --> 00:01:04.450
Well, normally, what you want is a nice,
continuous generative model,

00:01:04.450 --> 00:01:08.250
so what you need is
a likelihood density.

00:01:08.250 --> 00:01:09.470
Okay?
So you don't just have a history,

00:01:09.470 --> 00:01:10.580
I mean you actually want a density.

00:01:10.580 --> 00:01:15.300
And remember, it's likelihood,
so it's p of x given c.

00:01:15.300 --> 00:01:16.530
Not p of c given x.

00:01:16.530 --> 00:01:18.664
That we get from Bayes' rule.

00:01:18.664 --> 00:01:19.440
All right?

00:01:19.440 --> 00:01:24.570
So typically we represent those
likelihoods using some parametric form.

00:01:24.570 --> 00:01:27.710
Here it's written as a,
a mixture of Gaussians, right?

00:01:27.710 --> 00:01:33.120
So here, this particular class 2,
and see it says p of x, given c 2?

00:01:33.120 --> 00:01:34.340
That's its likelihood.

00:01:34.340 --> 00:01:36.610
This is one little Gaussian lump.

00:01:36.610 --> 00:01:39.590
And we do that because we said oh,
that looks you know, may,

00:01:39.590 --> 00:01:43.360
maybe most of the distribution of those
pixel values is actually like that.

00:01:43.360 --> 00:01:49.145
But over here for class 1,
you see that we've got two humps.

00:01:49.145 --> 00:01:50.570
Okay?

00:01:50.570 --> 00:01:52.250
And that's just a mixture of Gaussians.

00:01:52.250 --> 00:01:55.390
And it's pretty,
it's not too difficult to estimate

00:01:55.390 --> 00:01:59.100
a mixture of Gaussians from
a modest amount of data.

00:01:59.100 --> 00:02:03.735
One, so one of the reasons to use these
kinds of parameterized distributions is

00:02:03.735 --> 00:02:07.700
it allows you to use much less
data in order to fit your density.

00:02:07.700 --> 00:02:09.650
So some of you might ask,
well wait a minute.

00:02:09.650 --> 00:02:12.000
Why am I doing this whole
continuous fitting,

00:02:12.000 --> 00:02:15.670
why not just use some like k-nearest
neighbor or Parzen window method or

00:02:15.670 --> 00:02:18.350
something else that you
learned in probability class?

00:02:18.350 --> 00:02:20.482
Megan is not going to remember that
question so I'm not going to ask her.

00:02:20.482 --> 00:02:21.770
Never mind.
So why won't you do that?

00:02:21.770 --> 00:02:23.640
Well, you actually, you might do that.

00:02:23.640 --> 00:02:27.900
If you've got a lot of data, you might
actually try to do some histogram

00:02:27.900 --> 00:02:30.190
KNN based method of
estimating your density.

00:02:30.190 --> 00:02:33.760
But, you would really need lots and
lots of data.

00:02:33.760 --> 00:02:36.820
Sort of everywhere that you're likely
to get a point, because you need

00:02:36.820 --> 00:02:40.940
the densities pretty much everywhere
you're likely to get a data point.

00:02:42.060 --> 00:02:47.010
And in some sense the whole
point of generative models is

00:02:47.010 --> 00:02:51.580
to be able to use a modest
amount of data so the reason to

00:02:51.580 --> 00:02:54.495
use the parameterized thing is
that you don't need a lot of data.

00:02:54.495 --> 00:02:55.120
Okay?

00:02:55.120 --> 00:02:58.330
It allows you to generate a model
that you can use for each category.

