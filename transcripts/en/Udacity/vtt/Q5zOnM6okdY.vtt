WEBVTT
Kind: captions
Language: en

00:00:00.290 --> 00:00:05.047
So going back to our mani-core challenges, we have seen that when the number of

00:00:05.047 --> 00:00:07.816
cores grows, so does the coherence traffic on

00:00:07.816 --> 00:00:10.899
the chip. And for that, we needed a

00:00:10.899 --> 00:00:13.980
scalable on-chip network such as a mesh, and

00:00:13.980 --> 00:00:17.535
the directory coherence, which we have already seen

00:00:17.535 --> 00:00:21.826
in our cache coherence lesson. Another thing that

00:00:21.826 --> 00:00:25.474
having more cores will make more difficult is

00:00:25.474 --> 00:00:28.162
that as the number of cores goes up

00:00:28.162 --> 00:00:32.415
the off-chip traffic that we need goes up. To

00:00:32.415 --> 00:00:35.409
have decent performance we will, as we increase

00:00:35.409 --> 00:00:38.694
the number of cores, increase the number of on-chip

00:00:38.694 --> 00:00:41.058
caches. So if we have four cores, we

00:00:41.058 --> 00:00:44.367
have four level one and possibly level two caches.

00:00:44.367 --> 00:00:46.817
When we have 64 cores, we'll have 64

00:00:46.817 --> 00:00:51.760
level one and level two caches. So each core

00:00:51.760 --> 00:00:55.400
doesn't really generate more misses, but it generates the

00:00:55.400 --> 00:00:58.330
same number of misses regardless of how many cores we

00:00:58.330 --> 00:01:01.050
have. So we have the same number of misses

00:01:01.050 --> 00:01:04.209
per core. So as we have more cores, the number

00:01:04.209 --> 00:01:06.870
of memory requests which is what we do when

00:01:06.870 --> 00:01:09.606
we have a miss, would go up in proportion to

00:01:09.606 --> 00:01:13.270
the number of cores. Now note that the number

00:01:13.270 --> 00:01:16.904
of pins, the number of those little wires sticking out

00:01:16.904 --> 00:01:19.195
of a chip, does go up slowly but not

00:01:19.195 --> 00:01:22.750
in proportion, or anywhere close to in proportion to

00:01:22.750 --> 00:01:25.685
the number of cores. So if we double the

00:01:25.685 --> 00:01:28.760
number of cores possibly we get maybe 10% more

00:01:28.760 --> 00:01:31.760
pins but that's it, because these pins have to

00:01:31.760 --> 00:01:34.685
physically be large enough to not break when we

00:01:34.685 --> 00:01:37.610
are moving the chip around and plugging it into

00:01:37.610 --> 00:01:42.370
a motherboard. So what happens is we get some little

00:01:42.370 --> 00:01:46.500
improvement in off-chip throughput while our demand for

00:01:46.500 --> 00:01:48.850
it basically grows in proportion to the number

00:01:48.850 --> 00:01:52.910
of cores. So our off-chip available throughput becomes

00:01:52.910 --> 00:01:55.752
a bottleneck. So we need to reduce the

00:01:55.752 --> 00:02:01.208
number of memory requests that we generate per core and the way we do that is

00:02:01.208 --> 00:02:04.024
to make the last level cache, which in

00:02:04.024 --> 00:02:07.984
most two day's processors is the level three cache,

00:02:07.984 --> 00:02:11.152
shared so that all cores go to that cache and

00:02:11.152 --> 00:02:15.270
its size is shared equally by all the cores. And

00:02:15.270 --> 00:02:17.580
the size of that cache needs to go up in

00:02:17.580 --> 00:02:20.570
proportion to the number of cores as it goes up.

00:02:20.570 --> 00:02:22.990
So as we double the number of cores we should

00:02:22.990 --> 00:02:26.190
at least double the size of the last level cache.

00:02:26.190 --> 00:02:30.000
But there is a problem to having one huge LLC

00:02:30.000 --> 00:02:33.516
like this. Such a large cache would be slow, and

00:02:33.516 --> 00:02:36.480
if it's really one single cache, it will have

00:02:36.480 --> 00:02:39.368
one entry point where we enter the address and

00:02:39.368 --> 00:02:42.732
expect the data out. And that entry point needs

00:02:42.732 --> 00:02:45.172
to be somewhere on our chip that now possibly has

00:02:45.172 --> 00:02:48.796
a mesh or another advanced network. And that means

00:02:48.796 --> 00:02:52.144
that that one point will be where we send all

00:02:52.144 --> 00:02:55.492
of the level two misses, so it would become

00:02:55.492 --> 00:02:58.670
a bottleneck. Pretty much now we don't get to use

00:02:58.670 --> 00:03:01.270
all of the links equally. This set of links

00:03:01.270 --> 00:03:03.610
that go to this entry point for the cache will

00:03:03.610 --> 00:03:06.900
get much more traffic than the other links. If

00:03:06.900 --> 00:03:09.830
we double the number of cores, then those links around

00:03:09.830 --> 00:03:13.110
the entry point get double the traffic. So what

00:03:13.110 --> 00:03:16.500
we do is we don't have just one big last

00:03:16.500 --> 00:03:19.490
level cache, we have what is called a distributed

00:03:19.490 --> 00:03:22.580
last level cache. Now let's see what that looks like.

