WEBVTT
Kind: captions
Language: en

00:00:00.210 --> 00:00:01.970
Okay Michael so let me take you back to the pseudo code

00:00:01.970 --> 00:00:04.220
that we, we have for MIMIC and see if we can plugin some

00:00:04.220 --> 00:00:07.660
of the stuff that we we just talked about just as a way

00:00:07.660 --> 00:00:11.820
of refreshing. So as you recall, the goals to generate samples from some

00:00:11.820 --> 00:00:15.210
Pea soup theta sub T of X. So in this case were just

00:00:15.210 --> 00:00:19.500
going to estimate that by finding the best dependency tree we can, that is

00:00:19.500 --> 00:00:22.790
the best Pi function, okay? So we're going to be starting with some dependency

00:00:22.790 --> 00:00:25.220
tree And we're going to generate samples from that, and we know how to

00:00:25.220 --> 00:00:28.030
generate samples from that given a dependency tree

00:00:31.300 --> 00:00:33.040
like say this. We simply started a

00:00:33.040 --> 00:00:36.810
node, generate according to it's unconditional distribution and

00:00:36.810 --> 00:00:39.865
then generate samples from each according to it's

00:00:39.865 --> 00:00:43.090
conditional distribution given it's parents. So, first we

00:00:43.090 --> 00:00:44.960
generate a sample from here. Then we

00:00:44.960 --> 00:00:46.740
generate one from here, then from here, then

00:00:46.740 --> 00:00:48.970
from here, and then from here. Now where

00:00:48.970 --> 00:00:51.320
do we get these conditional probability tables, Michael?

00:00:51.320 --> 00:00:52.880
&gt;&gt; That's a good question. I think

00:00:52.880 --> 00:00:55.160
it's pretty direct from the mutual information.

00:00:55.160 --> 00:00:56.490
&gt;&gt; Exactly. In order to compute

00:00:56.490 --> 00:00:59.450
the mutual information, that is in order to compute the

00:00:59.450 --> 00:01:03.100
entropies, we have to know the probabilities. So, we generate

00:01:03.100 --> 00:01:05.470
all of these samples, and that tells us now, for

00:01:05.470 --> 00:01:09.410
every single feature, X of I, how often that was say,

00:01:09.410 --> 00:01:11.020
taking on a value of one, or taking on a

00:01:11.020 --> 00:01:14.270
value for zero. If we assume everything's binary. So that means

00:01:14.270 --> 00:01:17.520
we have unconditional probability distributions. Or at least estimates of

00:01:17.520 --> 00:01:19.350
unconditional probability distributions, for every

00:01:19.350 --> 00:01:21.270
single one of our features, yes?

00:01:21.270 --> 00:01:21.420
&gt;&gt; Yep.

00:01:21.420 --> 00:01:22.020
&gt;&gt; And

00:01:22.020 --> 00:01:24.540
at the same time, because we have these samples, we know

00:01:24.540 --> 00:01:28.690
for every pair of features. The probability that one took on a

00:01:28.690 --> 00:01:31.520
value, given a value for the other. So, already have the

00:01:31.520 --> 00:01:34.570
conditional probability table for each of those as well. So, just the

00:01:34.570 --> 00:01:38.760
act of generating these samples and building that mutual information graph,

00:01:38.760 --> 00:01:42.160
gives us all the conditional and unconditional probability tables that we need.

00:01:42.160 --> 00:01:44.400
So given that we can generate samples and time linear and

00:01:44.400 --> 00:01:46.660
the number of features, and we're done. So this part is easy.

00:01:47.730 --> 00:01:49.410
By the way you asked me a question before. I

00:01:49.410 --> 00:01:51.760
just want to make it clear, that we come back with an

00:01:51.760 --> 00:01:56.470
undirected graph. And, you can pick any single node as the

00:01:56.470 --> 00:02:00.700
root, and that will induce a specific directed tree. And it

00:02:00.700 --> 00:02:03.140
actually just follows from the chain rule. I'll leave that

00:02:03.140 --> 00:02:05.600
as an, an exercise to the, the viewer. But there you

00:02:05.600 --> 00:02:08.030
go. So now we know how to generate samples. From some

00:02:08.030 --> 00:02:11.090
tree that we have. We generate a bunch of these samples.

00:02:13.180 --> 00:02:17.080
We find the best ones. Retaining only those samples, and now we know

00:02:17.080 --> 00:02:21.820
how to estimate the next one, by simply doing the maximum spanning tree over

00:02:21.820 --> 00:02:25.620
all the mutual information. And then we just repeat. And we keep doing it.

00:02:25.620 --> 00:02:30.550
And there you go. A particular version of mimic. With dependency trees. Got it?

00:02:30.550 --> 00:02:31.890
&gt;&gt; Cool.

00:02:31.890 --> 00:02:35.290
&gt;&gt; Right. So, again, just to really drive this point home,

00:02:35.290 --> 00:02:38.610
you don't have to use dependency trees. You can use unconditional

00:02:38.610 --> 00:02:41.410
probability distributions, which are also very easy to sample from and

00:02:41.410 --> 00:02:44.580
very easy to estimate. You could come up with more complicated things,

00:02:44.580 --> 00:02:47.760
if you wanted to. Try to find the best Bayesian network. You

00:02:47.760 --> 00:02:50.010
can do all of these other kinds of things and that'll work

00:02:50.010 --> 00:02:53.260
just fine. Dependency Trees though are very powerful, because they do

00:02:53.260 --> 00:02:56.840
allow you to capture these relationships, that is to say they give

00:02:56.840 --> 00:03:00.100
you a probability distribution that has structure that we were looking for,

00:03:00.100 --> 00:03:03.140
while not having to pay an exponential cost for doing the estimation.

