WEBVTT
Kind: captions
Language: en

00:00:00.068 --> 00:00:05.390
So, by just naively having 10,000 threads incrementing 10 array elements, we got the wrong answer.

00:00:05.390 --> 00:00:09.218
One solution might be to sprinkle barriers throughout the code that I just showed you.

00:00:09.218 --> 00:00:12.495
But another solution is something called atomic memory operations.

00:00:12.495 --> 00:00:16.040
Atomic memory operations, or atomics for short, are special

00:00:16.040 --> 00:00:19.758
instructions that the GPU implements specifically for this problem.

00:00:19.758 --> 00:00:21.627
And, there's a bunch of atomics.

00:00:21.627 --> 00:00:23.925
You can get the list in the programing guide, but some of

00:00:23.925 --> 00:00:30.131
the examples that you might use are atomic add, atomic min, atomic xor, and so forth.

00:00:30.131 --> 00:00:38.106
A particularly interesting one is called atomic CAS, which stands for compare and swap.

00:00:38.106 --> 00:00:41.639
Let's look at how these work. Okay, so here's our code again.

00:00:41.639 --> 00:00:46.983
And this time, instead of calling it increment naive, let's call a new function increment atomic.

00:00:46.983 --> 00:00:49.036
This kernel is almost exactly the same.

00:00:49.036 --> 00:00:51.663
Every thread computes what thread it is,

00:00:51.663 --> 00:00:55.651
and then it mods that by the size of the array to figure out where it's going to write.

00:00:55.651 --> 00:01:00.665
But the difference is that, whereas before we were just doing a standard c operation--

00:01:00.665 --> 00:01:05.963
g sub i equals g sub i plus 1, which has 1 read, a modify, and a write,

00:01:05.963 --> 00:01:09.213
here we're going to use the atomic add operation.

00:01:09.213 --> 00:01:14.195
This atomic ad function is a CUDA built in that's going to take a pointer--

00:01:14.195 --> 00:01:18.165
which should be to somewhere in device memory, shared or global memory--

00:01:18.165 --> 00:01:21.969
and an amount to add, a value to add, okay?

00:01:21.969 --> 00:01:24.222
So, this has the same effect.

00:01:24.222 --> 00:01:30.745
Every thread is going to go and add 1 to the value at g sub i, but the difference is

00:01:30.745 --> 00:01:35.227
that this is using special hardware that the GPU has built in to perform atomic operations.

00:01:35.227 --> 00:01:42.092
So now we're guaranteed that only one thread at a time can do this read modify write operation.

00:01:42.092 --> 00:01:48.613
So now we'll scroll down, change our code, call increment atomic--

00:01:48.613 --> 00:01:52.868
that's the only change we'll make--compile the code again, and run it.

00:01:52.868 --> 00:01:55.867
So this looks more like it.

00:01:55.867 --> 00:02:02.495
Now we're writing a million total threads and a thousand blocks writing into 10 array elements.

00:02:02.495 --> 00:02:08.132
And this time, as we expect, every array element contains the number 100,000 when all is done.

00:02:08.132 --> 00:02:11.650
So the atomic operation prevented these collisions where

00:02:11.650 --> 00:02:16.104
multiple threads were trying to read and write from the same memory location at the same time,

00:02:16.104 --> 00:02:19.248
and instead serialized them, making sure that only one thread

00:02:19.248 --> 00:02:22.876
at a time has access during the course of that read modify write--

00:02:22.876 --> 00:02:26.915
that add operation--and we get the right result.

00:02:26.915 --> 00:02:29.968
Notice that took longer. Before it took about 3 milliseconds.

00:02:29.968 --> 00:02:34.800
Now we're taking about 3.7 milliseconds. So atomic operations come with a cost,

00:02:34.800 --> 00:02:37.336
and that's something I'm going to go into.

00:02:37.336 --> 00:02:40.322
Let's run this a few more times to make sure that we get same result.

00:02:40.322 --> 00:02:46.672
That time it took about 4 milliseconds, closer to 4 again, down around 3, and so forth.

