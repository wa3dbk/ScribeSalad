WEBVTT
Kind: captions
Language: en

00:00:00.450 --> 00:00:02.960
That's all we're going to talk
about in terms of HMMs in gesture.

00:00:02.960 --> 00:00:06.689
I'll say the good things about it were,
it was a learning paradigm, right.

00:00:06.689 --> 00:00:11.940
So, you define some features, by the way
you have to define the right features,

00:00:11.940 --> 00:00:15.900
you collect some well-annotated data,
which also takes some work.

00:00:15.900 --> 00:00:20.020
But given that, you're able to train
your HMMs by just giving it the data.

00:00:20.020 --> 00:00:22.380
So it's a learning paradigm and
that's good.

00:00:22.380 --> 00:00:25.190
The training is a little bit slow,
it's not very slow.

00:00:25.190 --> 00:00:29.290
But recognition can be very
fast because of that recursive

00:00:29.290 --> 00:00:31.290
those recursive algorithms
we talked about.

00:00:31.290 --> 00:00:33.860
Running those is very quick.

00:00:33.860 --> 00:00:35.890
So, so that works really well.

00:00:35.890 --> 00:00:38.500
You know, of course,
not everything is perfect.

00:00:38.500 --> 00:00:43.930
They're not as good for segmentation
labeling as some newer methods.

00:00:43.930 --> 00:00:46.150
I mentioned conditional
random fields earlier.

00:00:46.150 --> 00:00:49.270
If you actually have something
where your training data

00:00:49.270 --> 00:00:52.850
has particular underlying states and
you know what those states are.

00:00:52.850 --> 00:00:55.490
So it's not even really
a hidden marker model.

00:00:55.490 --> 00:00:57.400
It is a, It is a marker model, but

00:00:57.400 --> 00:01:00.340
your training data you know
the underlying states.

00:01:00.340 --> 00:01:04.620
And you're seeing an observation and
you want that state sequence for, for

00:01:04.620 --> 00:01:07.170
that segmentation,
there are now better methods and

00:01:07.170 --> 00:01:08.930
I mentioned in
conditioned random fields.

00:01:08.930 --> 00:01:10.290
You know, in general,

00:01:10.290 --> 00:01:13.630
this stuff requires a reasonable
amount of data to train one

00:01:13.630 --> 00:01:17.520
of the reasons it was used for speech is
we have lots and lots and lots of data.

00:01:17.520 --> 00:01:21.690
Although I will say that HMMs may
require less data than some of these

00:01:21.690 --> 00:01:23.410
more discriminative methods.

00:01:23.410 --> 00:01:26.980
And then I put down something
that I think is tonalogical,

00:01:26.980 --> 00:01:29.426
but also important to know.

00:01:29.426 --> 00:01:32.270
I said it works well
when the problem is easy.

00:01:32.270 --> 00:01:34.990
We did a lot of work on HMMs,
gesture, etc., whatever.

00:01:34.990 --> 00:01:37.570
And it's very powerful for

00:01:37.570 --> 00:01:40.450
capturing regularities,
when the regularities are quite clear.

00:01:41.620 --> 00:01:44.240
You have to do a lot more work
on your feature selection and

00:01:44.240 --> 00:01:48.600
a bunch of other things when those
differences are a little bit harder to,

00:01:48.600 --> 00:01:51.010
to see number of states and
those kind of things.

00:01:51.010 --> 00:01:55.640
So you know that, and that's probably
true of most classification methods.

00:01:55.640 --> 00:01:58.110
When the problem is easy,
they all work great.

00:01:58.110 --> 00:02:01.030
It's when the problem is not so
easy that, that the challenge comes in.

00:02:01.030 --> 00:02:03.349
And that's when people start
reporting on small differences.

00:02:04.750 --> 00:02:07.600
That means the real issue in def,
in choosing your classifiers

00:02:07.600 --> 00:02:11.988
to understand where the complexity and
where the difficulty of your problem is.

00:02:11.988 --> 00:02:15.414
But in general, people still use
HMMs a lot, and they use it for

00:02:15.414 --> 00:02:18.930
describing activity or,
or, or time series.

00:02:18.930 --> 00:02:21.310
And you know, it's, in some sense,

00:02:21.310 --> 00:02:24.896
your first line of defense
against activity recognition.

