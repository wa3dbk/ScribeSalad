WEBVTT
Kind: captions
Language: en

00:00:00.250 --> 00:00:04.342
Now let me focus on a specific aspect of writing high performance kernel code.

00:00:04.342 --> 00:00:07.140
At a high level, GPU programming looks like this.

00:00:07.140 --> 00:00:10.013
There's a bunch of data sitting in global memory,

00:00:10.013 --> 00:00:12.984
and you have an algorithm that you want to run on that data.

00:00:12.984 --> 00:00:16.050
That algorithm will get executed by threads running on SMs,

00:00:16.050 --> 00:00:20.312
and for various reasons, you might want to stage the data through shared memory.

00:00:20.312 --> 00:00:24.401
And you've seen for yourself that loading or storing global memory into shared memory

00:00:24.401 --> 00:00:26.731
or into local variables of the threads

00:00:26.731 --> 00:00:29.474
can be complicated if you are striving for really high performance.

00:00:29.474 --> 00:00:31.302
Think about problem set 2

00:00:31.302 --> 00:00:33.400
where you loaded an image tile into shared memory

00:00:33.400 --> 00:00:37.068
and also had to load a halo of extra cells around the image tile

00:00:37.068 --> 00:00:39.608
in order to account for the width of the blur,

00:00:39.608 --> 00:00:45.502
and this can be tricky because the number of threads that want to perform a computation

00:00:45.502 --> 00:00:48.050
on the pixels in that tile

00:00:48.050 --> 00:00:51.995
is naturally a different number than the number of threads

00:00:51.995 --> 00:00:56.600
you would launch if your goal was simply to load the pixels in the tile

00:00:56.600 --> 00:01:00.222
as well as the pixels outside of the tile.

00:01:00.222 --> 00:01:03.900
Or think about the tile transpose example in Unit 5

00:01:03.900 --> 00:01:10.223
where we staged through shared memory in order to pay careful attention to coalescent global memory.

00:01:10.223 --> 00:01:12.976
Think about our discussion of Little's Law

00:01:12.976 --> 00:01:15.545
and the trade offs that we went over between latency,

00:01:15.545 --> 00:01:18.447
bandwidth, occupancy, the number of threads,

00:01:18.447 --> 00:01:20.531
and the transaction size per thread.

00:01:20.531 --> 00:01:24.140
Remember that we saw a graph that looked sort of like this

00:01:24.140 --> 00:01:28.459
where the bandwidth that we achieved as we increased the number of threads

00:01:28.459 --> 00:01:33.037
was higher if we were able to access 4 floating point values in a single load

00:01:33.037 --> 00:01:35.431
versus 2 floating point values in a single load

00:01:35.431 --> 00:01:39.941
versus a single floating point value in a load.

00:01:39.941 --> 00:01:43.707
Finally, there are ninja level optimizations we haven't even talked about in this class,

00:01:43.707 --> 00:01:46.351
like using the Kepler LDG intrinsic.

00:01:46.351 --> 00:01:49.885
In short, CUDA's explicit use of user managed shared memory

00:01:49.885 --> 00:01:52.550
enables predictable high performance.

00:01:52.550 --> 00:01:55.981
In contrast, the hardware manage caches that you find on CPUs

00:01:55.981 --> 00:01:58.880
can result in unpredictable performance,

00:01:58.880 --> 00:02:02.565
especially when you have many multiple threads sharing the same cache, and they're interfering with each other.

00:02:02.565 --> 00:02:04.835
So that's an advantage of explicit shared memory,

00:02:04.835 --> 00:02:08.422
but that advantage comes at the cost of an additional burden on the programmer

00:02:08.422 --> 00:02:12.235
who has to explicitly manage the movement of data in and out from global memory.

00:02:12.235 --> 00:02:14.306
And this is a big part of where CUB comes in.

00:02:14.306 --> 00:02:18.255
CUB puts an abstraction around the algorithm and its memory access pattern

00:02:18.255 --> 00:02:22.846
and deals opaquely with the movement of data from global memory,

00:02:22.846 --> 00:02:27.650
possibly through shared memory, into the actual local variables of the threads.

00:02:27.650 --> 00:02:31.389
I want to mention another programming power tool called Cuda DMA

00:02:31.389 --> 00:02:35.693
that focuses specifically around the movement of data from global into shared memory.

