WEBVTT
Kind: captions
Language: en

00:00:00.490 --> 00:00:03.330
&gt;&gt; When you evaluate the network
that's been trained with drop out,

00:00:03.330 --> 00:00:06.220
you obviously no longer
want this randomness.

00:00:06.220 --> 00:00:08.270
You want something deterministic.

00:00:08.270 --> 00:00:10.940
Instead, you're going to
want to take the consensus

00:00:10.940 --> 00:00:13.190
over these redundant models.

00:00:13.190 --> 00:00:16.826
You get the consensus opinion
by averaging the activations.

00:00:16.826 --> 00:00:19.990
You want Y e here to be the average

00:00:19.990 --> 00:00:23.130
of all the yts that you
got during training.

00:00:23.130 --> 00:00:26.540
Here's a trick to make sure
this expectation holds.

00:00:26.540 --> 00:00:30.260
During training, not only do you use
zero out so the activations that you

00:00:30.260 --> 00:00:35.470
drop out, but you also scale the
remaining activations by a factor of 2.

00:00:35.470 --> 00:00:39.300
This way, when it comes time to
average them during evaluation,

00:00:39.300 --> 00:00:43.150
you just remove these dropouts and
scaling operations from your neural net.

00:00:43.150 --> 00:00:47.214
And the result is an average of these
activations that is properly scaled.

