WEBVTT
Kind: captions
Language: en

00:00:00.340 --> 00:00:04.050
Because we're lazy engineers, we're
going to take something that works,

00:00:04.050 --> 00:00:08.740
a logistic classifier and do the minimal
amount of change to make it non-linear.

00:00:08.740 --> 00:00:12.320
We're going to construct our new
function in the simplest way that we can

00:00:12.320 --> 00:00:13.310
think of.

00:00:13.310 --> 00:00:16.940
Instead of having a single matrix
multiplier as our classifier,

00:00:16.940 --> 00:00:19.780
we're going to insert
a RELU right in the middle.

00:00:19.780 --> 00:00:21.960
We now have two matrices.

00:00:21.960 --> 00:00:24.660
One going from the inputs to the RELUs,
and

00:00:24.660 --> 00:00:27.820
another one connecting
the RELUs to the classifier.

00:00:27.820 --> 00:00:29.650
We've solved two of our problems.

00:00:29.650 --> 00:00:33.450
Our function in now nonlinear thanks
to the RELU in the middle, and

00:00:33.450 --> 00:00:37.000
we now have a new knob that we can tune,
this number H which

00:00:37.000 --> 00:00:40.950
corresponds to the number of RELU
units that we have in the classifier.

00:00:40.950 --> 00:00:42.860
We can make it as big as we want.

00:00:42.860 --> 00:00:45.130
Congratulations, you've built
your first neural network.

00:00:46.150 --> 00:00:49.370
You might ask, wait a minute,
where's my neuron?

00:00:49.370 --> 00:00:53.170
In the past, when talking about neural
networks, I remember seeing diagrams

00:00:53.170 --> 00:00:58.900
with dendrites, axons, activation
functions, brains, neuroscience.

00:00:58.900 --> 00:00:59.500
Where is all that?

