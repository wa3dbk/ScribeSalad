WEBVTT
Kind: captions
Language: en

00:00:00.260 --> 00:00:03.400
So the process of producing
the dual of a linear program

00:00:03.400 --> 00:00:05.580
is just a mechanical process.

00:00:05.580 --> 00:00:08.290
There's a series of steps that
you go through where each

00:00:08.290 --> 00:00:11.890
constraint from the primal becomes
a variable in the dual and

00:00:11.890 --> 00:00:14.890
each variable in the primal
becomes a constraint in the dual.

00:00:14.890 --> 00:00:21.515
And certain maxes become mins and
bounds become objective function things.

00:00:21.515 --> 00:00:24.863
So what I've done here is I've
actually gone through that process,

00:00:24.863 --> 00:00:28.451
which we could go through in detail,
but let's just pretend that we did.

00:00:28.451 --> 00:00:32.952
[LAUGH] And we create a new linear
program from the old linear program and

00:00:32.952 --> 00:00:34.940
the new one is called the dual.

00:00:34.940 --> 00:00:39.835
So let's take a look at
this dual in detail.

00:00:39.835 --> 00:00:42.580
Because it turns out that the variables
and the constraints actually have some

00:00:42.580 --> 00:00:45.550
very nice interpretations that
are worth thinking about.

00:00:45.550 --> 00:00:49.330
Here in the dual, the thing that we're
trying to optimize is we're trying to

00:00:49.330 --> 00:00:55.130
maximize the sum over all
state-action pairs of the reward for

00:00:55.130 --> 00:01:00.140
that state-action pair times
this value little qsa.

00:01:00.140 --> 00:01:02.410
So little qsa,
probably not a great name for

00:01:02.410 --> 00:01:04.376
it because it's not
the same as a Q value.

00:01:04.376 --> 00:01:05.096
But it's something.

00:01:05.096 --> 00:01:08.065
It's something that we're
multiplying by the rewards and

00:01:08.065 --> 00:01:09.616
we're trying to maximize it.

00:01:09.616 --> 00:01:14.886
So one way I like to think
about qsa is as policy flow.

00:01:14.886 --> 00:01:19.960
Sort of how much [LAUGH] agentness is
flowing through each state-action pair.

00:01:19.960 --> 00:01:22.990
If it follows the policy it's
going to spend some time

00:01:22.990 --> 00:01:27.150
running around in the environment,
passing through each state-action pair.

00:01:27.150 --> 00:01:29.428
And each time it passes
through a state-action pair,

00:01:29.428 --> 00:01:32.990
we're going to get the reward
associated with that state-action pair.

00:01:32.990 --> 00:01:34.980
And what we want to do
is maximize that reward.

00:01:34.980 --> 00:01:37.562
So from this sort of,
policy flow concept,

00:01:37.562 --> 00:01:39.630
it sort of makes sense what it
is we're trying to maximize.

00:01:39.630 --> 00:01:42.200
It's in some ways even more
intuitive than in the primal,

00:01:42.200 --> 00:01:44.420
which we were saying we're
trying the minimize the value,

00:01:44.420 --> 00:01:46.620
which of course we're not really
trying to minimize the value.

00:01:46.620 --> 00:01:47.630
We're just trying to minimize it so

00:01:47.630 --> 00:01:50.720
that it doesn't end up being
an upper bound on the value.

00:01:50.720 --> 00:01:53.170
Here we're actually trying
to maximize expected reward,

00:01:53.170 --> 00:01:54.870
which seems like a really good thing.

00:01:54.870 --> 00:01:56.000
&gt;&gt; Right, right, right.

00:01:56.000 --> 00:01:59.480
&gt;&gt; Okay so this is now going to
be subject to the following idea,

00:01:59.480 --> 00:02:03.830
that for each state, in this case it's
easier to think of them as next states.

00:02:03.830 --> 00:02:05.800
For each possible next state,

00:02:05.800 --> 00:02:11.210
we want it to be true that the amount
of policy flow through that next state

00:02:11.210 --> 00:02:14.070
summed up over all the actions
that are going through it.

00:02:14.070 --> 00:02:16.900
That should be equal to in some
sense the number of times that

00:02:16.900 --> 00:02:20.460
next state is visited,
which we can get by summing over all

00:02:20.460 --> 00:02:24.190
states we might have started at, and
actions we might have taken form there.

00:02:24.190 --> 00:02:27.100
The policy flow through that
state-action pair times

00:02:27.100 --> 00:02:31.260
the transition probability that would
send us to s' as a result of that.

00:02:31.260 --> 00:02:34.500
And we're going to also
include in this equation

00:02:34.500 --> 00:02:37.900
sort of the sense that we can also
start a given state-action pair.

00:02:37.900 --> 00:02:39.630
So we're just going to stick a 1 in for
that.

00:02:39.630 --> 00:02:42.530
So there's some policy
flow that we're injecting

00:02:42.530 --> 00:02:45.090
into each state-action
pair of the system.

00:02:45.090 --> 00:02:47.700
And we're going to add to that any
policy flow that would be coming through

00:02:47.700 --> 00:02:49.330
it through other parts of the system.

00:02:49.330 --> 00:02:53.290
So it's kind of like we're dumping
policy flow all over our MDP.

00:02:53.290 --> 00:02:55.990
And then letting the MDP kind
of pump around what's left.

00:02:55.990 --> 00:02:58.080
And it's discounted,
there's a little discount here.

00:02:58.080 --> 00:03:01.980
So there's actually evaporation or
something happening all over the place.

00:03:01.980 --> 00:03:06.455
And what we want to know is what's
the way of letting the flow go through,

00:03:06.455 --> 00:03:10.655
kind of deciding at each state which
action it should flow through, so

00:03:10.655 --> 00:03:13.415
that we ultimately get
the maximum possible reward.

00:03:14.545 --> 00:03:16.875
And I think just one
additional constraint here,

00:03:16.875 --> 00:03:18.985
just to make sure that the policy
flow is not negative, so

00:03:18.985 --> 00:03:22.190
that we can't introduce a whole
lot of policy flow in one place.

00:03:22.190 --> 00:03:24.780
And then drain it out some other
place to make things balance.

00:03:24.780 --> 00:03:27.520
You could think of these constraints
as actually being a kind of

00:03:27.520 --> 00:03:29.480
conservation of flow.

00:03:29.480 --> 00:03:31.920
So the amount of stuff going into s'

00:03:31.920 --> 00:03:34.010
has to equal the amount of
stuff coming out of it.

00:03:34.010 --> 00:03:35.060
And that's all this is saying.

00:03:35.060 --> 00:03:36.590
It's just saying that
that has to balance.

00:03:36.590 --> 00:03:37.910
It has to be a meaningful flow.

00:03:37.910 --> 00:03:40.460
Subject to that constraint,
maximize reward.

00:03:40.460 --> 00:03:44.140
So, again, this is kind of a neat
interpretation of what it means to

00:03:44.140 --> 00:03:45.980
solve a mark off decision process.

00:03:45.980 --> 00:03:49.490
But it just comes mechanically
out of the primal version of

00:03:49.490 --> 00:03:51.390
setting up this as a linear program.

00:03:51.390 --> 00:03:54.330
&gt;&gt; Okay, does that suggest an algorithm

00:03:54.330 --> 00:03:56.985
that's different from the sort of
algorithms you've been using so far?

00:03:56.985 --> 00:03:59.040
&gt;&gt; Hm, interesting thought.

00:03:59.040 --> 00:04:02.660
So, well for one thing, it is an
algorithm in the sense that once we set

00:04:02.660 --> 00:04:05.520
things up this way, we can hit
it over the head with the linear

00:04:05.520 --> 00:04:08.560
programming stick and
a solution will come out.

00:04:08.560 --> 00:04:11.760
&gt;&gt; And in fact, it'll come out in
polynomial time, which is nice.

00:04:11.760 --> 00:04:12.690
But you're right.

00:04:12.690 --> 00:04:15.429
I mean in some sense, if you think
about value iteration as something

00:04:15.429 --> 00:04:18.279
that's propagating these values around,
this has a different form.

00:04:18.279 --> 00:04:23.105
This is really concentrating more
on the policy aspect of the MDP.

00:04:23.105 --> 00:04:28.103
And maybe we could make an algorithm
that more directly kind of

00:04:28.103 --> 00:04:33.007
searched in policy space for
the best behavior for the MDP.

00:04:33.007 --> 00:04:33.600
&gt;&gt; Let's do that.

