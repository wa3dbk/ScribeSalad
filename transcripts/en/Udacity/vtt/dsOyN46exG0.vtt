WEBVTT
Kind: captions
Language: en

00:00:00.330 --> 00:00:01.930
So how are we going to do this?

00:00:01.930 --> 00:00:03.200
Well, it's not going to surprise you.

00:00:03.200 --> 00:00:06.970
We're going to use principle component
analysis because when you see a picture

00:00:06.970 --> 00:00:10.410
that looks like this,
where I want the extended dimension,

00:00:10.410 --> 00:00:13.090
I want to use that extended variance.

00:00:13.090 --> 00:00:16.830
So here's how we're going to do
principle component analysis for this.

00:00:16.830 --> 00:00:19.040
Assume we have M data points, okay?

00:00:19.040 --> 00:00:21.360
So x1 through xM.

00:00:21.360 --> 00:00:25.310
Alright, where they are in
some D dimensional space.

00:00:25.310 --> 00:00:26.030
R to the D.

00:00:26.030 --> 00:00:28.940
Where D is big, really big.

00:00:28.940 --> 00:00:30.488
At least 10,000.

00:00:30.488 --> 00:00:33.740
Maybe it's a million, okay,
so if I had a 1k by 1k image.

00:00:33.740 --> 00:00:37.720
That would be a million pixels, so
I have a million dimensional space.

00:00:38.920 --> 00:00:44.000
So, what we want is some
direction in that big space,

00:00:44.000 --> 00:00:46.680
that captures most of the variation.

00:00:46.680 --> 00:00:47.480
Okay?

00:00:47.480 --> 00:00:49.740
And furthermore, and
this is sort of important.

00:00:50.900 --> 00:00:53.710
Not sort of important,
this is really important.

00:00:53.710 --> 00:00:57.480
If I talk about a direction, and
let's suppose u as my direction.

00:00:58.480 --> 00:01:01.520
How would I find out
where it lies along that?

00:01:01.520 --> 00:01:04.519
Well, whats nice about this is because
we're doing everything linearly

00:01:04.519 --> 00:01:08.190
we're just going to take the dot
product of x minus its mean.

00:01:08.190 --> 00:01:09.630
Why are we subtracting the mean?

00:01:09.630 --> 00:01:11.960
because none of this stuff works
if you don't subtract the mean.

00:01:11.960 --> 00:01:14.840
Remember, we're doing
variation about the mean.

00:01:14.840 --> 00:01:17.910
So we subtract the mean and
we take the dot product, and

00:01:17.910 --> 00:01:23.660
then u X of,
u of X i that would be the coefficient.

00:01:23.660 --> 00:01:28.430
So we want to find the u that
captures most of the variance.

00:01:28.430 --> 00:01:31.590
All right, well you know already how to
do this because we've been talking about

00:01:31.590 --> 00:01:34.930
it right, so here's this same
expression that we had before, but

00:01:34.930 --> 00:01:38.000
I sort of used the notation
we just developed, right?

00:01:38.000 --> 00:01:40.680
We have, we're going to say,
the variance along u.

00:01:40.680 --> 00:01:43.910
Well we want it over all
the possible points.

00:01:43.910 --> 00:01:47.430
So what we say is okay, well we're
just going to take that average.

00:01:47.430 --> 00:01:49.680
Here is x minus mu.

00:01:49.680 --> 00:01:53.700
Dotted with u,
we multiply it also by its transpose.

00:01:53.700 --> 00:01:56.350
So that we get the square value.

00:01:56.350 --> 00:01:59.720
Okay, so that's just the same thing
that we had from the previous slide,

00:01:59.720 --> 00:02:02.090
I'm just going to rearrange
that a little bit.

00:02:02.090 --> 00:02:06.030
Right, so
I can pull the U's out here, and

00:02:06.030 --> 00:02:09.350
the, the another U out there
because it's transpose, transpose.

00:02:09.350 --> 00:02:12.660
And what I have left here in the middle,
well what is that?

00:02:12.660 --> 00:02:15.120
Right, that's just
the covariance matrix and

00:02:15.120 --> 00:02:16.950
we'll call it the covariance
matrix sigmund.

00:02:16.950 --> 00:02:20.920
We've talked about how for
finding this, the axis of,

00:02:20.920 --> 00:02:24.370
of, maximum variance,
we need that covariance matrix.

00:02:24.370 --> 00:02:26.540
But think about this
just a little bit more.

00:02:26.540 --> 00:02:30.220
So that variance of u is u
transpose sigma u, all right?

