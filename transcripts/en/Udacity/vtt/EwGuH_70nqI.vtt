WEBVTT
Kind: captions
Language: en

00:00:00.170 --> 00:00:05.410
Okay Michael, so we've gone on a journey of discovery. [LAUGH] Through

00:00:05.410 --> 00:00:09.560
unsupervised learning, and so my question to you is, what have we learned?

00:00:09.560 --> 00:00:12.090
&gt;&gt; I think we learned a little bit about ourselves.

00:00:12.090 --> 00:00:14.110
&gt;&gt; And a little bit about America.

00:00:14.110 --> 00:00:15.640
So what A's have we learned today, Michael?

00:00:15.640 --> 00:00:18.300
&gt;&gt; So there was PCA.

00:00:18.300 --> 00:00:20.167
&gt;&gt; Mm-hmm.

00:00:20.167 --> 00:00:20.970
&gt;&gt; ICA.

00:00:20.970 --> 00:00:22.837
&gt;&gt; Mm-hmm.

00:00:22.837 --> 00:00:23.640
&gt;&gt; LDA.

00:00:23.640 --> 00:00:24.640
&gt;&gt;

00:00:23.640 --> 00:00:27.339
Mm-hmm. RCA and USA.

00:00:27.339 --> 00:00:29.950
&gt;&gt; [INAUDIBLE]

00:00:29.950 --> 00:00:33.010
USA, we're number one! Whoo!

00:00:33.010 --> 00:00:33.062
&gt;&gt; [LAUGH]

00:00:33.062 --> 00:00:38.170
&gt;&gt; Okay, we're just going to erase that little bit. [LAUGH]

00:00:38.170 --> 00:00:39.210
&gt;&gt; [LAUGH]

00:00:39.210 --> 00:00:41.660
&gt;&gt; Okay, yeah, okay, so we learned about a lot of A's today.

00:00:41.660 --> 00:00:42.413
&gt;&gt; Uh-huh.

00:00:42.413 --> 00:00:44.130
&gt;&gt; Which is the same grade that all our students

00:00:44.130 --> 00:00:47.490
are going to get, I am sure. That would be great.

00:00:47.490 --> 00:00:49.330
&gt;&gt; Or

00:00:49.330 --> 00:00:51.630
that's if they're truly independent. If they aren't independent, then the

00:00:51.630 --> 00:00:56.110
central limit theorem says, there will be a normal distribution across grades.

00:00:56.110 --> 00:00:56.152
&gt;&gt; Mm-mmm.

00:00:56.152 --> 00:00:56.200
&gt;&gt; Mm-mmm.

00:00:56.200 --> 00:00:57.080
&gt;&gt; Ring that bell curve.

00:00:57.080 --> 00:00:59.510
&gt;&gt; Aw, yeah, baby. Okay. So we learned about PCA,

00:00:59.510 --> 00:01:01.500
ICA, LDA and RCA. What else did we learn about?

00:01:01.500 --> 00:01:04.769
&gt;&gt; Well, I think that was it. But we talked about specifically.

00:01:05.980 --> 00:01:08.910
we, we talked in detail about the relationships between some of these.

00:01:08.910 --> 00:01:11.410
&gt;&gt; Mm-hm.

00:01:11.410 --> 00:01:14.396
&gt;&gt; In particular, these are all examples of

00:01:14.396 --> 00:01:17.468
feature transformation.

00:01:17.468 --> 00:01:20.713
&gt;&gt; That's right. Okay, so we found out about the relationships

00:01:20.713 --> 00:01:23.140
between different transformation analysis. Oh,

00:01:23.140 --> 00:01:24.570
here's something we learned. We learned

00:01:24.570 --> 00:01:28.530
that the A doesn't just stand for analysis. In the algorithms,

00:01:28.530 --> 00:01:30.900
but it actually does stand for the analysis of the data.

00:01:30.900 --> 00:01:34.350
&gt;&gt; Because that's unsupervised learning.

00:01:34.350 --> 00:01:37.290
&gt;&gt; That's right. And that in particular I

00:01:37.290 --> 00:01:39.430
gave some examples where ICA tells you what the

00:01:39.430 --> 00:01:43.310
underlying structure of the data is. You can use it to find structure.

00:01:45.480 --> 00:01:50.290
So that, for example, the independent components of natural scenes are edges.

00:01:50.290 --> 00:01:54.020
So it's interesting, because I feel like the other time that you

00:01:54.020 --> 00:01:58.990
emphasized structure was when you were talking about, mimic which was a

00:01:58.990 --> 00:02:01.710
piece of work that you did when you were a graduate student.

00:02:01.710 --> 00:02:02.440
&gt;&gt; Yep

00:02:02.440 --> 00:02:05.490
&gt;&gt; One would almost want to guess that maybe

00:02:05.490 --> 00:02:07.690
you worked on ICA when you were a graduate student.

00:02:07.690 --> 00:02:11.080
&gt;&gt; I actually did. My very first paper

00:02:11.080 --> 00:02:14.880
as a young graduate student was on mimic and my very last paper

00:02:14.880 --> 00:02:16.510
as a young graduate student. My

00:02:16.510 --> 00:02:18.940
actual dissertation, was on independent components analysis.

00:02:18.940 --> 00:02:21.460
&gt;&gt; I had that sense from the number of strong

00:02:21.460 --> 00:02:25.173
points you felt the need to make [LAUGH] about ICA.

00:02:25.173 --> 00:02:27.380
&gt;&gt; Well listen man, really, structure runs my life.

00:02:27.380 --> 00:02:29.830
As you know, everything about my life is well-structured.

00:02:29.830 --> 00:02:30.394
&gt;&gt; Yeah, sure.

00:02:30.394 --> 00:02:32.980
&gt;&gt; [LAUGH] Okay, did we learn anything else?

00:02:32.980 --> 00:02:36.230
&gt;&gt; So, yeah, so I mean I feel like we spent a lot of time talking about

00:02:36.230 --> 00:02:40.990
so P, ICA is a more probabilistic kind of modeling, method

00:02:40.990 --> 00:02:45.660
and PCA is a more I want to say linear algebraic, modeling model.

00:02:45.660 --> 00:02:49.960
&gt;&gt; That's a really good point Michael. So, we didn't say it explicitly this

00:02:49.960 --> 00:02:53.880
way, but actually, even in our own work it often comes up that sometimes,

00:02:53.880 --> 00:02:56.150
you want to think about, information theory. You

00:02:56.150 --> 00:02:58.360
want to think about probability. And sometimes, you

00:02:58.360 --> 00:03:01.760
really just want to think about linear algebra. And you could see PCA as being

00:03:01.760 --> 00:03:04.210
really about linear algebra. And sometimes only

00:03:04.210 --> 00:03:06.820
coincidentally being about probability. Where as ICA

00:03:06.820 --> 00:03:09.690
is all about probability and information theory,

00:03:09.690 --> 00:03:11.800
and only coincidentally ever about linear algebra.

00:03:11.800 --> 00:03:13.810
&gt;&gt; Yeah, that's helpful. That does seem to be a fundamental

00:03:13.810 --> 00:03:15.840
split in a lot of work that happens in machine learning.

00:03:15.840 --> 00:03:18.130
&gt;&gt; Yeah and it, and it makes some sense. I mean, we, we

00:03:18.130 --> 00:03:20.680
know what the right answer is in probability, but we know what the

00:03:20.680 --> 00:03:23.150
right answer is in linear algebra. And I guess it's, it's often the

00:03:23.150 --> 00:03:27.870
case Michael, would you agree that. That the linear algebra approach is often

00:03:27.870 --> 00:03:31.640
easier to think about, or easier to do in

00:03:31.640 --> 00:03:34.720
practice and sometimes, it can be interpreted as if

00:03:34.720 --> 00:03:37.560
it's probability. And that typically breaks down on the

00:03:37.560 --> 00:03:39.680
edge cases, but you know, you can kind of

00:03:39.680 --> 00:03:43.200
work around it for sort of common cases. Yeah,

00:03:43.200 --> 00:03:45.780
that, that the linear algebra algorithms are often cheaper

00:03:45.780 --> 00:03:49.380
to implement, cheaper to execute less prone to local

00:03:49.380 --> 00:03:53.060
minima issues. There's sort of a well defined answer

00:03:53.060 --> 00:03:55.950
that they're, that they're finding. But it's often not quite the

00:03:55.950 --> 00:03:58.600
answer that you want, and the probability methods give you the

00:03:58.600 --> 00:04:02.080
answer that you want, but can be very hard to find.

00:04:02.080 --> 00:04:04.410
Right. And in fact you can see that in ICA and PCA,

00:04:04.410 --> 00:04:07.080
in that PCA is very well understood. There are lots of

00:04:07.080 --> 00:04:10.230
fast algorithms for it. And you know that the principle components

00:04:10.230 --> 00:04:13.810
always exist. Interestingly, we didn't talk about this but by contrast,

00:04:13.810 --> 00:04:18.110
ICA with its more information, theoretic and probabilistic roots, has a very

00:04:18.110 --> 00:04:20.190
specific model. And it isn't always the case

00:04:20.190 --> 00:04:22.530
that that model fits, and so in fact, sometimes

00:04:22.530 --> 00:04:25.863
you can't find independent components. [INAUDIBLE] Because they don't

00:04:25.863 --> 00:04:29.050
actually exist, except in the most trivial sense. So

00:04:29.050 --> 00:04:31.330
it's both more expensive, because of the way

00:04:31.330 --> 00:04:33.330
you end up searching the space. And it doesn't

00:04:33.330 --> 00:04:35.390
always produce an answer. But when it does produce

00:04:35.390 --> 00:04:37.770
an answer, it tends to produce very satisfying ones.

00:04:37.770 --> 00:04:39.700
&gt;&gt; Well, I think that's a good place to stop, in the

00:04:39.700 --> 00:04:43.050
sense that I wanted to know just one more interesting fact about ICA.

00:04:43.050 --> 00:04:45.110
And now that I've got that, I feel

00:04:45.110 --> 00:04:47.670
fully satisfied. Well, there's another fact I can

00:04:47.670 --> 00:04:48.790
tell you which is that it's the only

00:04:48.790 --> 00:04:51.380
one of these algorithms that start with a vowel.

00:04:51.380 --> 00:04:53.460
&gt;&gt; And now I'm more than satisfied.

00:04:53.460 --> 00:04:56.170
&gt;&gt; It is always my goal to leave you more than satisifed.

00:04:56.170 --> 00:04:59.570
&gt;&gt; [LAUGH]. Alright, then!

00:04:59.570 --> 00:05:01.320
&gt;&gt; OK. Well I think we are done

00:05:01.320 --> 00:05:06.600
with this entire sub lesson mini course thingy.

00:05:06.600 --> 00:05:08.000
&gt;&gt; About unsupervised learning.

00:05:08.000 --> 00:05:08.570
Well that, well that's great!

00:05:08.570 --> 00:05:09.370
&gt;&gt; About unsupervised learning.

00:05:09.370 --> 00:05:12.310
&gt;&gt; What does that, what does that leave us to do? Doesn't lead us to do

00:05:12.310 --> 00:05:16.110
anything, at least not with this particular, mini-course.

00:05:16.110 --> 00:05:17.890
I think actually the description we had here, what

00:05:17.890 --> 00:05:21.570
we've learned for this particular, lesson actually applies,

00:05:21.570 --> 00:05:23.060
even going backwards to some of our other

00:05:23.060 --> 00:05:25.430
lessons. And what we're going to get to do

00:05:25.430 --> 00:05:30.040
next is, decision problems and reinforcement learning. Ooo, exciting.

00:05:30.040 --> 00:05:31.170
&gt;&gt; It is exciting.

00:05:31.170 --> 00:05:33.960
&gt;&gt; But I think first people probably have some

00:05:33.960 --> 00:05:37.980
homeworky stuff to do, projects to do in the context of this minicourse.

00:05:37.980 --> 00:05:39.820
&gt;&gt; Yes, and probably an exam of some sort.

00:05:39.820 --> 00:05:40.606
&gt;&gt; [LAUGH]

00:05:40.606 --> 00:05:41.392
&gt;&gt; [LAUGH]

00:05:41.392 --> 00:05:42.700
&gt;&gt; Good luck to everyone on that.

00:05:42.700 --> 00:05:45.160
&gt;&gt; Yes, we're absolutely sure you'll do fine.

00:05:45.160 --> 00:05:46.770
Be sure to go over these lessons and be

00:05:46.770 --> 00:05:48.550
sure to read all of the material, because there's

00:05:48.550 --> 00:05:51.250
a lot of detail in the material that wouldn't

00:05:51.250 --> 00:05:54.660
make a lot of sense for us to cover in this format. But do give it a

00:05:54.660 --> 00:05:57.090
read, come back, look at the stuff that we've

00:05:57.090 --> 00:05:59.370
talked about, it should help you understand the intuition

00:05:59.370 --> 00:06:01.600
behind whats really happening there. Excellent. Well this

00:06:01.600 --> 00:06:03.310
is great, thanks, thanks Charles, i learned a lot,

00:06:03.310 --> 00:06:08.790
&gt;&gt; I did too. Bye Michael i will hear from you soon. Alright, awesome.

00:06:08.790 --> 00:06:09.560
&gt;&gt; Bye.

