WEBVTT
Kind: captions
Language: en

00:00:00.067 --> 00:00:06.932
And the answer is that difference in values can, at most, be X. So if the difference in value between B and A is larger than X,

00:00:06.933 --> 00:00:11.799
value of A divided by X and the value of B divided by X, would round down to different numbers.

00:00:11.800 --> 00:00:17.966
So that's why we know the amount of value lost in each mistake that we make is actually smaller than X.

00:00:17.967 --> 00:00:24.532
And that is a very cool thing to know, because with these three things, knowing how many objects can be, at most,

00:00:24.533 --> 00:00:30.599
in an optimum solution, how many mistakes we can have made. And having quantified how much value

00:00:30.600 --> 00:00:37.766
we have lost in each mistake allows us to quantify the overall mistake that we can have made, at most.

00:00:37.767 --> 00:00:41.032
And I call this the maximum value lost, as opposed to an optimum solution.

00:00:41.033 --> 00:00:45.666
And that maximum value is, of course, well, there's, at most, N objects in an optimum solution.

00:00:45.667 --> 00:00:51.766
That means we can make, at most, N mistakes, and each of those mistakes costs us less than X.

00:00:51.767 --> 00:00:58.499
So the maximum value that we can lose overall by rounding down is smaller than N times X.

00:00:58.500 --> 00:01:03.432
And that, of course, is the absolute error that the algorithm can make by this rounding down technique.

00:01:03.433 --> 00:01:09.166
So now let's figure out the relative error here. And the relative error, since KNAPSACK is a maximization problem,

00:01:09.167 --> 00:01:14.832
is of course the value of an optimum solution divided by the value of an approximate solution.

00:01:14.833 --> 00:01:19.932
So let's just call the value of the optimum solution V opt, and opt stands for optimal here.

00:01:19.933 --> 00:01:25.932
And in that case we can write the approximate solution as V opt minus N times X.

00:01:25.933 --> 00:01:30.899
Now, this is not the precise value of the approximate solution, but it's a bound for the approximate solution.

00:01:30.900 --> 00:01:37.132
So we know it cannot get any worse than that. So, but to be precise here, of course then we'll have to write it like this here.

00:01:37.133 --> 00:01:41.199
Because we know that the approximate solution will have at least this value.

00:01:41.200 --> 00:01:46.866
Actually, it will always be bigger, because we have a strict smaller than here, so we should more precisely even write it like this.

00:01:46.867 --> 00:01:50.532
So it's smaller than V opt divided by V opt minus N times X.

00:01:50.533 --> 00:01:55.866
Now, we can simply write this as one plus N times X over V opt minus N times X.

00:01:55.867 --> 00:02:03.466
We know something about V opt because the value of an optimum solution can, at most, be the value of all of all objects combined,

00:02:03.467 --> 00:02:09.632
and we call that number V. So we can rewrite this as one plus N times X over V minus N times X.

00:02:09.633 --> 00:02:17.232
Now, let's have a closer look at X. How did we define X? Well, we said that X was equal to V over N

00:02:17.233 --> 00:02:20.932
times one minus C, where C was some number between zero and one.

00:02:20.933 --> 00:02:30.466
So if we plug the value of X into this equation here, what we get is one plus V times one minus C over V minus

00:02:30.467 --> 00:02:37.866
V times one minus C. Which means that Vs here simply cancel out, and you get one plus one minus C

00:02:37.867 --> 00:02:40.932
over one minus one plus C. Which is the same as one over C.

00:02:40.933 --> 00:02:47.766
So I didn't tell you about C earlier, but what we just found out is that the approximation factor of our algorithm

00:02:47.767 --> 00:02:52.632
is actually one over C. And you'll remember that the running time was also dependent on C.

00:02:52.633 --> 00:02:57.699
So we already figured this one out, right? It was O of N squared over one minus C.

00:02:57.700 --> 00:03:05.666
So a good approximation would be to set C close to one. Because then the relative error becomes almost one,

00:03:05.667 --> 00:03:10.966
but in return--and you see that in the running time here--the running time actually becomes very large,

00:03:10.967 --> 00:03:17.666
because if C is close to one, then you get a very small denominator here, which means that the overall running time,

00:03:17.667 --> 00:03:21.499
since it's a fraction depending on this, gets very large.

00:03:21.500 --> 00:03:28.799
But if you're happy with a--and I'm going to just call it bad approximation, then C can actually be smaller than one.

00:03:28.800 --> 00:03:34.999
C will always be larger than zero, so it'll always be a positive number, but it could be 0.5, 0.2 or even smaller,

00:03:35.000 --> 00:03:40.032
what will then happen is, if C is smaller than one--so it's this very tiny fraction,

00:03:40.033 --> 00:03:45.132
well, then actually this part here in the denominator will be very close to one.

00:03:45.133 --> 00:03:51.899
So the running time is very good, but this fraction here will actually increase.

00:03:51.900 --> 00:03:57.299
So in return, forgetting about the running time, you get a relative error that is much worse.

00:03:57.300 --> 00:04:03.399
And this is exactly the principle behind the PTAS. So we're trading on the quality of the approximation factor,

00:04:03.400 --> 00:04:10.132
which is this fraction here--one over C--against running time. More running time means we can get a better approximation factor.

00:04:10.133 --> 00:04:13.966
A worse approximation factor, in return, will mean that the algorithm runs faster.

00:04:13.967 --> 00:04:20.565
So this is already pretty amazing. Now, one last detail here: As you'll notice, the running time of the approximation algorithm

00:04:20.567 --> 00:04:28.332
is actually polynomially dependent on C, which is why this PTAS is sometimes also referred to as a fully polynomial

00:04:28.333 --> 00:04:34.566
time approximation scheme, or F PTAS. For the majority of polynomial time approximation schemes, the running time

00:04:34.567 --> 00:04:39.332
actually depends exponentially on the approximation factor that you want to achieve.

00:04:39.333 --> 00:04:44.299
But this is something for a more advanced course. I still sometimes find it mind-boggling that KNAPSACK

00:04:44.300 --> 00:04:49.899
is NP-complete, meaning that, in terms of intractability, KNAPSACK is likely to be just as hard to solve

00:04:49.900 --> 00:04:54.299
as nastier problems such as SAT or CLIQUE. So approximation, I think, shows very well

00:04:54.300 --> 00:04:58.966
that even within NP-completeness, there's a broad spectrum of difficulty, if you will.

00:04:58.967 --> 00:05:04.399
So, some of the most difficult problems to approximate appear to be CLIQUE and INDEPENDENT set.

00:05:04.400 --> 00:05:07.699
And then in the middle of the spectrum, we have problems like VERTEX COVER.

00:05:07.700 --> 00:05:12.532
And then we have problems which have a PTAS or even a fully polynomial time approximation scheme,

00:05:12.533 --> 00:05:16.599
such as KNAPSACK. All of these problems here are NP-complete.

00:05:16.600 --> 00:05:20.532
But some of them can be approximated with no constant factor at all.

00:05:20.533 --> 00:05:25.999
Others have algorithms that, if you don't look closely, could almost be mistaken for being polynomial.

00:05:26.000 --> 00:05:30.032
Of course, all of this is still under the assumption that P does not equal NP.

00:05:30.033 --> 00:05:34.899
If P equals NP, then all of these problems here would be polynomial time solvable, anyway.

00:05:34.900 --> 00:05:41.366
Now that we have looked at approximation, what else is there that you could try to tackle NP-complete problems?

00:05:41.367 --> 00:05:49.532
Next time we'll be basically poking around, meaning that we'll be looking at how randomization and random algorithms

00:05:49.533 --> 00:05:54.033
could maybe help us tackle these problems here as well. So, see you next time.

