WEBVTT
Kind: captions
Language: en

00:00:00.200 --> 00:00:02.680
So we have seen that performing these atomic read

00:00:02.680 --> 00:00:06.330
write operations on the lock repeatedly even while waiting

00:00:06.330 --> 00:00:09.150
for the lock is inefficient in terms of energy

00:00:09.150 --> 00:00:11.540
and it actually slows down the thread that is in

00:00:11.540 --> 00:00:14.490
the critical section. We want to make this more

00:00:14.490 --> 00:00:16.940
efficient and one way of doing that is to do

00:00:16.940 --> 00:00:19.670
a so-called test and atomic op approach in the

00:00:19.670 --> 00:00:25.510
locking function. So the original approach to using for example,

00:00:25.510 --> 00:00:31.610
an atomic exchange is to set R1 to 1, and as long as R1 is 1, try to swap

00:00:31.610 --> 00:00:38.280
between lock var, eventually we see a 0 and we exit. But as we now know, this

00:00:38.280 --> 00:00:41.711
keeps invalidating everybody else who is waiting on the

00:00:41.711 --> 00:00:43.974
lock, so we are getting a lot of cache

00:00:43.974 --> 00:00:47.382
misses while waiting. And active waiting like this is

00:00:47.382 --> 00:00:50.550
not only spending energy on the bus transfers all the

00:00:50.550 --> 00:00:52.998
time, but also keeps the bus busy so that

00:00:52.998 --> 00:00:55.590
useful work in other trays that are not busy

00:00:55.590 --> 00:00:58.630
waiting, you slow down. So this is kind of

00:00:58.630 --> 00:01:01.708
like a repeated atomic op approach. To make it

00:01:01.708 --> 00:01:04.459
more efficient, what we do is we do the

00:01:04.459 --> 00:01:08.330
same type of looping here, but we don't immediately

00:01:08.330 --> 00:01:11.790
try to swap. What we do first is use

00:01:11.790 --> 00:01:15.940
normal loads to wait for lockvar to become zero.

00:01:15.940 --> 00:01:19.440
So the idea is that were using normal loads

00:01:19.440 --> 00:01:23.760
while lockvar is busy. If we observe that its free,

00:01:23.760 --> 00:01:26.000
we try to grab it by doing an atomic

00:01:26.000 --> 00:01:29.351
exchange. We cannot try to grab it using the normal

00:01:29.351 --> 00:01:32.400
store, that's why we have to do the exchange.

00:01:32.400 --> 00:01:34.170
Bu the idea is that there is no point in

00:01:34.170 --> 00:01:37.610
trying to do the exchange unless the lock is actually

00:01:37.610 --> 00:01:41.200
free and the purpose of the exchange now is solely

00:01:41.200 --> 00:01:45.600
to ensure that only one of us grabs the lock once it becomes free. But the busy

00:01:45.600 --> 00:01:48.525
waiting while the lock is continuously free is

00:01:48.525 --> 00:01:51.846
done using normal reads. So now if we have

00:01:51.846 --> 00:01:54.634
core zero, one, and two, and core zero

00:01:54.634 --> 00:01:58.242
succeeds in doing an exchange, and gets the lockvar

00:01:58.242 --> 00:02:02.390
in the modified state. Core one will now reach

00:02:02.390 --> 00:02:06.480
this point and read lockvar. See that it's one.

00:02:06.480 --> 00:02:09.699
It's done using a normal load, so here we're

00:02:09.699 --> 00:02:12.260
moved to the own state, here we are moved

00:02:12.260 --> 00:02:14.966
to the shared state. And there is a cache

00:02:14.966 --> 00:02:18.068
to cache transfer here. Now we will keep doing

00:02:18.068 --> 00:02:21.032
this, but we'll keep getting cache hits so we

00:02:21.032 --> 00:02:25.420
no longer access the bus while waiting. Similarly core

00:02:25.420 --> 00:02:28.869
two will load the lockvar. It will also get

00:02:28.869 --> 00:02:33.180
the value and have it in the shared state.

00:02:33.180 --> 00:02:35.150
And now it also gets cached hits while

00:02:35.150 --> 00:02:38.610
waiting. So as long as core zero keeps the

00:02:38.610 --> 00:02:41.410
lock busy here, these two cores are no

00:02:41.410 --> 00:02:45.210
longer generating bus traffic. So core zero basically gets

00:02:45.210 --> 00:02:47.320
the entire coherence bus to itself so it

00:02:47.320 --> 00:02:50.950
can very quickly service cache misses. And cache hits

00:02:50.950 --> 00:02:54.900
here are much more efficient than bus accesses.

00:02:54.900 --> 00:02:58.260
And this is going to continue until core zero

00:02:58.260 --> 00:03:00.970
releases the lock, at which point, there is an

00:03:00.970 --> 00:03:04.640
invalidation, and these two will read again, see a

00:03:04.640 --> 00:03:07.150
value of one, try to compete for it using

00:03:07.150 --> 00:03:09.744
the exchange and so on. But those bursts of

00:03:09.744 --> 00:03:13.332
activity really happen only when they're actually trying to

00:03:13.332 --> 00:03:15.910
acquire the lock. As long as the lock is

00:03:15.910 --> 00:03:18.850
really busy, they just keep using normal loads in

00:03:18.850 --> 00:03:22.210
this loop. So this is much more energy efficient.

