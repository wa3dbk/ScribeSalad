WEBVTT
Kind: captions
Language: en

00:00:00.070 --> 00:00:01.830
We're going to make
general Rmax algorithm and

00:00:01.830 --> 00:00:03.860
it's going to be really simple
because it's going to be an awful lot

00:00:03.860 --> 00:00:05.670
like the Rmax algorithm
we already looked at.

00:00:05.670 --> 00:00:07.090
&gt;&gt; Good.
&gt;&gt; So I copied over the Rmax

00:00:07.090 --> 00:00:08.460
algorithm that we had before.

00:00:08.460 --> 00:00:11.190
And it went like this,
we're going to keep track of the MDP.

00:00:11.190 --> 00:00:14.820
Which is to say we're going to have
an estimate of the transitions and

00:00:14.820 --> 00:00:17.690
rewards for all the state action pairs.

00:00:17.690 --> 00:00:21.280
Any unknown state action pair,
we're going to assume has the reward of

00:00:21.280 --> 00:00:25.480
Rmax and then a self loop, so we can
just get like aahh for a long time.

00:00:25.480 --> 00:00:28.500
Then we actually build that MDP,
we solve that MDP and we take

00:00:28.500 --> 00:00:32.390
an action from the optimal policy with
respect to that MDP that we solve.

00:00:32.390 --> 00:00:35.560
The only difference is going to
be this notion of unknown.

00:00:35.560 --> 00:00:37.710
And the idea is that we're
going to have some parameter.

00:00:37.710 --> 00:00:42.210
Call it C, that state action pair
is unknown if it's been visited or

00:00:42.210 --> 00:00:44.530
if we've tried it out
fewer than C times.

00:00:44.530 --> 00:00:47.940
&gt;&gt; Okay, is that C different for
every state action pair?

00:00:47.940 --> 00:00:48.670
&gt;&gt; That's a good question.

00:00:48.670 --> 00:00:51.470
So I want to first give
the parametrized algorithm.

00:00:51.470 --> 00:00:54.680
Then what we have to do is, if we're
going to say that this algorithm is

00:00:54.680 --> 00:00:59.730
efficient, we have to derive some value
for C and show that it's not too big and

00:00:59.730 --> 00:01:02.290
not too small but-
&gt;&gt; Just right.

00:01:02.290 --> 00:01:03.370
&gt;&gt; Just right, yeah.

00:01:03.370 --> 00:01:07.700
So the way that we're defining unknown
here is that we have some parameter C.

00:01:07.700 --> 00:01:10.710
And we consider a state-action
pair SA unknown

00:01:10.710 --> 00:01:13.430
if it was tried fewer than c times.

00:01:13.430 --> 00:01:17.560
After that point then, we switch over
to the maximum likelihood estimate.

00:01:17.560 --> 00:01:21.670
So, we've tried it c times,
and we saw we went

00:01:23.310 --> 00:01:27.690
from state s with action
a to some state, S 53,

00:01:27.690 --> 00:01:30.360
so if we did that some number of times.

00:01:30.360 --> 00:01:33.300
If we only did that once, then we're
going to estimate a probability of 1

00:01:33.300 --> 00:01:35.910
over c for getting that transition.

00:01:35.910 --> 00:01:38.100
So this gives us a new estimate
of the transitions and

00:01:38.100 --> 00:01:41.070
rewards based on the data
that we've actually gathered.

00:01:41.070 --> 00:01:41.660
Okay.

00:01:41.660 --> 00:01:44.920
So I just want to point out that
this is the Rmax algorithm we

00:01:44.920 --> 00:01:49.050
used in the deterministic case, and
this very much is related to the hufting

00:01:49.050 --> 00:01:53.890
bound estimate we used in the stochastic
non-sequential case in the bandit case.

00:01:53.890 --> 00:01:55.550
Which is grafting
the two things together.

