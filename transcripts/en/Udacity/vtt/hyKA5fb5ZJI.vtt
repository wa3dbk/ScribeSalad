WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.306
So, this is NVVP. It's a very powerful tool. It has a lot of functionality.

00:00:04.306 --> 00:00:07.980
We're only going to touch on it today. My real point is simply to point out to

00:00:07.980 --> 00:00:11.147
you that these tools exist and that even though in this class we're going to

00:00:11.147 --> 00:00:15.219
focus on understanding the concepts and being able to reason about these things from scratch,

00:00:15.219 --> 00:00:18.818
I do want you to understand that you don't have to always do this from scratch,

00:00:18.818 --> 00:00:20.922
that you should use the tools that are available.

00:00:20.922 --> 00:00:24.061
So the first thing we're going to do is pick an executable to run on.

00:00:24.061 --> 00:00:29.030
And so this is where we tell it what executable to run, we're going to run our transpose code.

00:00:29.030 --> 00:00:34.038
Before we do that, I'm actually going to go into the transpose code and comment

00:00:34.038 --> 00:00:40.010
out the section of it that is running the transpose serial kernel,

00:00:40.010 --> 00:00:44.914
that first kernel that we figured out. That one is so slow that I don't really want to spend any more time on that.

00:00:44.914 --> 00:00:50.654
And as you'll see, the profiler takes a little time to run the code several times and acquire statistics about it.

00:00:50.654 --> 00:00:58.944
So I comment that out and save it. I go back and I recompile my code, and now I'm ready to go back to NVVP,

00:00:58.944 --> 00:01:10.357
tell it what executable we're going to run, and here we go.

00:01:10.357 --> 00:01:17.074
It runs the program, it measures all of the calls to the CUDA API, things like malloc and memcpy,

00:01:17.074 --> 00:01:20.368
and it measures the time taken for every kernel.

00:01:20.368 --> 00:01:24.660
Here's the transpose parallel per row kernel. Let me zoom In on that.

00:01:24.660 --> 00:01:32.079
You can see that the parallel per row kernel started a 110 milliseconds into the executional programs,

00:01:32.079 --> 00:01:34.927
finished at 119 milliseconds into the program,

00:01:34.942 --> 00:01:40.456
ran a single block at a grid size of 111 so it launched a single block,

00:01:40.456 --> 00:01:47.068
that block had 1024 threads. You can get a bunch of other statistics from this.

00:01:47.068 --> 00:01:52.905
If we scroll to the right, we can see the much shorter time taken by our transpose per element kernel.

00:01:52.905 --> 00:01:58.384
I'll zoom In even further. And this one, as you see,

00:01:58.384 --> 00:02:04.046
ran in a larger grid of 64 by 64 blocks, each of which had 16 by 16 threads.

00:02:04.046 --> 00:02:08.416
Now, I'll make the point that we measured a shorter time than that.

00:02:08.416 --> 00:02:10.785
And that the times that you see in the profile are not going to

00:02:10.785 --> 00:02:16.207
match up exactly to the timings that we measured a little bit ago, when we were outside the profiler,

00:02:16.207 --> 00:02:19.000
and that's to be expected.

