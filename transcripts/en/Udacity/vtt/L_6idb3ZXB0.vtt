WEBVTT
Kind: captions
Language: en

00:00:00.340 --> 00:00:01.705
Hey Charles. How's it goin'?

00:00:01.705 --> 00:00:03.810
&gt;&gt; It's going pretty well Michael. How are things going with you?

00:00:03.810 --> 00:00:07.470
&gt;&gt; Good. You know I'm excited to tell you about neural networks today.

00:00:07.470 --> 00:00:11.180
You may be familiar with neural networks because you have one, in your head.

00:00:11.180 --> 00:00:12.550
&gt;&gt; I do?

00:00:12.550 --> 00:00:15.930
&gt;&gt; Well, yeah. I mean, you have a network neurons. Like, you know,

00:00:15.930 --> 00:00:19.150
you know neurons, like brain cells. Let me, let me, I'll draw you one.

00:00:19.150 --> 00:00:20.310
&gt;&gt; Okay.

00:00:20.310 --> 00:00:22.760
&gt;&gt; So this is my template drawing, a nerve cell,

00:00:22.760 --> 00:00:26.290
a neuron. And you can, you know, you've got billions and

00:00:26.290 --> 00:00:29.180
billions of these inside your head. And they have

00:00:29.180 --> 00:00:30.460
you know, most of them have a pretty similar

00:00:30.460 --> 00:00:32.759
structure, that there's the, there's the kind of the

00:00:32.759 --> 00:00:35.680
main part of the cell called the cell body. And

00:00:35.680 --> 00:00:41.190
then there's this thing called an axon which kind of is like a wire going

00:00:41.190 --> 00:00:46.880
forward to a set of synapses which are kind of little gaps between

00:00:46.880 --> 00:00:51.940
this neuron and some other neuron. And what happens is, information spike

00:00:51.940 --> 00:00:52.430
trains

00:00:52.430 --> 00:00:53.540
&gt;&gt; Woo woo!

00:00:53.540 --> 00:00:57.270
&gt;&gt; Travel down the axon. When the

00:00:57.270 --> 00:00:59.780
cell body fires it has an electrical impulse

00:00:59.780 --> 00:01:02.090
it travels down the, the, the axon

00:01:02.090 --> 00:01:06.220
and then causes across the synapses excitation to

00:01:06.220 --> 00:01:08.240
occur on other neurons which themselves can

00:01:08.240 --> 00:01:11.860
fire. Again by sending out spike trains. And

00:01:11.860 --> 00:01:13.890
so they're very much a kind of

00:01:13.890 --> 00:01:17.820
a computational unit and they're very, very complicated.

00:01:17.820 --> 00:01:20.600
To a first approximation, as is often true with

00:01:20.600 --> 00:01:23.740
first approximations they're very simple. Sort of by definition

00:01:23.740 --> 00:01:26.750
of first approximation. So what, what, in the field

00:01:26.750 --> 00:01:29.710
of artificial neural networks we have kind of a

00:01:29.710 --> 00:01:34.640
cartoonish version of the neuron and networks of neurons

00:01:34.640 --> 00:01:37.820
and we actually. Put them together to compute various

00:01:37.820 --> 00:01:40.170
things. And one of the nice things about the,

00:01:40.170 --> 00:01:43.010
the way that they're set up is that they

00:01:43.010 --> 00:01:46.300
can be tuned or changed so that they fire under

00:01:46.300 --> 00:01:49.740
different conditions and therefore compute different things. And they can

00:01:49.740 --> 00:01:52.320
be trained through a learning process. So that's what we're

00:01:52.320 --> 00:01:54.610
going to talk through if you haven't heard about this before.

00:01:54.610 --> 00:01:55.880
&gt;&gt; Okay.

00:01:55.880 --> 00:01:58.250
&gt;&gt; So we can replace this sort of detailed version of a

00:01:58.250 --> 00:02:00.970
neuron with a very abstracted way kind of notion of a neuron.

00:02:00.970 --> 00:02:04.950
And here's how it's going to work. We're going to have inputs

00:02:04.950 --> 00:02:08.560
that are kind of you know, think of them as firing rates or

00:02:10.139 --> 00:02:15.470
the strength of inputs. X1, X2, and X3 in this case. Those are

00:02:15.470 --> 00:02:20.790
multiplied by weight, w1, w2, w3 correspondingly. And so the weights

00:02:20.790 --> 00:02:25.990
kind of turn up the gain or the sensitivity of the neuron, this unit,

00:02:28.800 --> 00:02:32.200
to each of the inputs respectively. Then what we're going to do is

00:02:32.200 --> 00:02:36.970
we're going to sum them up. So we're going to sum. Over all the inputs.

00:02:39.640 --> 00:02:42.750
The strength of the input times the weight,

00:02:42.750 --> 00:02:45.420
and that's going to be the activation. Then

00:02:45.420 --> 00:02:47.320
we're going to ask is that greater than,

00:02:47.320 --> 00:02:50.370
or equal to the firing threshold. And if it

00:02:50.370 --> 00:02:57.310
is, then we're going to say the output is one, and if it's not, we're going to

00:02:57.310 --> 00:02:59.420
say the output is zero. So this is

00:02:59.420 --> 00:03:04.800
a particular kind of neural net unit called Perceptron.

00:03:04.800 --> 00:03:07.880
Which is a very sexy name because they had very sexy names in the 50s

00:03:07.880 --> 00:03:09.940
&gt;&gt; They did.

00:03:09.940 --> 00:03:13.270
&gt;&gt; When this was first developed. Alright?

00:03:13.270 --> 00:03:15.500
So this, this whole neuron concept gets

00:03:15.500 --> 00:03:19.970
boiled down to something much simpler, which is just, a linear sum followed by

00:03:19.970 --> 00:03:22.960
a threshold, thresholding operation, right? So it's

00:03:22.960 --> 00:03:25.410
worth kind of thinking, how can we,

00:03:25.410 --> 00:03:30.160
what sort of things can this, can networks of these kinds of units compute? So,

00:03:30.160 --> 00:03:32.120
let's see if we can figure some of those things out.

