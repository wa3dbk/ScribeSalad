WEBVTT
Kind: captions
Language: en

00:00:00.190 --> 00:00:03.680
Behind the scenes deployments
manage replica sets.

00:00:03.680 --> 00:00:06.090
Each deployment is mapped
to one active replica set.

00:00:07.240 --> 00:00:11.710
Use kubectl get replicasets command
to view the current set of replicas.

00:00:12.835 --> 00:00:16.110
Replicasets are scaled to
the deployment for each service and

00:00:16.110 --> 00:00:18.450
can be scaled independently.

00:00:18.450 --> 00:00:22.010
As mentioned before, the real strength
of kubernetes come when you work in

00:00:22.010 --> 00:00:25.690
a declarative way, instead of using
imperative scale and expose commands.

00:00:26.760 --> 00:00:29.470
Let me show you how to scale
the front end deployment using

00:00:29.470 --> 00:00:32.259
an existing deployment
configuration file.

00:00:32.259 --> 00:00:34.934
First, let's see how many of
our hello pods are running.

00:00:34.934 --> 00:00:36.727
[BLANK_AUDIO]

00:00:36.727 --> 00:00:38.710
Currently we only have one.

00:00:38.710 --> 00:00:41.300
We'll change that by
updating the replicas field

00:00:41.300 --> 00:00:42.400
of our deployment manifest.

00:00:44.440 --> 00:00:45.610
Then we'll apply the change.

00:00:47.370 --> 00:00:51.290
Now we look at our replicasets, we can
see information about what's happening.

00:00:51.290 --> 00:00:54.200
That desired number of
replicas was updated.

00:00:54.200 --> 00:00:57.610
I can use to get pods command
to watch the pods come online.

00:00:57.610 --> 00:01:00.280
We'll see the correct
number of pods here.

00:01:00.280 --> 00:01:02.860
Now we can check that the deployment
of that it is the correct

00:01:02.860 --> 00:01:03.680
number of replicas.

00:01:05.660 --> 00:01:09.340
Yep, looks like we now have three
replicas that the deployment

00:01:09.340 --> 00:01:09.860
is managing.

00:01:10.910 --> 00:01:13.150
And we can still hit our
end point like before.

00:01:14.340 --> 00:01:18.390
At this point we now have multiple
copies of our Hello service

00:01:18.390 --> 00:01:20.000
running in kubernetes and

00:01:20.000 --> 00:01:25.110
we have a single front in service that's
proxying traffic to all three pods.

00:01:25.110 --> 00:01:29.330
This allows us to share the load and
scale our containers and kubernetes.

