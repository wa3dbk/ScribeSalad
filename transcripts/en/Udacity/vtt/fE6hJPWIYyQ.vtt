WEBVTT
Kind: captions
Language: en

00:00:00.250 --> 00:00:00.970
Okay, Michael.

00:00:00.970 --> 00:00:04.700
So, let me sort of jump in on that
last thing that you asked me.

00:00:04.700 --> 00:00:07.730
We've just been talking about
how to compute probabilities

00:00:08.800 --> 00:00:11.940
of optimal actions based upon
what human beings are giving us.

00:00:11.940 --> 00:00:13.070
And that's fine.

00:00:13.070 --> 00:00:16.540
But it turns out we actually have
multiple sources of information, and

00:00:16.540 --> 00:00:17.900
we would like to be
able to combine them.

00:00:17.900 --> 00:00:21.150
In particular,
we have some policy that we've learned

00:00:21.150 --> 00:00:25.380
that gives us a probability distribution
over actions that we learn from a human,

00:00:25.380 --> 00:00:27.500
that's why it says pi sub H here.

00:00:27.500 --> 00:00:30.320
And you notice here I'm treating
a policy as a probability distribution

00:00:30.320 --> 00:00:33.990
over actions, and that's all neat.

00:00:33.990 --> 00:00:37.040
And these, in fact, are the numbers
that we came up with in our last quiz.

00:00:37.040 --> 00:00:39.679
But while the agent is
running around in the world.

00:00:40.680 --> 00:00:44.130
The agent has information from
running around in the world, right?

00:00:44.130 --> 00:00:45.990
There were rewards.

00:00:45.990 --> 00:00:48.160
I've actually gone through and
played Pac-Man.

00:00:48.160 --> 00:00:50.700
And I've been eaten and I've won and
I've gone through rooms and

00:00:50.700 --> 00:00:52.160
I've done all these kinds of things.

00:00:52.160 --> 00:00:56.160
And it seems kind of silly to ignore
what I actually know about what's going

00:00:56.160 --> 00:00:59.360
on in the world, particularly in the
case where human being might, in fact,

00:00:59.360 --> 00:01:00.720
be incorrect sometimes,

00:01:00.720 --> 00:01:05.450
or may really believe that
the optimal policy is one thing,

00:01:05.450 --> 00:01:09.910
but in fact, for the underlying NVP,
the optimal policy is something else.

00:01:09.910 --> 00:01:13.280
So, I just want to ask you a couple
of really simple questions.

00:01:13.280 --> 00:01:16.880
Let's imagine that I've got some
algorithm that runs around in the world,

00:01:16.880 --> 00:01:22.440
on its own, and learns distributions
over actions in every given state, okay?

00:01:22.440 --> 00:01:27.510
And it says after a little while that
the probability that action x is optimal

00:01:27.510 --> 00:01:33.250
as 1/3, action y is optimal as 1/3, and
action z is being optimal is also 1/3.

00:01:33.250 --> 00:01:35.660
And so now I've got two
sources of information here,

00:01:35.660 --> 00:01:40.620
what the humans told me, and I know
that's 2/3, 1/6, 1/6 because math, and

00:01:40.620 --> 00:01:44.590
I've learned from experience so
far that all actions are equally likely.

00:01:44.590 --> 00:01:45.800
&gt;&gt; Which is pretty non-committal.

00:01:45.800 --> 00:01:46.840
&gt;&gt; It is pretty non-committal.

00:01:46.840 --> 00:01:48.970
In fact it might be what
happens when you start off.

00:01:48.970 --> 00:01:53.920
If you had to choose an action in this
case, which action would you choose?

00:01:53.920 --> 00:01:54.420
&gt;&gt; X.

00:01:54.420 --> 00:01:54.950
&gt;&gt; Why?

00:01:54.950 --> 00:01:56.100
&gt;&gt; Is that the quiz?

00:01:56.100 --> 00:01:56.950
&gt;&gt; No, that's a question.

00:01:56.950 --> 00:01:57.590
&gt;&gt; It's Y?

00:01:57.590 --> 00:02:00.270
Wow, I had no idea why it would be Y.

00:02:00.270 --> 00:02:01.963
&gt;&gt; Explain to me why you chose X.

00:02:01.963 --> 00:02:03.650
Sorry.

00:02:04.790 --> 00:02:07.580
&gt;&gt; I see, why I chose x?

00:02:07.580 --> 00:02:11.930
Because well according to the algorithm,
it doesn't really care, doesn't have any

00:02:11.930 --> 00:02:15.240
reason to believe one over the other,
but the human has reason to believe it.

00:02:15.240 --> 00:02:17.320
I guess there's one
missing parameter maybe,

00:02:17.320 --> 00:02:19.490
which is how much we
should believe the human.

00:02:19.490 --> 00:02:21.480
And so,
if we believe the human not at all,

00:02:21.480 --> 00:02:23.940
it's still not being bad to go for X.

00:02:23.940 --> 00:02:24.830
&gt;&gt; But here's the thing.

00:02:24.830 --> 00:02:26.090
We actually have that parameter.

00:02:26.090 --> 00:02:27.390
&gt;&gt; That was C!

00:02:27.390 --> 00:02:30.650
&gt;&gt; That was C, and in fact,
it doesn't matter what C is,

00:02:30.650 --> 00:02:33.530
because that's all inside
this distribution.

00:02:33.530 --> 00:02:36.960
This distribution, two-thirds,
one-sixth, one-sixth, actually already

00:02:36.960 --> 00:02:40.300
incorporates all the information
about the uncertainty of the human.

00:02:40.300 --> 00:02:41.680
I don't think it does.

00:02:41.680 --> 00:02:42.540
&gt;&gt; Yeah, absolutely does.

00:02:42.540 --> 00:02:46.020
The probability that X
is optimal is in fact

00:02:46.020 --> 00:02:49.780
a function of the data that
the human has given us.

00:02:49.780 --> 00:02:53.328
And how likely they are to be correct
in the data that they've given us.

00:02:53.328 --> 00:02:57.429
&gt;&gt; But it seems like it could change
over time as we've come to believe

00:02:57.429 --> 00:02:59.320
the algorithm more.

00:02:59.320 --> 00:03:01.144
Then we should believe the human less.

00:03:01.144 --> 00:03:04.293
&gt;&gt; It's really interesting you say that
because all the other various techniques

00:03:04.293 --> 00:03:06.800
that have been out there in
the world actually do that.

00:03:06.800 --> 00:03:12.370
They actually try to capture what
the human has said and what you've

00:03:12.370 --> 00:03:16.650
learned in the world and then over time
believe the human less and less often

00:03:16.650 --> 00:03:20.250
because whatever the real world is
telling you is got to be correct.

00:03:20.250 --> 00:03:24.260
What's actually beautiful about
this particular mechanism that we

00:03:24.260 --> 00:03:27.450
described in the last couple of
slides is that that parameter c,

00:03:27.450 --> 00:03:30.730
that confidence that we have that
probability the human is correct

00:03:30.730 --> 00:03:33.230
actually sort of captures
everything we need to know,

00:03:33.230 --> 00:03:36.690
if we are completely certain about
the human then these numbers will

00:03:36.690 --> 00:03:40.390
reflect that by peaking over
whatever action should be optimal.

00:03:40.390 --> 00:03:41.740
And if we're uncertain about the human,

00:03:41.740 --> 00:03:44.490
then these things will tend
to stay towards the uniform.

