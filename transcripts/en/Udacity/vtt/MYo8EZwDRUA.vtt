WEBVTT
Kind: captions
Language: en

00:00:01.290 --> 00:00:03.810
So, Sarah's just shown you the code. And one of

00:00:03.810 --> 00:00:06.510
the nice things about using hadoop streaming is that it's

00:00:06.510 --> 00:00:10.010
really easy to test your code outside of hadoop. So,

00:00:10.010 --> 00:00:12.980
let's take a look at how to do that. Our mapper

00:00:12.980 --> 00:00:16.870
takes input from standard input. So, in order to test

00:00:16.870 --> 00:00:19.060
it, we can just run it from the command line

00:00:19.060 --> 00:00:22.000
and type data in to test it. Here, I'm just

00:00:22.000 --> 00:00:26.990
typing, as standard input, a couple of lines of sample data.

00:00:26.990 --> 00:00:29.910
Six fields, separated by tabs. And then when

00:00:29.910 --> 00:00:32.560
I hit Ctrl+D, to simulate the end of

00:00:32.560 --> 00:00:35.260
input, you can see that the mapper outputs

00:00:35.260 --> 00:00:39.650
the results, just as I'd expect. Even better, we

00:00:39.650 --> 00:00:44.340
can just build a small sample data file, and pipe that into the mapper. So let's

00:00:44.340 --> 00:00:47.100
do that. I'm going to just take the

00:00:47.100 --> 00:00:52.140
first 50 lines of purchases.txt and save those into

00:00:52.140 --> 00:00:55.915
a test file, which I'll call testfile. Now,

00:00:55.915 --> 00:00:58.960
I can just pipe that to the mapper by

00:00:58.960 --> 00:01:03.420
saying cat testfile, pipe that to mapper.py. And

00:01:03.420 --> 00:01:06.790
that gives mapper that data a standard input, and

00:01:06.790 --> 00:01:08.960
as you can see, the mapper produces its

00:01:08.960 --> 00:01:12.360
output again to the command line. If we have

00:01:12.360 --> 00:01:14.370
problems, we could just go back and edit the

00:01:14.370 --> 00:01:17.470
mapper until it works. It's really nice and fast

00:01:17.470 --> 00:01:19.800
to be able to do this without having to run

00:01:19.800 --> 00:01:23.210
a complete hadoop job every time during the development phase.

00:01:24.450 --> 00:01:27.240
We can do a similar thing with the reducer. It's

00:01:27.240 --> 00:01:30.050
expecting a set of lines, each of which looks like

00:01:30.050 --> 00:01:33.130
the store name then a tab then the value. So,

00:01:33.130 --> 00:01:35.660
again, we can create a sample file which looks like

00:01:35.660 --> 00:01:38.390
that and pass it in to the reducer. But even

00:01:38.390 --> 00:01:42.620
nicer, we can test the entire pipeline. Remember that the mapper's

00:01:42.620 --> 00:01:45.840
output is sorted by the hadoop framework, and then passed

00:01:45.840 --> 00:01:48.950
to the reducer. So, we can simulate the entire thing

00:01:48.950 --> 00:01:52.410
on the Unix command line, like this. I cat testfile.

00:01:52.410 --> 00:01:54.810
I pipe it to the mapper. I then pass that

00:01:54.810 --> 00:01:58.080
to the Unix sort command, and pass that output to

00:01:58.080 --> 00:02:01.890
the reducer. When I run that, that simulates the entire

00:02:01.890 --> 00:02:05.540
map followed by shuffle and sort, followed by reduced phase,

00:02:05.540 --> 00:02:07.770
and as you can see, I've got the output from

00:02:07.770 --> 00:02:11.008
the reducer. So, now that we've tested this on the

00:02:11.008 --> 00:02:14.160
command line we can test it on the cluster. The best

00:02:14.160 --> 00:02:17.170
practice when you're developing map reduce jobs is first to

00:02:17.170 --> 00:02:20.550
test locally with a small data set before you run your

00:02:20.550 --> 00:02:24.460
code on the entire, huge, set of data. So, now

00:02:24.460 --> 00:02:27.020
that we've tested on the command line, we can test our

00:02:27.020 --> 00:02:30.460
code on the cluster. Best practice, when you're developing that

00:02:30.460 --> 00:02:33.300
reduced jobs, is to first test with a small data set.

00:02:33.300 --> 00:02:35.740
Before you run your code on your entire

00:02:35.740 --> 00:02:37.960
huge set of data. But, we're already, pretty,

00:02:37.960 --> 00:02:40.080
confident here. So, let's just run the thing

00:02:40.080 --> 00:02:44.300
on the wholepurchases.txt file. We'll use the HS

00:02:44.300 --> 00:02:47.380
alias to save some typing. I specified my

00:02:47.380 --> 00:02:51.670
mapper, my reducer, my input directory, and a

00:02:51.670 --> 00:02:54.190
new output directory which we'll call output two.

00:02:55.770 --> 00:02:58.289
And off hadoop goes. It starts running the job.

00:02:59.580 --> 00:03:03.930
It turns out that we can see what's going on on the cluster by taking a look at

00:03:03.930 --> 00:03:08.930
the hadoop job tracker web user interface. So, you point your

00:03:08.930 --> 00:03:14.160
web browser at the job tracker, which on our machine is just local host on

00:03:14.160 --> 00:03:19.300
port 50030. And here you can see that this just one running

00:03:19.300 --> 00:03:24.800
job. If we click on the job name, then that takes us to a page,

00:03:24.800 --> 00:03:27.390
which shows us the progress of the job, and a

00:03:27.390 --> 00:03:30.730
lot of other interesting information as well. We can drill

00:03:30.730 --> 00:03:34.120
down and look at the individual map, or reduce tasks

00:03:34.120 --> 00:03:37.110
just by clicking on the words map or reduce. And

00:03:37.110 --> 00:03:39.830
here we can see that two tasks have completed, two

00:03:39.830 --> 00:03:42.270
tasks are still running. If I click on one of

00:03:42.270 --> 00:03:45.860
the completed tasks, I can even go as far to

00:03:45.860 --> 00:03:50.240
look at the logs from that task. So, here's our job.

00:03:50.240 --> 00:03:53.090
We're now 25% of the way through our reducers.

00:03:53.090 --> 00:03:54.890
As you can see, we get graphs at the

00:03:54.890 --> 00:03:57.360
bottom of this page to show us exactly what's

00:03:57.360 --> 00:04:01.550
happening with the job. So, we nearly finish with

00:04:01.550 --> 00:04:05.370
the job, when the job's finished, hadoop fs minus

00:04:05.370 --> 00:04:08.590
ls of output 2 shows me the just I'd

00:04:08.590 --> 00:04:16.430
expect is a part dash 00000 file in that and just as we did before we can hadoop

00:04:16.430 --> 00:04:26.042
fs minus cat that file to see our actual results.

