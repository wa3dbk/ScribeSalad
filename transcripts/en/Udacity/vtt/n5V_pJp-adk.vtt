WEBVTT
Kind: captions
Language: en

00:00:00.430 --> 00:00:05.100
Now we have the kernel function given as K of x minus xn over h.

00:00:05.100 --> 00:00:09.810
Let's say we have total number of points, capital N,

00:00:09.810 --> 00:00:14.470
then the density estimate is given as a function of x.

00:00:15.520 --> 00:00:19.570
Here's the normalization factor given by the number of points

00:00:19.570 --> 00:00:25.490
inside the box with side h and dimension D.

00:00:25.490 --> 00:00:31.690
And we sum all the kernels from one to N for each xn.

00:00:31.690 --> 00:00:36.950
Here we can get the expectation value from the kernel density estimate.

00:00:36.950 --> 00:00:40.960
It's all right if you can't follow along the entire math here.

00:00:40.960 --> 00:00:44.070
The idea is to take the expectation value

00:00:44.070 --> 00:00:47.790
of the probability density derived from the kernel.

00:00:47.790 --> 00:00:50.750
Which is one over Ndh to the d.

00:00:51.760 --> 00:00:54.140
And the sum of the kernel and

00:00:54.140 --> 00:00:57.970
the expectation function has been taken inside the sum.

00:00:57.970 --> 00:01:02.310
Once we work this out, you can see that the expectation value

00:01:02.310 --> 00:01:07.780
is a convolution of the kernel, with the original probability density.

00:01:07.780 --> 00:01:11.570
We won't get into more details about the expectation value here.

00:01:11.570 --> 00:01:15.330
But we can discuss in the forum why this form is important.

00:01:16.590 --> 00:01:23.000
The kernel function we have defined here is known as the Parzen window kernel.

00:01:23.000 --> 00:01:26.489
This method is used in signal processing problems.

00:01:26.489 --> 00:01:33.793
You will also notice that the smoothness of this PDF depends on the value of h.

00:01:33.793 --> 00:01:38.010
We will now look at another kernel known as the Gaussian kernel.

