WEBVTT
Kind: captions
Language: en

00:00:00.340 --> 00:00:04.180
Here's F Michael, so
just like I wrote R is an expectation

00:00:04.180 --> 00:00:07.190
over the set of rewards that
you're going to see after k steps.

00:00:07.190 --> 00:00:08.960
I'm going to write F in the same way.

00:00:08.960 --> 00:00:11.870
And I've sort of got some
really funky notation here but

00:00:11.870 --> 00:00:13.170
let me try to explain it.

00:00:13.170 --> 00:00:18.095
In English,
F is simply the [LAUGH] expectation of

00:00:18.095 --> 00:00:22.140
me in ending up in s
prime after k steps.

00:00:22.140 --> 00:00:25.090
So this Delta is because I can't
draw lowercase deltas well,

00:00:25.090 --> 00:00:26.808
this is a delta function.

00:00:26.808 --> 00:00:31.220
It is one in the case where
the kth state that I see

00:00:31.220 --> 00:00:34.200
is actually equal to s prime,
and zero otherwise.

00:00:34.200 --> 00:00:39.930
And Delta to the k is just, well the
discount factor raised to the kth power.

00:00:39.930 --> 00:00:44.110
So, F is an expectation over after
I've executed the option that

00:00:44.110 --> 00:00:46.550
I will end up in state s prime.

00:00:46.550 --> 00:00:50.170
Having started in state s and
having taken k steps.

00:00:50.170 --> 00:00:52.430
Now notice, why do I even
have to go through all this?

00:00:52.430 --> 00:00:56.160
Why couldn't I just put T of s,o,
s prime here?

00:00:56.160 --> 00:00:59.140
Well first off, T is defined in terms
of A is not defined in terms of o,

00:00:59.140 --> 00:01:00.480
but that's okay.

00:01:00.480 --> 00:01:02.570
The reason I have to do this
little funky thing, and

00:01:02.570 --> 00:01:04.629
why we call it F instead of T,
is because.

00:01:04.629 --> 00:01:07.910
As you pointed out in the beginning,
there needs to be a discount factor

00:01:07.910 --> 00:01:13.270
attached to the value of the next state,
which, in this case, is s prime.

00:01:13.270 --> 00:01:17.130
So what we've done is, we've rolled
up that discount factor inside of F.

00:01:17.130 --> 00:01:21.360
So F is the expectation
of me starting in s,

00:01:21.360 --> 00:01:25.600
taking option zero,
ending up in s prime after k steps.

00:01:25.600 --> 00:01:29.409
And the Delta sub k gets sort of
brought along with that expectation so

00:01:29.409 --> 00:01:33.427
that I can appropriately discount the s
prime state that I will end up in.

00:01:33.427 --> 00:01:34.334
&gt;&gt; Hm, okay.

00:01:34.334 --> 00:01:37.988
&gt;&gt; Right, so in the end, both R and
F are just expectations,

00:01:37.988 --> 00:01:42.241
they're exactly doing what they
were doing in the original Bellman

00:01:42.241 --> 00:01:44.573
equation that we were using before.

00:01:44.573 --> 00:01:47.869
In the original Bellman equation
the reward was just the immediate reward

00:01:47.869 --> 00:01:49.580
that we got for taking an action here.

00:01:49.580 --> 00:01:54.515
The reward is just an expectation over
all the rewards that we're going to see,

00:01:54.515 --> 00:01:56.765
given that we've taken
this particular option.

00:01:56.765 --> 00:01:59.625
If the option is in
fact a single action,

00:01:59.625 --> 00:02:02.135
then it would be exactly the same
thing as a reward we would get for

00:02:02.135 --> 00:02:03.510
being in that state and
taking that action.

00:02:03.510 --> 00:02:07.756
Similarly, F is just the transition
function, except now it's sort of

00:02:07.756 --> 00:02:11.238
the accumulation of all
the transitions of starting in s and

00:02:11.238 --> 00:02:12.530
ending up in s prime.

00:02:12.530 --> 00:02:15.010
After k steps given that I took

00:02:15.010 --> 00:02:18.570
the particular actions that are inside
of the option, and dividing by pi.

00:02:18.570 --> 00:02:20.780
Now there's a couple of
neat facts about this and

00:02:20.780 --> 00:02:22.450
I will write them down
on the next slide.

00:02:22.450 --> 00:02:25.360
But I just want to point out what's
particularly cool about this,

00:02:25.360 --> 00:02:27.010
is that it really is a Bellman equation.

00:02:28.080 --> 00:02:32.130
It really does allow us to do all of
the things that the Bellman equation,

00:02:32.130 --> 00:02:33.920
Bellman operator allowed
us to do in MDPs.

00:02:35.470 --> 00:02:40.090
But it allows us to analyze or
to reason about or

00:02:40.090 --> 00:02:46.510
to plan over this multiple
variable time MDP.

00:02:46.510 --> 00:02:48.710
So, in fact,
this kind of MDP has a name,

00:02:48.710 --> 00:02:50.320
which I probably should
have written down earlier.

00:02:50.320 --> 00:02:55.860
And it's called SMDP, where the S
stands for, what do you think, Michael?

00:02:55.860 --> 00:02:56.910
Something?

00:02:56.910 --> 00:02:59.050
Yes, and that something is Semi, so

00:02:59.050 --> 00:03:01.810
this is called
a Semi-Markov Decision Process.

00:03:01.810 --> 00:03:03.920
And the difference between
the Markov Decision process and

00:03:03.920 --> 00:03:08.060
the Semi-Markov Decision Process is,
you get to make these jumps in time.

00:03:09.250 --> 00:03:12.580
And because we have this Bellman
operator that takes into account

00:03:12.580 --> 00:03:16.090
all the possible jumps in
time that you might take.

00:03:16.090 --> 00:03:20.080
In the end,
it grounds out to an underlying MDP and

00:03:20.080 --> 00:03:23.030
allows us to inherit all the things
that Bellman allowed us to in here.

