WEBVTT
Kind: captions
Language: en

00:00:00.400 --> 00:00:04.960
Let us look at another example of decision tree learning. Here is a data set of

00:00:04.960 --> 00:00:08.350
people who go to the beach, and some of them get sunburned, and

00:00:08.350 --> 00:00:13.090
others don't. In this data set, there are nine examples and

00:00:13.090 --> 00:00:17.640
each example is characterized by four features, hair, height, age and lotion.

00:00:17.640 --> 00:00:21.920
Once again, how can we construct an optimal decision tree that they classify all

00:00:21.920 --> 00:00:27.270
of those examples? One possible idea is to discriminate first on hair color.

00:00:27.270 --> 00:00:30.690
Hair color classifies all of these known examples into three categories,

00:00:30.690 --> 00:00:35.340
brown, red and blonde. The interesting thing about the choice of hair color is

00:00:35.340 --> 00:00:40.000
that in the case of brown, all of these sunburnt cases are negative. People with

00:00:40.000 --> 00:00:44.830
brown hair apparently don't get sunburned. In case of all the red haired people,

00:00:44.830 --> 00:00:49.620
there is sunburn. So hair color is a good choice for picking as a feature to

00:00:49.620 --> 00:00:53.870
discriminate on because it classifies things in such a way that some of

00:00:53.870 --> 00:00:58.540
the categories have only negative instances and no positive instances. And

00:00:58.540 --> 00:01:03.260
some of the categories are only positive instances and no negative instances.

00:01:03.260 --> 00:01:06.720
Of course, that still leaves blonde-haired people. In this case,

00:01:06.720 --> 00:01:10.270
there are both some positive instances and some negative instances, and

00:01:10.270 --> 00:01:13.600
therefore, will need another feature to discriminate between the positive and

00:01:13.600 --> 00:01:18.890
the negative instances. Here, lotion might be the second feature that we pick.

00:01:18.890 --> 00:01:22.690
Lotion now classifies the remaining examples into two categories, some people

00:01:22.690 --> 00:01:27.730
used lotion, other people did not. Those who used lotion did not get sunburnt.

00:01:27.730 --> 00:01:31.980
Those who did not use lotion did get sunburn. Once again, these are all

00:01:31.980 --> 00:01:36.080
negative instances. These are consisting of only positive instances.

00:01:36.080 --> 00:01:40.150
Thus, in this decision tree, simply by using two features, we were able

00:01:40.150 --> 00:01:44.450
to classify all of these nine examples. This is a different decision tree for

00:01:44.450 --> 00:01:49.340
this same data set. But because we use a different order, therefore, now we have

00:01:49.340 --> 00:01:55.040
to do more work. This decision tree is less optimal than the previous one.

00:01:55.040 --> 00:01:58.450
We could have chosen a different set of features in a different order. Perhaps,

00:01:58.450 --> 00:02:02.550
we could first discriminate on height then on hair color and age. In this case,

00:02:02.550 --> 00:02:08.600
we did a much bushier tree. Clearly, this tree is less optimal than this one.

00:02:08.600 --> 00:02:11.110
Note the trade off with the decision tree learning and

00:02:11.110 --> 00:02:13.947
discrimination tree learning that we covered in case-based reasoning.

00:02:14.960 --> 00:02:19.550
Decision tree learning leads to more optimal classification trees. But

00:02:19.550 --> 00:02:24.910
there is a requirement. You need all the examples right up front. Discrimination

00:02:24.910 --> 00:02:29.410
tree learning may lead to suboptimal trees, but you can learn incrementally.

