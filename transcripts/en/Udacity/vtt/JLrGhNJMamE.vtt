WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.073
So the examples that we're looking at get pretty good occupancy.

00:00:03.073 --> 00:00:05.875
In general, how do affect the occupancy of a kernel?

00:00:05.875 --> 00:00:10.948
Well, it's usually fairly easy to control the amount of shared memory needed by a thread block.

00:00:10.948 --> 00:00:15.916
For example, in our transpose kernel that would be the tile size, and, of course,

00:00:15.916 --> 00:00:19.862
you can change the number of threads and thread blocks that you launch when you launch your kernel.

00:00:19.862 --> 00:00:23.694
You can also work with a compiler to control the number of registers a kernel uses,

00:00:23.694 --> 00:00:28.264
though this qualifies as a ninja-level optimization and isn't usually worth the trouble.

00:00:28.264 --> 00:00:34.835
So increasing occupancy is usually, but not always, a good thing, and it only helps up to a point.

00:00:34.835 --> 00:00:39.758
It exposes more parallelism to the GPU and allows more memory transactions in flight,

00:00:39.758 --> 00:00:43.347
but it may force the GPU to run less efficiently when taken too far.

00:00:43.347 --> 00:00:50.884
This is always a tradeoff. For example, decreasing the tile size and the transpose code will let more thread blocks run.

00:00:50.884 --> 00:00:55.258
It decreases the amount of time spent waiting at barriers, but if we get to too small a tile,

00:00:55.258 --> 00:00:59.163
we'll lose the whole point of tiling, which was to coalesce the global memory accesses.

00:00:59.163 --> 00:01:01.736
So let's go back to that transpose code.

00:01:01.736 --> 00:01:06.738
So here's our kernel, and you'll recall that we concluded that the syncthreads was really the problem.

00:01:06.738 --> 00:01:10.674
We need it there, but we've got a large thread block, 32 by 32 threads,

00:01:10.674 --> 00:01:17.112
1024 total threads, and most of the time most of those threads are simply sitting at the barrier

00:01:17.112 --> 00:01:21.882
waiting for the rest of the threads to reach the barrier before they can proceed.

00:01:21.882 --> 00:01:31.660
So we conclude that maybe we can make this faster by simply going up and changing to 16 by 16 blocks,

00:01:31.660 --> 00:01:37.334
a very small change that should decrease the amount of time that we spend waiting at a barrier

00:01:37.334 --> 00:01:42.045
and therefore decrease the latency between memory operations, which,

00:01:42.045 --> 00:01:45.508
by Little's Law, we would expect to let us increase the total bandwidth.

00:01:45.508 --> 00:01:53.983
So now we'll compile and run, and sure enough, this time we took just over 0.53 milliseconds.

00:01:53.983 --> 00:01:58.384
So let's update our running tally of results. The tiled version of our code,

00:01:58.384 --> 00:02:02.659
we managed to get about half a millisecond by adjusting the tile size.

00:02:02.659 --> 00:02:04.759
So here's a programming exercise.

00:02:04.759 --> 00:02:10.268
Let's go back to the transpose code and try a few different tile sizes and see which ones work best.

00:02:10.268 --> 00:02:17.111
Try 8 by 8, 16 by 16, 32 by 32, and 64 by 64 tiles.

00:02:17.111 --> 00:02:21.857
This last one will be trickier because, of course, this is more elements than you can put threads in a thread block,

00:02:21.857 --> 00:02:24.644
so you'll have to think about how to pack this into a single thread block,

00:02:24.644 --> 00:02:31.723
and I should point out we are looking for the relative ranking of these on the Udacity servers,

00:02:31.723 --> 00:02:35.556
so using the Fermi-based GPUs as opposed to, for example,

00:02:35.556 --> 00:02:38.294
your own machine that you might be developing on at home.

