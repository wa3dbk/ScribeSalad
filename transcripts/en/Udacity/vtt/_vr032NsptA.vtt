WEBVTT
Kind: captions
Language: en

00:00:00.320 --> 00:00:04.120
Typically, the virtual address space of a process

00:00:04.120 --> 00:00:07.210
running on a processor, let's say your desktop, your

00:00:07.210 --> 00:00:09.750
PC, your laptop, and so on, is much

00:00:09.750 --> 00:00:13.200
larger than the physical memory that is allocated for

00:00:13.200 --> 00:00:15.980
a particular process. So the role of the

00:00:15.980 --> 00:00:19.480
virtual memory manager in the operating system is to

00:00:19.480 --> 00:00:22.760
give the illusion to the process that all of

00:00:22.760 --> 00:00:25.930
its virtual address space is contained in physical memory.

00:00:25.930 --> 00:00:28.982
But, in fact, only a portion of the virtual

00:00:28.982 --> 00:00:31.810
address space is really in the physical memory. And that's

00:00:31.810 --> 00:00:34.470
called the working set of the process. And the

00:00:34.470 --> 00:00:39.590
virtual memory manager supports this illusion for the process by

00:00:39.590 --> 00:00:42.970
paging in and out from the disk the pages

00:00:42.970 --> 00:00:45.266
that are being accessed by a process at any particular

00:00:45.266 --> 00:00:48.300
point of time so that the process is happy

00:00:48.300 --> 00:00:50.960
in terns of having its working set contained in the

00:00:50.960 --> 00:00:54.070
physical memory. Now when a node, a desktop,

00:00:54.070 --> 00:00:57.220
is connected on a local area network to

00:00:57.220 --> 00:00:59.840
other peer nodes that are also on the

00:00:59.840 --> 00:01:03.468
same local area network it is conceivable that

00:01:03.468 --> 00:01:05.852
at any point of time, the memory pressure,

00:01:05.852 --> 00:01:10.190
meaning the amount of memory that is required

00:01:10.190 --> 00:01:12.830
to keep all the processes running on this

00:01:12.830 --> 00:01:16.290
node happy, may be different from the memory pressure

00:01:16.290 --> 00:01:19.500
on another nodes. In other words this particular node

00:01:19.500 --> 00:01:23.140
may have a much higher memory pressure because the work

00:01:23.140 --> 00:01:25.320
load on this load consumes a lot of the physical

00:01:25.320 --> 00:01:29.220
memory whereas this work station may be idle and therefore

00:01:29.220 --> 00:01:31.460
all of this memory is not being utilized at

00:01:31.460 --> 00:01:35.920
this point for running anything useful because no applications are

00:01:35.920 --> 00:01:39.280
running on these nodes. So this opens up a new

00:01:39.280 --> 00:01:41.890
line of thought. Given that all these nodes are connected

00:01:41.890 --> 00:01:47.074
on a local area network and some nodes may be busy while other nodes may be

00:01:47.074 --> 00:01:51.060
idle. Is it possible if a particular node

00:01:51.060 --> 00:01:55.190
experiences memory pressure, can we use the idle

00:01:55.190 --> 00:01:58.370
cluster memory, and in particular, can we use

00:01:58.370 --> 00:02:01.790
the cluster memory that's available for paging in

00:02:01.790 --> 00:02:06.980
and out the working set of the processes on this node? Rather than going to the

00:02:06.980 --> 00:02:12.085
disk, can we page in and out to the cluster memories that are idle at this point

00:02:12.085 --> 00:02:17.825
of time? It turns out that with advances in local area networking gear, it's

00:02:17.825 --> 00:02:21.790
already made possible for gigabit ethernet connectivity

00:02:21.790 --> 00:02:24.850
to be available for desktops. And pretty soon,

00:02:24.850 --> 00:02:28.450
10 gigabit links are going to be common

00:02:28.450 --> 00:02:32.190
in connecting desktops to the local area network.

00:02:32.190 --> 00:02:35.430
This makes sending a page to a remote memory

00:02:35.430 --> 00:02:39.350
or fetching a page from a remote memory faster than

00:02:39.350 --> 00:02:42.310
sending it to a local disk. Typically, the local

00:02:42.310 --> 00:02:46.460
disk access speeds are in the order of 200 megabytes

00:02:46.460 --> 00:02:48.290
per second in terms of transfer rate, but on

00:02:48.290 --> 00:02:51.040
top of that, you have to add things like seek

00:02:51.040 --> 00:02:54.080
latency and rotation latency for accessing the data that you

00:02:54.080 --> 00:02:57.650
want from the disk. So all of this augers well

00:02:57.650 --> 00:03:03.000
for saying that perhaps paging in and out through the local area network to peer

00:03:03.000 --> 00:03:04.940
memories that are idle may be a good

00:03:04.940 --> 00:03:09.070
solution when we have memory pressure being experienced

00:03:09.070 --> 00:03:12.860
at any particular node in the cluster. So

00:03:12.860 --> 00:03:16.120
the global memory system, or GMS for short,

00:03:16.120 --> 00:03:20.870
uses cluster memory for paging across the network.

00:03:20.870 --> 00:03:23.680
So in other words in normal memory management,

00:03:23.680 --> 00:03:27.570
if virtual address to physical address translation fails then

00:03:27.570 --> 00:03:30.748
the memory manager knows that it can find this

00:03:30.748 --> 00:03:33.840
virtual address on the disk, meaning the page that

00:03:33.840 --> 00:03:36.220
contains this virtual address on the disk, it pages

00:03:36.220 --> 00:03:38.910
in that particular page from the disk. Now in

00:03:38.910 --> 00:03:42.440
GMS, what we're going to do is, if there

00:03:42.440 --> 00:03:45.060
is a page fault, that is, this translation, virtual

00:03:45.060 --> 00:03:48.810
address to physical address, fails then that means that

00:03:48.810 --> 00:03:51.295
the page is not in the physical memory of this

00:03:51.295 --> 00:03:54.770
node. In that case GMS is going to say, well

00:03:54.770 --> 00:03:57.210
it might be there in the cluster memory, in one

00:03:57.210 --> 00:03:59.710
of the nodes of my peers, or it could be on

00:03:59.710 --> 00:04:02.180
the disk if it is not in the cluster memory

00:04:02.180 --> 00:04:06.120
of my peers. So that's the idea that GMS is

00:04:06.120 --> 00:04:10.840
sort of integrating the cluster memory into this memory hierarchy.

00:04:10.840 --> 00:04:14.160
Normally, when we think about memory hierarchy in a computer system,

00:04:14.160 --> 00:04:17.110
we say there's a processor, there's the caches, and there's the

00:04:17.110 --> 00:04:19.890
main memory, and then there is the virtual memory sitting on

00:04:19.890 --> 00:04:23.070
the disk. But now in GMS, we're sort of extending that

00:04:23.070 --> 00:04:27.370
by saying in addition to the normal memory hierarchy that exists

00:04:27.370 --> 00:04:30.980
in any computer system, that is processor, caches and memory, there

00:04:30.980 --> 00:04:34.780
is also the cluster memory. And only if it is not

00:04:34.780 --> 00:04:37.040
in the cluster memory, we have to think of going to

00:04:37.040 --> 00:04:39.550
the disk in order to get the page that we're looking for.

00:04:39.550 --> 00:04:41.230
That's sort of the idea. So in other

00:04:41.230 --> 00:04:46.259
words, GMS trades network communication for disk I/O. And

00:04:46.259 --> 00:04:48.580
we are doing this only for reads, for

00:04:48.580 --> 00:04:51.890
reading across a network. GMS does not get in

00:04:51.890 --> 00:04:58.730
the way of writing to the disk, that is the disk always has copy of all the

00:04:58.730 --> 00:05:01.330
pages. The only pages that can be in the

00:05:01.330 --> 00:05:04.590
cluster memories are pages that have been paged out

00:05:04.590 --> 00:05:07.520
that are not dirty. And if there are dirty

00:05:07.520 --> 00:05:09.260
copies of pages, they are to be written onto the

00:05:09.260 --> 00:05:11.800
disk just like it will happen in a regular

00:05:11.800 --> 00:05:16.220
computer system. In other words, GMS does not add any

00:05:16.220 --> 00:05:20.130
new causes for worrying about failures because even if

00:05:20.130 --> 00:05:23.470
a node crashes all that you lose is clean copies

00:05:23.470 --> 00:05:26.610
of pages belonging to a particular process that may have

00:05:26.610 --> 00:05:29.598
been in the memory of this node. But those pages

00:05:29.598 --> 00:05:34.560
are on the disk as well. So the disk always has all the copies of the pages.

00:05:34.560 --> 00:05:37.170
It is just that the remote memories of

00:05:37.170 --> 00:05:41.060
the cluster is serving as yet another level in

00:05:41.060 --> 00:05:44.230
the memory hierarchy. So just to recap the

00:05:44.230 --> 00:05:48.790
top level idea of GMS. Normally, in a computer

00:05:48.790 --> 00:05:52.750
system if a process page faults that means

00:05:52.750 --> 00:05:54.630
that the page it is looking for is not

00:05:54.630 --> 00:05:57.010
in physical memory, it'll go to the disk to get it.

00:05:57.010 --> 00:06:00.260
But in GMS, what we're going to do is if a process

00:06:00.260 --> 00:06:03.580
page faults, we know that it is not in physical memory but

00:06:03.580 --> 00:06:06.280
it could be in one of the peer memories as well. So

00:06:06.280 --> 00:06:10.430
GMS is a way of locating the faulting page from one

00:06:10.430 --> 00:06:13.280
of the peer memories, if it is in fact contained in one

00:06:13.280 --> 00:06:16.090
of the peer memories. If not, it's going to be found on

00:06:16.090 --> 00:06:19.920
the disk. So that's the idea behind that. So when the virtual

00:06:19.920 --> 00:06:23.480
memory manager, the virtual memory manager that is part of

00:06:23.480 --> 00:06:27.780
GMS, decides to evict a page from physical memory to

00:06:27.780 --> 00:06:31.400
make room for the current working set of processes running

00:06:31.400 --> 00:06:34.134
on this node, then what the virtual memory manager is going

00:06:34.134 --> 00:06:36.620
to do is, instead of swapping it out to the

00:06:36.620 --> 00:06:38.890
disk, it is going to go out on the network

00:06:38.890 --> 00:06:42.390
and find a peer memory that's idle, and put that

00:06:42.390 --> 00:06:45.210
page in the peer memory so that later on if that

00:06:45.210 --> 00:06:48.120
page is needed, it can fetch it back from the peer and memory.

00:06:48.120 --> 00:06:52.100
That's sort of the big picture of how the global memory system works.

