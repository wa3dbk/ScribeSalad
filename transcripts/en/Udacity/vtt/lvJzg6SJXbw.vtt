WEBVTT
Kind: captions
Language: en

00:00:00.910 --> 00:00:02.580
Alright so here's how we're going to look at this. So as

00:00:02.580 --> 00:00:07.110
you may recall, in this housing example. If we look at different

00:00:07.110 --> 00:00:09.840
degrees of polynomials and how well they fit the data. Let's

00:00:09.840 --> 00:00:13.150
look at the training error. The per example training error. So how

00:00:13.150 --> 00:00:15.812
far off is it for each of the data points? And

00:00:15.812 --> 00:00:19.396
as we increase the degree of the polynomial from constant to linear

00:00:19.396 --> 00:00:22.276
to quadratic and all the way up to, when this case

00:00:22.276 --> 00:00:25.920
order six, the error's always falling. As you go up, you have

00:00:25.920 --> 00:00:28.080
more ability to fit the data, closer and closer and

00:00:28.080 --> 00:00:30.930
closer, right? because, each of these models is, is nested

00:00:30.930 --> 00:00:33.220
inside the other. We can always go back. If the

00:00:33.220 --> 00:00:37.070
zero fits best and I give you six degrees of freedom,

00:00:37.070 --> 00:00:40.420
you can still fit the zero. So, that's what happens

00:00:40.420 --> 00:00:42.240
with the training error, but now let's use this idea

00:00:42.240 --> 00:00:44.600
of cross validation to say what if we split the

00:00:44.600 --> 00:00:51.190
data up into chunks and have each chunk being predicted by

00:00:51.190 --> 00:00:53.170
the, the rest of the data? Train on the rest of the

00:00:53.170 --> 00:00:56.610
data, predict on the chunk. Repeat that for all the different chunks

00:00:56.610 --> 00:01:01.660
and average together. So, so I actually did that. And this is

00:01:01.660 --> 00:01:05.330
what I got with the cross validation error. So there's a I

00:01:05.330 --> 00:01:07.440
don't know there's a couple of interesting things to note about this

00:01:07.440 --> 00:01:09.810
plot. So that we see, we have this red plot that is

00:01:09.810 --> 00:01:13.640
constantly falling and the blue plot which is the cross validation error

00:01:13.640 --> 00:01:16.360
starts out a little bit higher than the, the red plot that's

00:01:16.360 --> 00:01:19.210
got higher error. So, why do you think that is Charles?

00:01:19.210 --> 00:01:20.620
&gt;&gt; Well that makes sense right?

00:01:20.620 --> 00:01:23.500
because we're actually training to minimize error.

00:01:23.500 --> 00:01:27.420
We're actually trying to minimize error on the training set. So the parts we

00:01:27.420 --> 00:01:30.190
aren't looking at, you're more likely to have some error with. That makes sense

00:01:30.190 --> 00:01:32.450
if you'd have a little bit more error on the data you haven't seen.

00:01:32.450 --> 00:01:35.990
&gt;&gt; Right, so, good. So, so, in the, on the, this red curve. We're

00:01:35.990 --> 00:01:39.720
actually predicting predicting all the different data

00:01:39.720 --> 00:01:41.910
points using all of those same data points.

00:01:41.910 --> 00:01:44.530
So it is using all the data to predict that data. This

00:01:44.530 --> 00:01:47.140
blue point, which is really only a little bit higher in this case,

00:01:47.140 --> 00:01:50.590
is using, in this particular case I used all but one of the

00:01:50.590 --> 00:01:54.842
examples to predict the remaining example. But it doesn't have that example when

00:01:54.842 --> 00:01:58.050
it's, when it's doing its fitting. So it's really predicting on a new

00:01:58.050 --> 00:02:00.732
example that it hasn't seen. And so of course you'd expect it to

00:02:00.732 --> 00:02:03.440
be a little bit worse. In this particular case, the averages are all

00:02:03.440 --> 00:02:06.870
pretty much the same so there's not a big difference. But now, let's,

00:02:06.870 --> 00:02:08.680
let's look at what happens as we start to increase the

00:02:08.680 --> 00:02:11.750
degree, we've got the ability to fit this data better and

00:02:11.750 --> 00:02:15.332
better and better, and, in fact, down at you know say,

00:02:15.332 --> 00:02:18.980
three and four, they're actually pretty close in terms of their

00:02:18.980 --> 00:02:23.630
ability to, to, to fit these examples. And then what's great,

00:02:23.630 --> 00:02:26.640
what's really interesting is what happens is now we start to

00:02:26.640 --> 00:02:28.890
give it more, the ability to fit the data closer and

00:02:28.890 --> 00:02:32.042
closer. And by the time we get up to, to order six

00:02:32.042 --> 00:02:34.870
polynomial, even though the error on the training set is

00:02:34.870 --> 00:02:37.470
really low, the error on this, on this cross validation

00:02:37.470 --> 00:02:41.320
error, the error that you, that you're measuring by predicting

00:02:41.320 --> 00:02:45.480
the examples that you haven't seen, is really high. And this

00:02:45.480 --> 00:02:48.710
is beautiful this, this inverted u, is, is exactly what

00:02:48.710 --> 00:02:50.820
you tend to see in these kinds of cases. That

00:02:50.820 --> 00:02:53.780
the error decreases as you have more power and then

00:02:53.780 --> 00:02:57.080
it starts to increase as you use too much [LAUGH] of

00:02:57.080 --> 00:03:00.280
that power. Does that make sense to you?

00:03:00.280 --> 00:03:03.740
&gt;&gt; It does make sense, so. The, the problem is that as

00:03:03.740 --> 00:03:05.930
we give it more and more power we're able to fit the

00:03:05.930 --> 00:03:09.530
data. But as it gets more and more and more power it

00:03:09.530 --> 00:03:16.260
tends to over fit the training data at the expense of future generalization.

00:03:16.260 --> 00:03:18.760
&gt;&gt; Right. So that's exactly how we, we referred to this is this

00:03:18.760 --> 00:03:22.020
sort of idea that if you don't give yourself enough degrees of freedom,

00:03:22.020 --> 00:03:25.250
you don't give yourself a model class that's powerful enough you

00:03:25.250 --> 00:03:27.680
will underfit the data. You won't be able to model what's actually

00:03:27.680 --> 00:03:30.440
going on and there'll be a lot of error. But if you

00:03:30.440 --> 00:03:33.610
give yourself too much you can overfit the data. You can actually

00:03:33.610 --> 00:03:36.850
start to model the error and it generalizes very poorly to

00:03:36.850 --> 00:03:41.410
unseen examples. And somewhere in between is kind of the goldilocks zone.

00:03:41.410 --> 00:03:42.750
Where we're not underfitting, and we're

00:03:42.750 --> 00:03:45.290
not overfitting. We're fitting just right.

00:03:45.290 --> 00:03:47.200
And that's the point that we really want to find. We want to

00:03:47.200 --> 00:03:50.820
find the model that fits the data without overfitting, and not underfitting.

00:03:50.820 --> 00:03:52.790
&gt;&gt; So what was the answer on the, housing exam?

00:03:52.790 --> 00:03:55.440
&gt;&gt; Well, so, it seems pretty clear in this,

00:03:55.440 --> 00:03:57.990
in this plot that it's somewhere, it's either three or

00:03:57.990 --> 00:03:59.660
four. It turns out, if you look at the

00:03:59.660 --> 00:04:02.700
actual numbers, three and four are really close. But three

00:04:02.700 --> 00:04:04.380
is a little bit lower. So three is actually

00:04:04.380 --> 00:04:06.190
the thing that fits it the best. And, in fact,

00:04:06.190 --> 00:04:08.430
if you look at what four does. It fits the

00:04:08.430 --> 00:04:12.340
data by more or less zeroing out the, the quartic

00:04:12.340 --> 00:04:14.550
term, right? It doesn't really use the, this power.

00:04:14.550 --> 00:04:16.779
&gt;&gt; Oh, but that's interesting. So that means

00:04:16.779 --> 00:04:19.940
it, it barely uses the, the, the extra degree

00:04:19.940 --> 00:04:22.079
of freedom you give it. But even using it

00:04:22.079 --> 00:04:24.880
a little bit, it still does worse than generalization.

00:04:24.880 --> 00:04:26.850
&gt;&gt; Just a tiny bit worse.

00:04:26.850 --> 00:04:26.910
&gt;&gt; Huh.

00:04:26.910 --> 00:04:27.750
&gt;&gt; Yup exactly so.

00:04:27.750 --> 00:04:29.560
&gt;&gt; That's actually kind of cool.

