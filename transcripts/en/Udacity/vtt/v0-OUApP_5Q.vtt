WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.000
How can we optimize these two terms over here?

00:00:04.000 --> 00:00:06.000
The idea is to use gradient descent.

00:00:06.000 --> 00:00:13.000
That is, every time step we take a small step in the direction of minimizing the error over here.

00:00:13.000 --> 00:00:15.000
Here's the expression for the first objective.

00:00:15.000 --> 00:00:20.000
When we iterate, we assigned to yi recursively the old yi,

00:00:20.000 --> 00:00:26.000
but we subtract a term that's proportional to the deviation of yi to xi,

00:00:26.000 --> 00:00:28.000
weighted by a weight function alpha.

00:00:28.000 --> 00:00:31.000
That's not exactly the same alpha as before.

00:00:31.000 --> 00:00:34.000
We set this alpha over here to 0.5.

00:00:34.000 --> 00:00:37.000
For the second term, we could implement this as follows

00:00:37.000 --> 00:00:47.000
where we retain the old y variable, but we move a little bit in the direction of yi+1 and away from yi.

00:00:47.000 --> 00:00:50.000
But an even better implementation looks as follows.

00:00:50.000 --> 00:00:53.000
This is combining the step on the left and the step on the right.

00:00:53.000 --> 00:00:58.000
Realizing that each yi occurs twice in this optimization term,

00:00:58.000 --> 00:01:03.000
one here and one here, we can now go and implement this in a single update rule

00:01:03.000 --> 00:01:09.000
where we wish yi to be as close to yi-1 and simultaneously be as close as yi+1

00:01:09.000 --> 00:01:11.000
by optimizing this combined term.

00:01:11.000 --> 00:01:15.000
Think about this as little bit, but that's what I want you to implement.

00:01:15.000 --> 00:01:18.000
We're going to set beta to 0.1.

00:01:18.000 --> 00:01:21.000
Now let's go and implement this.

00:01:21.000 --> 00:01:27.000
As a last hint, I don't want you to apply this optimization for the first or the last node in the sequence.

00:01:27.000 --> 00:01:30.000
I want those to be unchanged, as we'll see in a second.

00:01:30.000 --> 00:01:32.000
Here's the code I'll be giving you.

00:01:32.000 --> 00:01:36.000
There's a path in a 5 x 5 grid, starting at [0, 0] to [4, 4].

00:01:36.000 --> 00:01:41.000
If you look very carefully, it goes to the right at first, then straight down, then to the right again.

00:01:41.000 --> 00:01:45.000
This is exactly the path we discussed so far and looks like this graphically.

00:01:45.000 --> 00:01:51.000
I now want you to implement the function "smooth," which takes as an input the path,

00:01:51.000 --> 00:01:54.000
our two weighting factors, and a small tolerance variable,

00:01:54.000 --> 00:01:57.000
which I'll explain to you in a second, and it creates the new path,

00:01:57.000 --> 00:02:03.000
which are the y's in our equations so far from the old path.

00:02:03.000 --> 00:02:05.000
This is a deep copy over here.

00:02:05.000 --> 00:02:07.000
Then below the line, I want you to implement the smoother.

00:02:07.000 --> 00:02:11.000
What the smoother does is iteratively applies the two equations I just gave you to all nodes

00:02:11.000 --> 00:02:14.000
except for the first and the last,

00:02:14.000 --> 00:02:20.000
and it does so until the total change observed in the update step becomes smaller than tolerance,

00:02:20.000 --> 00:02:23.000
at which point we consider the smoother to have converged.

00:02:23.000 --> 00:02:28.000
Here is my command. I compute a new path as a result of the function smooth.

00:02:28.000 --> 00:02:31.000
In your test, you should uncomment the "newpath" smooth routine

00:02:31.000 --> 00:02:33.000
and the print routine that outputs my result,

00:02:33.000 --> 00:02:41.000
"Thank to EnTeer," a student who posted a much better way to output matrices on the discussion forum.

00:02:41.000 --> 00:02:44.000
I'm going to use his or her code. Thank you so much.

00:02:44.000 --> 00:02:52.000
Here's the result. After hitting "Run," I have the original path over here--zeros all the way to [4, 4].

00:02:52.000 --> 00:02:56.000
The two initial and the end position, should be the same as before, so please don't modify them,

00:02:56.000 --> 00:03:00.000
but in between we get these interpolation positions over here.

00:03:00.000 --> 00:03:05.000
If you look at this, my original path didn't vary the x's at all for the first three steps

00:03:05.000 --> 00:03:11.000
whereas this one goes from [0, 0] to 0.17, so it got closer. It went down a little bit.

00:03:11.000 --> 00:03:14.000
Also, it went less to the right side than my original path.

00:03:14.000 --> 00:03:17.000
We went all the way to 2 over here to 1.8 over here.

00:03:17.000 --> 00:03:22.000
What this means is that our new points lie a little bit like this.

00:03:22.000 --> 00:03:27.000
As you go through the list over here, you'll find that our new points really smooth out

00:03:27.000 --> 00:03:30.880
the path to something more like that.

00:03:32.055 --> 00:03:34.510
Hi, I'm Andy, the assistant instructor for this class,

00:03:34.510 --> 00:03:37.070
and I just wanted to provide some clarification about

00:03:37.070 --> 00:03:40.538
how you're going to use gradient descent to minimize these functions.

00:03:40.538 --> 00:03:43.956
First, I'd like to point out that there was a slight error

00:03:43.956 --> 00:03:46.119
in the path used in Sebastian's code.

00:03:46.119 --> 00:03:50.007
That [4,4] that he was using should actually be replaced with a [4,2].

00:03:50.007 --> 00:03:54.683
Also, those minus signs that you saw here should actually be replaced with plus signs

00:03:54.683 --> 00:03:57.571
if we want this gradient descent implementation to converge.

00:03:57.571 --> 00:04:01.803
Finally, even though these yi's and xi's are two-dimensional vectors,

00:04:01.803 --> 00:04:04.311
when you implement your code, it may be easier

00:04:04.311 --> 00:04:07.075
to just iterate over each individual entry.

00:04:07.075 --> 00:04:12.183
So, for example, these xi's would be these values here.

