WEBVTT
Kind: captions
Language: en

00:00:00.330 --> 00:00:02.490
You've already seen
some of those tricks.

00:00:02.490 --> 00:00:06.100
I asked you to make your inputs zero
mean and equal variance earlier.

00:00:06.100 --> 00:00:07.440
It's very important for SGD.

00:00:07.440 --> 00:00:10.860
I also told you to initialize
with random weights

00:00:10.860 --> 00:00:14.120
that have relatively small variance,
same thing.

00:00:14.120 --> 00:00:17.090
I'm going to talk about a few more
of those important tricks, and

00:00:17.090 --> 00:00:21.100
that should cover all you really need
to worry about to implement SGD.

00:00:21.100 --> 00:00:23.460
The first one is momentum.

00:00:23.460 --> 00:00:29.060
Remember that at each step, we're taking
a very small step in a random direction,

00:00:29.060 --> 00:00:33.120
but that on aggregate, those steps take
us toward the minimum of the loss.

00:00:33.120 --> 00:00:35.810
We can take advantage of
the knowledge that we've accumulated

00:00:35.810 --> 00:00:38.750
from previous steps about
where we should be headed.

00:00:39.840 --> 00:00:43.530
A cheap way to do that is to keep
a running average of the gradients, and

00:00:43.530 --> 00:00:46.770
to use that running average instead
of the direction of the current

00:00:46.770 --> 00:00:48.340
batch of the data.

00:00:48.340 --> 00:00:51.990
This momentum technique works very well
and often leads to better convergence.

00:00:53.170 --> 00:00:55.610
The second one is learning rate decay.

00:00:55.610 --> 00:01:00.110
Remember, when replacing gradient
descent with SGD, I said that we were

00:01:00.110 --> 00:01:04.420
going to take smaller,
noisier steps towards our objective.

00:01:04.420 --> 00:01:06.590
How small should that step be?

00:01:06.590 --> 00:01:08.150
That's a whole area of research as well.

00:01:09.260 --> 00:01:11.420
One thing that's always the case,
however,

00:01:11.420 --> 00:01:15.980
is that it's beneficial to make that
step smaller and smaller as you train.

00:01:15.980 --> 00:01:19.850
Some like to apply an exponential decay
to the learning rate some like to make

00:01:19.850 --> 00:01:23.890
it smaller every time the loss reaches
a plateau, there are lots of ways to go

00:01:23.890 --> 00:01:27.700
about it, but lowering it over
time is the key thing to remember.

