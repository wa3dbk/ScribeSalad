WEBVTT
Kind: captions
Language: en

00:00:00.280 --> 00:00:03.510
There's another way we can
build an ensemble of learners.

00:00:03.510 --> 00:00:08.300
We can build them using
the same learning algorithm but

00:00:08.300 --> 00:00:11.480
train each learner on
a different set of the data.

00:00:11.480 --> 00:00:14.840
This is what's called bootstrap
aggregating or bagging.

00:00:14.840 --> 00:00:20.190
It was invented by Bremen in
the late '80s, early '90s.

00:00:20.190 --> 00:00:22.170
Here's how bagging works.

00:00:22.170 --> 00:00:26.180
So what we do is we create
a number of subsets of the data.

00:00:26.180 --> 00:00:30.900
I've drawn little bags here
to represent bags of data.

00:00:30.900 --> 00:00:35.330
And each one of these is
a subset of the original data.

00:00:35.330 --> 00:00:37.280
Now how do we collect these?

00:00:37.280 --> 00:00:39.520
Well, we do it randomly.

00:00:39.520 --> 00:00:46.430
So for this subset it
contains n prime values and

00:00:46.430 --> 00:00:51.520
our original data set contains
n different instances.

00:00:51.520 --> 00:00:58.380
We grab n prime of them, at random, with
replacement from this original data.

00:00:58.380 --> 00:01:03.380
So what, with replacement means is,
let's say we had

00:01:03.380 --> 00:01:08.790
these values, we might grab this one and
put it in our bag.

00:01:08.790 --> 00:01:13.480
We might randomly grab this one and
put it in our bag, but each time we grab

00:01:13.480 --> 00:01:18.550
randomly, we randomly choose across
the whole collection of data.

00:01:18.550 --> 00:01:22.290
So we might choose this one again and
put it in the bag.

00:01:22.290 --> 00:01:27.910
So this one and this one are really
the same one and they're repeated twice.

00:01:27.910 --> 00:01:28.620
And that's okay.

00:01:28.620 --> 00:01:30.970
That's what with replacement means.

00:01:30.970 --> 00:01:36.990
So we crate all together m
of these groups or bags.

00:01:36.990 --> 00:01:39.990
And each one of them contains n prime

00:01:39.990 --> 00:01:44.710
different data instances chosen
at random with replacement.

00:01:44.710 --> 00:01:46.710
Let's note these things.

00:01:46.710 --> 00:01:51.140
So, n is the number of training
instances in our original data.

00:01:51.140 --> 00:01:56.960
N prime is the number of instances
that we put in each bag and

00:01:56.960 --> 00:01:59.580
m is the number of bags.

00:01:59.580 --> 00:02:05.090
We almost always want n
prime to be less than n.

00:02:05.090 --> 00:02:07.410
Usually about 60%.

00:02:07.410 --> 00:02:11.840
So each of these bags has about
60% as many training instances

00:02:11.840 --> 00:02:13.430
as our original data.

00:02:13.430 --> 00:02:15.490
That's just a rule of thumb.

00:02:15.490 --> 00:02:21.460
Now, we use each of these collections
of data to train a different model.

00:02:21.460 --> 00:02:24.490
We have now m different models,

00:02:24.490 --> 00:02:27.790
each one trained on a little
bit of different data.

00:02:27.790 --> 00:02:31.990
And just like when we have an ensemble
of different learning algorithms,

00:02:31.990 --> 00:02:36.180
here we have an ensemble of different
models we query in the same way.

00:02:36.180 --> 00:02:41.650
We query each model with the same x and
we collect all of their outputs.

00:02:41.650 --> 00:02:45.330
We take the y output of each model,
take their mean, and

00:02:45.330 --> 00:02:48.400
boom, that's our y for the ensemble.

00:02:48.400 --> 00:02:52.690
Now keep in mind we can
wrap this in a single API.

00:02:52.690 --> 00:02:58.090
Just like that API you wrapped your
[INAUDIBLE] in and your KNN learner in.

