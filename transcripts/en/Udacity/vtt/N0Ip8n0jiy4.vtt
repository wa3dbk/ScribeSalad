WEBVTT
Kind: captions
Language: en

00:00:00.420 --> 00:00:03.010
Okay, so, I want to talk about two things on this

00:00:03.010 --> 00:00:06.160
little slide, here. The first one is kind of a general

00:00:06.160 --> 00:00:10.300
observation about rewards and what makes the reinforcement learning MDP problem

00:00:10.300 --> 00:00:13.970
different from the supervised learning problems that we did before. And

00:00:13.970 --> 00:00:17.120
that's the notion of not just rewards, but this notion

00:00:17.120 --> 00:00:20.450
of delayed rewards. So, what do I mean by that. Well,

00:00:20.450 --> 00:00:22.540
so, if you think about the way we've set up kind

00:00:22.540 --> 00:00:25.120
of problem like, like we have here where we're trying to.

00:00:25.120 --> 00:00:29.440
Start in this little bottom left-hand square, and wind

00:00:29.440 --> 00:00:32.390
our way up into this plus one. There's really

00:00:32.390 --> 00:00:35.709
this notion of sequences of action. So in the

00:00:35.709 --> 00:00:39.863
reinforcement learning context, and all the things that we're talking

00:00:39.863 --> 00:00:43.213
about, at least in the foreseeable future, there's this

00:00:43.213 --> 00:00:45.759
sort of problem where you take some action and

00:00:45.759 --> 00:00:47.903
that puts you in some place and then you

00:00:47.903 --> 00:00:51.010
take another action and that puts you in some place.

00:00:51.010 --> 00:00:53.038
And then you take another action and that puts you in some place

00:00:53.038 --> 00:00:55.000
and maybe it puts you at a place where you get plus one.

00:00:55.000 --> 00:00:59.490
Or maybe it puts you at a place where you get minus one.

00:00:59.490 --> 00:01:03.200
And what really is going on here is this idea that you take

00:01:03.200 --> 00:01:06.420
actions that will set you up for other actions that will set you

00:01:06.420 --> 00:01:09.480
up for other actions. And only then do you know how good those

00:01:09.480 --> 00:01:13.600
particular actions you took were. So this reward is not just an idea

00:01:13.600 --> 00:01:16.710
of getting a reward at every state, it's an idea of getting delayed reward.

00:01:16.710 --> 00:01:19.130
So you don't know how your immediate action

00:01:19.130 --> 00:01:21.540
is going to lead to things down the road.

00:01:21.540 --> 00:01:22.830
So, let me give you a concrete example

00:01:22.830 --> 00:01:24.720
about that. So, have you ever played chess?

00:01:24.720 --> 00:01:25.470
&gt;&gt; Sure.

00:01:25.470 --> 00:01:26.770
&gt;&gt; So let's say we played a long game

00:01:26.770 --> 00:01:28.970
of chess and maybe took 60, 61, 62 moves. And

00:01:28.970 --> 00:01:33.270
then at the end, I win the game. So, what

00:01:33.270 --> 00:01:36.140
do you know about the way you played that game?

00:01:36.140 --> 00:01:39.350
&gt;&gt; That I probably made a bad decision around

00:01:39.350 --> 00:01:41.710
the time when I decided to play chess against you.

00:01:41.710 --> 00:01:44.040
&gt;&gt; That's possible, but maybe you made a

00:01:44.040 --> 00:01:47.080
good decision and you kept making good decisions, and

00:01:47.080 --> 00:01:49.450
you only messed up at the very last move,

00:01:49.450 --> 00:01:50.890
when you could have mated me but you didn't.

00:01:50.890 --> 00:01:52.970
&gt;&gt; I see. Well you, what, my

00:01:52.970 --> 00:01:55.290
experience in playing chess is that usually, that's

00:01:55.290 --> 00:01:58.900
not what happens. Usually it's not that I make a bad move and then there's

00:01:58.900 --> 00:02:02.250
a problem. It's usually I make a move that I think is reasonable that only

00:02:02.250 --> 00:02:05.980
later I discover put me in a position where I can't possibly do anything good.

00:02:05.980 --> 00:02:07.500
&gt;&gt; Right. Well that's actually

00:02:07.500 --> 00:02:09.229
the game that I played in New York many,

00:02:09.229 --> 00:02:11.840
many years ago that I lost was exactly like that.

00:02:11.840 --> 00:02:13.420
The game went on for like literally a hundred

00:02:13.420 --> 00:02:15.590
moves. I really lost the game on the third move.

00:02:15.590 --> 00:02:17.730
&gt;&gt; Oh.

00:02:17.730 --> 00:02:21.040
&gt;&gt; I made a mistake, i transposed two moves because it was a new

00:02:21.040 --> 00:02:24.800
opening that I was just learning and I knew at the time that i screwed

00:02:24.800 --> 00:02:28.340
up, i played beautiful chess from that point on, but the truth is the

00:02:28.340 --> 00:02:30.360
other player had a positional advantage from

00:02:30.360 --> 00:02:32.920
that point on that I could never overcome,

00:02:32.920 --> 00:02:36.710
so I lost the game, but not because I played poorly for,

00:02:36.710 --> 00:02:39.840
you know, 80 moves. It's because I paid, played poorly for one

00:02:39.840 --> 00:02:42.850
move, and that move happened to be fairly early. So this is

00:02:42.850 --> 00:02:45.810
this notion of delayed reward, that I played this long game of

00:02:45.810 --> 00:02:49.070
chess, and maybe it's I played well, and I screwed up in

00:02:49.070 --> 00:02:52.310
the end. Maybe I played medio, you know, mediocre game, but I

00:02:52.310 --> 00:02:54.960
had a couple of brilliant moves, and that's why I won, or

00:02:54.960 --> 00:02:58.130
maybe I played very well in the beginning, poorly at the end or

00:02:58.130 --> 00:03:00.170
the other way around. And the truth is you don't

00:03:00.170 --> 00:03:02.370
really know. All you know is that you're taking a bunch

00:03:02.370 --> 00:03:05.550
of actions. You get rewards signals back from the environment. Like

00:03:05.550 --> 00:03:07.710
I won the game or I lost the game. And then

00:03:07.710 --> 00:03:09.940
you have this problem of figuring out of all the actions

00:03:09.940 --> 00:03:13.990
that I took, what was the action that led to me

00:03:13.990 --> 00:03:17.990
ultimately winning or losing, or getting whatever reward that I, I

00:03:17.990 --> 00:03:20.020
got at the end of a sequence. Does that make sense?

00:03:20.020 --> 00:03:21.460
&gt;&gt; Yeah it seems like that could

00:03:21.460 --> 00:03:23.170
be really challenging. You probably need your

00:03:23.170 --> 00:03:26.310
own like sportscaster listening and, and commenting.

00:03:26.310 --> 00:03:29.100
&gt;&gt; Yeah and in fact the sportscaster who is listening and commenting

00:03:29.100 --> 00:03:32.500
at least in the NDP world is in fact the sequence of rewards

00:03:32.500 --> 00:03:34.750
that you get in the sequence of states that you see right?

00:03:34.750 --> 00:03:37.220
It really is sort of a play by play. In this state this

00:03:37.220 --> 00:03:39.680
action got this reward and you want to take all of that

00:03:39.680 --> 00:03:42.220
and figure out whether this was a good action you're first action or

00:03:42.220 --> 00:03:44.790
a poor action or your second action was good or poor and so

00:03:44.790 --> 00:03:48.170
on and so forth. Now contrast that with supervised learning, so in supervised

00:03:48.170 --> 00:03:51.720
learning what you would be getting is while I was in this state This

00:03:51.720 --> 00:03:54.840
first date and then this was the proper action I was supposed to take

00:03:54.840 --> 00:03:58.060
let's say action seventeen. And your goal then is just to simply learn a

00:03:58.060 --> 00:04:01.640
function from states to specific actions, that's

00:04:01.640 --> 00:04:03.400
how the supervised learning problem is setup, right?

00:04:03.400 --> 00:04:04.320
&gt;&gt; Yeah exactly.

00:04:04.320 --> 00:04:05.860
&gt;&gt; In this particular case you're in some

00:04:05.860 --> 00:04:09.130
state you take some action and you get

00:04:09.130 --> 00:04:13.090
some reward for the action you took. Or may be the state that you ended up

00:04:13.090 --> 00:04:15.590
in or something. And you get a sequence of these

00:04:15.590 --> 00:04:19.350
state action award triples, and ultimately you have to figure out

00:04:19.350 --> 00:04:23.900
for the given states you're in, what was the action

00:04:23.900 --> 00:04:26.460
you took that helped to deter, or actions you took that

00:04:26.460 --> 00:04:29.150
helped to determine the ultimate sequence of rewards that you

00:04:29.150 --> 00:04:32.780
saw. Perhaps this one plus one or this minus one that

00:04:32.780 --> 00:04:35.510
you got at the end. This is really difficult problem,

00:04:35.510 --> 00:04:38.560
its got its own name, its called the credit assignment problem.

00:04:38.560 --> 00:04:40.860
In fact, we're talking about a sequence

00:04:40.860 --> 00:04:42.930
of events over time, we typically refer to

00:04:42.930 --> 00:04:44.330
this type of problem as the temporal

00:04:44.330 --> 00:04:46.470
credit assignment problem. Does that make sense Michael.

00:04:46.470 --> 00:04:50.600
&gt;&gt; Yeah that's really cool. Although I guess its credit but also blame right?

00:04:50.600 --> 00:04:53.950
&gt;&gt; Well credit and blame you know, blame is just the minus of credit.

00:04:53.950 --> 00:04:56.490
So without loss of generality Assume that

00:04:56.490 --> 00:04:58.930
credit can assume a negative or positive vibe.

00:04:58.930 --> 00:05:01.990
&gt;&gt; And yet when I go to the supermarket they never say blame or credit.

00:05:01.990 --> 00:05:03.850
&gt;&gt; Well that's just because they don't understand

00:05:03.850 --> 00:05:04.940
the technical terms.

00:05:04.940 --> 00:05:05.160
&gt;&gt; Got it.

