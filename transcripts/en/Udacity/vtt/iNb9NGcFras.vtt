WEBVTT
Kind: captions
Language: en

00:00:00.100 --> 00:00:05.814
Okay, so before we proceed I want to fill in these numbers again and rerun this

00:00:05.814 --> 00:00:10.585
program to get values that correspond to what you'll see on the Udacity IDE.

00:00:10.585 --> 00:00:13.577
Until now, as I've mentioned, I've been using my laptop but I want to--

00:00:13.577 --> 00:00:18.936
we're going to let you play around with this transpose code, and I want you to see on

00:00:18.936 --> 00:00:21.193
the screen numbers that are a little bit more comparable to what you'll get on

00:00:21.193 --> 00:00:25.732
the much bigger GPUs that Udacity is using versus the one that's in my laptop.

00:00:25.732 --> 00:00:30.949
Okay, so here's the code we've looked at before. This time it's running in the Udacity IDE.

00:00:30.949 --> 00:00:34.463
And what I want to do now is do a test run.

00:00:34.463 --> 00:00:36.629
And here's the results we get.

00:00:36.629 --> 00:00:40.410
So you can see that now transpose serial took 82.6 milliseconds,

00:00:40.410 --> 00:00:49.354
all the way down to transpose parallel per element tiled. which took about 0.13 milliseconds.

00:00:49.354 --> 00:00:54.104
So here I've written those results down again, and now we can go through and fill in the DRAM utilization.

00:00:54.750 --> 00:00:57.636
So you can see that we've got the same pattern.

00:00:57.636 --> 00:01:02.512
The serial version of the code, which runs in a single thread, takes 82.7 milliseconds.

00:01:02.512 --> 00:01:08.382
As we get more parallel, we start moving into single milliseconds, and then sub-millisecond.

00:01:08.398 --> 00:01:12.684
And by the time we get here, we're running quite well at 0.35 milliseconds.

00:01:12.684 --> 00:01:16.151
And sure enough, by tiling that code we

00:01:16.151 --> 00:01:20.215
improve on this a little bit, but now we're still spending a lot of time waiting at barriers.

00:01:20.215 --> 00:01:26.287
And by going to a smaller tile size we can get that all the way down to 0.13 milliseconds.

00:01:26.287 --> 00:01:29.578
Now at this point we're geting about5 43.5%

00:01:29.578 --> 00:01:34.761
of the theoretical peak limit of our bandwidth, and that's actually doing pretty good.

00:01:34.761 --> 00:01:38.944
There's one last optimization effect that we're not going to talk about at this lecture,

00:01:38.944 --> 00:01:42.645
but you're welcome, in fact encouraged, to go look it

00:01:42.645 --> 00:01:46.997
up in the CUDA C Best Practices Guide, or on many presentations that you can

00:01:46.997 --> 00:01:49.557
find online if you're motivated and curious.

00:01:49.557 --> 00:01:56.048
I decided that it's a fairly subtle effect and not too important. And that's shared memory bank conflicts.

00:01:56.048 --> 00:02:01.468
Essentially what this says is that shared memory is organized into banks, and

00:02:01.468 --> 00:02:07.699
depending on how you line up your thread accesses that the array of threads is

00:02:07.699 --> 00:02:10.061
making to the memory that's actually stored on the tile,

00:02:10.061 --> 00:02:16.155
you can end up spending a lot of time sort of replaying over and over.

00:02:16.155 --> 00:02:19.793
You saw that shared memory replays statistic much earlier

00:02:19.793 --> 00:02:23.739
when we were looking at the NVVP output, and that's what this is about.

00:02:23.739 --> 00:02:28.668
The fix is relatively simple, and as I said, we're not going to go into it here,

00:02:28.668 --> 00:02:31.942
but there's a version of this code that you can download and play with on Wiki.

00:02:31.942 --> 00:02:37.414
The fix is actually simply to reserve a slightly larger chunk of shared memory than you're actually

00:02:37.414 --> 00:02:41.504
going to use, and this has the effect of padding the shared memory and striping it across the

00:02:41.504 --> 00:02:45.504
banks, which can actually improve our run time even slightly more.

