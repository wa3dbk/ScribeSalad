WEBVTT
Kind: captions
Language: en

00:00:00.180 --> 00:00:04.240
But what about the reliability and
the MTTF of Raid 1?

00:00:04.240 --> 00:00:08.490
Same as before, we'll assume that
some F is the failure rate for

00:00:08.490 --> 00:00:12.450
a single disk in failures
per disk per second.

00:00:12.450 --> 00:00:16.416
And for a single disk remember that
the MTTF is simply one over F,

00:00:16.416 --> 00:00:19.125
where F is that failure rate.

00:00:19.125 --> 00:00:23.485
And then the mean time to data loss,
which is the MTTF,

00:00:23.485 --> 00:00:25.375
really, of the whole disk array,

00:00:25.375 --> 00:00:30.155
in this case just one disk, would be
simply the MTTF of that one disk.

00:00:30.155 --> 00:00:34.200
Well, let's say now that we
have two disks in RAID 1.

00:00:34.200 --> 00:00:37.990
And remember that the failure rate
when we have more than one disk

00:00:37.990 --> 00:00:43.590
is N times the failure rate of one disk
as long as all of the disks are working.

00:00:43.590 --> 00:00:47.860
So what we have really is that
both of our disks in raid one

00:00:47.860 --> 00:00:52.150
are okay until the time,
entity of one disk over two.

00:00:52.150 --> 00:00:55.978
This is the expected time when
one of the disks will fail.

00:00:55.978 --> 00:00:57.530
Now, unlike raid zero,

00:00:57.530 --> 00:01:02.280
where this is the point where we would
lose data, now we have mirroring.

00:01:02.280 --> 00:01:07.750
So both of the disks need to fail in
order for the whole thing to lose data.

00:01:07.750 --> 00:01:12.824
One failure is not data loss yet,
so we don't have the failure

00:01:12.824 --> 00:01:17.520
of the array yet, but this time,
we are down to one disk.

00:01:18.540 --> 00:01:21.810
That one disk is a working disk,
and has this failure rate.

00:01:22.920 --> 00:01:29.190
So, that one remaining disk will live
on for another MTTF of a single disk.

00:01:29.190 --> 00:01:31.580
This is very important to understand.

00:01:31.580 --> 00:01:35.910
If we have a working disk at
some point in time, the MTTF for

00:01:35.910 --> 00:01:40.490
that disc is going to be,
the overall MTTF for one disc.

00:01:40.490 --> 00:01:44.010
This MTTF is not affected
by the fact that this disk

00:01:44.010 --> 00:01:46.420
has already been working for
all this time.

00:01:46.420 --> 00:01:47.080
Why?

00:01:47.080 --> 00:01:51.170
Well because the failure rate is
constant, it's not affected by time.

00:01:51.170 --> 00:01:54.500
So if we have a working disk,
its failure rate is always the same.

00:01:55.500 --> 00:01:59.079
The only thing that changes
the failure rate is if the disk fails.

00:02:00.250 --> 00:02:04.540
So if we had two disks and both work,
this is the expected time of them

00:02:04.540 --> 00:02:10.180
surviving, and then if we're down to
one disk that works, at the time when

00:02:10.180 --> 00:02:15.780
we have a single working disk this
is the MTTF that we can expect.

00:02:15.780 --> 00:02:22.300
So overall, the mean time to data loss
for RAID1 with two disks is going to be

00:02:22.300 --> 00:02:28.805
the MTTF of a single disk over two,
plus the MTTF of a single disk.

00:02:28.805 --> 00:02:30.765
And now, we bought two disks, and

00:02:30.765 --> 00:02:34.385
used them in an array that has
the capacity of only a single disk.

00:02:34.385 --> 00:02:37.515
So we basically paid for the extra disk,
just to get reliability.

00:02:37.515 --> 00:02:41.462
And what we get is just one
half more of the MTTF then

00:02:41.462 --> 00:02:44.082
we would normally get
with only a single disk.

00:02:44.082 --> 00:02:45.442
So what's the point?

00:02:45.442 --> 00:02:48.382
The point is that this calculation

00:02:48.382 --> 00:02:54.116
assumes that no disk is replaced
at the time when it fails.

00:02:54.116 --> 00:02:56.386
So we had the failure and

00:02:56.386 --> 00:03:01.006
we just let that fail disk just be
there and we were down to one disk and

00:03:01.006 --> 00:03:05.898
didn't fix it and then we were just down
to the entity F of one disk for awhile.

00:03:05.898 --> 00:03:09.528
But remember that when we want
reliability we definitely will

00:03:09.528 --> 00:03:12.358
replace the failed disks.

00:03:12.358 --> 00:03:16.848
So when we have the first failure we
will as soon as possible replace that

00:03:16.848 --> 00:03:18.328
failed disk.

00:03:18.328 --> 00:03:20.968
Copy the data from
the working disk to it so

00:03:20.968 --> 00:03:24.448
now we're back to RAID1
working properly.

00:03:24.448 --> 00:03:27.618
So what's the MTTF for that?

