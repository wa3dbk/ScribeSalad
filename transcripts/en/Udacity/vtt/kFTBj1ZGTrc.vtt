WEBVTT
Kind: captions
Language: en

00:00:00.068 --> 00:00:02.937
So let's have a programming exercise with the CUB library.

00:00:02.937 --> 00:00:06.113
Here's a simple example of the CUB blockscan primitive, okay,

00:00:06.113 --> 00:00:09.919
and this is doing a prefix sum in a single block.

00:00:09.919 --> 00:00:14.785
And you've seen scan a lot by now; you know a lot of the intricacies of how it can implemented,

00:00:14.785 --> 00:00:20.621
and so here's an example of how easy this is to implement with high performance in the IDE.

00:00:20.621 --> 00:00:23.900
And there's a couple different things about this from some of the examples you've seen;

00:00:23.900 --> 00:00:29.800
again, we're focusing on a single block, because the whole point of CUB is to give you intra-block primitives:

00:00:29.800 --> 00:00:34.505
Code for writing your own thread block level code within CUDA kernels.

00:00:34.505 --> 00:00:39.210
So we're just going to time a single block, and we're going to measure something we haven't quite seen before.

00:00:39.210 --> 00:00:44.715
We're actually going to measure the scan throughput, and we're measuring that in clocks per element scanned.

00:00:44.715 --> 00:00:48.351
These are SM clocks: How many actual clock cycles the SM took

00:00:48.351 --> 00:00:53.565
averaged across the total number of elements that were scanned by this single thread block.

00:00:53.565 --> 00:00:58.628
You'll see the CUDA built-in clock in this kernel, and the way it's used is obvious.

00:00:58.628 --> 00:01:01.431
And what I want you to do is fill out this matrix.

00:01:01.431 --> 00:01:05.292
This is a performance matrix where we have the number of threads per block

00:01:05.292 --> 00:01:07.771
which is represented in the code with block threads,

00:01:07.771 --> 00:01:10.475
and the number of items processed per thread,

00:01:10.475 --> 00:01:13.207
which is represented in the code with items per thread.

00:01:13.207 --> 00:01:16.286
And I want you to go ahead and look at the matrix here

00:01:16.286 --> 00:01:19.880
and experiment with different values of block threads and items per thread,

00:01:19.880 --> 00:01:23.954
and try to figure out how these 2 items affect the scan throughput.

00:01:23.954 --> 00:01:29.325
It would be too much work to fill out this whole matrix, so let me just guide you to a few sweet spots.

00:01:29.325 --> 00:01:32.878
So much of the interesting action is going to happen along this diagonal.

00:01:32.878 --> 00:01:38.743
So for example, 1,024 threads each representing a single item,

00:01:38.743 --> 00:01:42.840
each in charge of scanning a single item per thread

00:01:42.840 --> 00:01:45.205
is the kind of thing you might code up naturally.

00:01:45.205 --> 00:01:48.296
It might be the first thing you try; it's the simplest to code up.

00:01:48.296 --> 00:01:56.593
And then try to analyze what happens when you do 512 by 2 threads, 256 by 4 threads, 128 by 8 threads.

00:01:56.593 --> 00:02:00.933
and then let's go ahead and complete the diagonal--won't take long at all.

00:02:00.933 --> 00:02:05.228
All the way down to 32 threads each responsible for 32 items

00:02:05.228 --> 00:02:08.229
and 16 threads each responsible for 64 items.

00:02:08.229 --> 00:02:14.039
And so what I'm going to have you do is fill in this diagonal and from the choices along this diagonal,

00:02:14.039 --> 00:02:17.469
check off the option that gives you the best performance;

00:02:17.469 --> 00:02:21.469
in other words, the highest scan throughput which corresponds to the fewest clocks per elements scanned.

