WEBVTT
Kind: captions
Language: en

00:00:00.200 --> 00:00:03.880
So going back here, with out four issuing order processor with

00:00:03.880 --> 00:00:09.082
perfect branch prediction, we have seen that our loop, gets a CPI of 3 over 5,

00:00:09.082 --> 00:00:14.808
which is 0.6. Without scheduling, and it gets a CPI of 2 over 5.

00:00:14.808 --> 00:00:20.260
Which is 0.4, if we do a scheduling on the original loop. Now let's see what

00:00:20.260 --> 00:00:25.050
happens when we unroll the loop once, and try to execute it on this processor.

00:00:25.050 --> 00:00:29.250
This is the code we constructed, and now what happens in this loop is we,

00:00:29.250 --> 00:00:33.510
do the load. The add has to wait for that result. The store has to wait for

00:00:33.510 --> 00:00:39.280
that. This load is going to a different memory element, so it can proceed

00:00:39.280 --> 00:00:44.480
in parallel with the store. The R however, has to wait for that load. And then,

00:00:44.480 --> 00:00:48.470
this store has to wait for that add. And then, this decrement of the pointer,

00:00:48.470 --> 00:00:55.090
can proceed in parallel with the store, But the branch has to wait for R1.

00:00:55.090 --> 00:01:00.910
However then, the load can proceed. And now, we are back to where we started.

00:01:00.910 --> 00:01:05.900
So overall, it takes us five cycles. Do do these 8 instructions,

00:01:05.900 --> 00:01:11.440
which gives us a CP of 0.62 slightly lower than the original

00:01:11.440 --> 00:01:15.520
on schedule loop but not much lower. So these are very similar in CPI,

00:01:15.520 --> 00:01:19.510
we still gain significantly from eliminating some instructions. So

00:01:19.510 --> 00:01:24.630
overall this loop is going to perform better but the CPI has not improved. But

00:01:24.630 --> 00:01:28.900
let's see what scheduling does after enrolling. In the scheduled loop, we will

00:01:28.900 --> 00:01:34.590
have our load, for the first iteration of the two that we are really doing. And

00:01:34.590 --> 00:01:38.630
now if we put the add here, it would again have a dependence, so the load and

00:01:38.630 --> 00:01:43.180
whatever we did here would be the only thing that can be done in this cycle.

00:01:43.180 --> 00:01:48.240
So instead of doing the add next, we will look for what else can we do here that

00:01:48.240 --> 00:01:53.350
doesn't depend on this load. And it turns out that this load here, the load for

00:01:53.350 --> 00:01:58.440
the second iteration, can be done in parallel with this load. However, we cannot

00:01:58.440 --> 00:02:03.660
store the value into R2, so we will have to use another register, let's say R10,

00:02:03.660 --> 00:02:08.940
for that. So these two now can execute in the same cycle. And if you remember so

00:02:08.940 --> 00:02:13.320
could the branch. Now that we have done this we can do the add for the first

00:02:13.320 --> 00:02:17.510
iteration of the loop, and also the one for the second iteration of the loop,

00:02:17.510 --> 00:02:23.030
in the same cycle and in the same cycle you can do the add i instruction for

00:02:23.030 --> 00:02:28.950
moving the pointer. Now these two will produce the results that the stores need.

00:02:28.950 --> 00:02:32.600
So next cycle, we can do the stores. Note, however,

00:02:32.600 --> 00:02:38.330
that this is the store that was supposed to store with offset 0 from R1, but

00:02:38.330 --> 00:02:44.122
here, we have now decremented R1 by 8. So now we need to store to

00:02:44.122 --> 00:02:49.680
8 from R1. That's because his store needs to match the [UNKNOWN] of this load,

00:02:49.680 --> 00:02:53.920
but the R1 has moved in between. And then for the other store,

00:02:53.920 --> 00:02:58.880
we're storing R10 now, to the address that used to be minus 4 from R1.

00:03:00.330 --> 00:03:05.440
But R1 has already moved by minus 8. So now we need to store to 4 from R1,

00:03:05.440 --> 00:03:10.040
to match the address of this. This is basically minus 4 and then,

00:03:10.040 --> 00:03:15.270
we did minus 8, so now we need to add 4 to that, in order to get minus 4. And

00:03:15.270 --> 00:03:19.860
now, we can do our branch. So now let's see what can we get on our processor.

00:03:19.860 --> 00:03:23.770
In the first cycle we will be doing these two loads that can now proceed in

00:03:23.770 --> 00:03:29.070
parallel. The Add here, however, cannot proceed in that same cycle because

00:03:29.070 --> 00:03:33.860
it needs a result of this load. However, this Add, and the next one,

00:03:33.860 --> 00:03:38.790
and this one can proceed in the second cycle. Then the stores here have to

00:03:38.790 --> 00:03:42.470
wait the results of the adds so they cannot proceed in the same cycle.

00:03:42.470 --> 00:03:47.020
However they can be done in the next cycle and so can the branch because the R1

00:03:47.020 --> 00:03:52.820
has been produced, and so can the load because R1 has been produced. And

00:03:52.820 --> 00:03:57.660
unfortunately in this cycle we can not do this load because we already executed

00:03:57.660 --> 00:04:02.740
four things so unfortunately that load is done in the next cycle after which we

00:04:02.740 --> 00:04:08.090
get the same schedule as here. The three adds, and so on. So overall,

00:04:08.090 --> 00:04:14.160
it took us now 3 cycles to do 8 instructions. Which gives us a CP of 0.38,

00:04:14.160 --> 00:04:19.760
slightly better than the scheduled CPI for the loop that we didn't unroll.

00:04:19.760 --> 00:04:23.840
So what really happened is unrolling gives us more stuff that we can

00:04:23.840 --> 00:04:27.620
reorder here so that we can find more things that don't have dependencies and

00:04:27.620 --> 00:04:30.320
thus for example, we were able to do these two loads that were

00:04:30.320 --> 00:04:35.520
not available prior to unrolling. If we unroll this loop three times,

00:04:35.520 --> 00:04:40.000
then we will have four loads here that can all proceed in the same cycle.

00:04:40.000 --> 00:04:44.400
We will have four adds here that can proceed in the same cycle. Four stores that

00:04:44.400 --> 00:04:47.810
can proceed in the same cycle. And then the Add and branch equal. So

00:04:47.810 --> 00:04:52.770
the more we unroll, the more parallelism we can get, simply because there is no.

00:04:52.770 --> 00:04:57.760
More instructions among which we can try to find independent ones.

00:04:57.760 --> 00:05:02.440
Keep in mind, that unrolling already reduced a number of instructions and

00:05:02.440 --> 00:05:06.400
with scheduling it allows us to also reduce the CPI so

00:05:06.400 --> 00:05:10.340
it kind of creates a double whammy. For performance improvement our

00:05:10.340 --> 00:05:14.270
execution time gets reduced both because we have fewer instructions,

00:05:14.270 --> 00:05:19.380
8 instructions per 2 iterations as opposed to 5 instructions originally for

00:05:19.380 --> 00:05:24.750
one iteration. And also after scheduling it gives us usually a better CPI.

