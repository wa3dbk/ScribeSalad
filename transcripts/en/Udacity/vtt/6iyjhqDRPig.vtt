WEBVTT
Kind: captions
Language: en

00:00:00.210 --> 00:00:01.600
&gt;&gt; So, the second alternative I just wanted

00:00:01.600 --> 00:00:05.560
to mention very briefly is something called LDA, which

00:00:05.560 --> 00:00:09.350
is short for linear discriminant analysis. So, any

00:00:09.350 --> 00:00:11.870
guess on what linear discriminant analysis might do, Michael?

00:00:11.870 --> 00:00:14.970
&gt;&gt; It'll make, well, linear. So it's got a

00:00:14.970 --> 00:00:19.870
linear in it. Discriminant reminds me of, like, classification lines.

00:00:19.870 --> 00:00:22.870
&gt;&gt; Yes. So, what linear discriminant analysis

00:00:22.870 --> 00:00:25.690
does is, it finds a projection that

00:00:25.690 --> 00:00:29.690
discriminates based on the label. This actually will feel, this

00:00:29.690 --> 00:00:32.790
feels a lot like supervised learning, right? You take advantage

00:00:32.790 --> 00:00:35.490
of the label, and you come up with a transformation,

00:00:35.490 --> 00:00:39.400
such that you will, in fact, project things into different

00:00:39.400 --> 00:00:42.560
clumps or clusters, or otherwise separate them based upon their

00:00:42.560 --> 00:00:45.500
label. So, you can imagine using a lot of algorithms,

00:00:45.500 --> 00:00:47.120
as we've used in the past. In some sense, what

00:00:47.120 --> 00:00:51.050
they're doing is, linear discriminant analysis. They're finding lines or

00:00:51.050 --> 00:00:55.340
finding linear separators between different clumps of points. And

00:00:55.340 --> 00:00:56.870
if you think about the binary case, for example,

00:00:56.870 --> 00:00:59.050
you could think of SVM as doing something like

00:00:59.050 --> 00:01:03.850
this, where what you've done is, you transform your points

00:01:03.850 --> 00:01:07.112
not into a specific label plus or minus, one

00:01:07.112 --> 00:01:10.640
or zero, yes or no, but you instead project it

00:01:10.640 --> 00:01:12.680
onto the line such that it would clump things

00:01:12.680 --> 00:01:16.460
accordingly. And you use the value of that projection as

00:01:16.460 --> 00:01:19.840
a way of re-representing the data. Does that make sense?

00:01:19.840 --> 00:01:21.780
&gt;&gt; Yeah, and this, this approach seems a

00:01:21.780 --> 00:01:23.120
little different from all the other ones, in

00:01:23.120 --> 00:01:27.520
that it's actually paying attention to how the

00:01:27.520 --> 00:01:29.990
resulting components are going to get used. Right.

00:01:29.990 --> 00:01:31.170
&gt;&gt; That's exactly right.

00:01:31.170 --> 00:01:31.880
&gt;&gt; It actually is going to try

00:01:31.880 --> 00:01:34.690
to help you, specifically, with the classification.

00:01:34.690 --> 00:01:36.410
&gt;&gt; Alright, that's exactly right. All three

00:01:36.410 --> 00:01:37.860
of the examples that we gave before,

00:01:37.860 --> 00:01:39.700
feel almost like filter methods, right? In

00:01:39.700 --> 00:01:41.520
that they have some criterion they're trying

00:01:41.520 --> 00:01:45.100
to maximize. Even though randomized projections is, who knows

00:01:45.100 --> 00:01:48.070
what it's trying to maximize besides randomness. But they

00:01:48.070 --> 00:01:50.410
don't care about the ultimate learner, or the ultimate

00:01:50.410 --> 00:01:55.110
label that's associated with them. Whereas LDA does care explicitly

00:01:55.110 --> 00:01:57.080
about the labels and wants to find ways to

00:01:57.080 --> 00:02:00.750
discriminate, finds sort of projections or features that make it

00:02:00.750 --> 00:02:03.040
easier for you to do that discrimination. Now it

00:02:03.040 --> 00:02:06.580
also does not care about what learner's going to happen next,

00:02:06.580 --> 00:02:08.508
but it does care about the label. So

00:02:08.508 --> 00:02:11.090
it does end up doing something slightly different, and

00:02:11.090 --> 00:02:13.220
works pretty well in a world where you have

00:02:13.220 --> 00:02:15.960
very simple things that can be lineraly discriminated. Okay?

00:02:15.960 --> 00:02:16.890
&gt;&gt; Yeah.

00:02:16.890 --> 00:02:19.440
&gt;&gt; So that's it for the alternatives. I actually think that's pretty

00:02:19.440 --> 00:02:23.807
much it for, feature transformation. So why don't we wrap up. [CROSSTALK]

00:02:23.807 --> 00:02:25.690
&gt;&gt; Just a quick, just a quick question, though. [CROSSTALK] So, LDA, I've

00:02:25.690 --> 00:02:27.930
heard of LDA in this context before

00:02:27.930 --> 00:02:31.860
meaning something else. Like, latent Dirichlet allocation?

00:02:31.860 --> 00:02:33.450
Yeah, but that happened long after linear discriminant

00:02:33.450 --> 00:02:35.780
analysis and we haven't moved past the 90s.

00:02:35.780 --> 00:02:37.290
&gt;&gt; Well, I'm just saying that it does seem.

00:02:37.290 --> 00:02:40.230
And I think it is actually an unsupervised approach.

00:02:40.230 --> 00:02:42.850
&gt;&gt; Oh latent, oh no, absolutely. But it is well

00:02:42.850 --> 00:02:44.870
beyond the scope of the discussion that we're going to have here.

00:02:44.870 --> 00:02:46.010
&gt;&gt; All right. As you wish.

00:02:46.010 --> 00:02:49.620
&gt;&gt; But it is in fact. Every time you see a D you can think Dirichlet.

00:02:49.620 --> 00:02:50.700
&gt;&gt; Which is the computer science

00:02:50.700 --> 00:02:53.080
pronunciation. Other people pronounce it differently.

00:02:53.080 --> 00:02:55.400
&gt;&gt; Yes, because the correct way of pronouncing

00:02:55.400 --> 00:02:57.090
it is sort of beyond my. It has another

00:02:57.090 --> 00:02:59.020
syllable in there. Is it German or something?

00:02:59.020 --> 00:02:59.650
&gt;&gt; Yeah.

00:02:59.650 --> 00:03:03.414
&gt;&gt; Who is Dirichlet. Is it Dirichlet? [LAUGH] That

00:03:03.414 --> 00:03:06.220
sounded German. [LAUGH] Okay, so let's wrap up Michael.

00:03:06.220 --> 00:03:06.870
&gt;&gt; Alright.

00:03:06.870 --> 00:03:07.610
&gt;&gt; Okay.

