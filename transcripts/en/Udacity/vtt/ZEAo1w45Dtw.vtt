WEBVTT
Kind: captions
Language: en

00:00:00.014 --> 00:00:07.599
So here is my code for this. I'll start by setting k equal to 32 and here is the actual code.

00:00:07.599 --> 00:00:12.127
I begin by figuring out the locations of the tile corners.

00:00:12.127 --> 00:00:16.782
This is going to tell me where I need to start writing in the output and start reading from the input--

00:00:16.782 --> 00:00:22.522
so just a little book keeping and giving things variable names that mean something to me.

00:00:22.522 --> 00:00:25.001
But as you can see the, the place where I start

00:00:25.001 --> 00:00:32.037
reading the i value is a function of which block we're in times the width of the tile

00:00:32.037 --> 00:00:36.688
because each tile is responsible for 1 block and that's the case.

00:00:36.688 --> 00:00:42.814
And the j value is the same but in y, in y instead of x, and the output simply inverts y and x.

00:00:42.814 --> 00:00:47.400
Okay, so now that I know where I need to read to write my tile,

00:00:47.400 --> 00:00:51.236
I'm going to want to know which element of the tile to read and write from,

00:00:51.236 --> 00:00:54.057
and just a shorthand to make the code a little more readable.

00:00:54.057 --> 00:00:58.109
I'm going to set x to thread index .x, and y to threat index y.

00:00:58.109 --> 00:01:01.147
So now, the code itself is really simple.

00:01:01.147 --> 00:01:07.456
I declare my floating point array in shared memory k by k array of tiles,

00:01:07.456 --> 00:01:13.361
and I read from global memory, and write the result into shared memory.

00:01:13.361 --> 00:01:19.905
So here's my read from global memory and its function of where the tile starts

00:01:19.905 --> 00:01:25.103
plus which thread I'm responsible, or which element this thread is responsible for in the tile.

00:01:25.103 --> 00:01:28.015
To avoid an extra sync threads, I'm going to go

00:01:28.015 --> 00:01:31.151
ahead and write this into shared memory in transposed fashion.

00:01:31.151 --> 00:01:36.915
So it's not tile x y, it's tile y x. Okay, that saves one of these sync thread's barriers.

00:01:36.915 --> 00:01:39.481
Now I've got the transpose tile sitting in shared memory.

00:01:39.481 --> 00:01:42.892
It's already been transposed, and I want to write it out to global memory.

00:01:42.892 --> 00:01:47.558
And I want to write it out in coalesced fashion, so adjacent threads write adjacent locations and memory.

00:01:47.558 --> 00:01:52.649
In other words, adjacent threads are varying by x the way I've set this out.

00:01:52.649 --> 00:01:59.556
So here's my write to global memory after my read to from a shared memory.

00:01:59.556 --> 00:02:01.370
You could have done this in 2 sync threads.

00:02:01.370 --> 00:02:04.086
You could have read this in the shared memory, performed the transpose,

00:02:04.086 --> 00:02:05.955
and written it out to global memory.

00:02:05.955 --> 00:02:08.902
And you have needed a sync threads after reading it in to shared memory

00:02:08.902 --> 00:02:11.018
and again after performing the transpose.

00:02:11.018 --> 00:02:13.118
So, if you did that I encourage you to go back

00:02:13.118 --> 00:02:18.276
and convert it to this single version, and see how much faster it goes.

00:02:18.276 --> 00:02:26.697
Let's go ahead and run this on my laptop. Okay, there's 2 interesting things to note here.

00:02:26.697 --> 00:02:31.555
One is that the amount of time taken by the parallel per element code--

00:02:31.555 --> 00:02:34.604
the kernel that we had before--actually went up.

00:02:34.604 --> 00:02:36.853
It's almost twice as slow now as it was before.

00:02:36.853 --> 00:02:42.429
And if you think about it this code didn't change at all, except that we changed the value of k.

00:02:42.429 --> 00:02:46.257
We changed the size of the thread block that's being used in this code.

00:02:46.272 --> 00:02:49.495
We're going to come back to that. That's going to give us a hint as to a further optimization.

00:02:49.495 --> 00:02:54.961
In the meantime, transpose parallel per element tiled, our new version,

00:02:54.961 --> 00:02:59.464
is a little bit faster--not a lot faster and that's kind of disturbing.

00:02:59.464 --> 00:03:04.436
We should have gone to perfectly coalesced loads and stores which should have made a difference.

00:03:04.436 --> 00:03:08.001
Let's go ahead and fire up NVVP again and see what happened.

