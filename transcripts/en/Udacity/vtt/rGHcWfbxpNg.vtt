WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.000
Another question on the forum related to the use of lexer states.

00:00:05.000 --> 00:00:10.000
A lexer state like I'm in a comment or I'm not is actually

00:00:10.000 --> 00:00:14.000
more of a convenience function for allowing us to have different rules

00:00:14.000 --> 00:00:17.000
depending on where we are.

00:00:17.000 --> 00:00:21.000
In normal code for HTML or JavaScript, we want to recognize things like

00:00:21.000 --> 00:00:24.000
angle brackets, because they might be a part of tags

00:00:24.000 --> 00:00:28.000
or they might be the less than or greater than sign in a JavaScript mathematical computation.

00:00:28.000 --> 00:00:32.000
But inside a comment, they don't mean anything special at all.

00:00:32.000 --> 00:00:34.000
You want to just skip over them, because that's some documentation

00:00:34.000 --> 00:00:37.000
the programmer wrote down for later maintenance.

00:00:37.000 --> 00:00:41.000
So we used exclusive states like I'm in a comment or I'm not

00:00:41.000 --> 00:00:44.000
to have different rules depending on where we are.

00:00:44.000 --> 00:00:48.000
A totally legitimate question is: Are we going to use these exclusive states

00:00:48.000 --> 00:00:53.000
to have one lexer for HTML and another for JavaScript

00:00:53.000 --> 00:00:55.000
when we write our final web browser?

00:00:55.000 --> 00:00:57.000
This is a great question.

00:00:57.000 --> 00:01:02.000
It's going to turn out that we're not going to use explicit states,

00:01:02.000 --> 00:01:05.000
but we are going to use exactly the same concept.

00:01:05.000 --> 00:01:09.000
If you think about it under the hood, lexer states are really just a cute way

00:01:09.000 --> 00:01:12.000
of giving you two different finite state machines--

00:01:12.000 --> 00:01:14.000
two different sets of regular expressions--

00:01:14.000 --> 00:01:17.000
one for in comments and one for not, for example.

00:01:17.000 --> 00:01:23.000
We will do the same thing just by writing two different sets of token definition files.

00:01:23.000 --> 00:01:27.000
Here's one for HTML, and here's another one for JavaScript.

00:01:27.000 --> 00:01:32.000
We'll start with the entire web page that we download from a web server somewhere,

00:01:32.000 --> 00:01:36.000
and we will feed that to our HTML lexer and parser.

00:01:36.000 --> 00:01:39.000
Some parts of the web page will be embedded JavaScript,

00:01:39.000 --> 00:01:42.000
and we'll see this in great detail in later units.

00:01:42.000 --> 00:01:45.000
We will gather up those strings and keep them safe.

00:01:45.000 --> 00:01:51.000
And when it comes time to interpret them, we will pass them to another lexer,

00:01:51.000 --> 00:01:56.000
this time a JavaScript lexer, that will interpret that substring as JavaScript.

00:01:56.000 --> 00:02:01.000
And this is one of the great joys, really, of computer science--turtles all the way down,

00:02:01.000 --> 00:02:05.000
or nested Russian dolls--once you've learned a particular concept,

00:02:05.000 --> 00:02:10.000
like how to make a lexer, you can often nest it or have multiple of them at the same time.

00:02:10.000 --> 00:02:13.000
Once we know how to make a lexer for HTML,

00:02:13.000 --> 00:02:17.000
nothing stops us from making a lexer for JavaScript

00:02:17.000 --> 00:02:19.000
and having them both running as part of the same program.

00:02:19.000 --> 00:02:21.000
And that's what we'll do.

00:02:21.000 --> 00:02:24.000
So officially we won't be using exclusive lexer states,

00:02:24.000 --> 09:59:59.000
but we will end up with multiple different finite state machines used for lexing under the hood.

