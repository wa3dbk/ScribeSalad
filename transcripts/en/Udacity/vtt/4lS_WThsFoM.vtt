WEBVTT
Kind: captions
Language: en

00:00:00.178 --> 00:00:05.011
So with dynamic parallelism, programs can use other power patterns as well,

00:00:05.011 --> 00:00:07.048
and that's where it gets really interesting.

00:00:07.048 --> 00:00:12.880
There's nested parallelism, where I can call a parallel function from inside my parallel program.

00:00:12.880 --> 00:00:18.453
That's taking a parallel program, creating other parallel programs, so if I have kernel sequence of kernels

00:00:18.453 --> 00:00:25.006
A, B, and C inside a CUDA stream, and kernel B wants to do its own sequence of steps X, Y and Z,

00:00:25.006 --> 00:00:29.266
I can just launch that work in line from B exactly where I need it.

00:00:29.266 --> 00:00:31.670
Without nested powers I might have to work X, Y,

00:00:31.670 --> 00:00:37.399
and Z into Bs code and make a huge program or do some gymnastics with the CPU to launch the work.

00:00:37.399 --> 00:00:39.622
Like for example, I would split B in two.

00:00:39.622 --> 00:00:42.939
I would come out to X, Y, and Z, and I'd come back to the second half of B.

00:00:42.939 --> 00:00:46.076
That's all very complicated to write and very complicated to manage.

00:00:46.076 --> 00:00:49.880
It's much, much simpler if I can simply launch my work directly out of B.

00:00:49.880 --> 00:00:54.198
So here's an example. Suppose you want to adjust the volume of audio stream.

00:00:54.198 --> 00:00:57.755
We've got an incoming sound wave, this green wave right here.

00:00:57.755 --> 00:01:01.793
A lot of sound processing requires us to have a certain maximum volume,

00:01:01.793 --> 00:01:05.283
which means constraining the wave to fit within these two dashed lines right here.

00:01:05.283 --> 00:01:10.618
We could write a parallel program to deal with this by creating a kernel to process this wave,

00:01:10.618 --> 00:01:17.181
which would then perform sequences of parallel operations to cause the rescaling of this waveform.

00:01:17.181 --> 00:01:22.447
So I'd feed this wave into a kernel, and the kernel would break it down into a

00:01:22.447 --> 00:01:25.315
series of blocks like a normal CUDA kernel would do.

00:01:25.315 --> 00:01:30.957
The first step would be to find the maximum peak value in parallel, say 1.8 volts.

00:01:30.957 --> 00:01:35.999
So, having found the maximum peak voltage, my kernel would then launch a second kernel,

00:01:35.999 --> 00:01:42.505
which would rescale everything by the 1.8 volts to end up with a normalized 1 volt peak-to-peak audio.

00:01:42.505 --> 00:01:48.872
We're combining several bulk power algorithms into a single program operating on the whole wave at once.

