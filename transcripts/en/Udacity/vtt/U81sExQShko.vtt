WEBVTT
Kind: captions
Language: en

00:00:00.140 --> 00:00:02.640
Let's look at the solution
to our parity quiz.

00:00:02.640 --> 00:00:06.830
We're adding parity to our memory,
which consists of eight arrays.

00:00:06.830 --> 00:00:10.980
So now the question is should we
add parity to each row of data?

00:00:10.980 --> 00:00:14.860
And then when we read out the row into
the row buffer, we check for parity and

00:00:14.860 --> 00:00:16.140
report an error?

00:00:16.140 --> 00:00:19.950
Or should we have the modules as is and

00:00:19.950 --> 00:00:22.600
just add two more that
will hold a parity?

00:00:22.600 --> 00:00:27.460
Both approaches have exactly the same
number of extra bits we're adding, but

00:00:27.460 --> 00:00:31.940
the approach of having extra modules
as opposed to changing the modules

00:00:31.940 --> 00:00:34.170
is preferable for two reasons.

00:00:34.170 --> 00:00:37.940
One is that this way,
we can design these memory arrays.

00:00:37.940 --> 00:00:42.170
Both are unprotected and
for protective memories.

00:00:42.170 --> 00:00:46.070
The protected memories will just
have additional arrays, but

00:00:46.070 --> 00:00:48.030
the design of each array is the same.

00:00:48.030 --> 00:00:52.670
If we decided to add parity bits here,
then depending on exactly

00:00:52.670 --> 00:00:56.290
how many parity bits we are adding for
how many data bits, and whether we

00:00:56.290 --> 00:01:00.250
are having parity at all, we would need
to redesign our memory array every time.

00:01:01.310 --> 00:01:07.230
The second reason why using redundancy
at the coarser grain rather then

00:01:07.230 --> 00:01:12.530
finer grain tends to work better,
is because let's say the ten array fails

00:01:12.530 --> 00:01:17.170
by reading zeroes for the entire row,
this is not such a far fetched error.

00:01:18.180 --> 00:01:23.160
In that case what we have is if we have
the parity bits added to each row,

00:01:23.160 --> 00:01:25.370
the row still has correct parity.

00:01:25.370 --> 00:01:28.520
So a single failure, for
example, in the row decoder

00:01:28.520 --> 00:01:31.050
will just read the wrong row,
it still has the right parity.

00:01:32.670 --> 00:01:36.920
If however, we separate the modules
with a parity, then each of the modules

00:01:38.220 --> 00:01:43.030
will operate on its own so
the error of our row decoder here

00:01:43.030 --> 00:01:48.080
will cause this bit now to be wrong
in every eight bit word we read.

00:01:48.080 --> 00:01:50.700
But if this other arrays
are still working correctly,

00:01:50.700 --> 00:01:54.340
then we can still detect that because
the parity is somewhere else.

00:01:54.340 --> 00:01:57.820
So this is kind of similar to
doing geographical distribution

00:01:57.820 --> 00:02:00.520
of our computers, for redundancy.

00:02:00.520 --> 00:02:06.200
Pretty much the further away
the protection bits are, the more

00:02:06.200 --> 00:02:10.610
likely they will protect us against
large failures in any single component.

