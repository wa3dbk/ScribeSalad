WEBVTT
Kind: captions
Language: en

00:00:00.260 --> 00:00:03.320
The event-driven model doesn't
come without any challenges.

00:00:03.320 --> 00:00:06.610
Recall that when we talked about
the many to one multithreading model,

00:00:06.610 --> 00:00:09.740
we said that a single blocking
I/O call that's coming from

00:00:09.740 --> 00:00:14.310
one of the user level threads
can block the entire process,

00:00:14.310 --> 00:00:18.230
although there may be other user level
threads that are ready to execute.

00:00:18.230 --> 00:00:20.682
A similar problem can
occur here as well.

00:00:20.682 --> 00:00:24.752
If one of the handlers issues
a blocking I/O call to read data from

00:00:24.752 --> 00:00:29.575
the network or from disk, the entire
event-driven process can be blocked.

00:00:29.575 --> 00:00:35.240
One way to circumvent this problem,
is to use asynchronous I/O operations.

00:00:35.240 --> 00:00:39.830
Asynchronous calls have the property
that when the system call is made,

00:00:39.830 --> 00:00:44.240
the kernel captures enough information
about the caller and where and

00:00:44.240 --> 00:00:48.070
how the data should be returned
once it becomes available.

00:00:48.070 --> 00:00:52.250
Async calls also provide the caller
with an opportunity to precede

00:00:52.250 --> 00:00:56.700
executing something, and then come back
at a later time to check if the results

00:00:56.700 --> 00:00:59.440
of the asynchronous operation
are already available.

00:00:59.440 --> 00:01:03.554
For instance, the process or the thread
can come back later to check if a file

00:01:03.554 --> 00:01:07.429
has already been read and the data is
available in the buffer in memory.

00:01:07.429 --> 00:01:11.546
One thing that makes asynchronous
calls possible is that the OS kernel

00:01:11.546 --> 00:01:12.740
is multithreaded.

00:01:12.740 --> 00:01:17.488
So while the caller thread continues
execution, another kernel thread does

00:01:17.488 --> 00:01:22.164
all the necessary work and all the
waiting that's needed to perform the I/O

00:01:22.164 --> 00:01:25.961
operation, to get the I/O data,
and then, to also make sure

00:01:25.961 --> 00:01:30.589
that the results become available to
the appropriate user level context.

00:01:30.589 --> 00:01:35.523
Also, asynchronous operations can
benefit by the actual I/O devices.

00:01:35.523 --> 00:01:39.963
For instance, the caller thread can
simply pass some request data structure

00:01:39.963 --> 00:01:43.856
to the device itself, and
then the device performs the operation,

00:01:43.856 --> 00:01:46.178
and the thread at a later
time can come and

00:01:46.178 --> 00:01:49.547
check to see whether device
has completed the operation.

00:01:49.547 --> 00:01:53.115
We will return to a synchronous
I/O operations in a later lecture.

00:01:53.115 --> 00:01:54.864
What you need to know for

00:01:54.864 --> 00:01:59.416
now is that when we're using
asynchronous I/O operations,

00:01:59.416 --> 00:02:04.423
our process will not be blocked in
the kernel when performing I/O.

00:02:04.423 --> 00:02:06.278
In the event-driven model,

00:02:06.278 --> 00:02:10.952
if the handler initiates an asynchronous
I/O operation for network or for

00:02:10.952 --> 00:02:15.846
disk, the operating system can simply
use the mechanism like select or poll or

00:02:15.846 --> 00:02:20.300
epoll like we've mentioned
before to catch such events.

00:02:20.300 --> 00:02:23.450
Since summary asynchronous
I/O operations fit

00:02:23.450 --> 00:02:26.030
very nicely with the event-driven model.

00:02:26.030 --> 00:02:29.503
The problem with asynchronous
I/O calls is that they weren't

00:02:29.503 --> 00:02:31.608
ubiquitously available in the past.

00:02:31.608 --> 00:02:36.020
And even today, they may not be
available for all types of devices.

00:02:36.020 --> 00:02:40.749
In a general case, maybe the processing
that needs to be performed by our server

00:02:40.749 --> 00:02:45.350
isn't to read data from a file, where
there are asynchronous system calls.

00:02:45.350 --> 00:02:49.445
But instead maybe to call
processing some accelerator,

00:02:49.445 --> 00:02:52.430
some device that only
the server has access to.

00:02:52.430 --> 00:02:57.410
To deal with this problem,
paper proposed the use of helpers.

00:02:57.410 --> 00:03:01.960
But a handler needs to issue
an I/O operation that can block,

00:03:01.960 --> 00:03:07.600
it passes it to the helper, and
returns to the event dispatcher.

00:03:07.600 --> 00:03:11.710
The helper will be the one that will
handle the blocking I/O operation, and

00:03:11.710 --> 00:03:14.220
interact with the dispatcher
as necessary.

00:03:14.220 --> 00:03:18.171
The communication with the helper can
be via socket based interface, or

00:03:18.171 --> 00:03:22.580
via another type of messaging interface
that's available in operating systems

00:03:22.580 --> 00:03:23.438
called pipes.

00:03:23.438 --> 00:03:28.150
And both of these present a file
descriptor-like interface.

00:03:28.150 --> 00:03:29.730
So the same kind of select or

00:03:29.730 --> 00:03:33.720
poll mechanism that we mentioned can
be used for the event dispatcher

00:03:33.720 --> 00:03:37.529
to keep track of various events
that are occurring in the system.

00:03:37.529 --> 00:03:41.459
This interface can be used to track
whether the helpers are providing any

00:03:41.459 --> 00:03:44.120
kind of events to the event dispatcher.

00:03:44.120 --> 00:03:48.280
In doing this, the synchronous I/O
call is handled by the helper.

00:03:48.280 --> 00:03:51.550
The helper will be the one
that will block, and

00:03:51.550 --> 00:03:57.330
the main event dispatcher in the main
process will continue uninterrupted.

00:03:57.330 --> 00:04:01.260
So this way although we don't
have asynchronous I/O calls,

00:04:01.260 --> 00:04:03.040
through the use of helpers,

00:04:03.040 --> 00:04:08.220
we achieve the same kind of behavior
as if we had asynchronous calls.

00:04:08.220 --> 00:04:10.330
At the time of the writing of the paper,

00:04:10.330 --> 00:04:14.230
another limitation was that not
all kernels were multi-threaded.

00:04:14.230 --> 00:04:14.830
So basically,

00:04:14.830 --> 00:04:19.040
not all kernels supported the one
to one model that we talked about.

00:04:19.040 --> 00:04:21.470
In order to deal with this limitation,

00:04:21.470 --> 00:04:26.630
the decision in the paper was to make
these helper entities processes.

00:04:26.630 --> 00:04:29.134
Therefore, they call this model AMPED,

00:04:29.134 --> 00:04:32.290
Asymmetric Multi-Process Event-Driven
model.

00:04:32.290 --> 00:04:34.050
It's an event-driven model.

00:04:34.050 --> 00:04:36.020
It has multiple processes.

00:04:36.020 --> 00:04:37.531
And these processes are asymmetric.

00:04:37.531 --> 00:04:41.050
The helper ones only deal
with blocking I/O operation.

00:04:41.050 --> 00:04:44.370
And then,
the main one performs everything else.

00:04:44.370 --> 00:04:48.924
In principle, the same kind of idea
could have applied to the multi-threaded

00:04:48.924 --> 00:04:52.305
scenario where the helpers are threads,
not processes,

00:04:52.305 --> 00:04:55.549
so asymmetric multi-threaded
event-driven model.

00:04:55.549 --> 00:05:00.195
And in fact, there is a follow-on on
the Flash work that actually does this

00:05:00.195 --> 00:05:02.200
exact thing, the AMTED model.

00:05:02.200 --> 00:05:06.730
The key benefits of the symmetric model
that we described is that it resolved

00:05:06.730 --> 00:05:10.197
some of the limitations of
the pure event-driven model in

00:05:10.197 --> 00:05:13.382
terms of what is required
from the operating system,

00:05:13.382 --> 00:05:17.450
the dependence on asynchronous
I/O calls and threading support.

00:05:17.450 --> 00:05:21.956
In addition, this motto let's us achieve
concurrency with a smaller memory

00:05:21.956 --> 00:05:26.544
footprint than either the multi-process
or the multi-threading model.

00:05:26.544 --> 00:05:30.158
In the multi-process or
multi-threading model,

00:05:30.158 --> 00:05:33.943
a worker has to perform everything for
a full request.

00:05:33.943 --> 00:05:38.547
So its memory requirements will be
much more significant than the memory

00:05:38.547 --> 00:05:41.430
requirements of a helper entity.

00:05:41.430 --> 00:05:46.830
In addition, with the AMPED model,
we will have a helper entity only for

00:05:46.830 --> 00:05:50.510
the number of concurrent
blocking I/O operations.

00:05:50.510 --> 00:05:53.280
Whereas, in the multi-threaded or
multi-process models,

00:05:53.280 --> 00:05:58.690
we will have as many current entities,
as many processes, or as many threads

00:05:58.690 --> 00:06:03.530
as there are concurrent requests
regardless of whether they block or not.

00:06:03.530 --> 00:06:08.181
The downside is that audit works well
with the server pipe applications.

00:06:08.181 --> 00:06:12.539
It is not necessarily as generally
applicable to arbitrary applications.

00:06:12.539 --> 00:06:16.461
In addition, there are also some
complexities with the routing of events

00:06:16.461 --> 00:06:17.670
in multi CPU systems.

