WEBVTT
Kind: captions
Language: en

00:00:00.510 --> 00:00:04.930
The next organization that we
will be looking at is similar to

00:00:04.930 --> 00:00:09.570
distributed shared memory, except that
we are no longer sharing memory, so

00:00:09.570 --> 00:00:11.860
it's called distributed memory.

00:00:11.860 --> 00:00:18.010
No sharing of memory means that only
one core can access a memory slice,

00:00:18.010 --> 00:00:19.720
and the others can not.

00:00:19.720 --> 00:00:23.910
So what we now have is cores
that have their own caches, and

00:00:23.910 --> 00:00:29.040
each of them has its own memory that
can only be locally accessed here.

00:00:29.040 --> 00:00:33.280
So really, each of the cores pretty
much has what looks like a complete

00:00:33.280 --> 00:00:35.800
single core computer system.

00:00:35.800 --> 00:00:40.770
And a network interface card
that connects it to a network.

00:00:40.770 --> 00:00:43.510
But now when a cork has a cache miss,

00:00:43.510 --> 00:00:46.600
that cash miss goes
directly to this memory.

00:00:46.600 --> 00:00:50.900
And if the core wants to
access something that is

00:00:50.900 --> 00:00:52.760
in another core's memory.

00:00:52.760 --> 00:00:57.100
It can not simply issue an access that
message in the cache and goes there.

00:00:57.100 --> 00:01:01.710
Now what it needs to do is actually
create a network message using some sort

00:01:01.710 --> 00:01:06.840
of a send primiative in the operating
system to actually send a request.

00:01:06.840 --> 00:01:09.509
The program here needs to
receive that,see what is

00:01:09.509 --> 00:01:11.890
being requested,respond to it and so on.

00:01:11.890 --> 00:01:16.950
So now communication is explicit
it's no longer sufficient to simply

00:01:16.950 --> 00:01:21.450
put data in memory and then another
core just reads it from there.

00:01:21.450 --> 00:01:25.490
You have to actually send the data
explicitly to another core

00:01:25.490 --> 00:01:27.350
if we want to communicate.

00:01:27.350 --> 00:01:28.195
So.

00:01:28.195 --> 00:01:31.725
That means that you write
programs differently from this.

00:01:31.725 --> 00:01:33.225
Symmetric shared memory and

00:01:33.225 --> 00:01:38.185
distributed share memory pass
data around using shared memory.

00:01:38.185 --> 00:01:41.985
Meaning reads and writes to
memory are used to exchange data.

00:01:41.985 --> 00:01:46.365
Now we are using what is called
message passing for communication.

00:01:46.365 --> 00:01:49.970
So now you are pretty
much writing a program

00:01:49.970 --> 00:01:55.030
as if these were independent machines
that communicate over a network.

00:01:55.030 --> 00:01:57.940
If you have a distributed
memory supercomputer, for

00:01:57.940 --> 00:02:01.040
example, it's just that this network and

00:02:01.040 --> 00:02:07.300
these network cards are a lot faster
than your normal internet connection.

00:02:07.300 --> 00:02:11.500
This type of a system is also called
a multicomputer because really each of

00:02:11.500 --> 00:02:14.380
these is like a complete computer.

00:02:14.380 --> 00:02:19.380
It has the processor, the cache,
the memory, thumb I/O, and

00:02:19.380 --> 00:02:22.210
the processor only directly
accesses the local memory.

00:02:22.210 --> 00:02:25.280
So it looks like complete
uniprocessor system.

00:02:25.280 --> 00:02:30.650
These are also called cluster computers,
because really, you put a bunch of

00:02:30.650 --> 00:02:34.980
normal computers together into
a tightly networked cluster, and

00:02:34.980 --> 00:02:38.720
you get something like
a distributed memory system.

00:02:38.720 --> 00:02:45.500
These types of computers tend to scale
to a large number of processors.

00:02:45.500 --> 00:02:49.490
The reason is not that they're
fundamentally better at

00:02:49.490 --> 00:02:54.920
communicating than
shared memory systems.

00:02:54.920 --> 00:02:59.450
The reason is mainly that
the programmer is forced to explicitly

00:02:59.450 --> 00:03:03.830
think about communication because you
use a different type of primitive to

00:03:03.830 --> 00:03:07.500
communicate than you use to
access your local memory.

00:03:07.500 --> 00:03:11.400
So the programmer is aware of
communication going on and

00:03:11.400 --> 00:03:14.320
then naturally they will tend
to minimize communication.

00:03:14.320 --> 00:03:16.970
And if at all possible,
access the local memory.

00:03:18.120 --> 00:03:23.110
So this approach works better, primarily
because it forces the programmer.

00:03:23.110 --> 00:03:27.340
To figure out things that
otherwise it might not notice.

00:03:27.340 --> 00:03:30.500
Because it forces
the programmer to figure out

00:03:30.500 --> 00:03:32.620
how to communicate efficiently.

00:03:32.620 --> 00:03:34.250
Whereas in shared memory,

00:03:34.250 --> 00:03:39.600
the programmer may not even realize
that some of the accesses are not local.

00:03:39.600 --> 00:03:42.940
And thus are slow and causing
a lot of communication to happen.

