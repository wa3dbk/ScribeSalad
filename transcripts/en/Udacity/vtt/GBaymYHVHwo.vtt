WEBVTT
Kind: captions
Language: en

00:00:00.110 --> 00:00:03.719
So what happens if there
are repeated states?

00:00:03.719 --> 00:00:09.660
So if there were repeated states then,
oh I see.

00:00:09.660 --> 00:00:13.530
So let's imagine that we did
outcome based updates sort of then

00:00:13.530 --> 00:00:14.620
we saw repeated states.

00:00:14.620 --> 00:00:17.960
And all you're really doing is ignoring
anything, oh I see, okay, okay.

00:00:17.960 --> 00:00:18.890
How about this, Michael?

00:00:18.890 --> 00:00:21.617
Tell me if what I'm saying makes sense.

00:00:21.617 --> 00:00:22.383
&gt;&gt; Sure.

00:00:22.383 --> 00:00:27.101
&gt;&gt; So if we were doing outcome-based
updates, then basically, you're just

00:00:27.101 --> 00:00:31.818
looking at all the rewards that you see,
the values that you kind of expected

00:00:31.818 --> 00:00:36.120
along the way, and it's exactly
what you have written down before.

00:00:36.120 --> 00:00:40.360
But if we had a repeated state,
then you're basically ignoring

00:00:41.560 --> 00:00:44.332
anything you learned along the way.

00:00:44.332 --> 00:00:46.197
So [CROSSTALK]
&gt;&gt; During the episode, right.

00:00:46.197 --> 00:00:47.860
&gt;&gt; During the episode.

00:00:47.860 --> 00:00:52.840
So outcome-based updates learn
nothing during the episode.

00:00:52.840 --> 00:00:57.935
So if your sequence up there, instead
of being S1, S2, S3, SF, had been S1,

00:00:57.935 --> 00:01:02.127
S2, S3, S1, SF,

00:01:02.127 --> 00:01:06.800
then what would have happened
is I would have seen

00:01:06.800 --> 00:01:10.620
the rewards that I saw, r1, r2, r3, and
then some other reward that I would see.

00:01:10.620 --> 00:01:14.044
Let's call it r1 prime or
something, I don't know.

00:01:15.710 --> 00:01:20.390
I would be pretending as if I didn't
already know something about state one,

00:01:20.390 --> 00:01:23.670
because I also saw it go from
state one to state two, and

00:01:23.670 --> 00:01:25.630
saw a particular reward r1.

00:01:25.630 --> 00:01:26.840
&gt;&gt; Yes.

00:01:26.840 --> 00:01:28.040
&gt;&gt; I'd be ignoring that.

00:01:28.040 --> 00:01:33.420
So what this TD(1) update let's you
do is, when you see s one again,

00:01:33.420 --> 00:01:36.620
and sort of back up its value.

00:01:36.620 --> 00:01:40.551
You've actually captured the fact
that last time you were in s1,

00:01:40.551 --> 00:01:43.630
you actually went to s2 and
saw reward r1.

00:01:43.630 --> 00:01:46.990
&gt;&gt; Right, exactly, so
it's like the outcome-based on updates

00:01:46.990 --> 00:01:50.670
now with extra learning or
inside the episode learning.

00:01:50.670 --> 00:01:52.320
&gt;&gt; I like it, beautiful.

00:01:52.320 --> 00:01:55.034
So is that better?

00:01:55.034 --> 00:01:55.973
&gt;&gt; Let's say yes.

00:01:55.973 --> 00:01:57.400
&gt;&gt; Excellent.

00:01:57.400 --> 00:01:57.900
That makes me feel good about myself.

