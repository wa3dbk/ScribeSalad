WEBVTT
Kind: captions
Language: en

00:00:00.420 --> 00:00:01.970
So one of the things that goes wrong, when you try to

00:00:01.970 --> 00:00:07.120
actually run gradient descent on a complex network with a lot of data

00:00:07.120 --> 00:00:09.240
is that you can get stuck in these local minima and then

00:00:09.240 --> 00:00:11.580
you start to wonder, boy is there some other way that I can

00:00:11.580 --> 00:00:14.010
optimize these weights. I'm trying to find a set of weights for

00:00:14.010 --> 00:00:19.880
the neural network Well that what? That, that tries to ,minimize error on

00:00:19.880 --> 00:00:22.490
the training set. And so, gradient decent is one way to do

00:00:22.490 --> 00:00:25.720
it, and it can get stuck, but there's other kinds of advanced optimization

00:00:25.720 --> 00:00:29.290
methods that become very appropriate here. And in fact, there's

00:00:29.290 --> 00:00:32.159
a lot of people in machine learning who think of optimization

00:00:32.159 --> 00:00:34.490
and learning as kind of being the same thing. That, that

00:00:34.490 --> 00:00:36.510
what you're really trying to do in any kind of learning

00:00:36.510 --> 00:00:40.300
problem is solve this, this high order, very difficult optimization

00:00:40.300 --> 00:00:44.490
problem to figure out what the The learned ,representation needs to

00:00:44.490 --> 00:00:47.770
be. So, I need to mention in passing some kinds of

00:00:47.770 --> 00:00:50.835
advanced methods that people have brought to bear, there's things like

00:00:50.835 --> 00:00:53.780
,uh,using momentum terms in the gradient,

00:00:53.780 --> 00:00:56.860
which basically, where the idea in momentum

00:00:56.860 --> 00:00:59.250
is, as we're doing gradient decent, so let's imagine this is our error

00:00:59.250 --> 00:01:01.950
surface, we don't want to get stick on this ball here, we want to

00:01:01.950 --> 00:01:04.420
kind of pass all the way through it to get to this ball, so maybe

00:01:04.420 --> 00:01:07.470
we need to just continue in the direction we've been going. So, instead

00:01:07.470 --> 00:01:10.120
of You know think of it as a kind of physical analogy. Instead

00:01:10.120 --> 00:01:12.670
of just, just going to the bottom of this hill and getting stuck,

00:01:12.670 --> 00:01:15.890
it can kind of bounce out and pop over and come to, what might be

00:01:15.890 --> 00:01:19.010
a lower, minima, later. There's a lot of

00:01:19.010 --> 00:01:21.490
work in using higher order derivatives to, to better

00:01:21.490 --> 00:01:24.450
optimize things instead of just thinking about the,

00:01:24.450 --> 00:01:26.910
way that individual weights change the error function to

00:01:26.910 --> 00:01:31.000
look at combinations of weights. Hamiltonions and whatnot.

00:01:31.000 --> 00:01:33.930
There's various ideas for randomized optimization, which we're going

00:01:33.930 --> 00:01:37.340
to get to in a sister course, that can

00:01:37.340 --> 00:01:40.840
be applied to, to, to make things more robust.

00:01:40.840 --> 00:01:44.310
And sometimes it's worth thinking, you know what, we don't really want to just

00:01:44.310 --> 00:01:47.810
minimize the error on the training set, we may actually want to have some

00:01:47.810 --> 00:01:53.210
kind of penalty for using, using a structure that's too complex. I mean this,

00:01:53.210 --> 00:01:56.790
this ,uh, when did we, when did we see something like this before Charles?

00:01:56.790 --> 00:02:00.470
&gt;&gt; When we were doing regression, and we were talking about over fitting.

00:02:00.470 --> 00:02:03.020
&gt;&gt; So right. That's right. It came up in

00:02:03.020 --> 00:02:05.980
regression but something similar will also happen in the decision

00:02:05.980 --> 00:02:06.540
tree section.

00:02:06.540 --> 00:02:08.470
&gt;&gt; Sure. We, we had a, we had a issue with

00:02:08.470 --> 00:02:11.280
decision trees where if we had, we let the tree grow too

00:02:11.280 --> 00:02:15.330
much to explain every little quirk in the data. You'd over fit,

00:02:15.330 --> 00:02:17.970
and so ,we came up with a lot of ways of dealing

00:02:17.970 --> 00:02:23.340
with that, like pruning. No going too far deeply into the tree.

00:02:23.340 --> 00:02:25.630
You can either do that by filling out the tree and then

00:02:25.630 --> 00:02:28.270
backing up so you only have a little bit of small error

00:02:28.270 --> 00:02:31.350
Or by stopping once you've reached some sort of threshold as you

00:02:31.350 --> 00:02:34.410
grow the tree out. That's really the same

00:02:34.410 --> 00:02:36.810
as giving some kind of penalty for complexity.

00:02:36.810 --> 00:02:38.770
&gt;&gt; Yes, exactly, right. So complexity in the

00:02:38.770 --> 00:02:40.440
tree setting has to do with the size

00:02:40.440 --> 00:02:43.880
of the tree, in regression it had to do with the order of the polynomial. What

00:02:43.880 --> 00:02:47.210
do you suppose it would mean in the neural net setting? And, and how would you

00:02:47.210 --> 00:02:50.820
predict, what negative attributes it might have. So,

00:02:50.820 --> 00:02:53.550
what's, what's a more or less complex network?

00:02:53.550 --> 00:02:56.470
&gt;&gt; Well, there's two things you can do

00:02:56.470 --> 00:02:58.020
with networks, you can add more and more

00:02:58.020 --> 00:03:01.630
nodes, and you can add more and more layers.

00:03:01.630 --> 00:03:03.620
&gt;&gt; Good. So, right. So if we, the more

00:03:03.620 --> 00:03:05.980
nodes that we put into network, the more complicated

00:03:05.980 --> 00:03:08.640
the mapping becomes from input to output, the more

00:03:08.640 --> 00:03:11.180
local minima we get, the more we have the ability

00:03:11.180 --> 00:03:13.910
to actually model the noise, which brings up exactly

00:03:13.910 --> 00:03:16.770
the same overfitting issues. It turns out there's another one

00:03:16.770 --> 00:03:18.740
that's actually really interesting in the neural net setting

00:03:18.740 --> 00:03:21.470
which, I think didn't occur to people In the early

00:03:21.470 --> 00:03:24.220
days but it became clear and clear over time,

00:03:24.220 --> 00:03:27.870
which is that ,you can also have a complex network,

00:03:27.870 --> 00:03:31.740
just because the numbers, the weights, are very large. So

00:03:31.740 --> 00:03:33.850
same number of weights, same number of nodes, same number

00:03:33.850 --> 00:03:37.610
of layers, but larger numbers often leads to more complex.

00:03:38.610 --> 00:03:41.930
Network and the possibility of overfitting. And so ,sometimes we

00:03:41.930 --> 00:03:43.680
want to penalize a network not just by giving it

00:03:43.680 --> 00:03:46.750
fewer nodes or layers but also by keeping the numbers

00:03:46.750 --> 00:03:49.510
in a reasonable range. Does that, does that make sense?

00:03:49.510 --> 00:03:50.280
&gt;&gt; Makes perfect sense.

