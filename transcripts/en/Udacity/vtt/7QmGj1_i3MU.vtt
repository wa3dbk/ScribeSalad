WEBVTT
Kind: captions
Language: en

00:00:00.025 --> 00:00:05.120
While we're on the topic of
problems that are easy or hard for

00:00:05.120 --> 00:00:10.080
optimizers to solve, let's talk for
a moment about a particular class of

00:00:10.080 --> 00:00:16.910
problems that are indeed the most easy
for these types of algorithms to solve.

00:00:16.910 --> 00:00:19.770
And those are called convex problems.

00:00:19.770 --> 00:00:23.230
Here's the formal definition
of a convex function.

00:00:23.230 --> 00:00:25.628
I'm going to read it
to you from Wikipedia.

00:00:25.628 --> 00:00:29.010
And then I'll show you what it
means on these graphs here.

00:00:30.510 --> 00:00:35.000
A real valued function f
of x defined on an interval

00:00:35.000 --> 00:00:38.880
is called convex if
the line segment between

00:00:38.880 --> 00:00:43.220
any two points on the graph of
the function lies above the graph.

00:00:44.600 --> 00:00:46.540
A lot of words there.

00:00:46.540 --> 00:00:49.920
Let me show you what
that means more easily.

00:00:49.920 --> 00:00:54.150
First step, choose two points and
draw a line between them.

00:00:54.150 --> 00:00:59.365
Now, for each of these lines, if
the line is above the graph, everywhere

00:00:59.365 --> 00:01:04.720
between those two points, then the
function is convex between those points.

00:01:04.720 --> 00:01:07.105
So for this function, yes,

00:01:07.105 --> 00:01:11.630
it's convex because the line
is above the graph everywhere.

00:01:11.630 --> 00:01:17.456
In fact, any two points you chose on
this graph, we'll have that property.

00:01:17.456 --> 00:01:23.600
So this function is convex everywhere,
at least where we're looking at.

00:01:23.600 --> 00:01:28.820
Here, notice that this part of
the graph lies above the line.

00:01:29.820 --> 00:01:32.130
So this is non convex.

00:01:33.450 --> 00:01:37.740
Similarly, this one,
we've got this region here

00:01:37.740 --> 00:01:42.700
that lies above the line, so
this one is also non convex.

00:01:42.700 --> 00:01:45.040
And this one is of course convex.

00:01:45.040 --> 00:01:49.780
So a couple things to observe here,
some properties that emerged.

00:01:49.780 --> 00:01:54.300
One is in order for
the function to be convex,

00:01:55.610 --> 00:01:58.250
it has to have only one local minima.

00:01:58.250 --> 00:02:01.510
And in other words,
that local minima is the global minima.

00:02:01.510 --> 00:02:03.920
This one fails for that reason.

00:02:03.920 --> 00:02:07.760
We also can't have any flat regions

00:02:07.760 --> 00:02:10.350
that essentially don't
have any slope downward.

00:02:11.650 --> 00:02:15.950
Now, if the function you're trying
to find a minima for is convex,

00:02:15.950 --> 00:02:21.510
then these algorithms will find
the minima quickly and easily.

00:02:21.510 --> 00:02:26.640
But again, there are algorithms
that can still find the minima for

00:02:26.640 --> 00:02:29.340
more complicated examples like these.

00:02:29.340 --> 00:02:31.710
But they require a little
bit of randomness and

00:02:31.710 --> 00:02:37.208
they aren't necessarily guaranteed
to find the global minima.

00:02:37.208 --> 00:02:41.710
So far, we've been looking at functions
that just have one dimension in x.

00:02:41.710 --> 00:02:44.030
So for instance,
the parabola that we looked at.

00:02:45.290 --> 00:02:50.630
It’s just as easy for these optimizers
to work in multiple dimensions.

00:02:50.630 --> 00:02:54.480
Here’s an example of a function
that has two dimensions in x.

00:02:54.480 --> 00:02:57.320
It still has its y result.

00:02:57.320 --> 00:03:00.580
But the minimizers can solve these
problems with gradient descent

00:03:00.580 --> 00:03:02.420
just as easily.

00:03:02.420 --> 00:03:07.220
So instead of just one dimension,
we can have one,

00:03:07.220 --> 00:03:09.370
two, three, four, as many as we’d like.

