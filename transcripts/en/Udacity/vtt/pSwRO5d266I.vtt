WEBVTT
Kind: captions
Language: en

00:00:00.480 --> 00:00:02.340
Okay Michael. So let's try to be a little bit

00:00:02.340 --> 00:00:05.580
more detailed about, kind of, the mechanics of how you, how

00:00:05.580 --> 00:00:09.710
you would make this work. The algorithms for finding Independent Components

00:00:09.710 --> 00:00:12.100
Analysis, are in all the reading. But, I do think it's

00:00:12.100 --> 00:00:14.420
pretty easy to kind of get lost. If you don't, sort

00:00:14.420 --> 00:00:17.170
of, turn these matrices into actual numbers. So, let me just,

00:00:17.170 --> 00:00:18.880
sort of, give you a quick example of how we would

00:00:18.880 --> 00:00:21.770
do this specific thing here. And see if that helps, okay?

00:00:21.770 --> 00:00:22.746
&gt;&gt; Mm-hm.

00:00:22.746 --> 00:00:25.490
&gt;&gt; Okay. So, how would you go about turning

00:00:25.490 --> 00:00:27.760
this into a problem that something like, you

00:00:27.760 --> 00:00:30.670
know, an actual algorithm that uses numbers could

00:00:30.670 --> 00:00:32.940
do. Well, it turns out it's fairly straightforward.

00:00:32.940 --> 00:00:34.910
And let's just take this, this particular example

00:00:34.910 --> 00:00:37.920
here, with people talking into a microphone. Basically

00:00:37.920 --> 00:00:40.720
you create a matrix. Which are samples of

00:00:40.720 --> 00:00:42.720
all of the sounds. So, in our original

00:00:42.720 --> 00:00:46.110
space, we have, you know, this handsome person here.

00:00:48.630 --> 00:00:51.380
We have this other, let's say, similarly handsome but in a

00:00:51.380 --> 00:00:55.400
different way person here, and we have this other person, with

00:00:55.400 --> 00:00:59.680
my poor attempt to draw hair over here, and they're talking.

00:00:59.680 --> 00:01:02.700
Well, what we end up doing is we basically take the sound

00:01:02.700 --> 00:01:05.010
wave and we sample it. And what does it mean to

00:01:05.010 --> 00:01:08.700
sample it? Well, you know, you represent sound in a computer as

00:01:08.700 --> 00:01:11.150
a sequence of numbers, just like you represent pictures as a

00:01:11.150 --> 00:01:13.780
sequence of numbers, and in fact, you represent words as a sequence

00:01:13.780 --> 00:01:16.330
of numbers. And so we basically turn these sound waves

00:01:16.330 --> 00:01:18.900
into a matrix full of values. So, as I described

00:01:18.900 --> 00:01:22.410
before, Michael, each one of these microphones. Is actually getting

00:01:22.410 --> 00:01:25.570
a linear combination of speech from each of these three

00:01:25.570 --> 00:01:28.970
individuals. So if we think of microphone one, it is

00:01:28.970 --> 00:01:32.520
seeing some kind of sound wave, which is again a

00:01:32.520 --> 00:01:35.540
linear combination from each of these people generating a sound

00:01:35.540 --> 00:01:40.160
wave. The same is true for microphone two and similarly.

00:01:40.160 --> 00:01:43.670
For microphone three. Now, of course, we're talking about recording

00:01:43.670 --> 00:01:46.240
these and we're thinking about computer programs, which means we take

00:01:46.240 --> 00:01:49.890
this continuous sound wave and we sample it at a very

00:01:49.890 --> 00:01:52.530
fast rate. And what we end up with is a sequence

00:01:52.530 --> 00:01:56.750
of numbers that represent the particular pressure that we're getting

00:01:56.750 --> 00:02:00.360
in these sound waves. So, these sound waves get converted into

00:02:00.360 --> 00:02:02.830
a sequence of numbers and I'm just going to make up some

00:02:02.830 --> 00:02:05.900
numbers for this. And now what we have is a matrix

00:02:05.900 --> 00:02:09.690
where each row represents a feature in our original feature space.

00:02:09.690 --> 00:02:12.230
And by original here, I mean, the feature space that we

00:02:12.230 --> 00:02:17.594
see each of our microphones and every column represents a sample. Say

00:02:17.594 --> 00:02:23.180
at time0, time1, time2, and so on. Does that make sense?

00:02:23.180 --> 00:02:27.060
&gt;&gt; Yes. And so, since each of these is a linear combination

00:02:27.060 --> 00:02:31.100
of these voices we get here. Our goal is to find a projection

00:02:31.100 --> 00:02:33.350
like we did before and the original definition of the

00:02:33.350 --> 00:02:37.590
problem, such that if we projected this on to here, we

00:02:37.590 --> 00:02:41.440
would end up with a new feature space that corresponds individually

00:02:41.440 --> 00:02:43.570
to each of these people. And you could do this with

00:02:43.570 --> 00:02:45.320
anything, it doesn't have to be sounds, but if we think

00:02:45.320 --> 00:02:49.450
about the. Adhoc query problem that we went through earlier, each

00:02:49.450 --> 00:02:53.540
of these would be words, and say their presence of words

00:02:53.540 --> 00:02:56.350
would be your features, and each of these columns would be

00:02:56.350 --> 00:02:58.550
documents, say. And the goal would be to find

00:02:58.550 --> 00:03:01.450
a projection given a set of documents that allowed

00:03:01.450 --> 00:03:04.620
us to recover some underlying structure that was useful

00:03:04.620 --> 00:03:09.010
for classification, or for information retrieval. Does that make sense?

00:03:09.010 --> 00:03:10.930
&gt;&gt; Yeah. And can you say, what does

00:03:10.930 --> 00:03:14.320
it mean for that sequence to have mutual information?

00:03:15.730 --> 00:03:19.050
&gt;&gt; so as you recall. Because I know that you listen to Push Cars.

00:03:21.090 --> 00:03:24.060
A discussion about mutual information and entropy and so

00:03:24.060 --> 00:03:27.990
forth. All mutual information is is a measure of

00:03:27.990 --> 00:03:30.429
how much one variable tells you about another variable.

00:03:31.540 --> 00:03:35.460
And, the assumption here is that each of these people

00:03:35.460 --> 00:03:38.950
talking has no mutual information. That is, they're talking

00:03:38.950 --> 00:03:41.790
independently of one another, or the sounds that are coming

00:03:41.790 --> 00:03:44.110
out of their mouths, don't allow us to predict

00:03:44.110 --> 00:03:46.340
the sounds that are going to come out of another

00:03:46.340 --> 00:03:48.610
persons mouth. So it may mean that these people are

00:03:48.610 --> 00:03:50.960
talking to one another as we often do, Charles might

00:03:50.960 --> 00:03:52.880
be talking to Michael, who's talking to push cars, who's

00:03:52.880 --> 00:03:56.510
talking back to Michael, who's talking. To Charles, but the exact

00:03:56.510 --> 00:03:59.550
sound waves that we see are actually independent of one

00:03:59.550 --> 00:04:03.750
another. So if that turns out to be true, what independent

00:04:03.750 --> 00:04:06.380
component analysis is trying to do is trying to recover

00:04:06.380 --> 00:04:11.080
features, in this case. It turns out the corresponding individual speakers,

00:04:11.080 --> 00:04:13.190
such that their sound waves, or the values

00:04:13.190 --> 00:04:16.820
that you see, are statistically independent of one another.

00:04:16.820 --> 00:04:19.610
Okay? And that's what this does. But, since

00:04:19.610 --> 00:04:22.450
we can always come up with arbitrary projections that

00:04:22.450 --> 00:04:25.600
are statistically independent, it's important that whatever our

00:04:25.600 --> 00:04:30.340
new transformation gives us. It actually has some relationship

00:04:30.340 --> 00:04:33.950
with the original values that we saw. So some

00:04:33.950 --> 00:04:36.230
how we want to be able to learn from this,

00:04:36.230 --> 00:04:38.520
into this in a way that we don't lose any

00:04:38.520 --> 00:04:41.170
information. In much the same way that PCA was trying to

00:04:41.170 --> 00:04:44.190
minimize the loss of information, and so we not only want

00:04:44.190 --> 00:04:47.300
each of the individual new features, in this case people to

00:04:47.300 --> 00:04:50.270
be statistically independent, we want the amount of data that we

00:04:50.270 --> 00:04:52.960
get from these people. Compared to the amount of data that

00:04:52.960 --> 00:04:56.400
we got originally from the microphones, to actually strongly predict one

00:04:56.400 --> 00:05:01.750
another. And if the mutual information between the microphones and our

00:05:01.750 --> 00:05:05.360
candidates for the people's voices have high mutual information, then

00:05:05.360 --> 00:05:07.870
it means that we haven't lost anything. And so those

00:05:07.870 --> 00:05:11.390
two things together, those two constraints. Forces into a situation

00:05:11.390 --> 00:05:14.940
where, if this model is true, we construct the original voices.

