WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.000
Welcome to the Unit 2 office hours.

00:00:03.000 --> 00:00:09.000
One of the first questions I'd like to address deals with the Ply library

00:00:09.000 --> 00:00:12.000
that we're using under the hood for this class

00:00:12.000 --> 00:00:15.000
to help us out with lexing and parsing.

00:00:15.000 --> 00:00:20.000
In the lectures we talked about how to write down a token definition.

00:00:20.000 --> 00:00:23.000
You define a function t_something--like a number--

00:00:23.000 --> 00:00:27.000
and then you'd write out the regular expression--0 through 9-plus.

00:00:27.000 --> 00:00:30.000
Then you might do a little work modifying the token value--

00:00:30.000 --> 00:00:33.000
maybe converting it from a string to an integer.

00:00:33.000 --> 00:00:35.000
Then eventually you return the token.

00:00:35.000 --> 00:00:40.000
The basic question from students is how does that work? What's going on?

00:00:40.000 --> 00:00:44.000
After you write down a number of these token definitions,

00:00:44.000 --> 00:00:49.000
there is a library behind the scenes--part of the grading scripts that we're using,

00:00:49.000 --> 00:00:52.000
but also available for you to download if you'd like to try it out on your own--

00:00:52.000 --> 00:00:56.000
that gathers up all of your token definitions.

00:00:56.000 --> 00:01:00.000
One way to do this is using a technique called "reflection,"

00:01:00.000 --> 00:01:05.000
which actually sounds very philosophical but is a way for a computer program

00:01:05.000 --> 00:01:08.000
to look at itself and all of it's own capabilities.

00:01:08.000 --> 00:01:13.000
In essence, our Python program asks, has anyone defined any functions recently

00:01:13.000 --> 00:01:15.000
that start with t_?

00:01:15.000 --> 00:01:21.000
If so, the next thing we have to do is get that regular expression out of them.

00:01:21.000 --> 00:01:27.000
It turns out that Python functions allow you to write documentation or a brief explanation

00:01:27.000 --> 00:01:32.000
at the beginning of any function. This is sometimes a good software engineering practice.

00:01:32.000 --> 00:01:38.000
Our Python parsing library and our Python lexing library reuse this power.

00:01:38.000 --> 00:01:42.000
We're writing down this regular expression, and the library is treating it

00:01:42.000 --> 00:01:45.000
as if it were documentation.

00:01:45.000 --> 00:01:50.000
We use that to get access to the regular expressions you've written down.

00:01:50.000 --> 00:01:53.000
So step one: you write down a bunch of these token definitions.

00:01:53.000 --> 00:01:57.000
Then we look--using reflection, for example--to find all of them.

00:01:57.000 --> 00:02:02.000
Step two: we go through all of them and ask, do they have any strings at the beginning-

00:02:02.000 --> 00:02:06.000
anything that looks like documentation. The answer is yes.

00:02:06.000 --> 00:02:11.000
But for us it's not documentation. It's a regular expression specification.

00:02:11.000 --> 00:02:17.000
The next thing to do is convert each of those regular expressions to a finite state machine.

00:02:17.000 --> 00:02:21.000
Now, I hinted at this in class. Didn't give a full, formal proof on how to do it.

00:02:21.000 --> 00:02:28.000
But it turns out that every one of the regular expressions--plus, star, disjunction, concatenation--

00:02:28.000 --> 00:02:31.000
can be written out in a finite state machine.

00:02:31.000 --> 00:02:35.000
For every regular expression there is at least one--and in fact, typically, infinitely many--

00:02:35.000 --> 00:02:39.000
finite state machines that accept exactly the same language.

00:02:39.000 --> 00:02:43.000
We'll just apply that conversion in the background.

00:02:43.000 --> 00:02:48.000
But if you have a bunch of different token definitions--one for number, one for string,

00:02:48.000 --> 00:02:51.000
one for some keywords in your language like "if," "then," or "else"--

00:02:51.000 --> 00:02:54.000
we're going to end up with a bunch of different finite state machines.

00:02:54.000 --> 00:02:58.000
Now we have to combine them all together.

00:02:58.000 --> 00:03:02.000
If you're willing to humor me with nondeterministic finite state machines,

00:03:02.000 --> 00:03:05.000
we could actually do that just by putting a special state at the beginning--

00:03:05.000 --> 00:03:08.000
a special new start state--and having an epsilon transition

00:03:08.000 --> 00:03:11.000
go to the beginning of each of our old states.

00:03:11.000 --> 00:03:14.000
We'll glue them all together into sort of a Frankenstein's monster--

00:03:14.000 --> 00:03:17.000
an amalgamation of everything we've looked at.

00:03:17.000 --> 00:03:23.000
We're almost done except that it's not enough for a lexer to know this string is a token.

00:03:23.000 --> 00:03:26.000
You have to know which one it is.

00:03:26.000 --> 00:03:31.000
A real life lexer will have one more bit of information that we didn't talk about in class.

00:03:31.000 --> 00:03:36.000
A state isn't just an accepting state, it's an accepting state with a little label.

00:03:36.000 --> 00:03:39.000
If you accept, here it was a string.

00:03:39.000 --> 00:03:42.000
If you accept here in this over there, it was the token then.

00:03:42.000 --> 00:03:45.000
If you accept in this state down here, it's a number.

00:03:45.000 --> 00:03:47.000
Instead of just known what the accepting states are,

00:03:47.000 --> 00:03:50.000
we need to know what the accepting states are

00:03:50.000 --> 00:03:54.000
and what token each one corresponds to.

00:03:54.000 --> 00:03:56.000
All right. You did all your token definitions. We found them all.

00:03:56.000 --> 00:03:59.000
Each one had a regular expression. We found that.

00:03:59.000 --> 00:04:02.000
We converted each of those down into finite state machines.

00:04:02.000 --> 00:04:05.000
We joined them all together. We labeled all the accepting states.

00:04:05.000 --> 00:04:12.000
Now we represent that big finite state machine internally as edges.

00:04:12.000 --> 00:04:15.000
This is sometimes formally called a transition table,

00:04:15.000 --> 00:04:19.000
but it's just like the edges in coding that we used in class.

00:04:19.000 --> 00:04:23.000
Now when it comes time to actually do lexing--to break a string down into

00:04:23.000 --> 00:04:28.000
into important words and tokens--you just feed the characters of the string

00:04:28.000 --> 00:04:31.000
one at a time into that big finite state machine.

00:04:31.000 --> 00:04:36.000
It's exactly the same as the FSM sim procedure we went over in class.

00:04:36.000 --> 00:04:40.000
It's just that their finite state machine is much bigger.

00:04:40.000 --> 00:04:44.000
In fact, the FSM sim code that we wrote would work just as well,

00:04:44.000 --> 00:04:46.000
even on bigger finite state machines.

00:04:46.000 --> 00:04:50.000
I just didn't show it because they're harder to draw on screen.

00:04:50.000 --> 00:04:53.000
Under the hood, this library is basically just doing a bunch

00:04:53.000 --> 00:04:57.000
of grungy details, chores, busy work in the background.

00:04:57.000 --> 00:05:01.000
We did exercises with one or two regular expression definitions.

00:05:01.000 --> 00:05:04.000
The library gathers them all up together in one place.

00:05:04.000 --> 00:05:10.000
You're already familiar with all of the key concepts in making a lexical analysis library.

00:05:10.000 --> 09:59:59.000
The library just does a lot of the busy work for you. And that's the explanation.

