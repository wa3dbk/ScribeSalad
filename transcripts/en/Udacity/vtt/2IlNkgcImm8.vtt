WEBVTT
Kind: captions
Language: en

00:00:00.160 --> 00:00:03.865
In unit five, remember that we learned the, the GPU hardware processes groups

00:00:03.865 --> 00:00:07.767
of threads called warps. And at every thread in a warp, performs the same

00:00:07.767 --> 00:00:10.926
instruction at the same time. And this means, that is some threads are not

00:00:10.926 --> 00:00:13.427
actively doing computation, they will, they're still going to have to wait

00:00:13.427 --> 00:00:18.284
while the active threads in warp do there thing. Now todays GPUs, in fact all

00:00:18.284 --> 00:00:24.708
GPU that Nvidia has ever made, have 32 threads per warp. In this first case,

00:00:24.708 --> 00:00:29.313
only four out of every 32 threads are actually doing something. And, the rest

00:00:29.313 --> 00:00:33.508
of the threads are just along for the ride. In a compacted case, if we can

00:00:33.508 --> 00:00:37.245
compact this down to a dense array. Where all four of these elements, are next

00:00:37.245 --> 00:00:39.997
to each other and then there's, the remaining elements from elsewhere in the

00:00:39.997 --> 00:00:43.896
array all kind of compacted in into one dense array. In that case, all 32

00:00:43.896 --> 00:00:47.700
threads in the warp are going to be doing active work and so you'll finish the

00:00:47.700 --> 00:00:51.736
total work eight times faster. So that's our answer. We're going to go eight

00:00:51.736 --> 00:00:56.880
times faster, if every eighth element is active in the original array, and we

00:00:56.880 --> 00:01:02.838
compact it and operate and still in the compacted array. And by the same

00:01:02.838 --> 00:01:06.496
reasoning, if only one of these threads in a warp operating on the original

00:01:06.496 --> 00:01:10.154
array, has anything useful to do, then you'll go 32 times faster, which is a

00:01:10.154 --> 00:01:16.920
big deal. You can see why compaction can lead to big speed ups. But the other

00:01:16.920 --> 00:01:19.368
subtlety to remember about warps, is that if all the threads in the warp take

00:01:19.368 --> 00:01:24.149
the same branch, so in the code snippet I showed before. If all 32 threads in

00:01:24.149 --> 00:01:27.217
the warp check to see if they were active, decided they weren't and then

00:01:27.217 --> 00:01:31.760
exited. Then you pay almost no penalty for that. And so that, that warp is

00:01:31.760 --> 00:01:34.120
going to fire up, all threads are going to see they have nothing to do, it's

00:01:34.120 --> 00:01:38.993
going to disappear. And so, actually the warps that are primarily empty or that

00:01:38.993 --> 00:01:44.733
are entirely empty we'll exit immediately. And so, the case where only one in

00:01:44.733 --> 00:01:49.631
128 threads is doing something useful, actually still goes about 32 times

00:01:49.631 --> 00:01:54.607
faster. because there's one warp that's going to have some work to do. And 31

00:01:54.607 --> 00:01:57.380
of the threads in that warp, are going to be sitting around waiting for the

00:01:57.380 --> 00:02:01.327
single thread that has some useful work to do. But, all of the, all of the

00:02:01.327 --> 00:02:04.382
threads and the entirely empty warps, that have nothing to do, they exit quite

00:02:04.382 --> 00:02:08.846
quickly. So you'd pay a very slight penalty for having launched those warps and

00:02:08.846 --> 00:02:13.830
having them immediately exist. But, unbalance is still are going to be close to

00:02:13.830 --> 00:02:16.463
32 times as fast to operate on the compacted array than it would have been to

00:02:16.463 --> 00:02:19.443
operate on the original array.

