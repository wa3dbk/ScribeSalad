WEBVTT
Kind: captions
Language: en

00:00:00.403 --> 00:00:04.106
&gt;&gt; Steven, what kind of problems would be a good match for this dynamic parallelism capability?

00:00:04.106 --> 00:00:08.951
So as programmers are thinking about what techniques to use in designing the next-generation algorithms,

00:00:08.951 --> 00:00:13.052
what are the things where dynamic parallelism is going to really make a difference for them?

00:00:13.052 --> 00:00:17.520
&gt;&gt; I guess when I think about the features that I want to add to CUDA,

00:00:17.520 --> 00:00:21.791
I really want to make it easier to write the programs you want to write,

00:00:21.791 --> 00:00:29.177
and dynamic parallelism, I think, really gives you much more flexibility in how you write your code.

00:00:29.177 --> 00:00:35.911
So as well as things like recursive algorithms, which are extremely difficult without it,

00:00:35.911 --> 00:00:41.778
sometimes it's just easier to write a program wholly on the GPU, for example.

00:00:41.778 --> 00:00:46.449
If I know that my program is a small integrative loop launching lots of parallel work,

00:00:46.449 --> 00:00:49.445
it might be just simply be easier to write the whole thing on the GPU

00:00:49.445 --> 00:00:52.780
and have a GPU thread control the whole thing from scratch.

00:00:52.780 --> 00:00:56.677
So if I'm in a situation where my memory marshalling backwards and forwards

00:00:56.677 --> 00:00:59.813
between CPU and GPU would be complex or difficult--

00:00:59.813 --> 00:01:03.761
if I put my whole program on the GPU, all my memory's in one place, and it's much easier to write.

00:01:03.761 --> 00:01:06.453
So the first kind of problem I would look at

00:01:06.453 --> 00:01:11.225
is one which would just be made easier by keeping all of your code in one place.

00:01:11.225 --> 00:01:13.695
You may not necessarily go any faster.

00:01:13.695 --> 00:01:17.938
Remember serial execution on the GPU is slower than serial execution on the CPU.

00:01:17.938 --> 00:01:20.231
When you're developing code,

00:01:20.231 --> 00:01:22.967
even though you might be able to get something working faster

00:01:22.967 --> 00:01:25.874
if you do a CPU and GPU combination,

00:01:25.874 --> 00:01:28.673
typically--you know--as I said, I came from a science background.

00:01:28.673 --> 00:01:32.346
You don't have 6 months to tune your code; you've got 4 weeks before your paper deadline.

00:01:32.346 --> 00:01:34.947
So the question is not, how good can I get it?

00:01:34.947 --> 00:01:38.218
It's how good can I get it in 4 weeks before my paper is due?

00:01:38.218 --> 00:01:41.922
And so if I can give something like dynamic parallelism,

00:01:41.922 --> 00:01:45.792
which will make it easier to get as good as you can in 4 weeks--

00:01:45.792 --> 00:01:48.928
you might only get to 75% of performance in 4 weeks.

00:01:48.928 --> 00:01:51.672
But if without it, you'd only get to 50% of performance,

00:01:51.672 --> 00:01:54.399
you can do that much more science in the time you have available.

00:01:54.399 --> 00:01:59.075
So in spite of the new types of algorithm that it let's you do--and those are very interesting,

00:01:59.075 --> 00:02:02.776
because it let's you approach things on the GPU you simply couldn't do before.

00:02:02.776 --> 00:02:06.182
I think the first step is to say, "Well, does this just make my life easier?"

00:02:06.182 --> 00:02:09.780
If it makes your life easier, you can spend your time focusing on doing the science

00:02:09.780 --> 00:02:12.419
or the calculation you need to do,

00:02:12.419 --> 00:02:16.000
rather than wondering, how on Earth do I program this thing in the first place?

