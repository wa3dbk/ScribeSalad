WEBVTT
Kind: captions
Language: en

00:00:00.300 --> 00:00:04.059
Here's the big picture at a high
level of how we train a q learner.

00:00:05.310 --> 00:00:09.700
We have our data here and we select
which data we want to train on,

00:00:09.700 --> 00:00:15.790
of course this data in the case of
the stock market is time series.

00:00:15.790 --> 00:00:21.040
And so it's arranged from oldest
to newest vertically here.

00:00:21.040 --> 00:00:22.640
So we select the day
we want to train on.

00:00:22.640 --> 00:00:27.350
And then we iterate over
this data over time.

00:00:27.350 --> 00:00:31.870
So we evaluate the situation there and
for

00:00:31.870 --> 00:00:36.250
a particular stock that
gives us s our state.

00:00:36.250 --> 00:00:40.330
We consult our policy and
that gives us an action.

00:00:40.330 --> 00:00:43.950
So we take that action,
plug it into our system here,

00:00:45.210 --> 00:00:51.110
evaluate the next state, and
we get our s prime and our reward.

00:00:52.140 --> 00:00:59.090
So after one iteration here we've got
an s, an action, an s prime, and an r.

00:01:00.140 --> 00:01:06.280
Or an experience tuple, and we use that
experience tuple to update our Q table.

00:01:06.280 --> 00:01:11.380
Once we get all the way through the
training data, we test our policy and

00:01:11.380 --> 00:01:13.380
we see how well it
performs in a back test.

00:01:14.810 --> 00:01:19.340
If it's converged or it's not getting
any better then we say we're done.

00:01:19.340 --> 00:01:23.200
If not, we repeat this whole process
all the way through the training data.

00:01:24.430 --> 00:01:26.320
So what do we mean by converge?

00:01:26.320 --> 00:01:31.474
Well, each time we cycle through
the data training our Q table and

00:01:31.474 --> 00:01:36.828
then testing back across that same data,
we get some performance.

00:01:36.828 --> 00:01:41.130
And we expect that each time
we complete an iteration here,

00:01:41.130 --> 00:01:44.838
our performance is going
to get better and better.

00:01:44.838 --> 00:01:50.580
But after a point it finally stops
getting better and it converges.

00:01:50.580 --> 00:01:53.388
So overall the chart's going to
look something like this.

00:01:53.388 --> 00:01:57.632
Eventually we reach this regime
where more iterations doesn't

00:01:57.632 --> 00:02:01.178
make it better and
we call it converged at that point.

00:02:01.178 --> 00:02:05.728
Let's consider now in more
detail what happens here when

00:02:05.728 --> 00:02:08.389
we're iterating over the data.

00:02:09.789 --> 00:02:12.960
So here are the details as we
iterate over our training data.

00:02:14.190 --> 00:02:16.190
We start by setting our start time,

00:02:16.190 --> 00:02:21.280
which is right here at the beginning and
we initialize our Q table.

00:02:21.280 --> 00:02:25.830
The usual way to initialize a Q
table is with small random numbers,

00:02:25.830 --> 00:02:28.430
but variations of that are fine.

00:02:28.430 --> 00:02:35.590
Now we're here in time and
we observe the features of our stock or

00:02:35.590 --> 00:02:41.340
stocks and from those build
up together our state s.

00:02:41.340 --> 00:02:43.380
We consult our policy or

00:02:43.380 --> 00:02:48.790
in other words we consult Q to find
the best action in the current state.

00:02:48.790 --> 00:02:49.870
That gives us a.

00:02:50.910 --> 00:02:57.390
Then we step forward and we see what
reward we get and what's our new state.

00:02:57.390 --> 00:03:02.630
We now have a complete experience tuple
that we can use to update our Q table.

00:03:02.630 --> 00:03:05.880
So we take this information
that we just learned and

00:03:05.880 --> 00:03:10.120
we improve Q based on that information.

00:03:10.120 --> 00:03:12.650
Then we step to the next
point in time and

00:03:12.650 --> 00:03:15.080
the next point time and
the next next one time and so on.

00:03:16.350 --> 00:03:21.930
So these are all the details of what
happens in this step of the big picture.

