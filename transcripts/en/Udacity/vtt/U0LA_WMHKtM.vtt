WEBVTT
Kind: captions
Language: en

00:00:00.250 --> 00:00:01.370
So that was very astute of you.

00:00:01.370 --> 00:00:04.950
And in fact there's a very general
property that could be stated about

00:00:04.950 --> 00:00:06.700
learning algorithms of this type.

00:00:06.700 --> 00:00:10.870
So if we have a learning update rule
like the one that we have, which is that

00:00:10.870 --> 00:00:16.340
the value at time T is the value at
Time T minus one, plus some learning

00:00:16.340 --> 00:00:20.170
rate times the difference between the
new estimate and the previous estimate.

00:00:20.170 --> 00:00:24.663
Or the new observed value and
the previous estimate, then

00:00:24.663 --> 00:00:30.000
this Vt will actually go
to the true expectations,

00:00:30.000 --> 00:00:34.720
the actual average value of
the Rs once T is big enough.

00:00:36.150 --> 00:00:37.150
Okay, so that's good.

00:00:37.150 --> 00:00:38.980
That means it's actually
learning the right thing, but

00:00:38.980 --> 00:00:42.310
there are conditions that we have to
put on the learning rate sequence.

00:00:43.430 --> 00:00:46.300
And the learning rate sequence has to
have the property that if you sum up all

00:00:46.300 --> 00:00:49.320
the learning rates,
it actually sums up to infinity.

00:00:49.320 --> 00:00:50.176
It's unbounded.

00:00:50.176 --> 00:00:51.240
it diverges.

00:00:51.240 --> 00:00:52.930
&gt;&gt; As opposed to greater than infinity.

00:00:52.930 --> 00:00:54.270
&gt;&gt; Oh, good point.

00:00:54.270 --> 00:00:55.560
Yes, that it's equal to infinity.

00:00:55.560 --> 00:00:59.630
Very good, it's actually
greater than any finite number.

00:00:59.630 --> 00:01:04.300
Anyway, yeah it diverges, but
the square of the learning rates,

00:01:04.300 --> 00:01:06.910
if you sum those up,
it's less than infinity.

00:01:06.910 --> 00:01:09.790
And we're not going to get in
the details about why these

00:01:09.790 --> 00:01:13.580
are the standard properties for
making this kind of learning rule work.

00:01:13.580 --> 00:01:18.430
But the first approximation, the
learning rates have to be big enough so

00:01:18.430 --> 00:01:22.320
that you can move to what the true
value is, no matter where you start.

00:01:22.320 --> 00:01:25.750
But they can't be so big that
they don't damp out the noise and

00:01:25.750 --> 00:01:28.920
actually do a proper job of averaging.

00:01:28.920 --> 00:01:32.080
So, if you just accept that these
are the two conditions that make sure

00:01:32.080 --> 00:01:34.760
that that's true,
that gives us a way of choosing

00:01:34.760 --> 00:01:37.090
different kinds of
learning rate sequences.

00:01:37.090 --> 00:01:42.580
&gt;&gt; Okay so what's some alphas that tease
to satisfy that and some that don't.

