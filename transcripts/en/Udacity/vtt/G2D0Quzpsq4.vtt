WEBVTT
Kind: captions
Language: en

00:00:00.012 --> 00:00:02.819
All right, so here is a rule we're
going to call the TD (0) rule,

00:00:02.819 --> 00:00:04.875
which gives it a different
name from TD (1).

00:00:04.875 --> 00:00:07.970
But eventually we're going to connect
the zero and the one together.

00:00:07.970 --> 00:00:11.380
And the rule looks like this,
not so unfamiliar looking.

00:00:11.380 --> 00:00:14.210
That the way we're going to
compute our value estimate for

00:00:14.210 --> 00:00:20.630
the state that we just left,
when we make a transition at epoch T for

00:00:20.630 --> 00:00:25.040
trajectory T, big T,
is what the previous value was.

00:00:25.040 --> 00:00:27.940
Plus we're going to move a little bit
in the direction with our learning rate

00:00:27.940 --> 00:00:28.780
toward what?

00:00:28.780 --> 00:00:31.410
Toward the reward that we observed,

00:00:31.410 --> 00:00:35.810
plus the discounted estimated value
of the state that we ended up in,

00:00:35.810 --> 00:00:40.300
minus the estimated value of
this state that we just left.

00:00:40.300 --> 00:00:42.790
So does this look familiar?

00:00:42.790 --> 00:00:45.134
&gt;&gt; It's got Vs, and rs,
and Alphas is in it.

00:00:45.134 --> 00:00:46.489
And so- [CROSSTALK]
&gt;&gt; Well sure, all right, but

00:00:46.489 --> 00:00:48.090
let's maybe we can be a little
more precise about this.

00:00:48.090 --> 00:00:52.580
So what would we expect this outcome
to look like on average, right?

00:00:52.580 --> 00:00:56.076
So the thing that's random here,
at least the way we've been talking

00:00:56.076 --> 00:00:59.509
about it is, if we were in some
state St-1 and we make a transition,

00:00:59.509 --> 00:01:01.960
we don't know what state
we're going to end up in.

00:01:01.960 --> 00:01:04.239
But there's some probabilities of those.

00:01:04.239 --> 00:01:09.080
So basically this update is being done
more often in the states that we end up

00:01:09.080 --> 00:01:11.850
in more often, the more common Sts, and

00:01:11.850 --> 00:01:15.235
less often in the Sts that
are less common, or less likely.

00:01:15.235 --> 00:01:20.130
And so really if we take the expectation
of this, what is it going to look like?

00:01:20.130 --> 00:01:21.860
If we repeat this update over and
over again,

00:01:21.860 --> 00:01:25.810
what we're really doing is sampling
different possible St values, right?

00:01:25.810 --> 00:01:31.130
So really we're taking an expectation
over what we get as the next state

00:01:31.130 --> 00:01:34.650
of the reward plus the discounted
estimated value of that next state.

00:01:35.760 --> 00:01:38.140
&gt;&gt; That seems right,
that's kind of what we want.

00:01:38.140 --> 00:01:40.820
&gt;&gt; Yeah, so this is exactly what
the maximum likelihood estimate is

00:01:40.820 --> 00:01:41.950
supposed to be.

00:01:41.950 --> 00:01:45.450
As long as these probabilities for

00:01:45.450 --> 00:01:50.260
what the next state is going to be
match what the data has shown so

00:01:50.260 --> 00:01:54.180
far as the transition to St.
&gt;&gt; Is that what that blue

00:01:54.180 --> 00:01:56.890
stuff above the equations is about?

00:01:56.890 --> 00:02:00.890
&gt;&gt; Yeah, so here's the idea, is that if
we repeat this update rule on the finite

00:02:00.890 --> 00:02:04.400
data that we've got over and over and
over again, then we're actually taking

00:02:04.400 --> 00:02:08.590
an average with respect to how often
we've seen each of those transitions.

00:02:08.590 --> 00:02:11.290
So it really is computing
the maximum likelihood estimate.

00:02:11.290 --> 00:02:12.730
So this does the right thing.

00:02:12.730 --> 00:02:13.500
&gt;&gt; Oh, okay.

00:02:13.500 --> 00:02:17.362
So that finite data thing's important
because if we had infinite data,

00:02:17.362 --> 00:02:19.660
then everything would work.

00:02:19.660 --> 00:02:22.307
&gt;&gt; So right, in the infinite
data case this is also true, but

00:02:22.307 --> 00:02:24.750
also TD(1) does the right
thing in infinite data.

00:02:24.750 --> 00:02:27.130
&gt;&gt; Kind of everything does
the right thing in infinite data.

00:02:27.130 --> 00:02:29.100
&gt;&gt; Yeah, everything's
getting all averaged out, and

00:02:29.100 --> 00:02:30.115
it'll do the right thing.

00:02:30.115 --> 00:02:33.465
But here we've got a finite
set of data so far.

00:02:33.465 --> 00:02:37.497
And the issue is that if we run our
update rule over that data over and

00:02:37.497 --> 00:02:38.793
over and over again,

00:02:38.793 --> 00:02:43.138
then we're going to get the effect of
having a maximum likelihood model.

00:02:43.138 --> 00:02:45.590
And that's not true of
the outcome based model.

00:02:45.590 --> 00:02:49.612
Because in the data that we just saw
where we were only saw one transition

00:02:49.612 --> 00:02:53.920
from S2 to anything else, we can run
over that over and over and over again.

00:02:53.920 --> 00:02:57.820
But the estimate is not going to change,
it's always going to be exactly that.

00:02:57.820 --> 00:02:59.020
&gt;&gt; Okay, that makes sense.

00:02:59.020 --> 00:03:00.930
Okay, sure, sure, I'll buy that.

00:03:00.930 --> 00:03:04.037
&gt;&gt; We can contrast this with the outcome
based idea where we're not doing this

00:03:04.037 --> 00:03:05.080
sort of bootstrapping.

00:03:05.080 --> 00:03:08.340
We're not using the estimate that
we've gotten at some other state.

00:03:08.340 --> 00:03:11.720
We actually use literally
the reward sequence that we saw.

00:03:12.760 --> 00:03:17.690
And so as a result of that, if we've
only seen a reward sequence once,

00:03:17.690 --> 00:03:21.190
like in the case of S2,
repeating that update over and

00:03:21.190 --> 00:03:22.345
over again doesn't change anything.

00:03:22.345 --> 00:03:27.770
&gt;&gt; Right, sure, sure, right because
the expectation is the expectation.

00:03:27.770 --> 00:03:31.360
&gt;&gt; Right, whereas in here,
we're actually using the intermediate

00:03:31.360 --> 00:03:34.620
estimates that we've computed and
refined on all the intermediate nodes.

00:03:34.620 --> 00:03:37.890
All the states that we've encountered
along the way to improve our estimate

00:03:37.890 --> 00:03:40.040
of the value of every other state,
right?

00:03:40.040 --> 00:03:42.710
So this kind of is more self consistent,
right?

00:03:42.710 --> 00:03:45.684
It's actually kind of connecting the
values of states to the other values of

00:03:45.684 --> 00:03:47.710
the states which is what you'd want.

00:03:47.710 --> 00:03:51.880
And this is more just literally
using the experience that it saw and

00:03:51.880 --> 00:03:53.870
ignoring the fact that there's
these intermediate states.

00:03:53.870 --> 00:03:55.250
&gt;&gt; Okay, well, when you put it that way,

00:03:55.250 --> 00:03:58.502
it makes it sound like you should
always use TD(0) and never use TD(1).

00:03:58.502 --> 00:04:04.047
&gt;&gt; Yes, but that's oversimplifying.

00:04:04.047 --> 00:04:06.390
&gt;&gt; Okay, can you under-simplify it?

00:04:06.390 --> 00:04:09.870
&gt;&gt; [LAUGH] We'll get to that.

00:04:09.870 --> 00:04:11.230
&gt;&gt; Oh, okay, good, I look forward to it.

