WEBVTT
Kind: captions
Language: en

00:00:00.280 --> 00:00:03.719
Alright, so what we'd like to do is work up to an algorithm that can

00:00:03.719 --> 00:00:05.660
actually do some of these inference steps instead

00:00:05.660 --> 00:00:08.140
of having to think it through each time

00:00:08.140 --> 00:00:12.820
de novo. So what I'm going to do is, let's hearken back to an example that we

00:00:12.820 --> 00:00:15.500
looked at before which is about spam detection.

00:00:15.500 --> 00:00:16.880
Do you, do you remember the spam example?

00:00:16.880 --> 00:00:18.640
&gt;&gt; I do remember the spam example. That

00:00:18.640 --> 00:00:20.270
was way back in the boosting lecture, right?

00:00:20.270 --> 00:00:23.510
&gt;&gt; Yes, I think you did that one. I did, it was an excellent example.

00:00:23.510 --> 00:00:25.420
&gt;&gt; There you go. So,

00:00:25.420 --> 00:00:27.530
we didn't think about it in a Bayes net setting,

00:00:27.530 --> 00:00:29.370
it was in a classification setting we were trying to

00:00:29.370 --> 00:00:31.260
come up with the rule, but let's think of this

00:00:31.260 --> 00:00:33.990
as a Bayes Net where there's a bunch of different variables

00:00:33.990 --> 00:00:36.860
that can be true or false about any given email

00:00:36.860 --> 00:00:39.930
message. It can either be spam or not. It can contain

00:00:39.930 --> 00:00:42.560
the word Viagra or not. It can contain the word

00:00:42.560 --> 00:00:46.240
prince or not. It maybe contains the word Udacity, or not.

00:00:46.240 --> 00:00:46.890
&gt;&gt; Mm.

00:00:46.890 --> 00:00:51.260
&gt;&gt; Right? And, so, just as we think about these as these random variables.

00:00:51.260 --> 00:00:54.060
If we're trying to build a belief net or a Bayes net

00:00:54.060 --> 00:00:57.740
with these variables. We have to say. kind of, what's dependent on what. In

00:00:57.740 --> 00:01:01.270
terms of representing the probabilities. So how would you, how do you

00:01:01.270 --> 00:01:05.099
think we should draw arrows to, to relate these to quantities to each other.

00:01:05.099 --> 00:01:10.740
&gt;&gt; I think that the arrows should go down from spam to

00:01:10.740 --> 00:01:17.090
the other features of spam mail and I'll tell you why. Because if,

00:01:17.090 --> 00:01:19.950
I like this notion of generation that you talked about a

00:01:19.950 --> 00:01:23.500
little bit earlier. It seems to me if you know. Spam

00:01:23.500 --> 00:01:29.730
mail or not. It sort of generates certain words. And as

00:01:29.730 --> 00:01:32.050
written as these are like words I mean I know the, the

00:01:32.050 --> 00:01:35.250
spam example these are you know, kind of stand ins for

00:01:35.250 --> 00:01:37.990
features. But they're sort of features of spam mail. Yeah I

00:01:37.990 --> 00:01:40.010
think that's a really good way to think about it. So,

00:01:40.010 --> 00:01:42.090
in some sense what we're saying if we draw the bayes net

00:01:42.090 --> 00:01:43.850
in this way, then any given email

00:01:43.850 --> 00:01:46.900
message has some probability of being spam. And

00:01:46.900 --> 00:01:48.990
given that it's spam, it has some

00:01:48.990 --> 00:01:51.730
probability of containing different sets of possible words.

00:01:51.730 --> 00:01:52.430
&gt;&gt; Right.

00:01:52.430 --> 00:01:56.710
&gt;&gt; So, I would say that, well what, so what do you, oh

00:01:56.710 --> 00:02:00.060
let's see if we can actually fill in some of these values. So

00:02:00.060 --> 00:02:02.120
given that we have a spam message, how likely do you think it

00:02:02.120 --> 00:02:05.350
would be to contain a word like, well let's say the word viagra.

00:02:05.350 --> 00:02:06.190
&gt;&gt; Fairly high.

00:02:06.190 --> 00:02:07.415
&gt;&gt; It might

00:02:07.415 --> 00:02:14.715
be 0.3, but a non-spam message might be, I don't know, like 0.001.

00:02:14.715 --> 00:02:15.280
&gt;&gt; Right.

00:02:15.280 --> 00:02:18.260
&gt;&gt; Something like that. So how about a word like prince?

00:02:18.260 --> 00:02:20.870
&gt;&gt; Well I get a lot of email about Prince because I'm a Prince fan.

00:02:20.870 --> 00:02:22.510
&gt;&gt; Yeah, I was thinking that. That's

00:02:22.510 --> 00:02:24.950
why I thought it would an interesting example.

00:02:24.950 --> 00:02:28.790
So, if in your spam messages, how likely is it for Prince to come up?

00:02:28.790 --> 00:02:30.213
&gt;&gt; Fairly low.

00:02:30.213 --> 00:02:33.130
&gt;&gt; Maybe like 0.2

00:02:33.130 --> 00:02:35.820
because you're talking about the Nigerian princes

00:02:35.820 --> 00:02:38.610
and whatnot. On the other hand among your

00:02:38.610 --> 00:02:42.540
non spam messages how likely is it for prince to come up, do you think?

00:02:42.540 --> 00:02:45.430
&gt;&gt; Well I get a lot of non spam, so,

00:02:45.430 --> 00:02:48.539
its still relatively low, but not as low as .001.

00:02:48.539 --> 00:02:50.923
&gt;&gt; Alright, so, let's say .1.

00:02:50.923 --> 00:02:51.150
&gt;&gt; Okay.

00:02:51.150 --> 00:02:54.120
&gt;&gt; That's a lot of prince spam.

00:02:54.120 --> 00:02:55.840
&gt;&gt; You can never have enough prince spam.

00:02:57.160 --> 00:02:58.380
&gt;&gt; Alright, so in the messages

00:02:58.380 --> 00:03:01.500
that you have that are spam, how often does the word Udacity come up?

00:03:01.500 --> 00:03:03.380
&gt;&gt; I guess, it's pretty low.

00:03:03.380 --> 00:03:05.120
&gt;&gt; I don't think I've ever seen a spam

00:03:05.120 --> 00:03:09.500
that mentions Udacity. Alright, what about your non-spam email?

00:03:09.500 --> 00:03:11.950
&gt;&gt; Again, increasingly, it's getting higher and higher.

00:03:11.950 --> 00:03:12.191
&gt;&gt; [LAUGH]

00:03:12.191 --> 00:03:14.370
&gt;&gt; Almost as much as I get prince mail.

00:03:14.370 --> 00:03:16.130
All right, so we'll call that .1 as well then.

00:03:16.130 --> 00:03:16.650
&gt;&gt; Okay.

00:03:16.650 --> 00:03:18.270
&gt;&gt; All right, so now we have, oh an,

00:03:18.270 --> 00:03:20.850
an what's the probability of spam versus not spam?

00:03:20.850 --> 00:03:23.730
&gt;&gt; [INAUDIBLE] Probability to have spam is pretty low,

00:03:23.730 --> 00:03:26.630
I'm going to say, at this point, actually; it's not

00:03:26.630 --> 00:03:29.890
that low. At this point, it's probably half my mail.

00:03:29.890 --> 00:03:33.290
&gt;&gt; Wow. All right, I'm going to say .4

00:03:33.290 --> 00:03:35.870
Alright, so this is now, Bayesian network structure

00:03:35.870 --> 00:03:38.690
that actually is, it's not exactly generating spam,

00:03:38.690 --> 00:03:40.500
but it is kind of capturing features of email

00:03:40.500 --> 00:03:42.420
messages as they come in. So, we should

00:03:42.420 --> 00:03:44.070
be able to answer questions like what's the

00:03:44.070 --> 00:03:46.400
probability that a given message is spam, given

00:03:46.400 --> 00:03:48.860
that the message has Viagra in it but not

00:03:48.860 --> 00:03:51.810
prince or udacity. So, how would we work this out?

00:03:51.810 --> 00:03:55.580
&gt;&gt; Well, Since it says Naive Bays I think I would use Bayes rule.

00:03:56.670 --> 00:03:59.260
&gt;&gt; That would be naive of you. Now we have applied

00:03:59.260 --> 00:04:02.080
Bayes rule, we have flipped things around, why is this giving

00:04:02.080 --> 00:04:05.640
us an advantage? For this kind of network structure it actually

00:04:05.640 --> 00:04:08.850
has a huge advantage because we can break this first quantity up.

00:04:08.850 --> 00:04:13.950
&gt;&gt; Oh I do see that, so this is where those conditional independences

00:04:13.950 --> 00:04:18.950
come into play If I'm reading this network right, each one of those attribute

00:04:18.950 --> 00:04:22.480
values is conditionally independent of each other,

00:04:22.480 --> 00:04:24.070
given that you know the value of SPAM.

00:04:24.070 --> 00:04:25.100
&gt;&gt; Excellent.

00:04:25.100 --> 00:04:29.210
&gt;&gt; So then that means that the first quantity there

00:04:29.210 --> 00:04:33.760
is actually a product of each of those conditional probabilities.

00:04:33.760 --> 00:04:36.400
&gt;&gt; Yeah, so this is a really convenient structure.

00:04:36.400 --> 00:04:39.500
Because it really just decomposes into all these separate

00:04:39.500 --> 00:04:42.880
helpful quantities. So in particular, we can actually derive

00:04:42.880 --> 00:04:46.260
this by applying the chain rule. But what we end

00:04:46.260 --> 00:04:49.380
up with is that this joint probability over these

00:04:49.380 --> 00:04:52.550
three variables decomposes into a product of three independent joint

00:04:52.550 --> 00:04:56.100
probabilities. The probability that's, Contains viagra given that it's

00:04:56.100 --> 00:04:59.230
spam, which we have. That number is 0.3. That probability

00:04:59.230 --> 00:05:01.040
that prince doesn't appear in it, given that it's

00:05:01.040 --> 00:05:04.830
spam and that is that it doesn't contain prince given

00:05:04.830 --> 00:05:09.655
that it is spam. So that should 0.8, cause 1 minus the

00:05:09.655 --> 00:05:14.870
0.2. And that it's not udacity given that it's spam. Is

00:05:14.870 --> 00:05:19.971
going to be 1 minus this 0.0001, should be 0.9999. All right.

00:05:19.971 --> 00:05:25.750
So this is the case when things, when it is spam, and if it's not spam, we

00:05:25.750 --> 00:05:27.840
can do this same thing and get a product,

00:05:27.840 --> 00:05:29.920
and that we can normalize, to get what the,

00:05:29.920 --> 00:05:34.280
the relative probabilities between it being spam and not spam. So then I'm a

00:05:34.280 --> 00:05:37.910
big fan of normalization, but of course this makes me think about, since it's

00:05:37.910 --> 00:05:40.030
sort of a classification problem, we only

00:05:40.030 --> 00:05:42.360
really care about knowing which one's more

00:05:42.360 --> 00:05:43.650
likely. We don't really care about the

00:05:43.650 --> 00:05:45.310
probability, right? Do we have to normalize?

00:05:45.310 --> 00:05:47.470
&gt;&gt; Yeah, yeah because we do care about the probability.

00:05:47.470 --> 00:05:48.390
&gt;&gt; Oh we do?

00:05:48.390 --> 00:05:50.260
&gt;&gt; Yeah because we're... I asked" What is the

00:05:50.260 --> 00:05:53.160
probability of spam given these other quantities. Oh, I see.

00:05:53.160 --> 00:05:55.090
&gt;&gt; But you're right. So the observation

00:05:55.090 --> 00:05:57.730
that you're making is a really good one. Which is that we

00:05:57.730 --> 00:05:59.610
can do probability calculations in this

00:05:59.610 --> 00:06:01.010
setting, and that's actually going to give

00:06:01.010 --> 00:06:03.810
us answers to classification problems. And we're going to connect this back to

00:06:03.810 --> 00:06:07.480
machine learning. But but first let's write a general form of this formula.

00:06:07.480 --> 00:06:08.220
&gt;&gt; Okay.

00:06:08.220 --> 00:06:11.770
&gt;&gt; Because this this seems a little bit specific. Alright so

00:06:11.770 --> 00:06:13.400
the general form for this, is that if we're trying to figure

00:06:13.400 --> 00:06:15.780
out the probability of, of some kind of a a root node

00:06:15.780 --> 00:06:20.250
like this, when you have all these little bristly things coming down.

00:06:20.250 --> 00:06:21.550
You can think of it as a probability of a

00:06:21.550 --> 00:06:25.110
value given a bunch of attributes. And that's going to be equal

00:06:25.110 --> 00:06:27.980
to the product of the probability that each of those

00:06:27.980 --> 00:06:32.640
attributes would be generated by that. Underlying this v. This, this

00:06:32.640 --> 00:06:35.740
the label or the or the underlying class. Times the

00:06:35.740 --> 00:06:38.500
prior probability that v and then we just normalize by all

00:06:38.500 --> 00:06:41.920
the different possible values of, of v. This, this quantity across

00:06:41.920 --> 00:06:45.660
all the possible types of v. So so this is one

00:06:45.660 --> 00:06:50.306
way of actually getting a very general kind of inference done,

00:06:50.306 --> 00:06:53.020
and there's, as you were pointing out, Charles, there's a. There's

00:06:53.020 --> 00:06:55.940
a really nice reason to think about things in this form,

00:06:55.940 --> 00:06:58.550
because it does let you do a kind of classification. So

00:06:58.550 --> 00:07:01.810
essentially if you think of, of this top node as being

00:07:01.810 --> 00:07:03.800
the class, this is what was playing the role of V

00:07:03.800 --> 00:07:07.570
here, and these are all a bunch of attributes, then even

00:07:07.570 --> 00:07:11.380
if, if we have a way of generating attribute values from classes.

00:07:11.380 --> 00:07:13.350
What this let's us do is to go the other way.

00:07:13.350 --> 00:07:16.310
That we observe the attribute values and we can infer the class.

00:07:16.310 --> 00:07:18.670
&gt;&gt; Nice, so what's the equation for that?

00:07:18.670 --> 00:07:20.980
&gt;&gt; Right, so the, the maximum oposterior

00:07:20.980 --> 00:07:22.700
class if you're just trying to find whats

00:07:22.700 --> 00:07:27.390
the most likely class given the, the data that you've seen. You can just take

00:07:27.390 --> 00:07:31.430
an arg max over all the different possible values of that, that root node of

00:07:31.430 --> 00:07:33.880
the prob, its probability times the product

00:07:33.880 --> 00:07:36.650
of all the attribute values given that class.

00:07:36.650 --> 00:07:39.820
So this would actually let us if you're, if you're been paying attention,

00:07:39.820 --> 00:07:45.770
we could, in this particular case, compute map spam. Which is a palindrome.

00:07:45.770 --> 00:07:48.650
&gt;&gt; Wow. That is spectacular.

00:07:48.650 --> 00:07:50.350
&gt;&gt; You did not see that coming did you?

00:07:50.350 --> 00:07:51.680
&gt;&gt; No I did not.

