WEBVTT
Kind: captions
Language: en

00:00:00.180 --> 00:00:03.610
Usually when researchers
are evaluating a learning algorithm,

00:00:03.610 --> 00:00:05.560
they split their data into two chucks.

00:00:05.560 --> 00:00:08.700
The training chunk, and a testing chunk.

00:00:08.700 --> 00:00:13.409
Training usually is about 60% of
the data, and testing is about 40%.

00:00:13.409 --> 00:00:16.600
Now if you train and
then test on that data,

00:00:16.600 --> 00:00:21.690
that's one trial and in many
cases that's enough, you measured

00:00:21.690 --> 00:00:25.330
your root means square error and
that's an assessment of your algorithm.

00:00:25.330 --> 00:00:27.410
You might compare it
against another algorithm.

00:00:27.410 --> 00:00:30.490
But one problem researchers
sometimes encounter

00:00:30.490 --> 00:00:34.380
is they don't have enough data to
effectively analyze their algorithm.

00:00:34.380 --> 00:00:38.890
One thing they can do is effectively
create more data by slicing it up and

00:00:38.890 --> 00:00:40.170
running more trials.

00:00:40.170 --> 00:00:41.700
Here's how that works.

00:00:41.700 --> 00:00:47.170
So what we can do is we can slice our
data into say five different chunks,

00:00:47.170 --> 00:00:52.319
and then we can train here on 80%
of the data, and test on 20%.

00:00:52.319 --> 00:00:54.230
That's one trial.

00:00:54.230 --> 00:00:58.510
Then we can switch things up and
train on this 80% of the data.

00:00:58.510 --> 00:01:01.840
And test on that,
that's another trial, and so on.

00:01:01.840 --> 00:01:04.300
I'm sure you see how this is going.

00:01:04.300 --> 00:01:08.110
We can effectively get five different
trials out of this one set of data.

