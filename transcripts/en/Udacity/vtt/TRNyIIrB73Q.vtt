WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:06.000
In this question, you were asked to limit the depth the crawler searches to.

00:00:06.000 --> 00:00:11.000
In order to understand this question and what we need to do,

00:00:11.000 --> 00:00:19.000
I'm going to draw a graph of the links, so I'm going to start off with a seed page A,

00:00:19.000 --> 00:00:29.000
and from A I have links to another page B and another page C.

00:00:29.000 --> 00:00:35.000
Now B links to D, and C links to E.

00:00:35.000 --> 00:00:41.000
Now, just to make things a little tricky, E is going to link to D.

00:00:41.000 --> 00:00:45.000
Finally, D links to F.

00:00:45.000 --> 00:00:47.000
What do we actually need to do?

00:00:47.000 --> 00:00:55.000
Well, we need to keep track of what pages we have left to crawl, just like before.

00:00:55.000 --> 00:00:58.000
We need to keep track of the pages we've already crawled,

00:00:58.000 --> 00:01:02.000
because once we've crawled them we don't really want to have to crawl them again.

00:01:02.000 --> 00:01:10.000
I'm going to introduce a new list where we keep track of the pages at the next depth.

00:01:10.000 --> 00:01:14.000
I'm just going to call that next_depth.

00:01:14.000 --> 00:01:19.000
We start off at tocrawl, and we have just the seed page.

00:01:19.000 --> 00:01:23.000
Now, we haven't crawled anything yet, and we don't know what there is to come either.

00:01:23.000 --> 00:01:29.000
We start off with just tocrawl with the seed page in it.

00:01:29.000 --> 00:01:37.000
Now, from there we look at A, and we see that there are two pages that A links to.

00:01:37.000 --> 00:01:45.000
We're at A, and the next depth--the next pages at depth 1--are B and C.

00:01:45.000 --> 00:01:51.000
We're actually going to move A from tocrawl to crawled,

00:01:51.000 --> 00:01:53.000
because we've actually looked at it now.

00:01:53.000 --> 00:02:00.000
We've seen that the next pages are C and B.

00:02:00.000 --> 00:02:07.000
Those are going to be the ones that we crawl next. I'm going to put those into tocrawl.

00:02:07.000 --> 00:02:10.000
We'll look at those next.

00:02:10.000 --> 00:02:19.000
If we look at B, B has a neighbor D, and then C has a neighbor E.

00:02:19.000 --> 00:02:24.000
We do this in a couple of steps, but just to get the idea how the code is going to work.

00:02:24.000 --> 00:02:29.000
I'm just going to go through it looking at what's at each depth.

00:02:29.000 --> 00:02:37.000
B and C we've now crawled, so those two both go in to crawled.

00:02:37.000 --> 00:02:42.000
Now the ones at the next depth are D and E, so those are the ones we want to look at.

00:02:42.000 --> 00:02:45.000
We're going to move D and E over here,

00:02:45.000 --> 00:02:50.000
and then we're going to look at the next pages of D and E.

00:02:50.000 --> 00:02:58.000
D has neighbor F, and E has a neighbor D. Those are at depth 3.

00:02:58.000 --> 00:03:03.000
Now, we might not want to add D in here, because actually we've already look at D,

00:03:03.000 --> 00:03:08.000
but we'll see when we get to the actual code how we're going to handle that.

00:03:08.000 --> 00:03:15.000
Now we take D and E and put them into crawled.

00:03:15.000 --> 00:03:19.000
Then we've got F and D over here,

00:03:19.000 --> 00:03:24.000
and we can carry on, depending on what depth we want to go to.

00:03:24.000 --> 00:03:27.000
That's the idea behind how the code is going to work.

00:03:27.000 --> 00:03:30.000
We're going to start off with a page.

00:03:30.000 --> 00:03:34.000
We're going to crawl it and everything else at the same depth,

00:03:34.000 --> 00:03:41.000
and then add those pages to next depth and switch the lists around,

00:03:41.000 --> 00:03:44.000
which hopefully you'll be able to see more clearly in the code.

00:03:44.000 --> 00:03:51.000
This is the supplied code for crawl_web, and we're going to make a few changes

00:03:51.000 --> 00:03:56.000
to this to take account of the depth. So what do we do first?

00:03:56.000 --> 00:03:59.000
Well, we're going to need another list next depth,

00:03:59.000 --> 00:04:05.000
which will keep track of the next level of links.

00:04:05.000 --> 00:04:10.000
When we start we have a depth of 0.

00:04:10.000 --> 00:04:15.000
We're going to carry on through the while loop while there's something in tocrawl,

00:04:15.000 --> 00:04:18.000
because if there's nothing to crawl, then we can't carry on.

00:04:18.000 --> 00:04:27.000
Also while our depth is at most the maximum depth that's supplied as an input to the function.

00:04:27.000 --> 00:04:32.000
Just as before, we're going to pop from tocrawl,

00:04:32.000 --> 00:04:37.000
and we're not going to make any changes to this line either,

00:04:37.000 --> 00:04:42.000
but instead of adding to tocrawl--because we don't want to muddle up

00:04:42.000 --> 00:04:48.000
the pages we're searching through now and the pages at the next level--

00:04:48.000 --> 00:04:53.000
we're going to actually add the new links to next depth instead.

00:04:53.000 --> 00:04:59.000
Then just as before, we're going to add our page that we've crawled

00:04:59.000 --> 00:05:02.000
to the crawled list so that we know we've already looked through it,

00:05:02.000 --> 00:05:04.000
and we're not going to look through it again.

00:05:04.000 --> 00:05:08.000
Now, the last bit of code that we need to add--

00:05:08.000 --> 00:05:12.000
when we've finished going through all the lists at a given level,

00:05:12.000 --> 00:05:15.000
just like I showed you in the example,

00:05:15.000 --> 00:05:21.000
we're then going to change our tocrawl list and our depth list over.

00:05:21.000 --> 00:05:25.000
So tocrawl is now empty.

00:05:25.000 --> 00:05:32.000
Next depth contains all the new links, and I'm going to use a multiple assignment

00:05:32.000 --> 00:05:37.000
where I swap tocrawl and next_depth.

00:05:37.000 --> 00:05:42.000
Tocrawl becomes equal to next_depth and next_depth is going to be empty again,

00:05:42.000 --> 00:05:49.000
so that we can start the while loop again with a clean list.

00:05:49.000 --> 00:05:53.000
Then depth we're going to increase by 1.

00:05:53.000 --> 00:05:55.000
Then we're going to return crawled.

00:05:55.000 --> 00:06:03.000
Just to quickly recap--we carry on through the while loop as long as we've got some page

00:06:03.000 --> 00:06:08.000
to crawl and our depth is less than or equal to max_depth.

00:06:08.000 --> 00:06:12.000
We take page, and if we haven't crawled the page before,

00:06:12.000 --> 00:06:16.000
we add all the links from the page to the next_depth.

00:06:16.000 --> 00:06:22.000
Then we take our page, and we put it in the crawled list.

00:06:22.000 --> 00:06:26.000
We keep doing this. If we haven't got any pages left to crawl, then we switch them around.

00:06:26.000 --> 00:06:31.000
But if we have got pages left to crawl, we just carry on with the while loop

00:06:31.000 --> 00:06:34.000
until tocrawl is empty.

00:06:34.000 --> 00:06:39.000
Once tocrawl is empty, that means that that level is finished, that depth is finished,

00:06:39.000 --> 00:06:42.000
and we want to go on to start the next depth, and that's it.

