WEBVTT
Kind: captions
Language: en

00:00:00.310 --> 00:00:02.910
So why might we care to do this, Michael? So,

00:00:02.910 --> 00:00:05.130
there's a bunch of reasons but really it boils down

00:00:05.130 --> 00:00:08.360
to two. One is really about human beings and the

00:00:08.360 --> 00:00:11.660
other is really about machines and machine learning algorithms. And the

00:00:11.660 --> 00:00:14.330
first one really boils down to what I'll just call

00:00:14.330 --> 00:00:19.000
knowledge discovery, alright. It is often useful when you're thinking about

00:00:19.000 --> 00:00:21.000
a set of data in a problem, to be able

00:00:21.000 --> 00:00:25.590
to interpret the features. To figure out well, which subset features

00:00:25.590 --> 00:00:27.560
that I've been keeping track of say, when then

00:00:27.560 --> 00:00:31.220
I'm using on my sensors actually matter. And so really

00:00:31.220 --> 00:00:34.320
the feature selection problem for what other reasons you might

00:00:34.320 --> 00:00:37.070
think of doing it or often for interpretability and for

00:00:37.070 --> 00:00:40.970
insight. So it turns out that of the 1,000

00:00:40.970 --> 00:00:44.820
features that I keep track of only a few matter.

00:00:44.820 --> 00:00:47.650
Maybe only 10 of them matter for doing solving some

00:00:47.650 --> 00:00:50.850
problem of prediction. So let's think about our spam case

00:00:50.850 --> 00:00:53.640
for example. There are lots and lots and lots of features that

00:00:53.640 --> 00:00:56.430
we might care about in spam case, but it may turn out

00:00:56.430 --> 00:00:59.120
that after we do some kind of analysis that it really all

00:00:59.120 --> 00:01:03.160
boils down to something like whether you're getting mail from a Nigerian

00:01:03.160 --> 00:01:07.200
prince, for example. And some of these things that you thought mattered.

00:01:07.200 --> 00:01:11.120
Don't seem to matter. For for the purposes of solving some problem

00:01:11.120 --> 00:01:13.450
or, or for doing prediction. And, so, often, this is a very,

00:01:13.450 --> 00:01:15.850
very useful thing to do. You do this kind of thing all

00:01:15.850 --> 00:01:17.300
the time, Michael. Actually, if you go back

00:01:17.300 --> 00:01:18.940
and you look at some of the things that

00:01:18.940 --> 00:01:21.080
we have been giving for examples. You come up

00:01:21.080 --> 00:01:23.840
with problems and features taht are, for example, visible

00:01:23.840 --> 00:01:25.540
to two dimensions as opposed to three or

00:01:25.540 --> 00:01:28.200
four or five becdause it's easy for you to

00:01:28.200 --> 00:01:31.120
visualize them and to understand them. And for students

00:01:31.120 --> 00:01:33.450
to understand how the algorithms work in two d.

00:01:33.450 --> 00:01:34.520
&gt;&gt; Hm that makes sense.

00:01:34.520 --> 00:01:36.430
&gt;&gt; Does that all make sense? Okay. So that's one thing and there's, and

00:01:36.430 --> 00:01:38.540
this really should not be underestimated. People

00:01:38.540 --> 00:01:40.890
tend to ignore this a lot in the,

00:01:40.890 --> 00:01:43.880
in the, at least in the machine learning field, much less in the data

00:01:43.880 --> 00:01:47.910
mining field but this really is a sort of key and an important issue.

00:01:47.910 --> 00:01:51.390
But there's another reason we might care about if it's algorithmic and that is

00:01:51.390 --> 00:01:53.650
the curse of dimensionality. So do you

00:01:53.650 --> 00:01:55.160
remember what the Curse of Dimensionality is Michael?

00:01:55.160 --> 00:01:59.080
&gt;&gt; Yeah, as you add more features you may need

00:01:59.080 --> 00:02:01.550
exponentially need more data to kind of fill out the space.

00:02:01.550 --> 00:02:06.340
&gt;&gt; The Curse of Dimensionality says that the amount of data

00:02:06.340 --> 00:02:08.190
that you need grows exponentially in the

00:02:08.190 --> 00:02:10.970
number of features that you have. So it'd

00:02:10.970 --> 00:02:12.410
be really nice if you could have

00:02:12.410 --> 00:02:14.910
fewer features. So the problem of feature selection

00:02:14.910 --> 00:02:18.930
helps us, hopefully at least in a, in a nice world. To go from a

00:02:18.930 --> 00:02:21.550
whole bunch of features to just a few

00:02:21.550 --> 00:02:24.340
features, thus making the learning problem actually easier.

00:02:24.340 --> 00:02:27.750
&gt;&gt; [LAUGH] Looks a little bit like a smile, an alien smiley face now.

00:02:27.750 --> 00:02:30.140
&gt;&gt; That's right. So we came with an alien frowny face

00:02:30.140 --> 00:02:31.590
in the beginning and [LAUGH] then we [INAUDIBLE] with an alien

00:02:31.590 --> 00:02:34.190
smiley face. Okay, so you buy these reasons Michael?

00:02:34.190 --> 00:02:36.740
&gt;&gt; Yeah, I like how you managed to, you know if X

00:02:36.740 --> 00:02:39.790
is the input space here, you've managed to map X to Y.

00:02:39.790 --> 00:02:44.140
&gt;&gt; Michael uses the feature transformation function known as the pun.

00:02:44.140 --> 00:02:44.190
&gt;&gt; Mmm.

00:02:44.190 --> 00:02:49.750
&gt;&gt; That's really good, I like that. Okay. Anyway. Okay,

00:02:49.750 --> 00:02:52.200
so let's keep all this in mind when we're talking

00:02:52.200 --> 00:02:53.800
about all the algorithms that we're talking about in this

00:02:53.800 --> 00:02:56.910
lesson, and even in the future transformation lesson. The goal

00:02:56.910 --> 00:03:01.690
is to take, be able to use, in general, a bunch of features, because

00:03:01.690 --> 00:03:04.770
you think they might all be important, and then apply some kind of algorithm to

00:03:04.770 --> 00:03:08.320
get to just the important features. And if you can do that well, then not

00:03:08.320 --> 00:03:10.370
only will you understand your data better,

00:03:10.370 --> 00:03:12.330
But you'll also have an easier learning problem.

00:03:12.330 --> 00:03:13.090
&gt;&gt; Got it.

