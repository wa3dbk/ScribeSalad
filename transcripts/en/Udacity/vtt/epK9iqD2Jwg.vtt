WEBVTT
Kind: captions
Language: en

00:00:00.565 --> 00:00:03.356
So in the last problem of this homework assignment,

00:00:03.356 --> 00:00:06.202
we’re going to take a look at the complexity of

00:00:06.202 --> 00:00:09.731
parsing certain strings of input under certain

00:00:09.731 --> 00:00:12.942
grammars. Is it the case that certain grammars are

00:00:12.942 --> 00:00:15.797
more difficult to parse with the algorithms that we’ve

00:00:15.797 --> 00:00:19.604
given? Is it case that certain strings are more difficult

00:00:19.604 --> 00:00:23.998
to parse with the same algorithms? And just in case it

00:00:23.998 --> 00:00:27.595
wasn’t clear, this is definitely a challenge problem

00:00:27.595 --> 00:00:31.949
and very, very difficult. So please don’t be upset.

00:00:31.949 --> 00:00:34.623
If you want to able to get a good solution, if you dig

00:00:34.623 --> 00:00:36.794
your working solution I hope you shattered on the

00:00:36.794 --> 00:00:40.314
forms with your fellow students and discuss how you

00:00:40.314 --> 00:00:43.795
got there. So to recap, we’re trying to answer the

00:00:43.795 --> 00:00:47.479
question, how much work is it the parse? And before

00:00:47.479 --> 00:00:49.641
we answer that, we need to decide in someway to

00:00:49.641 --> 00:00:54.331
measure this work. We could take the time that it

00:00:54.331 --> 00:00:58.405
takes to run the parsing, maybe measure in seconds or

00:00:58.405 --> 00:01:02.528
minutes. We could take the energy when you have a

00:01:02.528 --> 00:01:06.265
web browser and something like my phone, then we

00:01:06.265 --> 00:01:10.180
need to decide whether or not our web browser is

00:01:10.180 --> 00:01:14.448
efficient enough not to drain my phone four hours

00:01:14.448 --> 00:01:17.147
into the day. I’m sure that happen to some of you

00:01:17.147 --> 00:01:22.050
before. So this might be measured in lots or some of

00:01:22.050 --> 00:01:24.059
the measurement of power.

00:01:24.059 --> 00:01:26.595
There is something wrong with each of these forms of

00:01:26.595 --> 00:01:29.103
measurements. And that is it doesn’t really get at the

00:01:29.103 --> 00:01:33.015
algorithm. If I buy a faster computer, does that mean

00:01:33.015 --> 00:01:37.699
my parser is better? No. if you build a more energy

00:01:37.699 --> 00:01:41.654
efficient phone, does that mean the parser is better?

00:01:41.654 --> 00:01:45.166
Not really. So measuring in such units would imply

00:01:45.166 --> 00:01:48.831
that our algorithms get better over time as the

00:01:48.831 --> 00:01:52.655
hardware does. But the truth is many algorithms

00:01:52.655 --> 00:01:54.313
haven’t gone better since when they were first

00:01:54.313 --> 00:01:57.697
developed and say the 70s. So we want a union of

00:01:57.697 --> 00:02:00.261
measurements that is independent on the hardware

00:02:00.261 --> 00:02:04.262
we’re running on. That way if I test my parser, I can

00:02:04.262 --> 00:02:06.544
compare it to your parser even though it’s very

00:02:06.544 --> 00:02:09.542
unlikely we’re running the exact same computer.

00:02:09.542 --> 00:02:11.647
Instead we’re going to find our own unit of

00:02:11.647 --> 00:02:12.706
measurement.

00:02:12.706 --> 00:02:15.289
For now let’s call it a work operation. We’re going to

00:02:15.289 --> 00:02:18.994
count the number of work operations it takes to parse,

00:02:18.994 --> 00:02:21.786
a given list of, a given string and our grammar. For

00:02:21.786 --> 00:02:25.870
this problem you were asked to give two things, a

00:02:25.870 --> 00:02:34.188
grammar and a list of tokens that takes at least two X,

00:02:34.188 --> 00:02:41.824
X, X work operations to parse where X is going to be

00:02:41.824 --> 00:02:48.256
defined as the maximum number of input tokens or

00:02:48.256 --> 00:02:51.687
the size of the largest grammar rule or the number of

00:02:51.687 --> 00:02:52.887
rules.

00:02:52.887 --> 00:02:56.589
So why is this definition special? It means that we

00:02:56.589 --> 00:03:00.589
can’t just give a really long list of tokens and expect

00:03:00.589 --> 00:03:06.717
to reach two times what is essentially x3 work

00:03:06.717 --> 00:03:09.785
operations. We have to find something that given the

00:03:09.785 --> 00:03:14.197
relative size of the grammar to tokens and the rules is

00:03:14.197 --> 00:03:18.521
more work to parse. The key to solving this problem

00:03:18.521 --> 00:03:22.919
is actually ambiguity. It turns out ambiguity really

00:03:22.919 --> 00:03:26.704
slows down the parsing process, given the set of

00:03:26.704 --> 00:03:30.558
rules that we’ve defined for parsing a given list of

00:03:30.558 --> 00:03:31.811
tokens.

00:03:31.811 --> 00:03:34.827
So let’s look at a really ambiguous grammar.

00:03:34.827 --> 00:03:39.432
This grammar matches a sequence of Xs, but

00:03:39.432 --> 00:03:41.725
because of this rule right here it’s very, very

00:03:41.725 --> 00:03:42.957
ambiguous.

00:03:42.957 --> 00:03:47.457
As it turns out that if you have the or more Xs in a

00:03:47.457 --> 00:03:51.452
row to actually break the two times x3 mark for

00:03:51.452 --> 00:03:52.931
parsing.

00:03:52.931 --> 00:03:55.318
But for the sake of demonstrating this,

00:03:55.318 --> 00:03:59.246
I’m going to just show you what the parse chart looks

00:03:59.246 --> 00:04:01.454
like with five Xs.

00:04:01.454 --> 00:04:05.134
So here I’ve taken the code given in the problem

00:04:05.134 --> 00:04:08.688
uncommented dissection that prints the chart and

00:04:08.688 --> 00:04:12.375
I’ve run the grammar that we had on a string

00:04:12.375 --> 00:04:13.525
of five tokens.

00:04:13.525 --> 00:04:16.313
The results, is as follows. Right from the get

00:04:16.313 --> 00:04:19.428
go there is a lot different rules that we can match.

00:04:19.428 --> 00:04:22.657
And with every step we keep getting more and more

00:04:22.657 --> 00:04:26.464
possibilities due to all the ambiguity. Everywhere that

00:04:26.464 --> 00:04:29.369
we could possibly have a P or putting in two more Ps

00:04:29.369 --> 00:04:32.883
and then from there even two more Ps. As it turns

00:04:32.883 --> 00:04:37.020
out for N characters we get about n2 possible

00:04:37.020 --> 00:04:38.521
Pastries.

00:04:38.521 --> 00:04:41.847
And that means the size of our chart alone includes a

00:04:41.847 --> 00:04:45.166
lot more than the possible Pastries and we do a lot

00:04:45.166 --> 00:04:48.539
more work operations as we try to go through that.

00:04:48.539 --> 00:04:54.826
So by the end, we’ve done 3,237 units of work. Now

00:04:54.826 --> 00:04:57.727
if we increase the number of tokens to something like

00:04:57.727 --> 00:05:04.435
50, we get a very long parse chart. And that will just

00:05:04.435 --> 00:05:09.522
keep scrolling and so hit the bottom. Get some

00:05:09.522 --> 00:05:16.833
longer and longer and longer, and oh my gosh,

00:05:16.833 --> 00:05:18.249
does it end.

00:05:18.249 --> 00:05:22.004
Here we go. Yeah, it gets pretty big. One thing to

00:05:22.004 --> 00:05:25.490
note that I didn’t do in my examples was that

00:05:25.490 --> 00:05:28.370
the length of your input needed to be greater than 10

00:05:28.370 --> 00:05:30.236
and less than 50.

00:05:30.236 --> 00:05:32.055
And by inputting mean the number X. So here I

00:05:32.055 --> 00:05:36.302
had Xs 50 and before I had a 5, but that’s why I

00:05:36.302 --> 00:05:38.183
neither of these were accepted. So if you did

00:05:38.183 --> 00:05:42.832
something say, 11 you will get a success that

00:05:42.832 --> 00:05:46.017
encourages you to copy and submit it. I hope you

00:05:46.017 --> 00:05:47.882
had fun doing this problem and it’s a bit more

00:05:47.882 --> 00:05:49.318
creative than what we’ve had in the past.

00:05:49.318 --> 00:05:51.618
And there is many, many right answers.

00:05:51.618 --> 00:05:53.836
I hope to see some of those answers on the form.

00:05:53.836 --> 00:05:57.836
Until then bye, bye. I will end with this manifest.

