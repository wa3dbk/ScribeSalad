WEBVTT
Kind: captions
Language: en

00:00:00.240 --> 00:00:02.160
&gt;&gt; Alright, so back to boosting, Michael. So as

00:00:02.160 --> 00:00:04.650
you recall the little teaser I left you with last

00:00:04.650 --> 00:00:07.800
time, is that it appears that boosting does not

00:00:07.800 --> 00:00:11.680
always over-fit. And a little graph. That's true, but it

00:00:11.680 --> 00:00:13.940
doesn't seem to over-fit in the ways that we

00:00:13.940 --> 00:00:16.910
would normally expect it to over-fit. And in particular we'd

00:00:16.910 --> 00:00:21.630
see a, you know, an error line on training And

00:00:21.630 --> 00:00:25.280
what we expect to see is a testing line that

00:00:25.280 --> 00:00:27.190
would, you know, hue pretty closely and then start to

00:00:27.190 --> 00:00:31.720
get bad. But what actually happens is that instead, this

00:00:31.720 --> 00:00:35.160
little bit at the end where you get over fitting

00:00:35.160 --> 00:00:39.470
seems to instead. Just keep doing well. In fact, getting better

00:00:39.470 --> 00:00:41.820
and better and better. And I promised you an explanation

00:00:41.820 --> 00:00:44.790
for why that was. So, given what we talked about

00:00:44.790 --> 00:00:47.000
with support vector machines, and what we spent most of

00:00:47.000 --> 00:00:49.960
our time thinking about, what do you think the answer is?

00:00:49.960 --> 00:00:50.960
&gt;&gt;

00:00:49.960 --> 00:00:53.180
Well I, I don't think I would have asked again if I, had

00:00:53.180 --> 00:00:55.880
a thought about it. But you mean You want me to connect it

00:00:55.880 --> 00:00:59.730
to support vector machines, somehow. Well the, the thing that was fighting over

00:00:59.730 --> 00:01:02.190
fitting in support vector machines, was

00:01:02.190 --> 00:01:05.050
trying to focus on maximum margin classifiers.

00:01:05.050 --> 00:01:07.360
&gt;&gt; Here, let me, let me try to explain to you

00:01:07.360 --> 00:01:10.550
why it is that you don't have this problem with With

00:01:10.550 --> 00:01:12.670
overfitting at least not in the, in the typical way as

00:01:12.670 --> 00:01:15.000
you keep applying it over and over again like you do

00:01:15.000 --> 00:01:17.990
with something like neural networks. And it really boils down

00:01:17.990 --> 00:01:21.950
to noticing that we've been ignoring some information. So, what we

00:01:21.950 --> 00:01:26.150
normally keep track of is error. So error on say

00:01:26.150 --> 00:01:30.300
a training set is just, you know, the The probability that

00:01:30.300 --> 00:01:32.510
you're going to come up with an incorrect answer or

00:01:32.510 --> 00:01:35.050
come up with an answer that disagrees with your training set.

00:01:35.050 --> 00:01:36.920
and that's a very natural thing to think about and it

00:01:36.920 --> 00:01:40.230
makes a lot of sense. But there's also something else that

00:01:40.230 --> 00:01:43.070
is actually captured inside of boosting and captured by a lot of learning

00:01:43.070 --> 00:01:44.620
algorithms we haven't been taking advantage

00:01:44.620 --> 00:01:46.209
of, and that's the notion of confidence.

00:01:47.620 --> 00:01:50.620
So confidence is not just whether you got it right or wrong. It's

00:01:50.620 --> 00:01:54.950
how strongly you believe in a particular answer that you given. Make sense?

00:01:54.950 --> 00:01:57.830
&gt;&gt; Yes, a lot of the algorithms we talked

00:01:57.830 --> 00:02:00.250
indirectly have something like that. So, like in a nearest

00:02:00.250 --> 00:02:02.620
neibhbor method, if you are doing five nearest neighbor

00:02:02.620 --> 00:02:05.420
and all five of the neighbor agree, that seems different

00:02:05.420 --> 00:02:08.570
than the case with vote one way and two vote the other.

00:02:08.570 --> 00:02:10.960
&gt;&gt; Right. And in fact, that's a really good example. If you think of that in

00:02:10.960 --> 00:02:13.550
terms of regression Then you could say something

00:02:13.550 --> 00:02:17.850
like the variance, between them is sort of

00:02:17.850 --> 00:02:20.230
a stand in for confidence. Low variance means

00:02:20.230 --> 00:02:23.780
everyone agrees, high variance means, there's some major

00:02:23.780 --> 00:02:26.270
disagreement. Okay. So what does that mean in

00:02:26.270 --> 00:02:30.600
the boosting case? Well as you recall, the final

00:02:30.600 --> 00:02:35.490
output of the boosted classifier is given by a very simple formula.

00:02:39.430 --> 00:02:44.510
And here's the equation here that h of x is equal to the sine of

00:02:44.510 --> 00:02:49.520
the sum over all of the weak hypothesis that you've gotten of alpha

00:02:49.520 --> 00:02:54.990
times h. So the weighted average of all of the hypothesis, right? And

00:02:54.990 --> 00:02:57.885
you just simply, if it's positive you produce a plus

00:02:57.885 --> 00:03:00.050
one. And if it's it negative you produce a minus and

00:03:00.050 --> 00:03:01.870
if it's exactly zero you don't know what to do

00:03:01.870 --> 00:03:05.190
so you just. Produces zero. Just throw up your hands. So

00:03:05.190 --> 00:03:08.211
I'm going to make a tiny change to this formula, Michael. Just, just for

00:03:08.211 --> 00:03:09.950
the purpose of sort of, explanation, that

00:03:09.950 --> 00:03:12.930
doesn't change the fundamental answer. And I'm

00:03:12.930 --> 00:03:16.420
just going to take exactly this equation as it is. And I'm going to divide

00:03:16.420 --> 00:03:19.980
it, by the weights that we use. Now what does that end up doing?

00:03:19.980 --> 00:03:23.380
&gt;&gt; Okay, so the weights. I'm getting a.

00:03:23.380 --> 00:03:26.110
There's Alphas in the SVM's too, so I'm

00:03:26.110 --> 00:03:28.130
getting a little confused. So that I'm. I

00:03:28.130 --> 00:03:30.563
think these Alphas all have to be non-negative.

00:03:30.563 --> 00:03:31.470
&gt;&gt; Right.

00:03:31.470 --> 00:03:35.270
&gt;&gt; But they kind of like this support vector values, in that

00:03:35.270 --> 00:03:38.750
there could be zero, if, if that hypothesis isn't come into play?

00:03:38.750 --> 00:03:41.230
&gt;&gt; Well, but they want in that case, the, the

00:03:41.230 --> 00:03:44.840
alpha is always set to be the natural log of something.

00:03:44.840 --> 00:03:48.620
&gt;&gt; Oh, oh, oh, and also these alphas are applied to hypothesis whereas

00:03:48.620 --> 00:03:51.980
the alphas in the, in the SVM settings were being applied to data points.

00:03:51.980 --> 00:03:54.030
&gt;&gt; That's right. So, unfortunately in machine

00:03:54.030 --> 00:03:56.415
learning, people in, invent things separately and

00:03:56.415 --> 00:04:00.970
re-use notation. Alpha's an easy Greek character to draw, so people use

00:04:00.970 --> 00:04:04.020
it all the time. But here, remember, alpha's the measure of how good

00:04:04.020 --> 00:04:07.670
a particular weak hypothesis was, and since it has to do better

00:04:07.670 --> 00:04:10.630
than chance, it works out that it will always be greater than zero.

00:04:10.630 --> 00:04:13.690
&gt;&gt; Gotcha, okay. So this, this normalization factor,

00:04:13.690 --> 00:04:16.930
this denominator doesn't, it's just a constant with respect

00:04:16.930 --> 00:04:19.220
to x, the input. So it won't actually change

00:04:19.220 --> 00:04:21.660
the answer. So it really is the same answer

00:04:21.660 --> 00:04:23.300
as we had before, just a different way of writing it.

00:04:23.300 --> 00:04:25.880
&gt;&gt; Right. And what it ends up doing like

00:04:25.880 --> 00:04:28.100
often is the case in these situations, is it

00:04:28.100 --> 00:04:31.190
normalizes the output. So it turns out that this

00:04:31.190 --> 00:04:35.010
value. Inside here is always going to be between minus

00:04:35.010 --> 00:04:39.640
one and plus one. Okay? But otherwise it doesn't

00:04:39.640 --> 00:04:42.880
change anything about what we've been doing for boosting. So

00:04:42.880 --> 00:04:44.510
you might ask why did I go through the

00:04:44.510 --> 00:04:46.850
trouble of normalizing it between minus one and plus one?

00:04:46.850 --> 00:04:47.940
&gt;&gt; Why indeed?

00:04:47.940 --> 00:04:50.020
&gt;&gt; Well it's makes it easier for me to draw

00:04:50.020 --> 00:04:53.000
what I want to draw next. So, we know that the

00:04:53.000 --> 00:04:56.760
output of this little bit inside the sign function is

00:04:56.760 --> 00:04:59.380
always going to be between minus one and plus one. Let's

00:04:59.380 --> 00:05:03.110
imagine that I take some particular data point x and

00:05:03.110 --> 00:05:05.190
I pass it through this function, I'm going to get some

00:05:05.190 --> 00:05:07.580
value between minus one and plus one. And let's just

00:05:07.580 --> 00:05:10.510
say for the sake of the argument, it ends up here.

00:05:11.990 --> 00:05:12.230
Okay?

00:05:12.230 --> 00:05:13.680
&gt;&gt; Is that an x or a plus?

00:05:13.680 --> 00:05:14.620
&gt;&gt; That's a plus.

00:05:14.620 --> 00:05:17.380
&gt;&gt; Okay. So it's a positive example and it's near plus one.

00:05:17.380 --> 00:05:18.290
&gt;&gt; Right.

00:05:18.290 --> 00:05:20.830
&gt;&gt; So this would be something that the algorithm is getting correct.

00:05:20.830 --> 00:05:22.580
&gt;&gt; Yes, and it's not just getting it correct,

00:05:22.580 --> 00:05:26.760
but it is very confident. In its correctness. because it

00:05:26.760 --> 00:05:29.460
gave it a very high value. By contrast there

00:05:29.460 --> 00:05:32.400
could have been another positive that ends up around here.

00:05:32.400 --> 00:05:33.580
&gt;&gt; Hmm.

00:05:33.580 --> 00:05:37.060
&gt;&gt; So it gets it correct but it doesn't have a lot

00:05:37.060 --> 00:05:39.550
of confidence so to speak in its correct answer because it's very

00:05:39.550 --> 00:05:44.140
near to zero. So that's the difference between error and confidence. Because

00:05:44.140 --> 00:05:47.050
for example I could also have a plus value way over here.

00:05:47.050 --> 00:05:51.770
So I am very, very confident in my very, very incorrect answer.

00:05:51.770 --> 00:05:52.100
&gt;&gt; Mm.

00:05:52.100 --> 00:05:54.270
&gt;&gt; So this is my daughter, for example.

00:05:54.270 --> 00:05:56.070
[LAUGH] She's very confident whether she's right or

00:05:56.070 --> 00:05:58.770
wrong. [LAUGH] Okay. And so now imagine there's

00:05:58.770 --> 00:06:02.100
lots of little points like this. And if

00:06:02.100 --> 00:06:04.140
you're doing well, you would expect that, you

00:06:04.140 --> 00:06:06.550
know, very, very often you're going to be

00:06:06.550 --> 00:06:08.520
correct. And so you end up shoving all

00:06:08.520 --> 00:06:10.330
the positives over here to the right, and all

00:06:10.330 --> 00:06:14.060
the negatives over here to the left. And it would be really nice if you were

00:06:14.060 --> 00:06:15.770
sort of confident in all of them. Okay,

00:06:15.770 --> 00:06:17.260
so does this make sense, Michael as a picture,

00:06:17.260 --> 00:06:17.550
&gt;&gt; Oh yeah.

00:06:17.550 --> 00:06:19.330
&gt;&gt; What, what might be going on? Absolutely.

00:06:19.330 --> 00:06:22.130
&gt;&gt; Okay, good. So now I want you to imagine that we've

00:06:22.130 --> 00:06:23.770
been going through these, these training

00:06:23.770 --> 00:06:27.260
examples, and we've gotten very, very good

00:06:27.260 --> 00:06:29.120
training error. In fact, let's imagine that

00:06:29.120 --> 00:06:31.953
we have negative training error. I'm [LAUGH]

00:06:31.953 --> 00:06:32.260
&gt;&gt; Wow.

00:06:32.260 --> 00:06:34.180
&gt;&gt; In fact, let's imagine that we have no

00:06:34.180 --> 00:06:37.360
training error at all. So we, we label everything correctly.

00:06:37.360 --> 00:06:38.700
So then the picture would look just a little

00:06:38.700 --> 00:06:41.840
bit different We're going to have all the pluses on one

00:06:41.840 --> 00:06:43.750
side, and all the minuses on the other. But

00:06:43.750 --> 00:06:47.350
we keep on training, we keep adding more and more

00:06:47.350 --> 00:06:50.230
weak learners into the mix. So here's what ends up

00:06:50.230 --> 00:06:52.810
happening in practice, right? What ends up happening in practice

00:06:52.810 --> 00:06:57.870
is, you have to do some kind of distribution

00:06:57.870 --> 00:07:00.680
on the hard examples. And the hard examples are

00:07:00.680 --> 00:07:02.800
going to be the one that are very near the

00:07:02.800 --> 00:07:06.340
boundary. So as you add more and more of these

00:07:06.340 --> 00:07:08.440
weak learners what seems to happen in practice is

00:07:08.440 --> 00:07:11.560
that these pluses that are near the boundary and these

00:07:11.560 --> 00:07:14.530
minuses that are near the boundary just start moving

00:07:14.530 --> 00:07:17.860
farther and farther away from the boundary. So, this minus

00:07:17.860 --> 00:07:20.340
starts drifting and drifting and drifting until it's all the way over

00:07:20.340 --> 00:07:23.660
here, this minus starts drifting and drifting and drifting untili it's all the

00:07:23.660 --> 00:07:27.340
way over here. And the same happens for the pluses. And as

00:07:27.340 --> 00:07:29.990
you keep going and you keep going, what ends up happening is that

00:07:29.990 --> 00:07:33.440
your error stays the same. It doesn't change at all, however your

00:07:33.440 --> 00:07:37.110
confidence keeps going up and up and up and up and up. Which

00:07:37.110 --> 00:07:39.410
has the effect, if you'll look at this little drawing over here of

00:07:39.410 --> 00:07:43.040
moving the pluses all around over here, so they're all in a bunch,

00:07:43.040 --> 00:07:44.870
and the minuses are on the other side.

00:07:44.870 --> 00:07:46.730
So what does that look like to you, Michael?

00:07:46.730 --> 00:07:47.380
&gt;&gt; This picture?

00:07:47.380 --> 00:07:47.820
&gt;&gt; Yeah.

00:07:47.820 --> 00:07:49.330
&gt;&gt; I mean that there's a, there's a

00:07:49.330 --> 00:07:52.590
big gap between the left most plus and the

00:07:52.590 --> 00:07:54.740
right most minus. Which, you know, in the

00:07:54.740 --> 00:07:56.710
context of this lecture reminds me of a margin.

00:07:56.710 --> 00:08:00.090
&gt;&gt; That's exactly right. Basically what ends up

00:08:00.090 --> 00:08:03.150
happening is that as you add more and more

00:08:03.150 --> 00:08:05.760
weak learners here the boosting out rhythm ends

00:08:05.760 --> 00:08:08.230
up becoming more and more confident in its answers

00:08:08.230 --> 00:08:12.460
which it's getting correct. And therefore effectively ends up creating a

00:08:12.460 --> 00:08:15.570
bigger and bigger margin. And what do we know about large margins?

00:08:15.570 --> 00:08:18.420
&gt;&gt; Large margins tend to minimize over fitting.

00:08:18.420 --> 00:08:22.870
&gt;&gt; That's exactly right. So it, counter intuitively, as we create

00:08:22.870 --> 00:08:26.530
more and more of these hypotheses, which you would think would

00:08:26.530 --> 00:08:29.620
make something more and more complicated, it turns out that you

00:08:29.620 --> 00:08:33.970
end up with something smoother, less likely to overfit and ultimately,

00:08:33.970 --> 00:08:38.200
less complicated. So the reason boosting tends to do well and tends to avoid

00:08:38.200 --> 00:08:42.308
over fitting even as you add more and more learners is that you're increasing

00:08:42.308 --> 00:08:45.840
the margin. And there you go. And if you look in the reading that

00:08:45.840 --> 00:08:49.000
we gave the students there's actually a

00:08:49.000 --> 00:08:51.070
detailed descritpion about this in a proof.

00:08:51.070 --> 00:08:51.440
&gt;&gt; Cool.

00:08:51.440 --> 00:08:53.060
&gt;&gt; Okay. So, there you go, Michael.

00:08:53.060 --> 00:08:55.530
Do you think, then, that boosting never overfits?

00:08:55.530 --> 00:08:59.790
&gt;&gt; [SOUND] Never seems like such a strong word.

00:08:59.790 --> 00:09:02.310
I mean, the story that you told says that it's going to try

00:09:02.310 --> 00:09:05.450
to separate those things out, but I guess I guess it doesn't have

00:09:05.450 --> 00:09:07.480
to be able to do that. I mean, it could be that

00:09:07.480 --> 00:09:09.140
for example all the weak learners

00:09:09.140 --> 00:09:13.220
are I dunno very unconfident very inconsistent.

00:09:13.220 --> 00:09:15.910
&gt;&gt; Hm. Okay, well you know, maybe, maybe it's worthwhile to

00:09:15.910 --> 00:09:18.380
take a little diversion here to take a five second quiz.

00:09:18.380 --> 00:09:20.780
&gt;&gt; I think it's worth the time.

00:09:20.780 --> 00:09:21.650
&gt;&gt; Done!

