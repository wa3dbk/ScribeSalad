WEBVTT
Kind: captions
Language: en

00:00:00.940 --> 00:00:04.152
Novel distributed multimedia applications tend to

00:00:04.152 --> 00:00:08.410
be sensor-based. And these sensors can be

00:00:08.410 --> 00:00:11.430
simple sensors, like temperature sensors and

00:00:11.430 --> 00:00:15.240
humidity sensors, or more interesting and complex

00:00:15.240 --> 00:00:20.690
sensors, like cameras and microphones and radars and so on. And these sensors

00:00:20.690 --> 00:00:26.140
are distributed, which means that you have to access them via the internet.

00:00:26.140 --> 00:00:31.518
And what we want to do with these sensor streams is live-stream analysis,

00:00:31.518 --> 00:00:35.110
and often, often such applications are

00:00:35.110 --> 00:00:39.920
also called situation awareness applications. Where what

00:00:39.920 --> 00:00:46.290
we are doing is we are gathering sensor data in order to analyze in

00:00:46.290 --> 00:00:51.760
real time what is happening in the environment and take appropriate decisions.

00:00:51.760 --> 00:00:56.590
So such situation awareness applications exhibit a

00:00:56.590 --> 00:01:01.150
control loop going from sensing, prioritizing the sense

00:01:01.150 --> 00:01:04.060
data to figure out what are important.

00:01:04.060 --> 00:01:06.720
For instance, if there are lots of cameras

00:01:06.720 --> 00:01:08.310
and there is no action in front

00:01:08.310 --> 00:01:11.070
of some cameras, some cameras are more interesting

00:01:11.070 --> 00:01:14.255
than others. That is the prioritization step,

00:01:14.255 --> 00:01:17.670
of figuring out what sensors are interesting and

00:01:17.670 --> 00:01:23.520
then devoting more computational resources to analyze the camera streams

00:01:23.520 --> 00:01:29.520
or other sensor streams and as a result of that, taking some actions. And this

00:01:29.520 --> 00:01:34.370
might involve actuating other sensors, actuating

00:01:34.370 --> 00:01:38.000
triggers, actuating other software entities, or even

00:01:38.000 --> 00:01:42.980
humans, to intervene in terms of what actions were needed to be taken.

00:01:44.310 --> 00:01:46.450
And part of this control loop may also

00:01:46.450 --> 00:01:50.110
be feedback to the sensors themselves to re-target them.

00:01:50.110 --> 00:01:52.700
For example, there may be a camera that you

00:01:52.700 --> 00:01:55.970
may want to point in a different direction. And

00:01:55.970 --> 00:01:58.320
there are cameras like pan, tilt, zoom cameras,

00:01:58.320 --> 00:02:01.240
which you may want to control depending on what

00:02:01.240 --> 00:02:04.070
you are observing in the environment. And such a

00:02:04.070 --> 00:02:09.449
control loop characterization of situation awareness applications can be

00:02:09.449 --> 00:02:10.590
applied to a lot of

00:02:10.590 --> 00:02:15.690
emerging distributed multimedia sensor-based, applications such

00:02:15.690 --> 00:02:19.880
as traffic analysis, emergency response, disaster

00:02:19.880 --> 00:02:23.570
recovery, robotics, acid tracking and so

00:02:23.570 --> 00:02:29.530
on and all such applications are computationally intensive. And because they

00:02:29.530 --> 00:02:34.576
are dealing with real live streams coming from the sensors, they have real-time

00:02:34.576 --> 00:02:39.990
properties, which means that there is a need to shrink the

00:02:39.990 --> 00:02:46.110
latency from sensing to actuation, so that we can take tiny

00:02:46.110 --> 00:02:51.020
actions based on the sensed data. Since they're computationally intensive, we

00:02:51.020 --> 00:02:53.465
need horsepower to run these

00:02:53.465 --> 00:02:59.520
large-scale novel multimedia distributed sensor-based applications.

00:02:59.520 --> 00:03:04.460
And so computational engines such as the clusters and the clouds may be deployed

00:03:04.460 --> 00:03:07.370
in order to cater to the needs

00:03:07.370 --> 00:03:11.144
of these large scale, sensor-based distributed applications.

