WEBVTT
Kind: captions
Language: en

00:00:00.280 --> 00:00:01.330
&gt;&gt; Alright Michael, what's the answer?

00:00:01.330 --> 00:00:03.661
&gt;&gt; Alright so of those others, well C

00:00:03.661 --> 00:00:06.055
is pretty good, because it does separate the

00:00:06.055 --> 00:00:09.910
pluses from the minuses. We, we even liked it so much we used it in round two.

00:00:09.910 --> 00:00:10.271
&gt;&gt; Mh-hm.

00:00:10.271 --> 00:00:14.500
&gt;&gt; But it doesn't as good to me as A, because A actually does a good

00:00:14.500 --> 00:00:16.630
job of separating the very, the more heavily

00:00:16.630 --> 00:00:19.780
weighted points. So I would, I would say A.

00:00:19.780 --> 00:00:26.000
&gt;&gt; So in fact that is what our little learning system shows.

00:00:26.000 --> 00:00:29.120
It shows A. Now, through the trick of animation, I

00:00:29.120 --> 00:00:31.740
leave you with A. And that is exactly the right

00:00:31.740 --> 00:00:35.350
answer. By the way, Michael, if you look at these

00:00:35.350 --> 00:00:39.490
three hypothesis and their weights, you end up with something kind

00:00:39.490 --> 00:00:43.690
of interesting. So if you look at this third hypothesis

00:00:43.690 --> 00:00:45.720
that's chosen here, turns out they have a very low

00:00:45.720 --> 00:00:47.650
error, you'll notice that the errors are going down over

00:00:47.650 --> 00:00:51.200
time, by the way, of 0.14. And it has a much

00:00:51.200 --> 00:00:55.620
higher alpha of 0.92. Now if you look at these weights and you add them up, you

00:00:55.620 --> 00:00:59.100
end up with a cute little combination. So,

00:00:59.100 --> 00:01:02.030
let me draw that for you. Okay Michael, so

00:01:02.030 --> 00:01:05.700
I cleaned up a little bit so that you could see it. If you take each of

00:01:05.700 --> 00:01:08.430
the three hypothesis that we produced, and you weight

00:01:08.430 --> 00:01:11.760
them accordingly, you end up with the bottom figure.

00:01:11.760 --> 00:01:12.650
&gt;&gt; No way.

00:01:12.650 --> 00:01:13.820
&gt;&gt; Absolutely.

00:01:13.820 --> 00:01:16.320
&gt;&gt; That's. Kind of awesome. So what you're saying

00:01:16.320 --> 00:01:19.850
is that, even though we were only using half planes,

00:01:19.850 --> 00:01:24.040
or, or axis-aligned semi planes, for all the weak learners, that

00:01:24.040 --> 00:01:25.930
at the end of the day it actually kind of bent

00:01:25.930 --> 00:01:29.460
the line around and captured the positive and negative examples perfectly.

00:01:29.460 --> 00:01:31.920
&gt;&gt; Right. Does that remind you of

00:01:31.920 --> 00:01:33.280
anything else we've talked about in the past?

00:01:33.280 --> 00:01:37.600
&gt;&gt; Sh. Everything. Nothing. No, I dunno, I mean so with,

00:01:37.600 --> 00:01:40.020
with decision trees you can make the shapes like that, and

00:01:40.020 --> 00:01:40.360
&gt;&gt; That's true.

00:01:40.360 --> 00:01:41.590
&gt;&gt; And the fact that we're doing a weighted

00:01:41.590 --> 00:01:43.640
combination of things reminds me of the neural net.

00:01:43.640 --> 00:01:46.060
&gt;&gt; Yeah. And it should remind you of one other thing.

00:01:46.060 --> 00:01:47.670
&gt;&gt; I'm imagining that you want me to say

00:01:47.670 --> 00:01:50.280
nearest neighbors, but I can't quite make the connection.

00:01:50.280 --> 00:01:52.750
&gt;&gt; Well, you recall in our discussion with nearest neighbors,

00:01:52.750 --> 00:01:56.640
when we did weighted nearest neighbor. In particular we did weighted

00:01:56.640 --> 00:02:00.112
linear regression, we were able to take a simple hypothesis,

00:02:00.112 --> 00:02:03.580
add it together in order to get a more complicated hypothesis.

00:02:03.580 --> 00:02:05.450
&gt;&gt; That's true, because it's local.

00:02:05.450 --> 00:02:07.140
&gt;&gt; Right, exactly because it's local, and

00:02:07.140 --> 00:02:11.590
this is a general feature of Ensemble methods that if you try to look

00:02:11.590 --> 00:02:14.510
at just some particular hypothesis class. Let's

00:02:14.510 --> 00:02:17.370
just call it H, because you're doing weighted

00:02:17.370 --> 00:02:21.690
averages over hypotheses drawn from that hypothesis

00:02:21.690 --> 00:02:26.723
class. This hypothesis class is almost all

00:02:26.723 --> 00:02:28.740
low, is at least as complicated as

00:02:28.740 --> 00:02:32.520
this hypothesis class and often is more complicated.

00:02:32.520 --> 00:02:34.980
So you're able to be more expressive, even though you're

00:02:34.980 --> 00:02:38.330
using simple hypotheses, because you're combining them in some way.

00:02:38.330 --> 00:02:40.800
&gt;&gt; I'm not surprised that you can combine simple things to get

00:02:40.800 --> 00:02:43.690
complicated things. But I am surprised that you can combine them just

00:02:43.690 --> 00:02:47.736
with sums. And get complicated things because sums often act very, you

00:02:47.736 --> 00:02:50.600
know, sort of, friendly. Right it's

00:02:50.600 --> 00:02:53.490
a linear combination not a nonlinear combination.

00:02:53.490 --> 00:02:54.920
&gt;&gt; Actually, Michael part of the reason you get something

00:02:54.920 --> 00:02:57.640
nonlinear here is because you're passing it through a non-linearity

00:02:57.640 --> 00:02:58.070
at the end.

00:02:58.070 --> 00:02:59.010
&gt;&gt; The sine.

00:02:59.010 --> 00:03:02.070
&gt;&gt; Yea, that's a good thing, we should, we should ponder that.

