WEBVTT
Kind: captions
Language: en

00:00:00.025 --> 00:00:04.360
Dyna-Q is an algorithm
developed by Rich Sutton

00:00:04.360 --> 00:00:09.900
intended to speed up learning or
model convergence for Q learning.

00:00:09.900 --> 00:00:13.560
Remember that Q learning is model free.

00:00:13.560 --> 00:00:18.760
Meaning that it does not rely on T or R.

00:00:18.760 --> 00:00:22.415
T being, our transition matrix and

00:00:22.415 --> 00:00:26.380
R being our reward function,
it doesn't know it.

00:00:26.380 --> 00:00:27.810
Q learning does not know T or R.

00:00:29.010 --> 00:00:34.060
Dyna ends up becoming a blend of
model free and model based methods.

00:00:34.060 --> 00:00:35.850
Here's how it works.

00:00:35.850 --> 00:00:38.950
So let's first consider
plain old Q learning.

00:00:38.950 --> 00:00:41.490
Dyna is really in
addition to Q learning.

00:00:41.490 --> 00:00:43.540
So let's start with
the regular Q learning and

00:00:43.540 --> 00:00:46.010
then look at how we add Dyna into it.

00:00:46.010 --> 00:00:50.770
So we initialize our Q table and
then we begin iterating.

00:00:50.770 --> 00:00:55.680
We observe s, we execute action a and

00:00:55.680 --> 00:00:59.990
then we have observe our new
state s prime and our reward R

00:01:01.220 --> 00:01:06.380
within updater our Q table with
this experience tubal and repeat.

00:01:08.220 --> 00:01:11.610
And we do that over and over and over
again, interacting with the world and

00:01:11.610 --> 00:01:14.640
our Q table becomes better and better.

00:01:14.640 --> 00:01:19.680
So when we augment Q
learning with Dyna-Q.

00:01:19.680 --> 00:01:24.810
We had three new components,
the first is that we

00:01:24.810 --> 00:01:30.650
add some logic that enables
us to learn models of T and

00:01:30.650 --> 00:01:37.100
R then for lack of a better term
we halucinate an experience.

00:01:37.100 --> 00:01:41.060
So rather than interacting with
the real world like we do appear with

00:01:41.060 --> 00:01:45.070
the Q learning part and
this is expensive by the way.

00:01:45.070 --> 00:01:49.580
We hallucinate these experiences,
update our queue table and

00:01:49.580 --> 00:01:54.540
repeat this many times,
maybe hundreds of times.

00:01:54.540 --> 00:02:00.700
This operation is very cheap compared
to interacting with the real world.

00:02:00.700 --> 00:02:05.490
So we can leverage
the experience we gain here

00:02:05.490 --> 00:02:10.280
from an interaction with the real world,
but then update our model

00:02:10.280 --> 00:02:15.710
more completely before we step out and
interact with the real world again.

00:02:15.710 --> 00:02:21.050
After we've iterated enough times
down here maybe 100 maybe 200 times.

00:02:21.050 --> 00:02:26.340
Then we return back up here and resume
our interaction with the real world.

00:02:26.340 --> 00:02:31.030
The key thing here is that for
each experience with the real world,

00:02:31.030 --> 00:02:35.310
we have maybe 100 or
200 updates of our model here.

00:02:35.310 --> 00:02:38.630
Let's look at each of these components
in a little more detail now.

00:02:38.630 --> 00:02:41.168
So in this part where
we update our model.

00:02:41.168 --> 00:02:47.511
What we really want to do is
find new values for T and R.

00:02:47.511 --> 00:02:51.900
The point where we update our
model includes the following.

00:02:51.900 --> 00:02:56.430
We want to update T, so I'm calling
it T prime for the moment, which

00:02:56.430 --> 00:03:01.270
represents our transition matrix and
we want to update our reward function.

00:03:01.270 --> 00:03:06.173
Now, remember that T is
the probability that if we

00:03:06.173 --> 00:03:11.558
are in state s and
we take action a will end up in s prime.

00:03:11.558 --> 00:03:17.678
And r is our expected reward if we
are in state s and we take action a.

00:03:17.678 --> 00:03:21.299
I'm going to show you in
a moment how we'll update T and

00:03:21.299 --> 00:03:24.440
R Here's how we
hallucinate an experience.

00:03:25.480 --> 00:03:30.820
First, we randomly select an s,
second, we randomly select an a,

00:03:30.820 --> 00:03:34.360
so our state and
action are chosen totally at random.

00:03:35.420 --> 00:03:40.500
Then we infer our new state s prime,
by looking at T.

00:03:40.500 --> 00:03:46.880
And we infer a reward, our immediate
reward R by looking at big R or R table.

00:03:47.940 --> 00:03:51.830
So, now we've got s, a, s prime and r.

00:03:51.830 --> 00:03:56.680
Or a complete experience tuple and
we can update our Q table using that.

00:03:56.680 --> 00:03:59.370
So, the Q table update
is our final step and

00:03:59.370 --> 00:04:02.970
this is really all there
is to die in a queue.

00:04:02.970 --> 00:04:07.110
We've added these three sections and
what's missing and I'll get to in just

00:04:07.110 --> 00:04:13.440
a moment is how do we update our
model of t and our model of r?

