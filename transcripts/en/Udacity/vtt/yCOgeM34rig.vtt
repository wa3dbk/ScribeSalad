WEBVTT
Kind: captions
Language: en

00:00:00.230 --> 00:00:04.620
Just like precision, we have another quantity that is called recall.

00:00:04.620 --> 00:00:07.520
The recall is the number of true positives,

00:00:07.520 --> 00:00:12.760
divided by the number of true positives plus false negatives.

00:00:12.760 --> 00:00:16.370
Let's quickly look at our confusion matrix again.

00:00:16.370 --> 00:00:20.070
In this case we are taking the true positives and

00:00:20.070 --> 00:00:24.560
dividing by the sum of true positives and false negatives.

00:00:24.560 --> 00:00:27.490
If you remember the definition of sensitivity,

00:00:27.490 --> 00:00:32.720
you will see that recall is the same quantity as sensitivity.

00:00:32.720 --> 00:00:35.360
The reason we have two different names for

00:00:35.360 --> 00:00:41.120
the same quantity is a consequence of how the entire field of model building.

00:00:41.120 --> 00:00:43.700
Was developed through many dif, different fields and

00:00:43.700 --> 00:00:48.940
disciplines and only recently we are beginning to consolidate the terminology.

00:00:48.940 --> 00:00:51.890
So usually if you look at statistics books,

00:00:51.890 --> 00:00:55.330
you will see the terminology of sensitivity.

00:00:55.330 --> 00:00:57.440
While if you look at a machine learning book,

00:00:57.440 --> 00:01:00.150
you will probably find the word recall.

00:01:00.150 --> 00:01:03.050
Also, when we are comparing with specificity.

00:01:03.050 --> 00:01:05.310
We tend to use the word sensitivity.

00:01:05.310 --> 00:01:10.190
When we are comparing with precision, we try to use the word recall.

00:01:10.190 --> 00:01:13.410
Mathematically, they are the same quantity.

00:01:13.410 --> 00:01:16.960
Also remember as we varied our threshold,

00:01:16.960 --> 00:01:21.460
we had different values of sensitivity and specificity.

00:01:21.460 --> 00:01:25.540
In the same way we can get different values of precision and

00:01:25.540 --> 00:01:29.490
recall by changing our threshold or cut.

00:01:30.630 --> 00:01:35.760
The resulting curve is very similar to our ROC curves.

00:01:35.760 --> 00:01:40.570
In this case these curves are called precision recall curves.

00:01:40.570 --> 00:01:43.340
Let's go back to our iPython Notebook.

00:01:43.340 --> 00:01:46.210
We had already built our models and

00:01:46.210 --> 00:01:51.170
we had received some accuracy numbers for the given classifiers.

00:01:51.170 --> 00:01:53.310
Let's look at this piece of code.

00:01:53.310 --> 00:01:57.030
When you run this piece of code you will get the following table.

00:01:57.030 --> 00:02:02.560
Now we have the same kind of confusion matrix that we had before.

00:02:02.560 --> 00:02:04.950
Let's look at the next piece of code.

00:02:04.950 --> 00:02:08.840
Here we are using this method of plot precision recall curve

00:02:08.840 --> 00:02:11.170
to plot a precision recall curve.

00:02:11.170 --> 00:02:15.800
We're using the scikit learns method to actually calculate the values of

00:02:15.800 --> 00:02:18.450
precision recall for every cut.

00:02:18.450 --> 00:02:20.590
Let's run this piece of code now.

00:02:21.610 --> 00:02:24.800
The next line should actually draw the curves for you.

00:02:26.100 --> 00:02:30.820
If everything runs properly you should get a curve that looks like this.

00:02:30.820 --> 00:02:34.410
In this case the recall has been plotted on the x axis.

00:02:34.410 --> 00:02:36.960
And the precision on the y axis.

00:02:36.960 --> 00:02:41.140
Remember the values of both range from zero to one.

00:02:41.140 --> 00:02:45.020
In this case the Logistic Regression, the Radom Forest Classifier, and

00:02:45.020 --> 00:02:51.040
the Support Vector classifiers are shown in blue, green, and red respectively.

00:02:51.040 --> 00:02:54.290
We have also calculated the area under the curves for

00:02:54.290 --> 00:02:57.390
each of these precision recall curves.

00:02:57.390 --> 00:03:02.890
This gives us an average measure of performance for these classifiers.

00:03:02.890 --> 00:03:06.790
It is often useful to use the PR plot if we

00:03:06.790 --> 00:03:11.770
are interested in the proportion of positive outcomes over the negative ones.

