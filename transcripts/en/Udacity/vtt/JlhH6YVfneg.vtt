WEBVTT
Kind: captions
Language: en

00:00:00.370 --> 00:00:04.500
It's important to point out that
when we say reinforcement learning,

00:00:04.500 --> 00:00:08.320
we're really describing a problem,
not a solution.

00:00:08.320 --> 00:00:13.970
In the same way that linear regression
is one solution to the supervised

00:00:13.970 --> 00:00:19.600
regression problem, there are many
algorithms that solve the RL problem.

00:00:19.600 --> 00:00:21.930
Because I started out as a roboticists,

00:00:21.930 --> 00:00:26.560
I'm going to first explain this in
terms of a problem for a robot.

00:00:26.560 --> 00:00:27.980
So here's our robot here and

00:00:27.980 --> 00:00:30.600
our robot is going to interact
with the environment.

00:00:30.600 --> 00:00:35.450
So we represent the environment
as this sort of cloud up here.

00:00:35.450 --> 00:00:38.940
So the robots going to take actions
that'll change the environment.

00:00:38.940 --> 00:00:42.380
It will sense the environment,
reason over what it sees and

00:00:42.380 --> 00:00:43.160
take another action.

00:00:44.350 --> 00:00:49.420
In robotics, we call this the sense,
think, act cycle and

00:00:49.420 --> 00:00:52.330
you don't have to implement it
only using reinforcement learning.

00:00:52.330 --> 00:00:56.310
There's many ways that you could
implement sense, think, act, but

00:00:56.310 --> 00:01:00.150
we're going to focus on how to do
that with reinforcement learning.

00:01:00.150 --> 00:01:03.300
Okay, so
our robot observes the environment and

00:01:03.300 --> 00:01:07.540
some form of description of
the environment comes in.

00:01:07.540 --> 00:01:11.070
Let's call that the state s, so

00:01:11.070 --> 00:01:14.800
s is our letter that represents
what we see in the environment.

00:01:14.800 --> 00:01:17.904
Now the robot has to process that
state somehow to determine what to do.

00:01:17.904 --> 00:01:22.909
And we call this pi or
policy, so the robot

00:01:22.909 --> 00:01:29.520
takes in the state s and
then outputs an action.

00:01:29.520 --> 00:01:31.670
We'll call that action a and

00:01:31.670 --> 00:01:36.210
it affects the environment
in some way and changes it.

00:01:36.210 --> 00:01:41.870
Now this is a sort of circular process,
the action a is taken

00:01:41.870 --> 00:01:47.900
into the environment and the environment
then transitions to a new state.

00:01:47.900 --> 00:01:52.210
So T is this transition
function that takes in

00:01:52.210 --> 00:01:57.240
what its previous state was and
the action and moves to a new state.

00:01:57.240 --> 00:02:01.540
And that new state comes out,
boom, back into the robot.

00:02:01.540 --> 00:02:04.170
Robot looks at his policy,
action comes out.

00:02:05.270 --> 00:02:09.740
Now there's a question,
how do we arrive at this policy?

00:02:09.740 --> 00:02:11.300
How do we find pi?

00:02:12.380 --> 00:02:16.140
Well, that's what we're going to
spend a couple lessons on, but

00:02:16.140 --> 00:02:20.870
this whole puzzle is missing a piece and
that's the thing that helps us find pi.

00:02:20.870 --> 00:02:23.940
And part of that piece is well,

00:02:23.940 --> 00:02:28.002
there's this other part
called r which is the reward.

00:02:28.002 --> 00:02:35.335
So everytime the robot is in a
particular state and it takes an action.

00:02:35.335 --> 00:02:39.180
There's a particular reward associated

00:02:39.180 --> 00:02:44.300
with taking that action in that state
and that reward comes into the robot.

00:02:44.300 --> 00:02:46.870
And you can think of
the robot has having a little

00:02:46.870 --> 00:02:51.200
pocket where it keeps cash and
that's what that reward is.

00:02:51.200 --> 00:02:53.070
And the robot's objective is,

00:02:53.070 --> 00:02:58.360
over time, to take actions
that maximize this reward.

00:02:58.360 --> 00:03:02.280
And somewhere within the robot,
there's an algorithm that takes all this

00:03:02.280 --> 00:03:06.140
information over time to figure
out what that policy ought to be.

00:03:07.320 --> 00:03:09.290
So let me recap a little bit.

00:03:09.290 --> 00:03:11.830
S is the state of our environment and

00:03:11.830 --> 00:03:15.650
that's what the robot senses
in order to decide what to do.

00:03:15.650 --> 00:03:20.370
It uses its policy pi to figure
out what that action should be.

00:03:20.370 --> 00:03:24.830
And by the way,
pi can be a simple look up table.

00:03:24.830 --> 00:03:30.300
Over time, each time the robot takes
an action, it gets a reward and

00:03:30.300 --> 00:03:34.560
it's trying to find the pi that
will maximize its reward over time.

00:03:34.560 --> 00:03:40.720
Now in terms of trading, our environment
really is the market and our actions

00:03:40.720 --> 00:03:45.130
are actions we can take in the market,
like buying and selling or holding.

00:03:45.130 --> 00:03:50.870
S are factors about our stocks that
we might observe and know about.

00:03:50.870 --> 00:03:55.200
And r is the return we get for
making the proper trades.

