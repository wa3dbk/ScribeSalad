WEBVTT
Kind: captions
Language: en

00:00:00.560 --> 00:00:04.260
Let's talk about convolutional networks,
or convnets.

00:00:04.260 --> 00:00:08.380
Convnets are neural networks that
share their parameters across space.

00:00:09.760 --> 00:00:11.560
Imagine you have an image.

00:00:11.560 --> 00:00:13.390
It can be represented as a flat pancake.

00:00:14.400 --> 00:00:16.850
It has a width and a height.

00:00:16.850 --> 00:00:18.970
And because you typically have red,
green, and

00:00:18.970 --> 00:00:22.040
blue channels, it also has a depth.

00:00:22.040 --> 00:00:25.590
In this instance, depth is 3,
that's your input.

00:00:26.700 --> 00:00:29.550
Now imagine taking a small
patch of this image and

00:00:29.550 --> 00:00:33.730
running a tiny neural network on it,
with say, K outputs.

00:00:33.730 --> 00:00:38.390
Let's represent those outputs
vertically, in a tiny column like this.

00:00:38.390 --> 00:00:41.350
Now let's slide that little
neural network across the image

00:00:41.350 --> 00:00:43.150
without changing the weights.

00:00:43.150 --> 00:00:46.330
Just slide across and vertically
like we're painting it with a brush.

00:00:47.440 --> 00:00:50.320
On the output,
we've drawn another image.

00:00:50.320 --> 00:00:53.310
It's got a different width,
a different height, and

00:00:53.310 --> 00:00:55.910
more importantly,
it's got a different depth.

00:00:55.910 --> 00:00:57.130
Instead of just R, G and

00:00:57.130 --> 00:01:01.800
B, now you have an output that's
got many color channels, K of them.

00:01:01.800 --> 00:01:04.269
This operation is called a convolution.

00:01:04.269 --> 00:01:07.700
If your patch size were
the size of the whole image,

00:01:07.700 --> 00:01:11.510
it would be no different than
a regular layer of a neural network.

00:01:11.510 --> 00:01:16.220
But because we have this small patch
instead, we have many fewer weights and

00:01:16.220 --> 00:01:17.550
they are shared across space.

00:01:18.740 --> 00:01:23.080
A convnet is going to basically be
a deep network where instead of having

00:01:23.080 --> 00:01:28.130
stacks of matrix multiply layers, we're
going to have stacks of convolutions.

00:01:29.470 --> 00:01:31.627
The general idea is that
they will form a pyramid.

00:01:31.627 --> 00:01:38.090
At the bottom you have this big image
but very shallow, just R, G, and B.

00:01:39.300 --> 00:01:43.190
You're going to apply convolutions that
are going to progressively squeeze

00:01:43.190 --> 00:01:47.680
the spatial dimensions while
increasing the depth, which corresponds

00:01:47.680 --> 00:01:50.750
roughly to the semantic complexity
of your representation.

00:01:52.000 --> 00:01:54.640
At the top you can put your classifier.

00:01:54.640 --> 00:01:58.130
You have a representation where all the
spatial information has been squeezed

00:01:58.130 --> 00:02:01.730
out and only parameters that map
to contents of the image remain.

00:02:02.850 --> 00:02:03.934
So that's the general idea.

00:02:03.934 --> 00:02:08.156
If you're going to implement this,
there are lots of little details to get

00:02:08.156 --> 00:02:11.220
right and
a fair bit of lingo to get used to.

00:02:11.220 --> 00:02:14.360
You've met the concept of patch and
depth.

00:02:14.360 --> 00:02:17.230
Patches are sometimes called kernels.

00:02:17.230 --> 00:02:21.150
Each pancake in your stack
is called a feature map.

00:02:21.150 --> 00:02:24.916
Here, you're mapping three
feature maps to K feature maps.

00:02:24.916 --> 00:02:28.200
Another term that you
need to know is stride.

00:02:28.200 --> 00:02:32.620
It's the number of pixels that you're
shifting each time you move your filter.

00:02:32.620 --> 00:02:37.290
A stride of 1 makes the output
roughly the same size as the input.

00:02:37.290 --> 00:02:39.760
A stride of 2 means it's
about half the size.

00:02:40.980 --> 00:02:41.950
I say roughly,

00:02:41.950 --> 00:02:45.830
because it depends a bit about what
you do at the edge of your image.

00:02:45.830 --> 00:02:47.190
Either, you don't go past the edge,

00:02:47.190 --> 00:02:50.720
and it's often called valid
padding as a shortcut.

00:02:51.730 --> 00:02:56.150
Or you go off the edge and pad with
zeros in such a way that the output

00:02:56.150 --> 00:03:00.800
map size is exactly the same
size as the input map.

00:03:00.800 --> 00:03:03.340
That is often called same
padding as a shortcut.

