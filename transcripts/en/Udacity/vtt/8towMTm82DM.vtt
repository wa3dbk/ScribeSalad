WEBVTT
Kind: captions
Language: en

00:00:00.284 --> 00:00:03.673
So what are the kind of problems you've looked at recently as you were designing this

00:00:03.673 --> 00:00:08.642
where dynamic parallelism really makes a difference either in terms of usability or performance?

00:00:08.642 --> 00:00:12.626
&amp;gt;&amp;gt; In terms of usability, obviously, it's simpler to program when you

00:00:12.626 --> 00:00:14.958
don't have to keep going backwards and forwards to the CPU.

00:00:14.958 --> 00:00:21.958
So any kind of problem which dynamically discovers the work as it goes--iteratively works it's way through something.

00:00:21.958 --> 00:00:24.127
Imagine you're constructing a tree.

00:00:24.127 --> 00:00:29.369
You're partitioning something into an Octree, which is a common 3D spatial problem.

00:00:29.369 --> 00:00:35.138
You don't necessarily know how many objects are going into which part of your tree

00:00:35.138 --> 00:00:37.914
until you reach that point in your tree.

00:00:37.914 --> 00:00:41.676
So typically the approach would be to do this level by level by level

00:00:41.676 --> 00:00:45.120
which is not the most balanced way of doing things necessarily.

00:00:45.120 --> 00:00:49.052
So as you discover the type of work that you need to do,

00:00:49.052 --> 00:00:52.160
the ability to simply launch that work is much, much simpler.

00:00:52.160 --> 00:00:56.660
We were very motivated by irregular parallelism, if you like;

00:00:56.660 --> 00:01:00.335
problems which did not have nice, well-balanced things.

00:01:00.335 --> 00:01:05.870
A similar sort of category or problem to that is task parallelism

00:01:05.870 --> 00:01:12.042
where I might be wanting not just to do one thing that fills my whole GPU, which is often difficult.

00:01:12.042 --> 00:01:15.178
Now the GPUs these days are a teraflop of performance.

00:01:15.178 --> 00:01:20.791
It might be much easier to have half a dozen or a dozen things running on my GPU at a time,

00:01:20.791 --> 00:01:24.962
and so if each of these can autonomously make forward progress,

00:01:24.962 --> 00:01:27.588
it's much easier to manage them if they're just managing themselves

00:01:27.588 --> 00:01:32.128
instead of having my CPU now juggle 12 different different things instead of just 1.

00:01:32.128 --> 00:01:38.535
And finally there's the there's the new type of algorithm that you can approach.

00:01:38.535 --> 00:01:40.937
You can approach recursive types of algorithm--

00:01:40.937 --> 00:01:45.175
things where the category is generally the divide and conquer algorithm,

00:01:45.175 --> 00:01:49.443
where you take all of the work that you need to do,

00:01:49.443 --> 00:01:53.117
and you conquer it by subdividing and subdividing and subdividing repeatedly,

00:01:53.117 --> 00:01:55.952
and a typical example would be quicksort, for example--

00:01:55.952 --> 00:02:01.290
a well-known problem where you take your data that you want to sort

00:02:01.290 --> 00:02:06.364
and recursively you progress through the data until you end up with a final sorted data set.

00:02:06.364 --> 00:02:09.598
&amp;gt;&amp;gt; So 1 of the demos you guys showed when you launched this

00:02:09.598 --> 00:02:12.435
was N body simulation interstellar,

00:02:12.435 --> 00:02:18.372
a bunch of stars moving around, being attracted to each other with gravity.

00:02:18.372 --> 00:02:25.249
And so, with dynamic parallelism, you were able to write that in a way that you hadn't written it before.

00:02:25.249 --> 00:02:28.686
So how did that come about--like, what was the cool thing you could do with it?

00:02:28.686 --> 00:02:31.922
&amp;gt;&amp;gt; I mentioned octree spatial partitioning just now,

00:02:31.922 --> 00:02:34.824
and that is a key component of an N body simulation.

00:02:34.824 --> 00:02:39.798
So the way that you approach an N body simulation with a very large number of bodies

00:02:39.798 --> 00:02:42.932
is instead of doing an all to all comparison

00:02:42.932 --> 00:02:46.600
where you just calculate the gravity between all the bodies--

00:02:46.600 --> 00:02:49.003
so for N bodies that's an N squared problem.

00:02:49.003 --> 00:02:51.474
You can cut down the complexity of the problem

00:02:51.474 --> 00:02:54.804
to an N log N or an order of an N problem

00:02:54.804 --> 00:03:01.252
by partitioning things into space, doing a local interaction of gravity between your close neighbors,

00:03:01.252 --> 00:03:05.155
and doing an approximation to a center of mass at a more distant neighbor.

00:03:05.155 --> 00:03:11.384
To do this, you build an octree, partitioning your bodies up into small octans, little cubes.

00:03:11.384 --> 00:03:15.104
Everyone inside your cube, you do the N squared problem,

00:03:15.104 --> 00:03:21.567
and for all of the other cubes you then only have an order of an N expansion for.

00:03:21.567 --> 00:03:26.576
So what we did with dynamic parallelism in this N4 body problem

00:03:26.576 --> 00:03:32.249
was we optimize the tree build which is about half of the time in the whole simulation.

00:03:32.249 --> 00:03:35.895
We optimize the tree build by using this recursive property

00:03:35.895 --> 00:03:39.456
by using this irregular parallelism ability

00:03:39.456 --> 00:03:44.591
where the tree might--in the case of a galaxy of stars, for example, might be very, very dense in some regions

00:03:44.591 --> 00:03:48.129
and very, very sparse in others to much more efficiently build the tree.

00:03:48.129 --> 00:03:50.941
So instead of building the tree level by level by level

00:03:50.941 --> 00:03:57.070
so that you're wasting work building the tree for areas of where the bodies are sparse,

00:03:57.070 --> 00:04:00.343
you focus the compute performance on the area where you need it.

00:04:00.343 --> 00:04:02.977
&amp;gt;&amp;gt; Could you have done this without dynamic parallelism?

00:04:02.977 --> 00:04:06.516
&amp;gt;&amp;gt; Yes, I guess you could,

00:04:06.516 --> 00:04:10.820
but the overhead of moving backwards and forwards between the CPU and GPU

00:04:10.820 --> 00:04:14.000
would probably have negated the performance gain that we got.

