WEBVTT
Kind: captions
Language: en

00:00:00.130 --> 00:00:03.370
The answer is, all choices are true except for the

00:00:03.370 --> 00:00:06.030
first one. The reason the first one's not true, is

00:00:06.030 --> 00:00:08.220
this really doesn't take long to compute. We only need

00:00:08.220 --> 00:00:11.460
to look at one letter and do a simple modular computation,

00:00:11.460 --> 00:00:14.710
that's very efficient. But the other three reasons are true

00:00:14.710 --> 00:00:17.960
and we'll go through each of these showing what happens

00:00:17.960 --> 00:00:21.430
looking at how things evaluate in the Python Interpreter. So

00:00:21.430 --> 00:00:25.500
the first correct reason is that it produces an error for

00:00:25.500 --> 00:00:28.350
one input keyword. When we write code we should think

00:00:28.350 --> 00:00:31.880
about whether it works for all possible inputs. And the one

00:00:31.880 --> 00:00:35.230
that's usually the trickiest to think about is the boundary case.

00:00:35.230 --> 00:00:38.770
For a string, that's often the empty string. So, if we

00:00:38.770 --> 00:00:42.110
pass in a string with no characters in it, which

00:00:42.110 --> 00:00:44.990
is a perfectly valid string, we'll then, when we try to

00:00:44.990 --> 00:00:48.200
index element zero, that would be an error. So let's see

00:00:48.200 --> 00:00:51.020
what happens when we try that in the Python Shell. So

00:00:51.020 --> 00:00:56.240
we'll try to evaluate bad_hash_string, passing in the keyword the

00:00:56.240 --> 00:00:59.250
empty string, which is a perfectly valid string. And let's see

00:00:59.250 --> 00:01:01.530
there are 100 buckets. And we do get an error,

00:01:01.530 --> 00:01:03.340
we get the error that the string index is out of

00:01:03.340 --> 00:01:06.690
range because we tried to access the character at position

00:01:06.690 --> 00:01:09.640
zero, but there is no character at position zero in the

00:01:09.640 --> 00:01:13.200
empty string. So to understand the other two reasons, I have

00:01:13.200 --> 00:01:16.720
defined a procedure called test hash function. It takes three inputs.

00:01:16.720 --> 00:01:19.410
The first input is a function, so we can pass functions around,

00:01:19.410 --> 00:01:22.400
just like any other value, so what we're going to pass in for this,

00:01:22.400 --> 00:01:25.430
is the bad_hash_string function that we've defined, but we can also use

00:01:25.430 --> 00:01:28.780
it to test other hash functions, which we'll see later. We're going to pass

00:01:28.780 --> 00:01:31.730
in a list of keys, those are the keywords for the hash

00:01:31.730 --> 00:01:34.000
table, and we're going to pass in the size, this is the number of

00:01:34.000 --> 00:01:37.760
buckets. What we do in test hash function, is we're going to keep

00:01:37.760 --> 00:01:41.710
results as a list of the number of times each bucket is used.

00:01:41.710 --> 00:01:46.410
So initially, they're all zeroes. And we initialize it with zero

00:01:46.410 --> 00:01:49.820
times the size. We're going to use keys_used as a list

00:01:49.820 --> 00:01:52.430
of the keys that have already been used. We don't want to

00:01:52.430 --> 00:01:55.490
count a duplicate key more than once. So now we're going to

00:01:55.490 --> 00:01:57.730
loop through the keys. We're going to check if a key

00:01:57.730 --> 00:02:01.140
was used already, and if the key was not used, then

00:02:01.140 --> 00:02:04.340
we're going to figure out by calling the hash function where

00:02:04.340 --> 00:02:06.870
that key would hash to. So, if we passed in

00:02:06.870 --> 00:02:08.910
bad_hash_string, that would be the function here.

00:02:08.910 --> 00:02:11.330
And we're calling that passing in the keyword, and

00:02:11.330 --> 00:02:13.960
the number of buckets. We're storing the result in

00:02:13.960 --> 00:02:18.250
the variable hv. And then we're increasing the value

00:02:18.250 --> 00:02:21.300
of the element at results position hv by one.

00:02:21.300 --> 00:02:24.510
And this is a shorthand syntax, means the

00:02:24.510 --> 00:02:28.880
same thing as doing a new assignment where we're

00:02:28.880 --> 00:02:32.460
assigning to results[hv], the value currently end results[hv]

00:02:32.460 --> 00:02:37.340
plus one, and then we're adding the word that

00:02:37.340 --> 00:02:39.340
we just used to the list of keys_used, so

00:02:39.340 --> 00:02:41.790
we don't use it again. This is similar to what

00:02:41.790 --> 00:02:44.430
we did in the web crawler, to avoid crawling the

00:02:44.430 --> 00:02:46.694
same page more than once. And if end will return

00:02:46.694 --> 00:02:49.585
the results. So what we'll have as the result of

00:02:49.585 --> 00:02:54.020
test_hash_function, is a list where the values in that list

00:02:54.020 --> 00:02:57.690
are numbers, giving the number of times a key hashes

00:02:57.690 --> 00:03:00.950
to that bucket. So let's try this with

00:03:00.950 --> 00:03:04.550
an example using the bad_hash_string function. So to test

00:03:04.550 --> 00:03:07.190
our hash function, we need some content. We

00:03:07.190 --> 00:03:09.660
need content that represents the kind of words that

00:03:09.660 --> 00:03:11.210
we think we're going to be using the hash

00:03:11.210 --> 00:03:14.610
table on. I've picked this one, which, perhaps is

00:03:14.610 --> 00:03:18.850
represented, perhaps not. And what's there at that link,

00:03:18.850 --> 00:03:22.750
is Gutenberg's text of The Adventures of Sherlock Holmes.

00:03:22.750 --> 00:03:28.710
And you can see from the scroll bar, it's quite long. So this is all the text

00:03:28.710 --> 00:03:35.096
there is. And so on. So, we're going to get all the words on this page using

00:03:35.096 --> 00:03:38.368
getpage. We're going to split them into words like

00:03:38.368 --> 00:03:40.272
we were doing in the crawler, and we'll

00:03:40.272 --> 00:03:42.660
store that in the variable words. And the

00:03:42.660 --> 00:03:47.860
length of that is over 100,000 words. Now they're

00:03:47.860 --> 00:03:49.780
not all unique, so the number of entries in our

00:03:49.780 --> 00:03:52.610
hash table will be smaller then that. But let's see

00:03:52.610 --> 00:03:55.430
how the distribution is for those words. So we'll use

00:03:55.430 --> 00:03:58.440
the test_hash_function that we defined,

00:03:58.440 --> 00:04:01.540
passing in bad_hash_string, the words that

00:04:01.540 --> 00:04:04.650
we got from Sherlock Holmes, and we'll pick, for now

00:04:04.650 --> 00:04:08.060
we'll use size 12, definitely too small, but that'll give

00:04:08.060 --> 00:04:10.030
us a good sense of how the distribution goes for

00:04:10.030 --> 00:04:12.960
a small number of buckets. So now we have the result.

00:04:12.960 --> 00:04:16.459
Let's look at what the counts are. And

00:04:16.459 --> 00:04:20.260
you can see, we've got 12 entries, which corresponds

00:04:20.260 --> 00:04:22.560
to the number of buckets. And they vary quite

00:04:22.560 --> 00:04:27.580
a bit. The smallest one has only 754 elements.

00:04:27.580 --> 00:04:30.140
The largest one has over 2000. So the gap

00:04:30.140 --> 00:04:33.210
between the smallest and largest is nearly a factor

00:04:33.210 --> 00:04:35.440
of three. If our hash function was good, we

00:04:35.440 --> 00:04:37.800
will want these to be about the same size.

00:04:37.800 --> 00:04:41.230
Here's what that looks like graphically. We have our twelve buckets.

00:04:41.230 --> 00:04:44.730
The ones that are red, are too full. The ones that are

00:04:44.730 --> 00:04:47.510
blue, are not full enough. We would like this to be a

00:04:47.510 --> 00:04:51.420
fairly flat graph, distributing all of the words evenly between the buckets.

