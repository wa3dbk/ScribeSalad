WEBVTT
Kind: captions
Language: en

00:00:00.590 --> 00:00:03.880
So how many overall
comparisons was that?

00:00:03.880 --> 00:00:06.680
Well, let's try to find a pattern here.

00:00:06.680 --> 00:00:08.500
It looks like at each step,

00:00:08.500 --> 00:00:12.510
we did one less comparison than
the array we were building.

00:00:12.510 --> 00:00:17.300
So when we were building an array of
seven, we only did six comparisons.

00:00:17.300 --> 00:00:21.586
When we're building an array of three,
we did two comparisons, an array of two,

00:00:21.586 --> 00:00:25.299
one comparison, and an array of four,
we did three comparisons.

00:00:26.430 --> 00:00:27.580
I'm going to go ahead and

00:00:27.580 --> 00:00:31.259
call the length of each array m just
to make it a little easier for me.

00:00:32.409 --> 00:00:36.510
So if m is the size of
the array that we're building,

00:00:36.510 --> 00:00:40.440
the number of comparisons is always
going to be one less than that.

00:00:40.440 --> 00:00:44.470
It's going to be hard to get
an exact number of comparisons here.

00:00:44.470 --> 00:00:49.120
However, we have a good friend that
we can count on, approximation.

00:00:49.120 --> 00:00:52.379
Since we know that every iteration
just cycles through the same

00:00:52.380 --> 00:00:56.730
seven elements again and again,
we know that the size of our arrays,

00:00:56.730 --> 00:01:01.500
or m, are always going to
add up to seven at the end.

00:01:01.500 --> 00:01:03.989
So again,
here we have an array of seven.

00:01:03.990 --> 00:01:08.191
Here we have an array of four and an
array of three, which adds up to seven.

00:01:08.191 --> 00:01:12.370
And here, one, two, two,
and two adds up to seven.

00:01:13.410 --> 00:01:14.780
Since we're just doing divide and

00:01:14.780 --> 00:01:18.780
conquer, splitting up the array at each
step, we can say that approximately

00:01:18.780 --> 00:01:21.400
we're going to do seven
comparisons at each step.

00:01:22.380 --> 00:01:24.240
Again, seven is an upper bound.

00:01:24.240 --> 00:01:27.390
We're never going to do more
than seven comparisons, but

00:01:27.390 --> 00:01:29.260
at many steps we get close to seven.

00:01:30.740 --> 00:01:34.890
So to get the run time of a sorting
algorithm, you can normally multiply

00:01:34.890 --> 00:01:40.370
the number of overall iterations by the
number of comparisons at each iteration.

00:01:40.370 --> 00:01:44.940
In bubble sort,
we were doing n comparisons and n steps.

00:01:44.940 --> 00:01:48.750
We just proved that we're doing at
most n comparisons at every step.

00:01:49.940 --> 00:01:52.500
But how many steps overall are we doing?

00:01:53.180 --> 00:01:55.735
Okay, so to sort an array of seven,

00:01:55.735 --> 00:02:00.600
we had to do three steps,
one, two, and three.

00:02:01.600 --> 00:02:04.475
Also I should note some of
the sub-processes that happened here.

00:02:05.485 --> 00:02:09.520
In order to sort this array of two,
we only had to do one step.

00:02:11.140 --> 00:02:13.455
When we were sorting
an array of three or

00:02:13.455 --> 00:02:17.550
an array of four,
we needed to do two steps.

00:02:17.550 --> 00:02:20.295
We can use a nice table to
keep track of all of that.

00:02:20.295 --> 00:02:25.350
Again, so when our array size was seven,
we had to do three different steps.

00:02:25.350 --> 00:02:30.230
When our array was three or four
numbers large, we had to do two steps.

00:02:30.230 --> 00:02:33.852
When our array was two large,
we had to do one step.

00:02:33.853 --> 00:02:38.500
And if we just had one element, we don't
need to do any comparisons, so no steps.

00:02:39.342 --> 00:02:43.600
I'm going to go ahead and fill in a few
numbers here just for the sake of time.

00:02:43.600 --> 00:02:46.420
You can assume that I did these
calculations on my own and

00:02:46.420 --> 00:02:48.609
I encourage you to do them as well.

00:02:48.610 --> 00:02:52.100
Okay, so this might look
a little bit familiar to you.

00:02:52.100 --> 00:02:56.300
Remember in binary search when
we had a similar table and

00:02:56.300 --> 00:03:00.700
the number of iterations
incremented at every power of two.

00:03:00.700 --> 00:03:03.679
Well, we actually have
something pretty similar here.

00:03:03.679 --> 00:03:05.912
Instead of incrementing
at the powers of two,

00:03:05.912 --> 00:03:08.845
this time we're incrementing
one after every power of two.

00:03:08.845 --> 00:03:11.137
[BLANK_AUDIO]

00:03:11.137 --> 00:03:13.540
So, in the binary search video,

00:03:13.540 --> 00:03:16.850
I proved why the number of
iterations would be equal to log(n).

00:03:16.850 --> 00:03:18.810
So if you need a refresher,
you can go back there.

00:03:20.550 --> 00:03:23.230
Again, this time is a little
different because it increments

00:03:23.230 --> 00:03:26.220
after the power of two instead
of at the power of two.

00:03:26.220 --> 00:03:29.765
Yet again, we're kind of
relying on approximation here.

00:03:29.765 --> 00:03:33.665
We don't exactly care
where the change happens

00:03:33.665 --> 00:03:36.655
as long as we capture the interval
at which the change is happening.

00:03:38.195 --> 00:03:45.594
So in summary, we're doing roughly
n comparisons for log(n) steps.

00:03:45.594 --> 00:03:49.236
That makes an overall
complexity of nlog(n).

00:03:49.236 --> 00:03:52.570
This is definitely better than the n
squared we got in bubble sort.

00:03:53.610 --> 00:03:58.640
In bubble sort, we had n times n or
n squared as our efficiency.

00:04:00.105 --> 00:04:02.840
log(n) is generally going to
be less than n, but

00:04:02.840 --> 00:04:05.650
it's definitely never going
to be greater than n.

00:04:05.650 --> 00:04:08.450
So we can say that the efficiency
of merge sort is better than

00:04:08.450 --> 00:04:09.750
the efficiency of bubble sort.

00:04:11.800 --> 00:04:15.800
However, the space efficiency of merge
sort is actually worse than bubble sort.

00:04:16.260 --> 00:04:18.630
In bubble sort,
we were sorting in place, so

00:04:18.630 --> 00:04:20.690
we didn't need to use any extra arrays.

00:04:21.860 --> 00:04:24.770
Here we frequently copied
our values into new arrays.

00:04:26.500 --> 00:04:32.420
You can say that the auxiliary space or
the extra space we used was linear.

00:04:32.420 --> 00:04:36.909
This complexity assumes that we got rid
of arrays after we were done using them.

00:04:36.909 --> 00:04:40.605
At each step, we were copying
values into new arrays, so

00:04:40.605 --> 00:04:42.926
we needed new arrays at some point.

00:04:42.926 --> 00:04:46.195
However, we could say that we were
getting rid of the old ones whenever we

00:04:46.196 --> 00:04:47.340
were done.

00:04:47.340 --> 00:04:49.940
So we're not using a ton
of new arrays every time.

00:04:49.940 --> 00:04:54.210
We really only need two different arrays
at every step, the array that our

00:04:54.210 --> 00:04:57.400
numbers were in and the new array
that we're copying the values into.

