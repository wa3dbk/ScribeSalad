WEBVTT
Kind: captions
Language: en

00:00:00.500 --> 00:00:02.560
It's often the case that MapReduce code is written in

00:00:02.560 --> 00:00:05.730
Java. However, to make things a little easier for us,

00:00:05.730 --> 00:00:09.260
we've actually written our mapper and reducer in Python instead.

00:00:09.260 --> 00:00:11.660
And we can do that thanks to a feature called Hadoop

00:00:11.660 --> 00:00:14.360
streaming, which allows you to write your code in pretty

00:00:14.360 --> 00:00:17.585
much any language you'd like. So first of all, let's

00:00:17.585 --> 00:00:21.460
double-check that we have our input data in HDFS. So,

00:00:21.460 --> 00:00:26.250
if I Hadoop fs minus ls, then there's my input directory.

00:00:26.250 --> 00:00:28.160
And if I look at that directory, then

00:00:28.160 --> 00:00:33.092
yes, there's purchases.txt in there. And in my local

00:00:33.092 --> 00:00:36.524
directory, I have mapper.py and reducer.py, that's the

00:00:36.524 --> 00:00:40.190
code for the mapper and reducer, written in Python.

00:00:40.190 --> 00:00:41.600
We'll look at the actual code in the

00:00:41.600 --> 00:00:45.270
next lesson. Okay, to submit the job we have

00:00:45.270 --> 00:00:48.090
to give this rather cumbersome command. We say hadoop

00:00:48.090 --> 00:00:51.410
jar, a path to a jar, then I specify

00:00:51.410 --> 00:00:57.390
the mapper, I specify the reducer, I need to say -file, for both the mapper and

00:00:57.390 --> 00:01:04.170
the reducer code. I specify the input directory in HDFS and I specify the output

00:01:04.170 --> 00:01:06.790
directory to which the reducers will write their

00:01:06.790 --> 00:01:10.330
output data. And we're calling that joboutput. I

00:01:10.330 --> 00:01:16.570
hit Enter and off we go. Hadoop's pretty verbose, as you can see. As the job

00:01:16.570 --> 00:01:19.840
runs, you'll see a bunch of output which shows us how

00:01:19.840 --> 00:01:22.880
far along the job is. It turns out that for this

00:01:22.880 --> 00:01:26.000
job Hadoop will be running four mappers. And our virtual machine

00:01:26.000 --> 00:01:28.430
here can only run two at a time. So the job is

00:01:28.430 --> 00:01:31.510
going to take longer than it would on a larger cluster.

00:01:31.510 --> 00:01:34.780
Actually, that's worth mentioning here. With the size of the data we

00:01:34.780 --> 00:01:38.000
have for this example which is only 200 megs, realistically, we

00:01:38.000 --> 00:01:41.800
could probably have solved this problem faster by just importing the data

00:01:41.800 --> 00:01:44.550
into a relational database and querying it from there.

00:01:44.550 --> 00:01:47.030
And that's often the case when we're developing and testing

00:01:47.030 --> 00:01:50.280
code. Because the test data sets are pretty small, Hadoop

00:01:50.280 --> 00:01:53.140
isn't necessarily the optimal tool for the job. But when

00:01:53.140 --> 00:01:55.000
we're done testing and we need to process our full

00:01:55.000 --> 00:01:58.040
production data, that's when Hadoop really comes into its own.

00:01:59.150 --> 00:02:01.230
So, as you can see the job is now nearly

00:02:01.230 --> 00:02:06.125
complete, and when the job has finished we'll get a

00:02:06.125 --> 00:02:06.240
[UNKNOWN]

00:02:06.240 --> 00:02:08.820
output. And we'll see that the last line tells

00:02:08.820 --> 00:02:13.230
me that the output directory is called joboutput. Let's

00:02:13.230 --> 00:02:16.225
take a look at what we've got in there.

00:02:16.225 --> 00:02:18.265
Hadoop fs minus ls, shows me that yes I do

00:02:18.265 --> 00:02:21.340
have a job output directory. And if we look

00:02:21.340 --> 00:02:23.390
at the job output directory, you'll see that it

00:02:23.390 --> 00:02:27.890
contains three things. It contains a file called _SUCCESS,

00:02:27.890 --> 00:02:31.240
which just tells me that the job has successfully completed.

00:02:31.240 --> 00:02:34.210
It contains a directory called _logs, which

00:02:34.210 --> 00:02:37.050
contains some log information about what happened during

00:02:37.050 --> 00:02:39.730
the job's run. And then, it contains a

00:02:39.730 --> 00:02:45.480
file called part-00000. That file is the output

00:02:45.480 --> 00:02:50.210
from the one reducer that we had for this job. Let's take a look at that by

00:02:50.210 --> 00:02:56.380
saying hadoop fs minus cat part 00000, and we'll pipe that to less on

00:02:56.380 --> 00:02:59.540
our local machine. That's the contents of that file,

00:02:59.540 --> 00:03:01.640
which is the output from our reducer. It's the

00:03:01.640 --> 00:03:05.490
sum total sales broken down by store exactly as

00:03:05.490 --> 00:03:09.320
we want it. Incidentally, if you want to retrieve data

00:03:09.320 --> 00:03:12.210
from HFDS and put it onto your local disk,

00:03:12.210 --> 00:03:14.590
you can do that with Hadoop fs minus get.

00:03:14.590 --> 00:03:18.080
Hadoop fs minus get is the opposite of Hadoop

00:03:18.080 --> 00:03:21.830
fs minus put. It just pulls data from HDFS and

00:03:21.830 --> 00:03:24.740
puts it on the local disk. So as you can see, now I have

00:03:24.740 --> 00:03:30.000
my local file.txt on my local disk And I can manipulate that however I'd like.

