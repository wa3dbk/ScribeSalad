WEBVTT
Kind: captions
Language: en

00:00:00.270 --> 00:00:02.960
So now that I have that,
I backed everything up and

00:00:02.960 --> 00:00:04.300
I can just do selection again.

00:00:04.300 --> 00:00:07.110
Now this time around because I've
changed sort of my estimates

00:00:07.110 --> 00:00:09.240
of the Q functions along the way.

00:00:09.240 --> 00:00:11.260
I might want to make
a different selection.

00:00:11.260 --> 00:00:15.650
And so that actually tells us where
this original policy comes from.

00:00:15.650 --> 00:00:20.750
This original policy is just
an estimate of two values

00:00:20.750 --> 00:00:24.860
that I have where I happen to feel
very comfortable about those Q values.

00:00:24.860 --> 00:00:26.900
I've visited these states many times,

00:00:26.900 --> 00:00:30.340
I've backed up a lot of information from
them so I have a decent estimate and I'm

00:00:30.340 --> 00:00:36.550
willing to do the kind of greedy policy
that is implied by these Q functions.

00:00:36.550 --> 00:00:41.090
&gt;&gt; I see, so we're taking greedy
steps down the black part of the tree

00:00:41.090 --> 00:00:45.490
when we hit a leaf them we
do the roll out policy pi r

00:00:45.490 --> 00:00:47.920
to get estimates at
those different values.

00:00:47.920 --> 00:00:51.660
Do we now know anything
new about the tree?

00:00:51.660 --> 00:00:55.160
&gt;&gt; Yes, and in fact,
if we did this 100 bazillion times,

00:00:55.160 --> 00:01:00.010
or however many number of times we sort
of wanted to, we just can say well,

00:01:00.010 --> 00:01:03.390
I know the actions that I
can take from this node.

00:01:03.390 --> 00:01:08.560
And so I feel comfortable in expanding
my policy all the way down to here for

00:01:08.560 --> 00:01:10.120
this particular node as well.

00:01:10.120 --> 00:01:13.840
And when if I did this again, and let's
say the selection algorithm had me go

00:01:13.840 --> 00:01:16.740
here, here, here and
I got back to here again.

00:01:16.740 --> 00:01:19.400
I would actually know which
action I wanted to take,

00:01:19.400 --> 00:01:21.820
say it ended up with me being here.

00:01:21.820 --> 00:01:24.920
But then once again I've gotten to
a place where I don't feel comfortable

00:01:24.920 --> 00:01:27.310
about what to do next, and
I would do the expansion and

00:01:27.310 --> 00:01:30.000
the rollout again, over and
over and over again.

00:01:30.000 --> 00:01:32.310
So I just keep doing this again and
again and again and again and again and

00:01:32.310 --> 00:01:33.830
again and again and again.

00:01:33.830 --> 00:01:38.910
And what this gives me is an ever
expanding tree where I feel more and

00:01:38.910 --> 00:01:41.480
more comfortable about my cue
values and, therefore more and

00:01:41.480 --> 00:01:44.240
more comfortable about
a policy that I should take.

00:01:44.240 --> 00:01:46.850
&gt;&gt; And it seems like it's
getting more accurate over time.

00:01:46.850 --> 00:01:49.280
&gt;&gt; Yeah, and in fact if you're
at each of these nodes, right?

00:01:49.280 --> 00:01:51.400
and you took those actions and
you did the expansion and

00:01:51.400 --> 00:01:54.320
you did that well let's say
an infinite number of times, and

00:01:54.320 --> 00:01:57.000
then you do an infinite number of
roll outs over and over again.

00:01:57.000 --> 00:02:00.080
Eventually you would converge
to a property two value.

00:02:00.080 --> 00:02:03.550
&gt;&gt; Cool that's right so this is
a planning algorithm we need to know

00:02:03.550 --> 00:02:06.530
the transition model to be
able to do those roll outs.

00:02:06.530 --> 00:02:09.770
&gt;&gt; Right, or you have to have
some way of doing the simulation.

00:02:09.770 --> 00:02:12.390
So when you say you need to
know the transition model,

00:02:12.390 --> 00:02:14.370
you could mean a couple of things.

00:02:14.370 --> 00:02:16.130
You could mean I actually
have the transition model and

00:02:16.130 --> 00:02:17.920
I can do calculations with it.

00:02:17.920 --> 00:02:22.200
Or you could just mean I have some
way of simulating the world and

00:02:22.200 --> 00:02:24.120
I can just sample from it.

00:02:24.120 --> 00:02:25.180
So this is pretty neat, right?

00:02:25.180 --> 00:02:27.410
I think it has some neat
little features to it.

00:02:27.410 --> 00:02:29.590
It allows me to kind of
build up a model the world.

00:02:29.590 --> 00:02:31.270
Do a bunch of simulation and
figure out what to do.

00:02:31.270 --> 00:02:34.620
But there are a couple of questions
that should pop out to you even

00:02:34.620 --> 00:02:37.150
though I kind of ran
over them fairly quickly.

00:02:37.150 --> 00:02:39.070
I think the first one is,
well, when do you stop?

00:02:39.070 --> 00:02:40.480
What does this actually, I mean,

00:02:40.480 --> 00:02:42.440
this is kind of an inner
loop of a process, right?

00:02:42.440 --> 00:02:45.480
I'm at this state I
have an estimate of Q

00:02:45.480 --> 00:02:48.770
values that I'm comfortable
enough in that I'm going to take

00:02:48.770 --> 00:02:51.900
actual actions following
the policy that's implied by them.

00:02:51.900 --> 00:02:54.700
And then I'm going to keep expanding and
learning more and more about it.

00:02:54.700 --> 00:02:57.070
Well in principle I could
just do that for eternity.

00:02:57.070 --> 00:02:58.670
But at some point you have to stop.

00:02:58.670 --> 00:02:59.730
And when do you stop?

00:02:59.730 --> 00:03:01.040
Well you stop when you get bored.

00:03:01.040 --> 00:03:02.840
&gt;&gt; And what do you do when you stop?

00:03:02.840 --> 00:03:06.520
&gt;&gt; Well, once you stop so next
question well what I do after I stop?

00:03:06.520 --> 00:03:07.460
Well, you execute.

00:03:07.460 --> 00:03:11.420
You do a sort of one step policy
based upon what you've learned and

00:03:11.420 --> 00:03:13.210
then you throw it all away and
you do it all over again.

00:03:13.210 --> 00:03:15.900
&gt;&gt; Wait, so
you take that step in the world

00:03:15.900 --> 00:03:19.070
using policy the whole
chain that you figured out?

00:03:19.070 --> 00:03:22.350
&gt;&gt; Well, you could but and
in fact sometimes I mean,

00:03:22.350 --> 00:03:24.370
well you can do actually
any number of things there.

00:03:24.370 --> 00:03:27.790
But sort of the simplest version of this
is I started out with an empty tree.

00:03:27.790 --> 00:03:28.880
I'm in some particular state,

00:03:28.880 --> 00:03:32.140
I'm going to hallucinate all
the things that I might do from there.

00:03:32.140 --> 00:03:34.810
As I do that I'll be learning more and
more about the world.

00:03:34.810 --> 00:03:37.560
This will allow me to do better and
better exploration

00:03:37.560 --> 00:03:41.840
because I do this kind of Monte Carlo
search and simulation and hallucination.

00:03:41.840 --> 00:03:44.770
And then eventually I learn enough that
I know what the right thing to do is

00:03:44.770 --> 00:03:46.980
from this state and then I do it.

00:03:46.980 --> 00:03:49.960
And then I end up wherever it is
I end up in the real world and

00:03:49.960 --> 00:03:52.770
when I'm asked to do something
again I just do it all over again.

00:03:52.770 --> 00:03:56.200
&gt;&gt; Wow, so it does a lot of
thinking each time it takes a step.

00:03:56.200 --> 00:03:58.020
&gt;&gt; Yeah but
it's very fast thinking because

00:03:58.020 --> 00:03:59.290
you know I'm just flipping coins.

00:03:59.290 --> 00:04:03.690
If I have a fast simulator and I'm
able to do addition pretty quickly and

00:04:03.690 --> 00:04:08.490
maybe some averaging I can actually
do this estimate fairly well.

00:04:08.490 --> 00:04:13.570
What's going to impact this a lot
is sort of how far down I can go

00:04:13.570 --> 00:04:15.590
before I run out of computational power?

00:04:15.590 --> 00:04:18.649
Typically that's what board
really means board means okay

00:04:18.649 --> 00:04:22.760
I've taken 15 seconds that's too much
time or maybe I've taken three seconds.

00:04:22.760 --> 00:04:25.640
That's too much time it's time to do
the best thing that I can based upon

00:04:25.640 --> 00:04:26.970
what I know.

00:04:26.970 --> 00:04:29.910
But if I can do that every single time,
if I can expand this tree pretty

00:04:29.910 --> 00:04:33.730
deeply at every time step,
then I lose 50 milliseconds.

00:04:33.730 --> 00:04:37.340
But 50 milliseconds is a long time for
a computer and not any time at all for

00:04:37.340 --> 00:04:38.180
human beings.

00:04:38.180 --> 00:04:41.580
And the alternative would be to
actually explore the entire space and

00:04:41.580 --> 00:04:42.850
try to solve the underlying MDP.

00:04:42.850 --> 00:04:45.170
And if you have a whole
lot of states and

00:04:45.170 --> 00:04:50.170
a whole lot of actions then that
can take a very, very long time.

00:04:50.170 --> 00:04:54.710
So do you take 50 or 200 milliseconds
every time you take an action and

00:04:54.710 --> 00:04:55.450
do it online?

00:04:55.450 --> 00:04:58.880
Or do you take three and a half months
of super computing power to figure out

00:04:58.880 --> 00:05:00.770
the optimal policy and then use that?

00:05:00.770 --> 00:05:01.850
And the answer is, it depends.

