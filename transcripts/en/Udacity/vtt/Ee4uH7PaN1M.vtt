WEBVTT
Kind: captions
Language: en

00:00:00.125 --> 00:00:01.425
All right.
So that's supervised learning and

00:00:01.425 --> 00:00:02.087
unsupervised learning.

00:00:02.087 --> 00:00:02.921
That's pretty good.

00:00:02.921 --> 00:00:04.640
The last one is reinforcement learning.

00:00:04.640 --> 00:00:05.442
&gt;&gt; [SOUND].

00:00:05.442 --> 00:00:07.587
&gt;&gt; Now reinforcement learning
is what we both do, so

00:00:07.587 --> 00:00:10.561
Michael does a little bit of
reinforcement learning here and there.

00:00:10.561 --> 00:00:12.906
You've got how many papers published
in reinforcement learning?

00:00:12.906 --> 00:00:13.545
&gt;&gt; All of them.

00:00:13.545 --> 00:00:14.902
[LAUGH] Several.

00:00:14.902 --> 00:00:15.704
I have several.

00:00:15.704 --> 00:00:19.455
&gt;&gt; The man has like a hundred papers of
reinforcement learnings and in fact he

00:00:19.455 --> 00:00:24.299
wrote with his colleagues the great
summary journal article bringing every

00:00:24.299 --> 00:00:28.012
one up to date on what reinforcement
learning was like back in 1990.

00:00:28.012 --> 00:00:29.718
&gt;&gt; Yeah like 112 years ago.

00:00:29.718 --> 00:00:30.417
&gt;&gt; 1992.

00:00:30.417 --> 00:00:33.386
&gt;&gt; People are saying yeah we should
probably somebody should write a new one

00:00:33.386 --> 00:00:35.733
because the other ones getting
a little long in the dude.

00:00:35.733 --> 00:00:37.520
&gt;&gt; But there's been books written
on machine learning system.

00:00:37.520 --> 00:00:38.055
&gt;&gt; That's right.

00:00:38.055 --> 00:00:38.659
&gt;&gt; It's a very popular field.

00:00:38.659 --> 00:00:39.760
That's why we're both in it.

00:00:39.760 --> 00:00:41.573
Michael tends to prove a lot of things,.

00:00:41.573 --> 00:00:43.521
&gt;&gt; It is not, that is not why I'm in it.

00:00:43.521 --> 00:00:45.391
&gt;&gt; What, I didn't, wait, what?

00:00:45.391 --> 00:00:47.701
&gt;&gt; You said it's a very popular
field and that's why we're in it.

00:00:47.701 --> 00:00:48.561
&gt;&gt; No, no, no, no, no.

00:00:48.561 --> 00:00:49.212
Did I say that?

00:00:49.212 --> 00:00:49.908
&gt;&gt; That's what I heard.

00:00:49.908 --> 00:00:50.976
&gt;&gt; I didn't mean to say that.

00:00:50.976 --> 00:00:52.323
&gt;&gt; [SOUND] Let's run it back and see.

00:00:52.323 --> 00:00:52.903
&gt;&gt; It's a very popular, yeah,

00:00:52.903 --> 00:00:53.981
let's do that again because
I did not mean to say that.

00:00:53.981 --> 00:00:55.096
It is a very popular field.

00:00:55.096 --> 00:00:56.535
Perhaps because you're in it Michael.

00:00:56.535 --> 00:00:57.107
&gt;&gt; I don't think that's it.

00:00:57.107 --> 00:00:58.518
When I was an undergraduate,

00:00:58.518 --> 00:01:01.008
I thought the thing that I
really want to understand.

00:01:01.008 --> 00:01:03.040
I liked AI,
I liked the whole idea of AI.

00:01:03.040 --> 00:01:06.457
But what I really want to understand
is how can you learn to be

00:01:06.457 --> 00:01:08.009
better from experience?

00:01:08.009 --> 00:01:10.595
And like I, I built a tic-tac-toe
playing program, and like,

00:01:10.595 --> 00:01:13.802
I want this tic-tac-toe playing program
to get really good at tic-tac-toe.

00:01:13.802 --> 00:01:17.438
because I was always interested
in the most practical society

00:01:17.438 --> 00:01:18.885
impacting problems.

00:01:18.885 --> 00:01:22.323
&gt;&gt; I think that generalized
pretty well to world hunger.

00:01:22.323 --> 00:01:22.939
&gt;&gt; Eventually.

00:01:22.939 --> 00:01:24.831
So so that is what got
me interested in it, and

00:01:24.831 --> 00:01:27.144
I was, I didn't even know what
it was called for a long time.

00:01:27.144 --> 00:01:29.391
So I started doing
reinforcement learning, and

00:01:29.391 --> 00:01:31.927
then discovered that it was
interesting and popular.

00:01:31.927 --> 00:01:33.111
&gt;&gt; Right.

00:01:33.111 --> 00:01:35.799
Well, I certainly wouldn't suggest that
we're doing the science that we're doing

00:01:35.799 --> 00:01:36.592
because it's popular.

00:01:36.592 --> 00:01:37.951
We're doing it because
we're interested in it.

00:01:37.951 --> 00:01:38.508
&gt;&gt; Yes.

00:01:38.508 --> 00:01:41.792
&gt;&gt; And I'm interested in reinforcement
learning because in some sense,

00:01:41.792 --> 00:01:44.592
it kind of encapsulates all
the things I happen to care about.

00:01:44.592 --> 00:01:48.587
I come from a sort of general AI
background, and I care modeling people.

00:01:48.587 --> 00:01:51.567
I care about building smart agents
that have to live in in world that

00:01:51.567 --> 00:01:54.813
other smart agents, thousands of them,
hundreds of thousand of them,

00:01:54.813 --> 00:01:55.737
thousands of them.

00:01:55.737 --> 00:01:57.362
Some of them might be human and

00:01:57.362 --> 00:02:00.230
have to feel some way to
predict what to do over time.

00:02:00.230 --> 00:02:03.386
So, from a sort a technical point
of view, if we can think of re,

00:02:03.386 --> 00:02:06.545
in, supervised learning as
function approximation and

00:02:06.545 --> 00:02:08.485
unsupervised learning as, you know.

00:02:08.485 --> 00:02:09.362
&gt;&gt; Concise-
&gt;&gt; Concise,

00:02:09.362 --> 00:02:12.076
impact description, what's
the difference between something like

00:02:12.076 --> 00:02:13.640
reinforcement learning and those two?

00:02:13.640 --> 00:02:14.464
Supervised learning.

00:02:14.464 --> 00:02:17.284
&gt;&gt; So often the way that
supervised learning oh, sorry,

00:02:17.284 --> 00:02:21.290
reinforcement learning is described is,
is learning from delayed reward.

00:02:21.290 --> 00:02:22.969
&gt;&gt; Mm-hm.
So instead of the feedback that you get

00:02:22.969 --> 00:02:25.249
in supervised learning which
is here's what you should do.

00:02:25.249 --> 00:02:28.364
And the feedback that you get in
unsupervised learning which is

00:02:28.364 --> 00:02:31.481
the feedback in reinforcement
learning may come several steps

00:02:31.481 --> 00:02:33.968
after the decisions that
you've actually made.

00:02:33.968 --> 00:02:36.257
&gt;&gt; So a good example of that, or
the easy example of that would be,

00:02:36.257 --> 00:02:38.044
actually your tic-tac-toe program,
right?

00:02:38.044 --> 00:02:42.452
So, you do something in tic-tac-toe,
you put an X in the center and

00:02:42.452 --> 00:02:45.266
then you put a, let's say,
an O over here.

00:02:45.266 --> 00:02:45.766
&gt;&gt; Oh.

00:02:45.766 --> 00:02:47.762
&gt;&gt; And then I put an X right here.

00:02:47.762 --> 00:02:48.401
&gt;&gt; Nice.

00:02:48.401 --> 00:02:51.327
&gt;&gt; And then you ridiculously
put an O in the center.

00:02:51.327 --> 00:02:53.728
&gt;&gt; Which allows me to put
an X over here and I win.

00:02:53.728 --> 00:02:54.309
&gt;&gt; All right.

00:02:54.309 --> 00:02:56.219
&gt;&gt; Now what's interesting about that is,

00:02:56.219 --> 00:02:59.487
I didn't tell you what happened until
the very end when I said X wins.

00:02:59.487 --> 00:03:00.509
&gt;&gt; Right.

00:03:00.509 --> 00:03:03.161
And now I know I made a mistake
somewhere along the way but

00:03:03.161 --> 00:03:04.584
I don't know exactly where.

00:03:04.584 --> 00:03:07.650
I may have to kind of roll back the game
in my mind and eventually figure out

00:03:07.650 --> 00:03:10.389
where it is that I went off track,
and what it is that I did wrong.

00:03:10.389 --> 00:03:12.339
&gt;&gt; And in the full generality
of reinforcement learning,

00:03:12.339 --> 00:03:13.547
you may have never made a mistake.

00:03:13.547 --> 00:03:15.650
It may simply be that's
the way games go but

00:03:15.650 --> 00:03:18.584
you would like to know which of
the moves you made mattered.

00:03:18.584 --> 00:03:21.307
Now, if it were a civilized learning
problem, I would have put the X here,

00:03:21.307 --> 00:03:24.004
he would have put the O there, and
it would have been called that's Good.

00:03:24.004 --> 00:03:25.727
I would have put the X here, and

00:03:25.727 --> 00:03:29.781
when he put the O there, it would have
been that's Bad, the O goes here.

00:03:29.781 --> 00:03:30.339
&gt;&gt; Mm-hm.

00:03:30.339 --> 00:03:31.382
Right.
&gt;&gt; Or something like that.

00:03:31.382 --> 00:03:33.777
It would have told you where
he should have put the O.

00:03:33.777 --> 00:03:36.059
But here, all he gets is eventually
some kind of signal saying,

00:03:36.059 --> 00:03:36.962
you did something well.

00:03:36.962 --> 00:03:39.955
You did something poorly and even
then it's only relative to the other

00:03:39.955 --> 00:03:41.536
signals that you might have gotten.

00:03:41.536 --> 00:03:44.428
&gt;&gt; Right, so then reinforcement
learning is in some sense harder

00:03:44.428 --> 00:03:46.400
because nobody's telling you what to do.

00:03:46.400 --> 00:03:47.782
You have to work it out on your own.

00:03:47.782 --> 00:03:51.624
&gt;&gt; Yeah it's like playing a game
without knowing any of the rules.

00:03:51.624 --> 00:03:56.478
Or at least knowing how you win or lose.

00:03:56.478 --> 00:04:00.342
But being told every once in awhile that
you've won or you've lost, okay, now-

00:04:00.342 --> 00:04:01.283
&gt;&gt; Sometimes I feel like that.

00:04:01.283 --> 00:04:03.130
&gt;&gt; I know man.

