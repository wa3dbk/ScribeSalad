WEBVTT
Kind: captions
Language: en

00:00:00.280 --> 00:00:03.700
Whenever we do recognition machine
learning, what's going to help me?

00:00:03.700 --> 00:00:04.780
It's the data.

00:00:04.780 --> 00:00:06.670
The data are the first thing
that's going to help me.

00:00:06.670 --> 00:00:07.880
There's somebody else
that's going to help me.

00:00:07.880 --> 00:00:09.540
I'm going to bring
them along in a minute.

00:00:09.540 --> 00:00:14.580
So here we have some images of pictures,
and as you can see in the red circles,

00:00:14.580 --> 00:00:17.667
there are some pixels that are from,
say, skin.

00:00:17.667 --> 00:00:20.600
And there are pixels
that are not from skin.

00:00:20.600 --> 00:00:25.330
And let's suppose I just want to
label a pixel or little patches.

00:00:25.330 --> 00:00:27.480
Is that skin, or is there not skin?

00:00:27.480 --> 00:00:31.084
By the way, this is a cute,
little thing for those of you out there.

00:00:31.084 --> 00:00:33.376
We did a bunch of skin
detection et cetera.

00:00:33.376 --> 00:00:38.406
And in Sandy Pentland's lab, and
I think Thad Starner was involved back

00:00:38.406 --> 00:00:44.310
when he was a student, it turns out
the color of skin is pretty constant.

00:00:44.310 --> 00:00:46.320
The luminance changes, right?

00:00:46.320 --> 00:00:49.530
So some people, you may have noticed
are darker than other people.

00:00:49.530 --> 00:00:50.910
But they're darker,

00:00:50.910 --> 00:00:56.550
they just have more of the same pignent,
pigment, right, the melanin pigment.

00:00:56.550 --> 00:01:01.620
So when you look at skin in a color
space, if you remove the illuminant,

00:01:01.620 --> 00:01:07.900
it's actually possible to detect
skin for a very wide range

00:01:07.900 --> 00:01:12.800
of how dark the skin is, using the same
color removing the illuminant.

00:01:12.800 --> 00:01:15.310
Now you have to worry about lighting and
a bunch of other things.

00:01:15.310 --> 00:01:17.520
But it makes sense, right,
because there's only one pigment.

00:01:17.520 --> 00:01:19.220
Anyway, we're going back,
we're looking at skin color.

00:01:19.220 --> 00:01:22.480
There's a bunch of pixels that are skin
color and a bunch of pixels are not.

00:01:22.480 --> 00:01:23.410
So, what am I going to do?

00:01:23.410 --> 00:01:24.210
Well, what I do is,

00:01:24.210 --> 00:01:28.290
I'm going to take the skin pixels, all
the pixels that I, somebody supervised,

00:01:28.290 --> 00:01:31.495
remember supervised learning,
have classified as being skin pixels.

00:01:31.495 --> 00:01:35.450
And I"m going to build a little
histogram of their feature.

00:01:35.450 --> 00:01:38.390
Now, the feature that
we're using here is hue.

00:01:38.390 --> 00:01:41.460
Hue is the, essentially the color
with the amount of brightness and

00:01:41.460 --> 00:01:42.550
saturation removed.

00:01:42.550 --> 00:01:46.860
It's, you know, how red, greenish,
bluish, yellowish is, is the color.

00:01:46.860 --> 00:01:51.920
And what this is, is this is a little
box that's essentially the percentage.

00:01:51.920 --> 00:01:56.050
It's a histogram that's
the percentage of pixels,

00:01:56.050 --> 00:01:59.830
of the skin pixels, that have this hue.

00:01:59.830 --> 00:02:05.020
And a histogram is an approximation
to the probability distribution.

00:02:05.020 --> 00:02:08.180
But notice that it's
the probability of getting

00:02:08.180 --> 00:02:11.320
that particular hue
given that it's skin.

00:02:11.320 --> 00:02:16.900
It is not the probability that
it's skin given these hues, okay?

00:02:16.900 --> 00:02:19.700
This is the,
remember the likelihood, all right?

00:02:19.700 --> 00:02:21.680
Well what do I do after
I've done the skin pixels,

00:02:21.680 --> 00:02:26.210
well I do the not skin pixels,
I build another graph, right?

00:02:26.210 --> 00:02:28.920
And in fact what's interesting
is you'll notice there's

00:02:28.920 --> 00:02:31.240
a little bit of overlap here, right?

00:02:31.240 --> 00:02:33.330
That's not too surprising.

00:02:33.330 --> 00:02:36.810
There are some, in fact in real life
there actually would have been some down

00:02:36.810 --> 00:02:38.270
here too, et cetera, whatever.

00:02:38.270 --> 00:02:38.890
Because, you know,

00:02:38.890 --> 00:02:43.340
there are skin colored pixels
in the non skin world, okay?

00:02:43.340 --> 00:02:46.940
And there are some skin that, you know,
people go and get tattooed or something.

00:02:46.940 --> 00:02:47.740
I dont know, whatever.

00:02:47.740 --> 00:02:51.600
You know, but in general I'm not
going to have a clean separation, but

00:02:51.600 --> 00:02:54.370
I'm going to have a distribution
that hopefully has some

00:02:54.370 --> 00:02:55.800
differences between them.

00:02:55.800 --> 00:02:56.770
And again,

00:02:56.770 --> 00:03:01.330
this is the probability of getting
that hue given that it's not skin.

00:03:01.330 --> 00:03:04.580
And it's not the probability
of the other way around.

00:03:04.580 --> 00:03:07.480
So all we have at the moment
is the probability of getting

00:03:07.480 --> 00:03:10.450
a hue given that it's skin, probability
of giving a hue given it's not skin.

00:03:10.450 --> 00:03:12.460
But what, what do we really want to do?

00:03:12.460 --> 00:03:14.490
Well, somebody gives us a picture.

00:03:14.490 --> 00:03:16.160
This is a stunning picture
of the Beatles, actually.

00:03:16.160 --> 00:03:17.583
I've never seen this picture before.

00:03:17.583 --> 00:03:18.460
Well, that's not totally true.

00:03:18.460 --> 00:03:20.833
I saw it the last time I gave this
lecture, but you know what I mean.

00:03:20.833 --> 00:03:23.134
Anyway, we get in a new picture, and

00:03:23.134 --> 00:03:27.960
what we want to do is we want to label
each pixel as being skin or not skin.

00:03:27.960 --> 00:03:32.180
Or more precisely, we want to know the
probability that it's skin or not skin,

00:03:32.180 --> 00:03:33.650
given say, some of the color.

00:03:33.650 --> 00:03:36.920
So, do I have the probability
that something's skin,

00:03:36.920 --> 00:03:39.030
given its hue on this slide?

00:03:39.030 --> 00:03:40.350
Megan, do I have it?

00:03:40.350 --> 00:03:40.990
Yes.

00:03:40.990 --> 00:03:41.560
&gt;&gt; Yes.

00:03:41.560 --> 00:03:42.810
&gt;&gt; Wrong.

00:03:42.810 --> 00:03:47.833
No, I have the probability of
a hue given that it's skin, right?

00:03:47.833 --> 00:03:48.625
What do we have to do?

00:03:48.625 --> 00:03:50.390
Well you've all seen this company wor,
coming.

00:03:50.390 --> 00:03:52.840
Where else do I get help from,
besides the data?

00:03:52.840 --> 00:03:56.570
I get a help from Dr.
Thomas Bayes, okay?

00:03:56.570 --> 00:03:58.120
Bayes rule, remember Bayes rule?

00:03:58.120 --> 00:04:00.160
We did Bayes rule now a couple of times?

00:04:00.160 --> 00:04:04.216
Bayes rule says that the probability
that it's skin, given x and

00:04:04.216 --> 00:04:06.500
that's referred to as the posterior.

00:04:06.500 --> 00:04:08.293
It can be written as a prior.

00:04:08.293 --> 00:04:10.451
If I pull out some pixel and
I don't tell you anything about it,

00:04:10.451 --> 00:04:12.300
what's the probability it's skin?

00:04:12.300 --> 00:04:13.520
That's the prior.

00:04:13.520 --> 00:04:18.820
And I multiply that by the likelihood,
P of x given that it is skin.

00:04:18.820 --> 00:04:20.899
Now that we do have that.

00:04:20.899 --> 00:04:22.340
Remember those histograms?

00:04:22.340 --> 00:04:25.630
That's the probability of getting
a hue given that it's skin.

00:04:25.630 --> 00:04:28.450
So that we have from the data,
all right?

00:04:28.450 --> 00:04:30.640
But the prior is added in here.

00:04:30.640 --> 00:04:33.690
And in fact, if we remove,
since we've got our measurement,

00:04:33.690 --> 00:04:35.090
we can sort of pull that out.

00:04:35.090 --> 00:04:37.870
We did that last time for
our particle filtering, actually.

00:04:37.870 --> 00:04:42.040
We can say that the probability that
something is skin, given a measurement,

00:04:42.040 --> 00:04:46.720
is the likelihood, or is proportional
to the likelihood times the prior.

00:04:46.720 --> 00:04:51.333
Which all brings us to the question
of where do you get your prior from?

00:04:51.333 --> 00:04:54.707
Okay, we've run into this problem before
when we told you to go to get in your

00:04:54.707 --> 00:04:57.850
car, drive to Greece,
go to the oracle, get a prior.

00:04:57.850 --> 00:05:01.590
Well that's one thing you could do,
but airfare to Greece is expensive.

00:05:01.590 --> 00:05:03.350
So we have to do something else and

00:05:03.350 --> 00:05:06.190
Steve sites,
wrote this Bayes rule in use/abuse.

00:05:06.190 --> 00:05:07.470
It was kind of cute, I kept the joke.

00:05:07.470 --> 00:05:07.970
Thanks Steve.

