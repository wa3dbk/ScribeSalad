WEBVTT
Kind: captions
Language: en

00:00:00.150 --> 00:00:03.340
First let's look at how we can reduce

00:00:03.340 --> 00:00:05.930
the overhead and marshaling the arguments and the

00:00:05.930 --> 00:00:09.530
data copying. Just to jog your memory, marshaling

00:00:09.530 --> 00:00:13.830
refers to the fact that the semantics of

00:00:13.830 --> 00:00:16.590
the RPC call being made between the client

00:00:16.590 --> 00:00:19.500
and the server. It's something that the operating

00:00:19.500 --> 00:00:22.100
system doesn't have any clue about. So in

00:00:22.100 --> 00:00:26.040
other words, the arguments that are actually passed

00:00:26.040 --> 00:00:29.000
between the client and the server, has semantic meaning

00:00:29.000 --> 00:00:32.060
only between the client and the server. The operating system

00:00:32.060 --> 00:00:35.440
has no knowledge of it. And therefore marshaling is

00:00:35.440 --> 00:00:39.750
the term that is used to say, let's accumulate. All

00:00:39.750 --> 00:00:42.780
of the arguments that is there in the call

00:00:42.780 --> 00:00:46.380
and make one contiguous network packet out of it, so

00:00:46.380 --> 00:00:48.040
that we can give it to the kernel and the

00:00:48.040 --> 00:00:51.140
kernel can send it out. That's what is being described

00:00:51.140 --> 00:00:54.450
as marshaling, and the biggest source of overhead in marshaling

00:00:54.450 --> 00:00:57.060
is the data copying that's going to happen, and I'll explain

00:00:57.060 --> 00:01:00.130
that in a minute. Potentially, in doing the marshaling, there

00:01:00.130 --> 00:01:03.780
could be three copies involved. Where are these three copies coming

00:01:03.780 --> 00:01:07.190
aboard? Well, first of all, the client is executing. When

00:01:07.190 --> 00:01:10.710
a client is executing a procedure, all the arguments for

00:01:10.710 --> 00:01:13.440
the procedure call that it wants to do is living

00:01:13.440 --> 00:01:16.400
on the stack of the client. And there is an entity,

00:01:16.400 --> 00:01:19.300
we'll introduce this terminology even before, called the

00:01:19.300 --> 00:01:22.600
client stub, and the role of the client stub

00:01:22.600 --> 00:01:24.930
is to take the arguments of the call which

00:01:24.930 --> 00:01:28.300
are living on the stack. And convert it into

00:01:28.300 --> 00:01:32.320
a contiguous sequence of bytes called an RPC

00:01:32.320 --> 00:01:36.130
message, so the RPC message has no semantic meaning,

00:01:36.130 --> 00:01:38.870
and it's just a contiguous string of bytes, which

00:01:38.870 --> 00:01:41.490
you can pass to the kernel, and the kernel

00:01:41.490 --> 00:01:43.470
can then send it out on the wire, just

00:01:43.470 --> 00:01:47.110
like any other message. So that's the first thing that

00:01:47.110 --> 00:01:50.810
the stub does, and that's the first source of overhead.

00:01:50.810 --> 00:01:53.420
The client stub is making the first copy, from the

00:01:53.420 --> 00:01:57.430
stack, in order to create an RPC message. Now remember

00:01:57.430 --> 00:02:00.300
that the client is a user program, so it is

00:02:00.300 --> 00:02:03.140
living in the user space outside the kernel, and so

00:02:03.140 --> 00:02:06.500
this RPC message, which is being created by the stub,

00:02:06.500 --> 00:02:09.630
it is pulled off the client's address space.

00:02:09.630 --> 00:02:12.250
Which is living outside the kernel. So, this RPC

00:02:12.250 --> 00:02:15.510
message is in user space and the kernel has

00:02:15.510 --> 00:02:18.760
to make a copy of the RPC message from

00:02:18.760 --> 00:02:22.290
the user space into its own buffer, the kernel

00:02:22.290 --> 00:02:25.840
buffer. And that's a second source of overhead. The

00:02:25.840 --> 00:02:29.222
second source of copy in doing the marshalling of

00:02:29.222 --> 00:02:31.670
the arguments. So now it is in the buffer

00:02:31.670 --> 00:02:34.730
of the operating system kernel, now the operating system can

00:02:34.730 --> 00:02:37.480
kick the network controller and say, hey, go ahead, take

00:02:37.480 --> 00:02:39.340
this buffer, send it out in the wire to the

00:02:39.340 --> 00:02:43.360
desired destination. And the network controller, at that point, is going

00:02:43.360 --> 00:02:46.030
to move the bits from the buffer, which is in

00:02:46.030 --> 00:02:49.860
the system memory, of the operating system, into its internal

00:02:49.860 --> 00:02:52.730
buffer using DMA. And this is the third copy that

00:02:52.730 --> 00:02:56.380
is happening. The copy that is done by the network controller,

00:02:56.380 --> 00:02:59.430
using DMA to move the bits of the

00:02:59.430 --> 00:03:03.180
RPC message, copied from the user space into the

00:03:03.180 --> 00:03:06.310
internal buffer of the kernel. And now this movement

00:03:06.310 --> 00:03:09.820
is being orchestrated by the hardware to move it

00:03:09.820 --> 00:03:13.500
from the kernel buffer into the internal buffer of

00:03:13.500 --> 00:03:15.390
the network controller, so that it can then get

00:03:15.390 --> 00:03:17.000
out on the wire. So those are the three

00:03:17.000 --> 00:03:21.440
copies involved in marshaling the arguments of the call,

00:03:21.440 --> 00:03:24.160
before it can be put out on the wire.

00:03:24.160 --> 00:03:27.170
And the copying overhead is the biggest source of overhead

00:03:27.170 --> 00:03:29.750
for RPC latency. Now, how do we reduce the

00:03:29.750 --> 00:03:33.240
number of copies? Well, it turns out that the third

00:03:33.240 --> 00:03:35.860
copy that you're looking at here, moving the bits

00:03:35.860 --> 00:03:39.160
from the system memory into the network controller, there's a

00:03:39.160 --> 00:03:42.472
hardware action. That is unavoidable, and therefore, we're going to

00:03:42.472 --> 00:03:47.240
live with it. Unless the network controller is completely redesigned,

00:03:47.240 --> 00:03:49.890
if the network controller is saying, well, I need

00:03:49.890 --> 00:03:53.150
to DMA the bits from the system memory into my

00:03:53.150 --> 00:03:56.610
buffer. Well, I have to DMA the bits from

00:03:56.610 --> 00:03:59.910
the system memory into my internal buffer, before I can

00:03:59.910 --> 00:04:02.440
put it all on the wire. Then this third

00:04:02.440 --> 00:04:05.290
copy is inevitable so we live with it. But we

00:04:05.290 --> 00:04:07.790
would like to see if we can try to reduce

00:04:07.790 --> 00:04:12.150
the number of copies involved here. The first idea is,

00:04:12.150 --> 00:04:15.450
can we eliminate this copy that is done by

00:04:15.450 --> 00:04:19.120
the client stub? Why is that happening? Well, it has

00:04:19.120 --> 00:04:21.200
to create a network message in order to send it

00:04:21.200 --> 00:04:24.220
out on the wire. As we said. That the semantics

00:04:24.220 --> 00:04:27.570
of this call is only known to the client and

00:04:27.570 --> 00:04:31.540
the server, and the client stub is taking the argument

00:04:31.540 --> 00:04:34.760
and making a network packet out of it. And it

00:04:34.760 --> 00:04:37.160
was doing it in user space. And so what we're

00:04:37.160 --> 00:04:40.080
going to do is, we're going to marshal it directly

00:04:40.080 --> 00:04:42.380
into the kernel buffer. In other words, we've now

00:04:42.380 --> 00:04:46.310
moved this stub, the client side stub, from

00:04:46.310 --> 00:04:49.250
the user space down into the kernel. If you

00:04:49.250 --> 00:04:51.900
can move it into the kernel, then from

00:04:51.900 --> 00:04:56.720
the stack a stub can directly marshal it into

00:04:56.720 --> 00:04:59.450
the kernel buffer. And so that intermediate copy

00:04:59.450 --> 00:05:02.390
that we had here creating an RPC message and

00:05:02.390 --> 00:05:08.260
copying it. Again into the kernel buffer is avoided if the stub can

00:05:08.260 --> 00:05:12.920
directly work on the stack, and write it into the kernel memory. So what this

00:05:12.920 --> 00:05:20.530
means is that, at instantiation time, the client stub is installed inside the

00:05:20.530 --> 00:05:27.480
kernel. At bind time, when the client binds with the server at the bind time,

00:05:27.480 --> 00:05:29.980
what we going to do is, we going to say that

00:05:29.980 --> 00:05:32.710
here is the client stub, please put it inside

00:05:32.710 --> 00:05:35.790
the kernel so that, later on, you can use

00:05:35.790 --> 00:05:39.070
that in order to do the marshaling. So the synthesized

00:05:39.070 --> 00:05:43.160
procedure is installing the kernel for each client call,

00:05:43.160 --> 00:05:46.140
so for each client server relationship. We synthesize the

00:05:46.140 --> 00:05:48.950
procedure, which is the client's job, install it in

00:05:48.950 --> 00:05:53.310
the kernel for use every time, I make this call.

00:05:53.310 --> 00:05:56.910
This stub can be invoked to convert the argument

00:05:56.910 --> 00:05:59.180
that are living on the stack, into a network

00:05:59.180 --> 00:06:01.960
message, and directly put it into the kernel buffer.

00:06:01.960 --> 00:06:06.030
So this, obviously, will eliminate. From the two copies

00:06:06.030 --> 00:06:09.530
down to one copy because, the intermediate copy of

00:06:09.530 --> 00:06:12.950
converting the arguments into an RPC message is now

00:06:12.950 --> 00:06:16.070
eliminated. Now, the problem with this idea is that

00:06:16.070 --> 00:06:18.920
we're seeing, let's dump some code into the kernel and

00:06:18.920 --> 00:06:23.895
that may not be something that is, so palatable. So this is a solution

00:06:23.895 --> 00:06:28.730
that's possible if the RPC service that is being provided between the client and

00:06:28.730 --> 00:06:33.190
the server, the trusted service and therefore we can trust, who is putting the

00:06:33.190 --> 00:06:38.490
stub into the kernel. In that case the solution maybe a reasonable one to adopt.

