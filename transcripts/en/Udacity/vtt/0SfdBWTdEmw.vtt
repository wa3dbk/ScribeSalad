WEBVTT
Kind: captions
Language: en

00:00:00.390 --> 00:00:05.163
If you think about RNNs in general,
they are a very general, trainable model

00:00:05.163 --> 00:00:09.370
that maps variable-length
sequences to fixed-length vectors.

00:00:09.370 --> 00:00:12.490
In fact, thanks to the sequence
generation and Bing search that we've

00:00:12.490 --> 00:00:16.940
just discussed, they can also be made to
map fixed-length vectors to sequences.

00:00:18.028 --> 00:00:23.110
Start from a vector, map it to the state
of your RNN, then produce a prediction.

00:00:23.110 --> 00:00:25.500
Then sample from it,
or use Bing search, and

00:00:25.500 --> 00:00:27.940
feed them back into the RNN
to get the next one.

00:00:29.180 --> 00:00:32.700
Now you have those two building blocks,
and you can stitch them together.

00:00:32.700 --> 00:00:35.550
It gives you a new model that maps
sequences of arbitrary lengths to other

00:00:35.550 --> 00:00:36.870
sequences of arbitrary length.

00:00:36.870 --> 00:00:40.450
And it's fully trainable.

00:00:41.590 --> 00:00:43.080
What can you do with?

00:00:43.080 --> 00:00:44.160
Many things.

00:00:44.160 --> 00:00:46.845
Imagine that your input is
a sequence of English words and

00:00:46.845 --> 00:00:48.850
you output a sequence of French words.

00:00:48.850 --> 00:00:50.690
You've just built a machine
translation system.

00:00:51.840 --> 00:00:53.660
All you need is some parallel text.

00:00:54.890 --> 00:00:58.010
Imagine that your input is sounds and
your output words.

00:00:58.010 --> 00:01:00.300
You've just built an end-to-end
speech recognition system.

00:01:01.350 --> 00:01:06.160
Real systems based on variations on this
design exist and are very competitive.

00:01:06.160 --> 00:01:08.990
In practice,
they do require a lot of data and

00:01:08.990 --> 00:01:10.204
a lot of compute to work very well.

00:01:11.455 --> 00:01:14.935
We've also talked about conv nets,
which basically map images

00:01:14.935 --> 00:01:18.335
into vectors that represent
the content of that image.

00:01:18.335 --> 00:01:23.655
So picture what would happen if you took
a conv net and connected it to an RNN.

00:01:23.655 --> 00:01:26.959
You have an image going in, and
a sequence of things coming out.

00:01:26.959 --> 00:01:28.675
A sequence of words maybe.

00:01:28.675 --> 00:01:30.015
Maybe the caption for that image.

00:01:31.230 --> 00:01:34.440
To do that,
you need training images and captions.

00:01:34.440 --> 00:01:37.190
There are a few data sets out
there that you can use for that.

00:01:37.190 --> 00:01:38.793
Most notably the coco data set.

00:01:38.793 --> 00:01:42.740
It does images and
crowdsourced captions for them.

00:01:42.740 --> 00:01:46.393
You can train a model that uses
a cov net to analyze the image and

00:01:46.393 --> 00:01:48.229
generate captions from them.

00:01:48.229 --> 00:01:49.390
And it works.

00:01:49.390 --> 00:01:53.250
You can get great captions,
generated completely automatically, and

00:01:53.250 --> 00:01:55.470
it sometimes fails in very,
very funny ways.

