WEBVTT
Kind: captions
Language: en

00:00:00.400 --> 00:00:01.589
Alright, what do you think, Charles?

00:00:01.589 --> 00:00:03.940
&gt;&gt; Okay, so I think a couple of things. Let me

00:00:03.940 --> 00:00:07.850
just talk this out loud. So we're trying to learn incrementally.

00:00:07.850 --> 00:00:10.270
We're trying to figure out, how if we just kind of

00:00:10.270 --> 00:00:12.270
keep adding these random variables in a sort of way to

00:00:12.270 --> 00:00:15.050
what you described before what we would end up at. So

00:00:15.050 --> 00:00:19.300
why do we want to choose alpha t with a particular

00:00:19.300 --> 00:00:22.750
kind of property well if you look at it, it's clear

00:00:22.750 --> 00:00:25.410
that alpha t at each time step is moving closer and closer

00:00:25.410 --> 00:00:27.510
to zero. So it means you start out learning a

00:00:27.510 --> 00:00:30.150
lot and you sort of believe things less and less and

00:00:30.150 --> 00:00:32.670
less or you let things change you less and less and

00:00:32.670 --> 00:00:36.800
less over time. So what is that likely to end up

00:00:36.800 --> 00:00:38.870
at? Well the first thing I notice that well if

00:00:38.870 --> 00:00:42.220
it's going to do that then it has to converge. Some

00:00:42.220 --> 00:00:45.680
point. It just makes sense. So I, that helps me to

00:00:45.680 --> 00:00:50.630
eliminate the second answer. But actually, that also let's me eliminate

00:00:50.630 --> 00:00:54.660
the fourth answer because in some ways they, that says the same thing.

00:00:54.660 --> 00:00:59.200
So that leaves us with the expected value of x and the expected value

00:00:59.200 --> 00:01:02.750
of x squared. For the expected value of x squared. Whichever one it is

00:01:02.750 --> 00:01:07.660
that is variance. Okay so, I'm going to go with, the expected value of x.

00:01:07.660 --> 00:01:11.230
&gt;&gt; So, the expected value of x, right. So, so it turns out that this

00:01:11.230 --> 00:01:13.510
is actually a wave computing the average

00:01:13.510 --> 00:01:15.790
by just, you know, repeatedly sampling and updating

00:01:15.790 --> 00:01:17.410
your values. And, and the way to think about it,

00:01:17.410 --> 00:01:20.370
is that sometimes the x values that we draw are going

00:01:20.370 --> 00:01:22.530
to be a bit above the average, and it's going to

00:01:22.530 --> 00:01:25.200
pull the v value up. Sometimes it's a little bit below

00:01:25.200 --> 00:01:27.310
the average, and it's going to pull the value down.

00:01:27.310 --> 00:01:30.430
But in the limit, all those pulls and pushes are going

00:01:30.430 --> 00:01:33.060
to cancel out, and it's going to settle in on the

00:01:33.060 --> 00:01:36.730
actual average value or the expected value of this random variable.

00:01:36.730 --> 00:01:38.700
&gt;&gt; Right, because in principle, you're going

00:01:38.700 --> 00:01:41.060
to see an infinite number of those things.

00:01:41.060 --> 00:01:43.550
And effectively all you are doing is adding them all up, and sort

00:01:43.550 --> 00:01:47.300
of, you know. Well you're effectively averaging them one little bit at a time.

00:01:47.300 --> 00:01:48.660
&gt;&gt; Yeah, that's a good thing about it, that

00:01:48.660 --> 00:01:50.960
in fact it's adding them up and it's computing

00:01:50.960 --> 00:01:54.810
a weighted average, but the weights are these alpha's

00:01:54.810 --> 00:01:56.540
and the weights are decaying over time so we're

00:01:56.540 --> 00:01:58.460
going to put more weight on the more recent

00:01:58.460 --> 00:02:00.280
ones, but some more weight on the further away

00:02:00.280 --> 00:02:02.960
ones but it sort of doesn't matter because the

00:02:02.960 --> 00:02:06.240
order, since we're drawing this iid, the order doesn't

00:02:06.240 --> 00:02:08.520
matter and the thing that has to converge to is the mean.

00:02:08.520 --> 00:02:10.008
&gt;&gt; Right. I like it.

00:02:10.008 --> 00:02:14.234
&gt;&gt; So let's relate that back to what we do in Q-learning.

