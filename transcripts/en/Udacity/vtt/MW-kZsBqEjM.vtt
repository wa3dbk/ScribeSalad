WEBVTT
Kind: captions
Language: en

00:00:00.290 --> 00:00:02.320
Let's think about the binary
search algorithm.

00:00:02.320 --> 00:00:07.400
We're given a sorted array, A,
containing let's say n unique elements.

00:00:07.400 --> 00:00:09.070
Then, given a target value, v,

00:00:09.070 --> 00:00:13.950
you want to find the largest index
i such that a sub i is at most v.

00:00:13.950 --> 00:00:16.560
You can find this index
by binary search.

00:00:16.560 --> 00:00:20.380
To compute the number of cache misses,
you'd set up recurrence, like so.

00:00:20.380 --> 00:00:22.910
Once the search interval
falls within a cache line,

00:00:22.910 --> 00:00:25.340
there would be just one cache miss.

00:00:25.340 --> 00:00:26.980
Otherwise, you'd pay for one miss,

00:00:26.980 --> 00:00:30.690
plus any additional misses to search
the remaining half of the array.

00:00:30.690 --> 00:00:33.000
Solving the recurrence,
you'd find the following.

00:00:33.000 --> 00:00:35.000
Big O of log n over l.

00:00:35.000 --> 00:00:37.490
Compare this to the lower bound.

00:00:37.490 --> 00:00:41.980
Binary search differs from the lower
bound by about a factor of log L.

00:00:41.980 --> 00:00:43.260
So it's not optimal, but

00:00:43.260 --> 00:00:47.650
one nice thing about binary search is
that it's already cache oblivious.

00:00:47.650 --> 00:00:51.330
It makes no references to the cache
size Z or the line size L.

00:00:51.330 --> 00:00:54.280
But it begs the question,
how do we get to the lower bound?

00:00:54.280 --> 00:00:56.200
In fact, there's a way to do it.

00:00:56.200 --> 00:00:58.990
Leave the logic of the binary
search algorithm intact, but

00:00:58.990 --> 00:01:00.940
change the data layout.

00:01:00.940 --> 00:01:05.620
Remember that a binary search tree
maintains some ordering of its elements.

00:01:05.620 --> 00:01:09.220
Let's number this tree according
to an in-order traversal.

00:01:09.220 --> 00:01:12.530
If you interpret these numbers
as addresses or index positions,

00:01:12.530 --> 00:01:16.480
then the layout of the tree nodes
is equivalent to sorted order.

00:01:16.480 --> 00:01:19.170
But there's nothing
sacred about this layout.

00:01:19.170 --> 00:01:20.050
In fact let's consider

00:01:20.050 --> 00:01:23.210
a different ordering which is
called the Vanamdebos layout.

00:01:23.210 --> 00:01:25.210
Iâ€™ll sketch this now but
if you want some details,

00:01:25.210 --> 00:01:27.480
see the nice tutorial by Erik Demaine.

00:01:27.480 --> 00:01:30.870
The idea is to use
the following recursive layout.

00:01:30.870 --> 00:01:34.050
Suppose you start with
a complete binary search tree.

00:01:34.050 --> 00:01:37.640
If it has about n nodes it
should have log n levels.

00:01:37.640 --> 00:01:39.820
Now divide the levels in half.

00:01:39.820 --> 00:01:43.350
So there would be about one half
log n levels above the cut line and

00:01:43.350 --> 00:01:45.500
about one half log n below.

00:01:45.500 --> 00:01:49.430
This also means the upper subtree
will have about root n nodes.

00:01:49.430 --> 00:01:54.850
Below the cut line, there will about
root n subtrees, each of size root n.

00:01:54.850 --> 00:01:56.690
Here's the Van Emde Boas layout idea.

00:01:56.690 --> 00:02:01.850
You have a binary search tree that you'd
like to lay out linearly in slow memory.

00:02:01.850 --> 00:02:03.390
After partitioning the levels,

00:02:03.390 --> 00:02:06.270
lay out all of the upper
sub-tree elements together.

00:02:06.270 --> 00:02:09.810
And then concatenate them with
the lower sub-tree elements.

00:02:09.810 --> 00:02:11.850
And when I say lay out
the elements together,

00:02:11.850 --> 00:02:16.550
I mean recursively apply the Van
Emde Boas layout to each sub-tree.

00:02:16.550 --> 00:02:17.700
So, what does this buy you?

00:02:18.730 --> 00:02:20.020
Let's zoom in on the tree,

00:02:20.020 --> 00:02:24.390
looking at the point where
the sub-trees fit within cache lines.

00:02:24.390 --> 00:02:28.950
That is, in the figure the elements in
each of the smallest sub trees shown

00:02:28.950 --> 00:02:31.750
fit within a cache line size hole.

00:02:31.750 --> 00:02:36.210
A binary search in this tree takes
some path from the root to the leaf.

00:02:36.210 --> 00:02:38.040
Since the sub trees are a size L,

00:02:38.040 --> 00:02:42.340
you only generate a cache miss when you
hit the root of one of the sub trees.

00:02:42.340 --> 00:02:45.930
Now the maximum height of one of these
little cache line size sub trees is

00:02:45.930 --> 00:02:47.230
log L.

00:02:47.230 --> 00:02:49.000
So, starting at the root of the tree,

00:02:49.000 --> 00:02:52.150
how many of these size L
sub trees will you visit?

00:02:52.150 --> 00:02:54.280
Well, the height of the tree is log-n.

00:02:54.280 --> 00:02:56.370
So, on any path from root to leaf,

00:02:56.370 --> 00:03:00.860
you'll encounter log-n
over log-l sub-trees.

00:03:00.860 --> 00:03:03.580
That's totally awesome
because that is optimal.

00:03:03.580 --> 00:03:04.450
Woot!

00:03:04.450 --> 00:03:07.610
The important lesson is
that data layout matters.

00:03:07.610 --> 00:03:10.710
What we did hear that was so
cool is we took the standard binary

00:03:10.710 --> 00:03:14.640
search algorithm, reshuffled how we
stored the data in order to get an IO

00:03:14.640 --> 00:03:18.384
optimal algorithm, and
the layout is itself cache-oblivious.

