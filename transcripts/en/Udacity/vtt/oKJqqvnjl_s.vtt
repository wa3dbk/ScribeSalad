WEBVTT
Kind: captions
Language: en

00:00:00.260 --> 00:00:04.010
So we have threads at the user level, we have threads at the kernel level.

00:00:04.010 --> 00:00:07.610
We will now see what are some of the interactions that are necessary in

00:00:07.610 --> 00:00:10.410
order to efficiently manage threads.

00:00:10.410 --> 00:00:12.540
Consider we have a multithreaded process.

00:00:12.540 --> 00:00:16.420
And let's say that process has four user-level threads.

00:00:16.420 --> 00:00:19.910
However, the process is such that, at any given point of time,

00:00:19.910 --> 00:00:23.460
the actual level of concurrency is just two.

00:00:23.460 --> 00:00:25.590
Basically, if you look at the process,

00:00:25.590 --> 00:00:29.910
it always happens that two of its user-level threads are waiting on I/O, and

00:00:29.910 --> 00:00:32.470
then some other two are actually executing.

00:00:32.470 --> 00:00:36.430
So, if our operating system has a limit on the number of kernel threads that it

00:00:36.430 --> 00:00:42.060
can support, it would be nice if the user-level process actually said,

00:00:42.060 --> 00:00:43.950
I just really need two threads.

00:00:43.950 --> 00:00:48.380
So when the process starts, the kernel will first give it, let's say, a default

00:00:48.380 --> 00:00:52.440
number of kernel-level threads and the accompanying lightweight threads.

00:00:52.440 --> 00:00:54.100
And let's say that is one.

00:00:54.100 --> 00:00:58.600
Then the process will request additional kernel-level threads, and the way it's

00:00:58.600 --> 00:01:03.670
done is that the kernel now supports a system call called set_concurrency.

00:01:03.670 --> 00:01:07.930
In response to this system call the kernel will create additional threads and

00:01:07.930 --> 00:01:10.510
it will allocate those to this process.

00:01:10.510 --> 00:01:14.230
Now let's consider this scenario in which the two user-level threads that were

00:01:14.230 --> 00:01:17.690
mapped on the underlying kernel-level threads block.

00:01:17.690 --> 00:01:22.960
They needed to perform some I/O operation and then they were basically

00:01:22.960 --> 00:01:27.560
moved on the wait queue that's associated with that particular I/O event.

00:01:27.560 --> 00:01:30.120
So the kernel level threads are blocked as well.

00:01:30.120 --> 00:01:33.790
Now let's say we have a situation in which the two user-level threads that

00:01:33.790 --> 00:01:38.080
were running on kernel level threads issued an I/O request, and

00:01:38.080 --> 00:01:40.040
now have to wait for that to complete.

00:01:40.040 --> 00:01:41.190
So it's a blocking I/O.

00:01:41.190 --> 00:01:44.950
What that means is that the kernel-level threads themselves,

00:01:44.950 --> 00:01:47.140
they're also blocked on that I/O operation.

00:01:47.140 --> 00:01:51.900
They're waiting in a queue somewhere in the kernel for that I/O event to occur.

00:01:51.900 --> 00:01:55.350
Now we have a situation where the process as a whole is blocked,

00:01:55.350 --> 00:01:59.440
because it only had two kernel-level threads, both of them are blocked, and

00:01:59.440 --> 00:02:03.700
there are user-level threads that are ready to run and make progress.

00:02:03.700 --> 00:02:07.810
The reason why this is happening is because the user-level library doesn't know

00:02:07.810 --> 00:02:09.320
what is happening in the kernel,

00:02:09.320 --> 00:02:13.610
it doesn't know that the kernel threads are about to block.

00:02:13.610 --> 00:02:16.990
What would have really been useful is if the kernel had

00:02:16.990 --> 00:02:22.520
notified the user-level library before it blocks the kernel-level threads.

00:02:22.520 --> 00:02:25.850
And then the user-level library can look at its run queue,

00:02:25.850 --> 00:02:29.320
it can see that it has multiple runnable user-level threads, and,

00:02:29.320 --> 00:02:33.700
in response, can let the kernel know, so, call a system call

00:02:33.700 --> 00:02:37.620
to request more kernel-level threads or lightweight processes.

00:02:37.620 --> 00:02:40.915
Now in response to this call, the kernel can allocate an extra

00:02:40.915 --> 00:02:44.905
kernel-level thread, and the library can start scheduling the remaining

00:02:44.905 --> 00:02:48.590
user-level threads onto the associated lightweight process.

00:02:48.590 --> 00:02:53.220
At a later time when the I/O operation completes, at some point the kernel will

00:02:53.220 --> 00:02:57.960
notice that one of the kernel-level threads is pretty much constantly idle,

00:02:57.960 --> 00:03:02.050
because we said that that's the natural state of this particular application.

00:03:02.050 --> 00:03:05.310
So maybe the kernel can tell the kernel-level library that, you no

00:03:05.310 --> 00:03:09.220
longer have access to this kernel-level thread, so you can't schedule on it.

00:03:09.220 --> 00:03:12.240
By going through these examples you realize that

00:03:12.240 --> 00:03:15.630
both the user-level library doesn't know what's happening in the kernel, but

00:03:15.630 --> 00:03:19.440
also the kernel doesn't know what's happening at the user level.

00:03:19.440 --> 00:03:22.390
Both of these fact cause for some problems.

00:03:22.390 --> 00:03:26.680
To correct for these issues, we saw how in the Solaris threading implementation,

00:03:26.680 --> 00:03:30.300
they introduced certain system calls and special signals that

00:03:30.300 --> 00:03:35.430
can be used to pass or request certain things among these two layers.

00:03:35.430 --> 00:03:38.042
And basically this is how the kernel-level and

00:03:38.042 --> 00:03:41.510
the user-level thread management interact and coordinate.

