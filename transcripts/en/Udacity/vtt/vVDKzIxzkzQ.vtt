WEBVTT
Kind: captions
Language: en

00:00:00.240 --> 00:00:04.610
So I think it will help if we think
through what the basic update rule is

00:00:04.610 --> 00:00:06.980
when you're doing reinforcement
learning,or cue learning.

00:00:06.980 --> 00:00:10.870
And then to try to adapt to this
notion that are we're representing our

00:00:10.870 --> 00:00:13.490
value function using some kind
of function approximater.

00:00:13.490 --> 00:00:15.350
&gt;&gt; All right.
&gt;&gt; So here's a way to think about this,

00:00:15.350 --> 00:00:19.340
we've got Q function that we're
going to try to represent as

00:00:19.340 --> 00:00:23.100
some function of a set of parameters,
or weights.

00:00:23.100 --> 00:00:25.920
Maybe let's say one set of
parameters for each action.

00:00:25.920 --> 00:00:27.010
&gt;&gt; Sure.

00:00:27.010 --> 00:00:29.420
&gt;&gt; And
what F is doing is taking a state and

00:00:29.420 --> 00:00:31.080
translating it into a set of features.

00:00:31.080 --> 00:00:32.400
We're going to feed those features and

00:00:32.400 --> 00:00:34.600
the parameters of the function
into something called F.

00:00:34.600 --> 00:00:37.280
And what will come out of
that will be the Q value.

00:00:37.280 --> 00:00:39.720
So this is intended
to be fairly general.

00:00:39.720 --> 00:00:42.880
And we'll talk about some specific
examples of this in a moment.

00:00:42.880 --> 00:00:46.210
But let's just imagine that when Q
is being represented by some kind of

00:00:46.210 --> 00:00:48.998
function approximater, and
here's how Q learning works.

00:00:48.998 --> 00:00:53.220
We have an experienced couple that
comes in, state, action, reward next,

00:00:53.220 --> 00:00:55.710
state and then we do an update.

00:00:55.710 --> 00:00:57.690
Now what's the update usually?

00:00:57.690 --> 00:01:02.230
We try to move the the value of
that state action pair a little bit

00:01:02.230 --> 00:01:06.890
towards the reward plus the discounted
value of the next state S prime.

00:01:06.890 --> 00:01:10.310
So, that's the bellman equation
kind of hidden in here and

00:01:10.310 --> 00:01:14.190
the difference between that and
the current Q value is the TD error.

00:01:14.190 --> 00:01:15.830
And what do we want to
do with the TD error?

00:01:15.830 --> 00:01:19.380
The TD error tells us is our
current prediction too high,

00:01:19.380 --> 00:01:20.680
too low, just right.

00:01:20.680 --> 00:01:22.450
You know does it like porridge?

00:01:22.450 --> 00:01:25.175
And what we want to do is change.

00:01:25.175 --> 00:01:27.670
[LAUGH]
&gt;&gt; That's the three bears I guess.

00:01:27.670 --> 00:01:31.320
But the fact that matter is we actually
need to know the kind of the direction

00:01:31.320 --> 00:01:32.260
that we should be going.

00:01:32.260 --> 00:01:36.770
Are we too high or too low and we want
to move the parameters that represent

00:01:36.770 --> 00:01:41.840
the Q function, you know so
a given weight say W super a sub i.

00:01:41.840 --> 00:01:43.310
We want to move it a little bit,

00:01:43.310 --> 00:01:47.540
this is a learning rate alpha,
in the direction of that TD Error.

00:01:47.540 --> 00:01:49.060
But how much do we move the weight,

00:01:49.060 --> 00:01:53.410
depends on how that weight actually
influences the Q function, right?

00:01:53.410 --> 00:01:57.210
So what's the partial derivative of
the Q value for that state action pair

00:01:57.210 --> 00:02:00.040
as a function of that particular
weight that we want to change?

00:02:00.040 --> 00:02:03.550
And we're going to change that weight
proportion, we're going to change things

00:02:03.550 --> 00:02:06.050
proportionately to that,
to that gradient.

00:02:06.050 --> 00:02:07.370
So this should look kind of familiar.

00:02:07.370 --> 00:02:08.229
Does this look familiar?

00:02:09.259 --> 00:02:13.030
&gt;&gt; Yeah, it looks exactly like the,
you know, we drew up the update rule.

00:02:13.030 --> 00:02:15.580
The things that we did with perceptrons.

00:02:15.580 --> 00:02:18.780
Yeah, it's a normal kind
of gradient update rule.

00:02:19.810 --> 00:02:20.540
&gt;&gt; Yeah, right.

00:02:20.540 --> 00:02:21.460
Exactly so.

00:02:21.460 --> 00:02:22.020
Right and so

00:02:22.020 --> 00:02:25.110
when you're representing a function in
terms of a set of parameters we want to

00:02:25.110 --> 00:02:28.840
know how did the parameters impact the
value that we're predicting and let's

00:02:28.840 --> 00:02:32.590
move those parameters in a direction
that makes the prediction more accurate.

00:02:32.590 --> 00:02:38.030
So, this is a way that we can actually
view Q learning as an update rule for

00:02:38.030 --> 00:02:39.699
underlying parameters of a function or
proxy.

00:02:41.900 --> 00:02:47.910
And I guess the only difference is that,
unlike the sort of y star minus

00:02:47.910 --> 00:02:53.320
y thing that we used to do, this is
what we got on this particular step.

00:02:53.320 --> 00:02:56.790
So that's what makes a TD Error
as opposed to actual error,

00:02:56.790 --> 00:02:57.610
if that makes any sense.

00:02:57.610 --> 00:02:58.410
Does that make sense?

00:02:58.410 --> 00:02:59.660
&gt;&gt; Yeah, I think that's exactly right.

00:02:59.660 --> 00:03:03.530
So when we were thinking about
supervised learning, what was happening

00:03:03.530 --> 00:03:06.600
is we have some kind of output for
our current input and we'll call that y.

00:03:06.600 --> 00:03:09.290
And then we have a target output, y*.

00:03:09.290 --> 00:03:11.860
And we want to change
the parameters of the function so

00:03:11.860 --> 00:03:15.190
that instead of getting something like
y we get something much more like y*.

00:03:15.190 --> 00:03:20.775
So I think think what your point is is
that this Bellman equation kind of value

00:03:20.775 --> 00:03:25.065
and this kind of current prediction
do play the rules of y* and y.

00:03:25.065 --> 00:03:26.775
So this I think is
a really good analogy.

00:03:26.775 --> 00:03:29.705
But one way that it's different is that
why star when we're doing supervised

00:03:29.705 --> 00:03:32.535
learning is actually the right value,
right?

00:03:32.535 --> 00:03:36.530
It's coming from some kind of
trusted source, and it is the label.

00:03:36.530 --> 00:03:41.210
Here, we're bootstrapping,
which I think is a kind of wrapping.

00:03:41.210 --> 00:03:46.610
And what we're doing with bootstrapping
is we're using our current predictions

00:03:46.610 --> 00:03:48.770
as a kind of way of making a target for

00:03:48.770 --> 00:03:51.730
what our other prediction
should be moving toward.

00:03:51.730 --> 00:03:55.090
So, this this should look
a little bit unnerving.

00:03:55.090 --> 00:03:59.260
But, it's not a bad idea but
it's also not clearly of great idea

00:03:59.260 --> 00:04:03.630
because instead of using real label data
we're actually making up our own labels

00:04:03.630 --> 00:04:06.580
using the current prediction
from the Q function.

00:04:06.580 --> 00:04:07.440
&gt;&gt; Right.
&gt;&gt; And we'll see

00:04:07.440 --> 00:04:09.520
sometimes this can really
run us in a trouble and

00:04:09.520 --> 00:04:11.880
we have to be a little bit careful
to make sure that it doesn't.

00:04:11.880 --> 00:04:12.660
Okay.
&gt;&gt; Right?

00:04:12.660 --> 00:04:14.630
So this is in a very general form.

00:04:14.630 --> 00:04:17.450
I think it would be helpful though to
make it make more sense if we dive in

00:04:17.450 --> 00:04:21.269
and think about some particular
representations of the Q function.

00:04:21.269 --> 00:04:21.769
&gt;&gt; Okay.

