WEBVTT
Kind: captions
Language: en

00:00:00.440 --> 00:00:03.320
So I want to look at
HMMs in the context of

00:00:03.320 --> 00:00:05.500
what's referred to as
Gesture Recognition.

00:00:05.500 --> 00:00:09.250
It's sort of an old approach, an old
problem, especially using HMMs but it's,

00:00:09.250 --> 00:00:10.410
it's, it's a good paradigm.

00:00:11.560 --> 00:00:15.850
There's a conference called Face and
Gesture, in, Computer Vision.

00:00:15.850 --> 00:00:19.080
So, obviously gesture must be important
because there's a whole conference

00:00:19.080 --> 00:00:20.610
called Face and Gesture.

00:00:20.610 --> 00:00:23.400
And it's a long story going back to
college of mine about why face and

00:00:23.400 --> 00:00:24.940
gesture are put together.

00:00:24.940 --> 00:00:29.757
I'll tell it to you some other
time over drinks of some flavor.

00:00:29.757 --> 00:00:36.672
[COUGH] the typical scenario was you
do some examples of each gesture.

00:00:36.672 --> 00:00:40.135
The system learns or is trained to
have some sort of model for each.

00:00:40.135 --> 00:00:42.005
And like said lots of things used HMMs.

00:00:42.005 --> 00:00:46.885
And at run time one would compare the
output of each of the different model,

00:00:46.885 --> 00:00:50.570
you would evaluate what you saw in
terms of the likelihood sequence.

00:00:50.570 --> 00:00:53.210
And if that probability
exceeded some threshold,

00:00:53.210 --> 00:00:55.500
you'd say, uh-huh,
a particular gesture happened.

00:00:55.500 --> 00:00:57.380
So this was done, as I said awhile ago,

00:00:57.380 --> 00:00:59.730
sort of the mid-90s,
then it started to taper off.

00:00:59.730 --> 00:01:02.860
Because partially this whole
idea of having sort of

00:01:02.860 --> 00:01:07.630
individual trained gestures that people
would do to, to make something happen,

00:01:07.630 --> 00:01:11.430
that didn't feel like just the
recognition of individual vocabularies.

00:01:11.430 --> 00:01:13.320
So it's sort of tapered off.

00:01:13.320 --> 00:01:17.250
But then all of a sudden, new life was
injected into the field of gesture

00:01:17.250 --> 00:01:20.455
recognition and it was because
of human robotics interaction.

00:01:20.455 --> 00:01:21.640
Okay?

00:01:21.640 --> 00:01:26.910
People really love the idea of
being able to gesture to a robot.

00:01:26.910 --> 00:01:30.230
Which if you've ever been to my lab we
sometimes gesture to the robot in a not

00:01:30.230 --> 00:01:31.350
so friendly way.

00:01:31.350 --> 00:01:33.300
But this idea that you
could do things and

00:01:33.300 --> 00:01:37.400
control a robot, first of all,
speech is very difficult

00:01:37.400 --> 00:01:39.850
in a noisy environment unless
you're wearing a microphone.

00:01:40.892 --> 00:01:43.180
Gesture, you might be able to
do something more remotely.

00:01:43.180 --> 00:01:46.450
And it was just this general
idea that it was kind of cool,

00:01:46.450 --> 00:01:51.110
to be able to make some gestures and
have a robot respond appropriately.

00:01:51.110 --> 00:01:53.930
So and here I just pulled out a paper,
Reed 2014.

00:01:53.930 --> 00:01:57.420
This was at some conference
that just happened, and

00:01:57.420 --> 00:02:03.500
since I'm recording this in 2014, it's
a, you know, I'm not making this up.

00:02:03.500 --> 00:02:05.940
This is, you know,
what's currently going on.

00:02:05.940 --> 00:02:09.160
I'm not advocating one way or
the other the importance or

00:02:09.160 --> 00:02:11.000
whatever of this
particular piece of work.

