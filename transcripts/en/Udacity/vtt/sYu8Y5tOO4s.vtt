WEBVTT
Kind: captions
Language: en

00:00:00.220 --> 00:00:03.550
So, we've determined that we want to figure out which set of parameters provide

00:00:03.550 --> 00:00:06.680
the best predictions for our output variable, but how can we do that?

00:00:07.870 --> 00:00:10.520
We'll use an algorithm called gradient descent.

00:00:10.520 --> 00:00:14.930
First, we need to define some cost function, which we'll call J of big theta.

00:00:16.260 --> 00:00:20.320
I'm going to use big theta here, to represent our entire set of thetas, and

00:00:20.320 --> 00:00:22.820
I'll use this notation throughout the rest of this lesson.

00:00:22.820 --> 00:00:25.490
The cost function is meant to provide a measure of how well our

00:00:25.490 --> 00:00:28.850
current set of thetas does, at modeling the observed data.

00:00:28.850 --> 00:00:31.020
So, we want to minimize the cost function's value.

00:00:32.310 --> 00:00:35.470
As we discussed just a moment ago, when we're doing linear regression,

00:00:35.470 --> 00:00:39.350
our cost function J of theta, can simply measure the sum of the squares of

00:00:39.350 --> 00:00:42.290
the differences between our predicted and observed values.

00:00:42.290 --> 00:00:47.090
I am going to formalize this a little bit, and say this, J of big theta, is

00:00:47.090 --> 00:00:53.230
equal to one half, times the sum from i equals one to m, of y predicted of x i.

00:00:53.230 --> 00:00:55.840
Minus Y observed i squared.

00:00:55.840 --> 00:01:02.150
Where Y predicted x i, equals the sum of n from zero to big N, of theta n x n i.

00:01:03.400 --> 00:01:06.140
So, there's a lot going on here, and I have color coded this.

00:01:06.140 --> 00:01:08.010
So, why don't we walk through it.

00:01:08.010 --> 00:01:11.580
First, we're just saying that J of big theta is equal to one half,

00:01:11.580 --> 00:01:15.530
times the sum over all of our data points, of the predicted Y,

00:01:15.530 --> 00:01:20.230
given our Xs, our input variables, minus the observed Y squared.

00:01:20.230 --> 00:01:22.660
So, this is just our error squared term.

00:01:22.660 --> 00:01:24.650
Summed over all the data points.

00:01:24.650 --> 00:01:26.380
No different than the equation that we have up here.

00:01:28.200 --> 00:01:31.100
Down here, we're just defining the way that we calculate the predicted

00:01:31.100 --> 00:01:32.250
value of Y.

00:01:32.250 --> 00:01:33.960
Given our input variables.

00:01:33.960 --> 00:01:36.890
And the way that we do that, is that we say we sum from N equals zero,

00:01:36.890 --> 00:01:39.340
to big N, of theta N times X N.

00:01:39.340 --> 00:01:44.020
So, we're just saying that we sum up the X N each input variable,

00:01:44.020 --> 00:01:46.450
times its weight, theta.

00:01:46.450 --> 00:01:49.790
This is no different that the sum that we had, on the last slide.

00:01:49.790 --> 00:01:53.540
Note that we include an N equals zero term here, which corresponds to a constant

00:01:53.540 --> 00:01:57.220
term in our model, which doesn't correspond to any of the input variables.

00:01:57.220 --> 00:02:00.190
For those of you familiar with linear algebra, if we wanted to,

00:02:00.190 --> 00:02:02.700
we could also express this as theta transposed times x.

00:02:03.720 --> 00:02:05.990
If you don't know linear algebra, don't worry about this,

00:02:05.990 --> 00:02:08.669
this is just another way of expressing the same thing here,

00:02:08.669 --> 00:02:10.740
that might be easier for some students to comprehend.

