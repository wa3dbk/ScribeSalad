WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.000
The next thing we did to increase the speed of the site was we added another database,

00:00:05.000 --> 00:00:11.000
and this was using a piece of software called Slony, I believe, to replicate our database.

00:00:11.000 --> 00:00:14.000
We still only had one app server at this time, and we made lots of database requests.

00:00:14.000 --> 00:00:16.000
Our caching was not nearly as advanced.

00:00:16.000 --> 00:00:19.000
We had some basic caching, it was just at the application level.

00:00:19.000 --> 00:00:21.000
This machine was hitting both of these Postgres machines.

00:00:21.000 --> 00:00:24.000
This is the first time we ran into the notion of replication lag,

00:00:24.000 --> 00:00:27.000
because we have these two separate machines, and Slony would normally keep them in sync,

00:00:27.000 --> 00:00:31.000
but sometimes, the slave machine could lag by about 5 seconds or so.

00:00:31.000 --> 00:00:34.000
If you had just submitted the link and then we redirected you to the permalink,

00:00:34.000 --> 00:00:38.000
and then we'd hit the slave machine that didn't have the permalink we would send the user a 404,

00:00:38.000 --> 00:00:40.000
which is really annoying right after you submitted a piece of content.

00:00:40.000 --> 00:00:42.000
That's when we started thinking about writing to our cache

00:00:42.000 --> 00:00:44.000
at the same time as writing to our database.

00:00:44.000 --> 00:00:47.000
We also had the issue with these two processes of keeping these caches in sync.

00:00:47.000 --> 00:00:50.000
We weren't using memcached yet--the way we kept our caches in sync

00:00:50.000 --> 00:00:55.000
was we used the library called Spread, and Spread is this network library that basically says

00:00:55.000 --> 00:00:58.000
if you send a message to one host, it will send it to all of these other hosts.

00:00:58.000 --> 00:01:01.000
Around this time was when we added our second app server

00:01:01.000 --> 00:01:05.000
that was running Python on as well, and we used Spread to keep the two hash tables

00:01:05.000 --> 00:01:07.000
that we were using as our cache kind of in sync.

00:01:07.000 --> 00:01:10.000
This worked sort of okay in terms of keeping these two caches in sync,

00:01:10.000 --> 00:01:13.000
but it wasn't going to scale very well, because eventually, we would add

00:01:13.000 --> 00:01:16.000
a couple of more app servers to deal with more load and Spread

00:01:16.000 --> 00:01:18.000
like every time one of these app servers would update its cache,

00:01:18.000 --> 00:01:21.000
you would have to tell all of the other app servers about it and so turned in to the huge mess,

00:01:21.000 --> 00:01:25.000
a lot of network traffic keeping all of these caches in sync.

00:01:25.000 --> 00:01:28.000
It was a total mess, and so eventually, we would switch to memcached,

00:01:28.000 --> 00:01:31.000
but that was not before we rewrote our database.

00:01:31.000 --> 00:01:35.000
Shortly after we switched a couple of machines and a couple of databases, we added comments,

00:01:35.000 --> 00:01:38.000
and the first version of comments--it was just another database table.

00:01:38.000 --> 00:01:42.000
There's nothing complicated about it--it had a link ID of what link it was on

00:01:42.000 --> 00:01:46.000
and had some contents, author ID--you can kind of guess the columns it would have,

00:01:46.000 --> 00:01:48.000
and we still did lots of joins--it was a relational database.

00:01:48.000 --> 00:01:51.000
We changed around this time to a more flexible database architecture,

00:01:51.000 --> 00:01:54.000
or at least a more flexible table design, because the challenge we had

00:01:54.000 --> 00:01:57.000
was every time we added a new feature, you might have to add a new table

00:01:57.000 --> 00:01:59.000
or you might add a new column to one of these tables.

00:01:59.000 --> 00:02:03.000
Adding a column would take time, and we might have to do a data migration

00:02:03.000 --> 00:02:06.000
or update an index and add all these load--it was just a total pain.

00:02:06.000 --> 00:02:09.000
We felt like the rate at which we could add features was limited

00:02:09.000 --> 00:02:13.000
by our ability to update our database and to do these big migrations.

