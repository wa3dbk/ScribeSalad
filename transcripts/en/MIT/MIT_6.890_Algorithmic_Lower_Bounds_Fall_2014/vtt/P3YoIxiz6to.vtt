WEBVTT
Kind: captions
Language: en

00:00:00.080 --> 00:00:02.430
The following content is
provided under a Creative

00:00:02.430 --> 00:00:03.820
Commons license.

00:00:03.820 --> 00:00:06.050
Your support will help
MIT OpenCourseWare

00:00:06.050 --> 00:00:10.150
continue to offer high quality
educational resources for free.

00:00:10.150 --> 00:00:12.690
To make a donation or to
view additional materials

00:00:12.690 --> 00:00:16.600
from hundreds of MIT courses,
visit MIT OpenCourseWare

00:00:16.600 --> 00:00:17.305
at ocw.mit.edu.

00:00:26.920 --> 00:00:28.900
PROFESSOR: Today, we
begin a new topic, which

00:00:28.900 --> 00:00:30.185
is parameterized complexity.

00:00:32.900 --> 00:00:38.730
This is one of the
most recent areas

00:00:38.730 --> 00:00:41.430
of hardness that
has been invented

00:00:41.430 --> 00:00:44.915
that we will cover so far.

00:00:44.915 --> 00:00:48.940
It started around 1999,
and then since the 2000s,

00:00:48.940 --> 00:00:50.780
it's been getting
really big, originally

00:00:50.780 --> 00:00:52.960
by Downey and Fellows.

00:00:52.960 --> 00:00:54.510
And the general
idea is we're going

00:00:54.510 --> 00:00:56.570
to take problems,
which normally we

00:00:56.570 --> 00:00:59.990
think of as being parameterized
by the problem size, n,

00:00:59.990 --> 00:01:02.995
and thinking about polynomial
versus exponential growth of n.

00:01:02.995 --> 00:01:05.330
And we're going to
add another parameter.

00:01:05.330 --> 00:01:07.350
So you could say this
is a two parameter

00:01:07.350 --> 00:01:08.910
analysis in some sense.

00:01:08.910 --> 00:01:12.140
Usually, we call problem
size n and then we

00:01:12.140 --> 00:01:14.980
call the parameter k.

00:01:14.980 --> 00:01:17.530
In general, a parameter
is just some way

00:01:17.530 --> 00:01:19.690
of measuring instances.

00:01:19.690 --> 00:01:26.520
So parameter, k, is a
function from instances

00:01:26.520 --> 00:01:31.950
to non-negative
integers, 0, 1, 2.

00:01:31.950 --> 00:01:38.030
And usually, this number k
is just part of the instance,

00:01:38.030 --> 00:01:49.060
for example-- well, let me
go over here to examples.

00:01:49.060 --> 00:01:57.260
Let's say problem that's
called k-vertex cover--

00:01:57.260 --> 00:02:00.450
and usually, I'll just call it
vertex cover for simplicity.

00:02:00.450 --> 00:02:03.230
You know the problem
vertices to cover edges.

00:02:03.230 --> 00:02:06.900
And usually, the decision
version of that is

00:02:06.900 --> 00:02:10.759
are there k vertices
that cover all edges?

00:02:18.240 --> 00:02:20.200
So k is an input
to this problem,

00:02:20.200 --> 00:02:22.580
and it appears in the
statement of the problem.

00:02:22.580 --> 00:02:24.360
But in particular,
it's an input.

00:02:24.360 --> 00:02:27.480
And so this function, k,
of a vertex cover instance

00:02:27.480 --> 00:02:31.580
just pulls out that one number
and throws away the graph.

00:02:31.580 --> 00:02:34.560
That's the typical parameter.

00:02:34.560 --> 00:02:37.350
But in general, a parameter
could be some hard to compute

00:02:37.350 --> 00:02:38.560
thing.

00:02:38.560 --> 00:02:41.610
Maybe k-- you think
of the parameter--

00:02:41.610 --> 00:02:43.570
you take vertex cover
as your problem,

00:02:43.570 --> 00:02:45.030
but you define your
parameter to be

00:02:45.030 --> 00:02:48.770
the smallest independent set--
or largest independent set.

00:02:48.770 --> 00:02:53.810
Smallest is pretty small, 0.

00:02:53.810 --> 00:02:55.750
Or, pick your favorite.

00:02:55.750 --> 00:03:01.700
You could, for example, think
about vertex cover with respect

00:03:01.700 --> 00:03:07.780
to-- this is how we'll write
the parameter-- crossing number.

00:03:14.220 --> 00:03:19.590
So the idea here is-- sorry
I forgot to covet-- we always

00:03:19.590 --> 00:03:21.090
measure our instances
with respect

00:03:21.090 --> 00:03:23.340
to the problem size, which
is the size of the graph

00:03:23.340 --> 00:03:25.860
for vertex cover instance.

00:03:25.860 --> 00:03:27.840
But we're also going
to parameterize

00:03:27.840 --> 00:03:29.351
by some other quantity here.

00:03:29.351 --> 00:03:31.600
Maybe it's the minimum
crossing number for your graph,

00:03:31.600 --> 00:03:34.000
so 0 if it's planar and so on.

00:03:34.000 --> 00:03:37.390
We prove that that's
NP-complete to compute.

00:03:37.390 --> 00:03:41.610
So this parameter may not be
easy to compute in general.

00:03:41.610 --> 00:03:44.690
But a lot of the times it's
just part of the problem.

00:03:44.690 --> 00:03:49.386
And in general, this is called
the natural parameterization.

00:03:54.840 --> 00:03:56.450
If you have an
optimization problem,

00:03:56.450 --> 00:03:59.320
like vertex cover is--
minimum vertex cover--

00:03:59.320 --> 00:04:01.930
and you convert the optimization
problem into a decision

00:04:01.930 --> 00:04:04.580
problem, you're asking is
opt less than or equal to k.

00:04:04.580 --> 00:04:07.790
That k is the natural parameter
for an optimization problem.

00:04:07.790 --> 00:04:10.360
So most the time, we'll
be thinking about that.

00:04:10.360 --> 00:04:13.820
But things like this also arise.

00:04:13.820 --> 00:04:21.300
So just to be a
little more precise

00:04:21.300 --> 00:04:27.095
a parameterized problem
is a decision problem

00:04:27.095 --> 00:04:27.920
plus the parameter.

00:04:36.270 --> 00:04:39.370
So when I said vertex cover
with respect to crossing number,

00:04:39.370 --> 00:04:42.330
I meant my decision
problem is vertex cover,

00:04:42.330 --> 00:04:45.070
and I'm going to choose to
use this particular parameter

00:04:45.070 --> 00:04:48.210
function to parameterize
those instances.

00:04:48.210 --> 00:04:52.850
And in general, we'll call them
parameter k and problem size n.

00:04:52.850 --> 00:04:57.280
And our goal is to get a
very good dependence on n

00:04:57.280 --> 00:05:00.420
at the cost of a
bad dependence on k.

00:05:00.420 --> 00:05:07.930
So let me start with a
so-so dependents, XP, would

00:05:07.930 --> 00:05:19.570
be the set of all prioritized
problems solvable in n

00:05:19.570 --> 00:05:23.450
to the f of k time for some
function k-- for any function

00:05:23.450 --> 00:05:26.150
k, any function f.

00:05:26.150 --> 00:05:31.370
And FPT is the set of
parameterized problem solvable

00:05:31.370 --> 00:05:37.250
in f of k times
n to the order 1.

00:05:37.250 --> 00:05:39.000
So this is considered
a good running time.

00:05:39.000 --> 00:05:42.820
This is considered
a bad running time.

00:05:42.820 --> 00:05:44.620
F is presumably exponential.

00:05:44.620 --> 00:05:46.010
Assuming your
problem is NP-hard,

00:05:46.010 --> 00:05:48.040
you have to have
exponential in something.

00:05:48.040 --> 00:05:50.890
And the goal is to get the
exponential away from n

00:05:50.890 --> 00:05:52.410
because n is hard to control.

00:05:52.410 --> 00:05:55.837
We like to solve big problems,
but maybe we can somehow

00:05:55.837 --> 00:05:57.670
characterize that the
problems we care about

00:05:57.670 --> 00:06:02.280
have some small measure, k--
for some interesting measure k.

00:06:02.280 --> 00:06:04.626
If such an algorithm is
possible and, in practice,

00:06:04.626 --> 00:06:06.250
your k's are small,
then you're golden.

00:06:06.250 --> 00:06:07.624
These give really
good algorithms

00:06:07.624 --> 00:06:09.750
for solving lots of problems.

00:06:09.750 --> 00:06:13.660
Any problem in FPT, if k
is somewhat reasonable,

00:06:13.660 --> 00:06:15.810
even when n is huge, we
can solve the problem.

00:06:18.840 --> 00:06:23.130
This running time is also
polynomial for fixed k,

00:06:23.130 --> 00:06:26.140
but the polynomial
changes depending on k.

00:06:26.140 --> 00:06:28.420
So here, we typically get
linear time algorithms

00:06:28.420 --> 00:06:29.890
for any fixed k.

00:06:29.890 --> 00:06:31.940
Here, as k increases,
the polynomial

00:06:31.940 --> 00:06:32.990
gets bigger and bigger.

00:06:32.990 --> 00:06:34.720
And in practice,
you probably can't

00:06:34.720 --> 00:06:37.960
handle more than an n squared
algorithm for a large n.

00:06:37.960 --> 00:06:41.340
So this is considered not
useful even for small k.

00:06:41.340 --> 00:06:43.860
This is considered very
useful for small k,

00:06:43.860 --> 00:06:47.740
depending on your notion
of small, of course.

00:06:47.740 --> 00:06:50.690
And parameterized complexity,
the main name of the game

00:06:50.690 --> 00:06:53.320
is distinguishing between FPT.

00:06:53.320 --> 00:06:57.310
And XP is actually rather large,
but distinguishing between when

00:06:57.310 --> 00:06:59.445
this is possible and
when it is impossible.

00:07:03.280 --> 00:07:03.780
Cool.

00:07:03.780 --> 00:07:07.551
So for example,
vertex cover is FPT.

00:07:07.551 --> 00:07:09.800
Vertex cover with respect
to crossing number, I think,

00:07:09.800 --> 00:07:10.630
is FPT.

00:07:10.630 --> 00:07:13.080
I have to think about
that a little bit more.

00:07:13.080 --> 00:07:18.960
AUDIENCE: Can you say what
XP and FPT roughly stand for?

00:07:18.960 --> 00:07:21.450
PROFESSOR: Oh, right.

00:07:21.450 --> 00:07:25.790
FPT is fixed
parameter tractable.

00:07:25.790 --> 00:07:27.150
That's the good word.

00:07:30.750 --> 00:07:33.790
intuitive--
etymological that means

00:07:33.790 --> 00:07:37.360
when you fix the parameter k,
you get a tractable problem

00:07:37.360 --> 00:07:40.300
although it's a strong
sense of tractable.

00:07:40.300 --> 00:07:43.410
You can say it is n
squared for any fixed k.

00:07:43.410 --> 00:07:45.570
Or, it's n to the
fifth for any fixed k.

00:07:45.570 --> 00:07:48.320
This constant does
not depend on k.

00:07:51.720 --> 00:07:52.980
But the lead constant does.

00:07:56.720 --> 00:07:58.740
So that's FPT.

00:07:58.740 --> 00:08:03.980
XP, experience points.

00:08:10.047 --> 00:08:10.630
Good question.

00:08:13.230 --> 00:08:14.760
One other good thing to know.

00:08:17.440 --> 00:08:21.840
You might wonder why f
of k times polynomial n.

00:08:21.840 --> 00:08:26.350
Maybe I could hope for better
like f of k plus polynomial n.

00:08:26.350 --> 00:08:29.540
That actually is the same class.

00:08:29.540 --> 00:08:35.440
So FPT also equals the set
of all parameterized problems

00:08:35.440 --> 00:08:44.310
solvable in f of k
plus n to order 1.

00:08:44.310 --> 00:08:45.560
That's a nice general theorem.

00:08:45.560 --> 00:08:47.020
I won't bother proving it here.

00:08:47.020 --> 00:08:50.180
I think the f ends up becoming
square of the original f.

00:08:50.180 --> 00:08:52.410
It's a very easy case
analysis based on whether

00:08:52.410 --> 00:08:55.376
which of these two
things is bigger.

00:08:55.376 --> 00:08:56.750
I will leave that
as an exercise.

00:08:56.750 --> 00:08:58.375
It would be a good
problem set problem.

00:09:01.460 --> 00:09:07.390
Cool, so that answers
one plausible question.

00:09:07.390 --> 00:09:09.520
Let me give you an example
of an FPT algorithm,

00:09:09.520 --> 00:09:12.760
let's say, for vertex cover.

00:09:12.760 --> 00:09:17.240
So back over here
to vertex cover.

00:09:17.240 --> 00:09:20.240
How about an XP algorithm
for vertex cover.

00:09:20.240 --> 00:09:23.570
So that's really easy,
not very interesting.

00:09:23.570 --> 00:09:27.610
You could just guess a vertex
cover, try all possible vertex

00:09:27.610 --> 00:09:28.690
covers of size k.

00:09:28.690 --> 00:09:31.023
Remember, we want to know
whether there's a vertex cover

00:09:31.023 --> 00:09:34.370
size k. k is given
to us, so we could

00:09:34.370 --> 00:09:38.980
try all vertex covers--
or, I shouldn't

00:09:38.980 --> 00:09:42.310
say that-- all vertex
sets of size k.

00:09:46.660 --> 00:09:49.120
There's only n to the k of them.

00:09:49.120 --> 00:09:52.470
And for each of them,
we'll spend linear time

00:09:52.470 --> 00:09:54.270
to check whether
it's a vertex cover.

00:09:54.270 --> 00:09:57.240
So this proves that
the problem is in XP.

00:09:57.240 --> 00:09:58.140
That's trivial.

00:09:58.140 --> 00:09:59.980
Most problems you'll
find are in XP

00:09:59.980 --> 00:10:02.210
with reasonable
parameterization.

00:10:02.210 --> 00:10:04.650
On some, it's not obvious.

00:10:04.650 --> 00:10:07.714
And now, let's prove
that it's in FPT.

00:10:07.714 --> 00:10:09.130
This is a little
more interesting.

00:10:09.130 --> 00:10:11.230
There are lots of FPT
algorithms for vertex cover.

00:10:11.230 --> 00:10:12.600
It's an active area.

00:10:12.600 --> 00:10:15.630
Every year there's a better
algorithm pretty much.

00:10:15.630 --> 00:10:16.488
Question?

00:10:16.488 --> 00:10:19.764
AUDIENCE: So in the inputs of
the problem, the k is an input?

00:10:22.399 --> 00:10:24.190
PROFESSOR: For vertex
cover, k is an input.

00:10:24.190 --> 00:10:26.242
AUDIENCE: Oh, because you
can also think of vertex

00:10:26.242 --> 00:10:27.382
cover where k is not
an input, but you

00:10:27.382 --> 00:10:28.294
can analyze the run time.

00:10:28.294 --> 00:10:28.750
PROFESSOR: Yes.

00:10:28.750 --> 00:10:29.249
Right.

00:10:29.249 --> 00:10:36.990
So you could also think about
vertex cover with respect

00:10:36.990 --> 00:10:40.180
to vertex cover.

00:10:40.180 --> 00:10:44.850
So here, let's say-- this
is a little bit funny,

00:10:44.850 --> 00:10:50.510
but here the idea
is k, the parameter,

00:10:50.510 --> 00:10:52.440
is the minimum vertex cover.

00:10:52.440 --> 00:10:54.750
Let me write minimum
vertex cover.

00:10:54.750 --> 00:10:56.920
And here, you're given
some other value.

00:10:56.920 --> 00:10:58.320
Let's call it j.

00:10:58.320 --> 00:11:01.960
And you want to decide whether
vertex cover is at most j,

00:11:01.960 --> 00:11:03.510
but you're not told what k is.

00:11:03.510 --> 00:11:06.010
You're not told what
the optimal value is.

00:11:06.010 --> 00:11:07.440
So this is a little bit subtle.

00:11:07.440 --> 00:11:08.990
It's essentially
the same problem.

00:11:08.990 --> 00:11:14.810
You're just not told
the critical value.

00:11:14.810 --> 00:11:19.680
But if you can solve k
vertex cover when k is given,

00:11:19.680 --> 00:11:23.410
you can just run that
algorithm for k equals 1, 2, 4,

00:11:23.410 --> 00:11:25.290
and you'll end up
solving this problem.

00:11:25.290 --> 00:11:27.830
So even though--
so in this problem,

00:11:27.830 --> 00:11:30.560
you can say, well, if I discover
there's no vertex cover of size

00:11:30.560 --> 00:11:33.030
k, I can just return no.

00:11:33.030 --> 00:11:35.440
Here, it's a little
more annoying, but just

00:11:35.440 --> 00:11:38.266
by iterating and stopping when
you get the first vertex cover,

00:11:38.266 --> 00:11:39.640
your algorithm
will end up having

00:11:39.640 --> 00:11:41.520
a good running time
with respect to k

00:11:41.520 --> 00:11:43.440
even though you didn't know it.

00:11:43.440 --> 00:11:44.620
So it's a subtlety.

00:11:44.620 --> 00:11:46.580
In general, this is
messy to work with,

00:11:46.580 --> 00:11:49.150
so we usually think
about k being an input.

00:11:49.150 --> 00:11:52.320
But that reduction shows
they're more or less the same.

00:11:52.320 --> 00:11:54.660
But the complexity theory
for this is annoying.

00:11:54.660 --> 00:11:58.800
In the same way that for NP, we
restricted decision problems,

00:11:58.800 --> 00:12:00.460
here we're going
to usually restrict

00:12:00.460 --> 00:12:01.580
to the natural parameter.

00:12:04.900 --> 00:12:05.700
So good question.

00:12:08.270 --> 00:12:12.240
Let me prove vertex cover is in
FPT, so a much better running

00:12:12.240 --> 00:12:13.160
time than n to the k.

00:12:13.160 --> 00:12:17.820
I think I will get something
like 2 to the k times n.

00:12:17.820 --> 00:12:20.260
Look at an edge of
the graph, any edge.

00:12:20.260 --> 00:12:22.000
Pick an edge, any edge.

00:12:22.000 --> 00:12:24.330
And we know for this
to be a vertex cover,

00:12:24.330 --> 00:12:26.480
one of the two endpoints
has to be in there.

00:12:26.480 --> 00:12:27.120
Which one?

00:12:27.120 --> 00:12:27.680
I don't know.

00:12:27.680 --> 00:12:28.920
Guess.

00:12:28.920 --> 00:12:33.680
So I'm going to guess either
the left guy or the right vertex

00:12:33.680 --> 00:12:36.490
is in the vertex
cover, and then repeat.

00:12:36.490 --> 00:12:38.600
So when I put somebody
in the vertex cover,

00:12:38.600 --> 00:12:41.575
I delete all of
the incident edges.

00:12:45.360 --> 00:12:46.800
That takes linear time.

00:12:46.800 --> 00:12:48.820
Over here, I would
delete whatever

00:12:48.820 --> 00:12:51.600
edges are incident to this
guy and the vertex itself,

00:12:51.600 --> 00:12:54.300
of course.

00:12:54.300 --> 00:12:58.010
And so I get a smaller graph.

00:12:58.010 --> 00:13:04.430
I do this k times, so in
general, my execution tree

00:13:04.430 --> 00:13:09.000
will have two branches, every
time I pick an edge and say,

00:13:09.000 --> 00:13:11.550
do I put the left guy
or the right guy in.

00:13:11.550 --> 00:13:16.246
But I only need to worry
about a tree of height k

00:13:16.246 --> 00:13:17.620
because after I
make k decisions,

00:13:17.620 --> 00:13:19.900
I'm supposed to get a
vertex cover of size k.

00:13:19.900 --> 00:13:21.400
After I make k
decisions, if there's

00:13:21.400 --> 00:13:22.941
anything left to
the graph that means

00:13:22.941 --> 00:13:25.720
that wasn't a vertex cover of
size k and I should backtrack.

00:13:25.720 --> 00:13:27.670
So I just explore
this tree, look

00:13:27.670 --> 00:13:29.030
at all the different leaves.

00:13:29.030 --> 00:13:32.890
If there's any vertex cover
of size k, I will find it.

00:13:32.890 --> 00:13:37.340
And so I only spend 2 to
the k to explore this tree.

00:13:37.340 --> 00:13:39.270
2 to the k is the size
of the tree times n

00:13:39.270 --> 00:13:43.380
because I maybe delete
n things at every node.

00:13:43.380 --> 00:13:45.964
And that's one of
the simplest FPT

00:13:45.964 --> 00:13:47.130
algorithms for vertex cover.

00:13:47.130 --> 00:13:48.759
There are much
cooler, fancier ones.

00:13:48.759 --> 00:13:50.550
But this is now going
to be about hardness,

00:13:50.550 --> 00:13:51.841
so I will stop with algorithms.

00:13:55.470 --> 00:14:01.770
Also related is a
stronger notion of PTAS.

00:14:04.910 --> 00:14:09.470
So I mentioned
before, with PTAS, you

00:14:09.470 --> 00:14:14.501
could have a running time
something like n to the 2

00:14:14.501 --> 00:14:16.390
to the 1 over epsilon--
which is actually

00:14:16.390 --> 00:14:20.120
fairly common in a lot
of PTASes, not so great.

00:14:20.120 --> 00:14:23.100
If epsilon is anything
good, then this

00:14:23.100 --> 00:14:26.400
is going to be a
really huge polynomial.

00:14:26.400 --> 00:14:30.090
Well, the corresponding
notion in FPT land

00:14:30.090 --> 00:14:37.770
is a running time
of f of epsilon--

00:14:37.770 --> 00:14:39.020
or, I'll write 1 over epsilon.

00:14:39.020 --> 00:14:43.630
It doesn't matter--
times n to the order 1.

00:14:43.630 --> 00:14:50.000
So this is really saying that
the approximation scheme is FPT

00:14:50.000 --> 00:14:52.750
with respect to 1 over epsilon.

00:14:52.750 --> 00:14:56.630
This is to get a 1 plus
epsilon approximation.

00:14:56.630 --> 00:14:58.470
So this is relating
the two worlds.

00:14:58.470 --> 00:15:01.262
If you want a FPT algorithm
for approximating a problem

00:15:01.262 --> 00:15:03.470
and getting within a factor
of 1 over epsilon, if you

00:15:03.470 --> 00:15:06.190
get this kind of
running time verses

00:15:06.190 --> 00:15:10.940
this kind of running time, I
consider this good, this bad.

00:15:10.940 --> 00:15:16.690
And in general, these are called
Efficient PTASes, or EPTAS.

00:15:16.690 --> 00:15:20.420
And in general, if
you have an EPTAS,

00:15:20.420 --> 00:15:25.510
you also have an FPT
algorithm with respect

00:15:25.510 --> 00:15:36.480
to the natural parameter,
which is to basically compute

00:15:36.480 --> 00:15:42.180
what the optimal value is,
because you can let epsilon be

00:15:42.180 --> 00:15:50.060
something like 1 over 2k--
that ought to do it-- something

00:15:50.060 --> 00:15:52.800
less than 1 over k.

00:15:52.800 --> 00:15:55.070
You're trying to
decide whether you

00:15:55.070 --> 00:15:57.640
have a solution of less
than or equal to k or not,

00:15:57.640 --> 00:15:59.880
whether optimal is
strictly greater than k.

00:15:59.880 --> 00:16:03.940
And I said k is an
integer, so this

00:16:03.940 --> 00:16:06.940
is an integer valued problem
you're trying to optimize.

00:16:06.940 --> 00:16:09.080
And so the difference
became k and k plus 1,

00:16:09.080 --> 00:16:13.762
multiplicatively, is about
1 plus 1 over k factor.

00:16:13.762 --> 00:16:15.770
It's exactly 1 plus 1 over k.

00:16:15.770 --> 00:16:18.470
So if we let epsilon be
smaller than 1 over k,

00:16:18.470 --> 00:16:21.130
then to get within a
multiplicative factor of 1

00:16:21.130 --> 00:16:24.200
plus 1/2 1 over k,
you actually have

00:16:24.200 --> 00:16:28.010
to be within an additive plus
1 of the right answer at k.

00:16:28.010 --> 00:16:36.957
So this will decide whether
opt is less than or equal to k

00:16:36.957 --> 00:16:39.040
based on whether that
approximation algorithm will

00:16:39.040 --> 00:16:42.720
give you something of value
at most k or strictly bigger.

00:16:42.720 --> 00:16:46.480
Strictly bigger
would be k plus 1.

00:16:46.480 --> 00:16:49.200
So this is particularly
interesting

00:16:49.200 --> 00:16:54.630
because it means if a
problem is not in FPT,

00:16:54.630 --> 00:16:59.510
then it does not have
an Efficient PTAS.

00:16:59.510 --> 00:17:03.340
So we can use-- before,
in previous lectures,

00:17:03.340 --> 00:17:05.690
PTAS was considered gold.

00:17:05.690 --> 00:17:08.740
Now, we can distinguish
between EPTAS and PTAS.

00:17:08.740 --> 00:17:10.660
And there are problems
that have PTASs

00:17:10.660 --> 00:17:14.460
that do not have FPT
algorithms assuming

00:17:14.460 --> 00:17:17.810
FPT does not equal w1-- an
assumption we will get to

00:17:17.810 --> 00:17:19.220
in a moment.

00:17:19.220 --> 00:17:21.690
So if you can, in general,
establish something is probably

00:17:21.690 --> 00:17:24.190
not in FPT, then we get
it probably does not

00:17:24.190 --> 00:17:25.480
have an Efficient PTAS.

00:17:25.480 --> 00:17:28.020
So this is useful for lower
bounds about approximation.

00:17:28.020 --> 00:17:30.710
Even if you don't care about
fixed parameter tractability,

00:17:30.710 --> 00:17:33.584
this will give us cool
things about approximability.

00:17:33.584 --> 00:17:38.312
AUDIENCE: Can you give
a few examples of EPTAS?

00:17:38.312 --> 00:17:39.770
PROFESSOR: A few
examples of EPTAS.

00:17:42.870 --> 00:17:45.880
For example, most of the planar
results I've talked about,

00:17:45.880 --> 00:17:48.840
planar independence set, planar
vertex cover, those things all

00:17:48.840 --> 00:17:50.230
have Efficient PTASs.

00:17:52.724 --> 00:17:54.890
But that gets us into the
world of bounded treewidth

00:17:54.890 --> 00:17:56.510
algorithms.

00:17:56.510 --> 00:17:59.240
But in general, if you have like
a-- a lot of bounded treewidth

00:17:59.240 --> 00:18:00.990
algorithms are FPT.

00:18:00.990 --> 00:18:04.640
There will be some exponential
in treewidth times polynomial n

00:18:04.640 --> 00:18:07.490
So a lot of times,
you can use those

00:18:07.490 --> 00:18:11.300
to get EPTASs for planar
and h-minor free problems.

00:18:11.300 --> 00:18:13.170
So that's one set of examples.

00:18:13.170 --> 00:18:15.260
There's no short
example I can give you,

00:18:15.260 --> 00:18:18.370
but that's a big class.

00:18:18.370 --> 00:18:21.870
So for either reason,
either you want

00:18:21.870 --> 00:18:24.180
to solve problems
for small k or you

00:18:24.180 --> 00:18:26.820
want to figure out
whether there's an EPTAS,

00:18:26.820 --> 00:18:29.900
how can we prove that problems
are hard in this world using

00:18:29.900 --> 00:18:33.200
reductions like usual?

00:18:33.200 --> 00:18:36.120
So in this context,
luckily, there's

00:18:36.120 --> 00:18:37.770
only one type of
reduction we need

00:18:37.770 --> 00:18:41.110
to learn unlike approximation
where there were a lot.

00:18:45.860 --> 00:18:47.560
And I'll call it
parameterized reduction

00:18:47.560 --> 00:18:49.690
although throughout this
lecture and the next one

00:18:49.690 --> 00:18:52.860
I'll just say reduction usually
because we're always talking

00:18:52.860 --> 00:18:54.960
about parameterized problems.

00:18:54.960 --> 00:18:58.920
So in general, we have some
problem A-- decision problem A

00:18:58.920 --> 00:19:00.330
and parameter k.

00:19:00.330 --> 00:19:03.160
And we want to convert into
some decision problem B

00:19:03.160 --> 00:19:06.810
with parameter k prime.

00:19:06.810 --> 00:19:11.800
And of course, as usual,
the set up is we're

00:19:11.800 --> 00:19:13.720
given an instance x.

00:19:13.720 --> 00:19:17.570
This is going to look almost
identical to NP Karp-style

00:19:17.570 --> 00:19:21.590
reductions, but then we're going
to have one extra condition.

00:19:21.590 --> 00:19:24.460
So instance x of
A gets mapped to

00:19:24.460 --> 00:19:31.540
by a function f to an
instance x prime of B. X prime

00:19:31.540 --> 00:19:36.140
is f o x as usual.

00:19:36.140 --> 00:19:42.150
This needs to be a
polynomial time function

00:19:42.150 --> 00:19:46.460
just like for NP reductions,
which means, in particular,

00:19:46.460 --> 00:19:49.840
x prime has polynomials
size reduced back to x.

00:19:49.840 --> 00:19:51.200
It should be answer preserving.

00:19:57.180 --> 00:20:03.340
So x is yes instance
for A if and only

00:20:03.340 --> 00:20:07.440
x prime is a yes instance for B.

00:20:07.440 --> 00:20:10.480
So, so far, exactly
NP reductions.

00:20:10.480 --> 00:20:13.350
And then when we
need one extra thing

00:20:13.350 --> 00:20:15.970
which is parameter preserving.

00:20:21.740 --> 00:20:43.880
This is there's some function
g, which I'll call the parameter

00:20:43.880 --> 00:20:46.490
blow up-- or, I guess you
call it parameter growth

00:20:46.490 --> 00:20:49.990
for g-- such that
the new parameter

00:20:49.990 --> 00:20:52.450
value for the
converted instance is,

00:20:52.450 --> 00:20:54.510
at most, that function
of the original parameter

00:20:54.510 --> 00:20:56.440
value of the original instance.

00:20:56.440 --> 00:20:56.940
Question?

00:20:56.940 --> 00:20:59.106
AUDIENCE: Are there any
limits on the amount of time

00:20:59.106 --> 00:21:01.930
that g can take to compute?

00:21:01.930 --> 00:21:05.240
PROFESSOR: g should be
a computable function.

00:21:05.240 --> 00:21:08.290
I think that's all we need.

00:21:08.290 --> 00:21:10.970
Probably polynomial
time is also a fine,

00:21:10.970 --> 00:21:16.660
but usually this is going to
be like linear or polynomial

00:21:16.660 --> 00:21:17.940
or exponential.

00:21:17.940 --> 00:21:20.560
It's rarely some
insanely large thing,

00:21:20.560 --> 00:21:22.500
but computable would be nice.

00:21:28.730 --> 00:21:29.490
Cool.

00:21:29.490 --> 00:21:33.520
So that is our notion of
parameterized reduction.

00:21:33.520 --> 00:21:37.350
And the consequence,
if this exists

00:21:37.350 --> 00:21:40.160
and B is fixed
parameter tractable,

00:21:40.160 --> 00:21:47.140
then A is because we can
take an instance of A

00:21:47.140 --> 00:21:50.900
converting it to B. If
the original parameter was

00:21:50.900 --> 00:21:53.720
founded by some k,
this new parameter

00:21:53.720 --> 00:21:56.446
will be bounded by g of k.

00:21:56.446 --> 00:21:58.070
New instance will be
bounded of g of k.

00:21:58.070 --> 00:22:01.120
So we run the FPT
algorithm for B,

00:22:01.120 --> 00:22:06.890
and that gives us the answer
to the original instance of A.

00:22:06.890 --> 00:22:15.020
So if we don't care about
what this function is,

00:22:15.020 --> 00:22:16.950
we are basically
composing functions.

00:22:16.950 --> 00:22:21.310
So there's some f dependence
on k in this algorithm,

00:22:21.310 --> 00:22:25.470
and we're taking that function
of g of k is our new function.

00:22:25.470 --> 00:22:30.610
And we get a new dependence on
k and the running time over FPT.

00:22:30.610 --> 00:22:32.700
So what that means
is if we believe

00:22:32.700 --> 00:22:36.820
it A does not have an FPT, then
B does not have an FPT if we

00:22:36.820 --> 00:22:37.820
can do these reductions.

00:22:37.820 --> 00:22:39.360
So same style, we're
going to reduce

00:22:39.360 --> 00:22:42.240
from a problem we know is
hard, A, into a problem

00:22:42.240 --> 00:22:46.280
that we don't know about, and
that proves B is hard as well.

00:22:46.280 --> 00:22:46.780
Yeah?

00:22:46.780 --> 00:22:48.196
AUDIENCE: What is
the relationship

00:22:48.196 --> 00:22:49.200
between FPT and XP?

00:22:51.912 --> 00:22:53.037
Is there like a difference?

00:22:53.037 --> 00:22:53.661
PROFESSOR: Yeah

00:22:53.661 --> 00:22:54.910
AUDIENCE: If there's overlap.

00:22:54.910 --> 00:22:58.600
PROFESSOR: Well, FPT
is contained in XP.

00:22:58.600 --> 00:23:02.530
And they are different if you
believe the exponential time

00:23:02.530 --> 00:23:03.760
hypothesis.

00:23:03.760 --> 00:23:06.440
If you believe SAT
does not have two

00:23:06.440 --> 00:23:13.290
to the little of n algorithms--
if that's not possible for SAT,

00:23:13.290 --> 00:23:17.457
then XP and FPT are different,
and much, much more things

00:23:17.457 --> 00:23:18.040
are different.

00:23:18.040 --> 00:23:21.320
But we'll talk about that
more next class-- relating

00:23:21.320 --> 00:23:23.540
to exponential time hypothesis.

00:23:23.540 --> 00:23:27.330
So at this moment, we don't
really care what g is.

00:23:27.330 --> 00:23:30.020
But if you assume
exponential time hypothesis,

00:23:30.020 --> 00:23:34.800
then g matters, and you
can get very explicit lower

00:23:34.800 --> 00:23:39.260
bounds about how good an
algorithm you could hope for.

00:23:39.260 --> 00:23:42.160
So we won't just prove you're
probably not an-- we won't just

00:23:42.160 --> 00:23:45.025
prove you're not an FPT, but we
will give an actual lower bound

00:23:45.025 --> 00:23:46.680
and how much running
time you need,

00:23:46.680 --> 00:23:48.600
something like n
to the little k.

00:23:48.600 --> 00:23:51.180
But the running time we get
will depend on this blow

00:23:51.180 --> 00:23:52.340
up function.

00:23:52.340 --> 00:23:55.320
So next class, we'll
care about what g is.

00:23:55.320 --> 00:23:59.040
So I do try to minimize it,
but so far we don't care.

00:23:59.040 --> 00:24:00.016
Any g is fine.

00:24:05.620 --> 00:24:07.570
XP will not turn
out to be the class

00:24:07.570 --> 00:24:08.890
that we think about very often.

00:24:08.890 --> 00:24:11.770
There are things in between that
are a lot easier to work with,

00:24:11.770 --> 00:24:13.050
which we will get to.

00:24:13.050 --> 00:24:17.540
In particular, W1
is the most common.

00:24:17.540 --> 00:24:20.830
So let me do two examples.

00:24:20.830 --> 00:24:23.870
One of which is a correct
example and the other is not.

00:24:23.870 --> 00:24:26.580
And you can help me
figure out which is which.

00:24:26.580 --> 00:24:30.150
Independent set to
vertex cover, these

00:24:30.150 --> 00:24:31.840
are reductions
we've seen before.

00:24:31.840 --> 00:24:33.790
We have tons of
reductions, but usually we

00:24:33.790 --> 00:24:35.830
weren't thinking
about parameters.

00:24:35.830 --> 00:24:41.540
And independent set
to clique, these

00:24:41.540 --> 00:24:45.090
are basically identical
problems left and right.

00:24:45.090 --> 00:24:47.540
And I want the
natural parameter.

00:24:47.540 --> 00:24:51.589
Which of these is a parameter
preserving reduction?

00:24:51.589 --> 00:24:52.755
I need some quiz show music.

00:24:58.242 --> 00:24:59.190
AUDIENCE: The second.

00:24:59.190 --> 00:25:00.940
PROFESSOR: The second, yeah.

00:25:00.940 --> 00:25:03.580
Because if we take a
graph and a parameter k,

00:25:03.580 --> 00:25:06.620
what we convert it to
is the complement graph

00:25:06.620 --> 00:25:09.200
with the same parameter.

00:25:09.200 --> 00:25:11.340
So that's obviously
parameter preserving.

00:25:11.340 --> 00:25:13.590
Whereas independence
set, these are

00:25:13.590 --> 00:25:15.230
complementary in
a different sense

00:25:15.230 --> 00:25:17.646
that if you take everything
that's not in the independence

00:25:17.646 --> 00:25:18.750
set is a vertex cover.

00:25:18.750 --> 00:25:21.940
So it's actually the same
graph but with n minus k

00:25:21.940 --> 00:25:23.540
as the new parameter.

00:25:23.540 --> 00:25:27.950
So this is not a
parameterized reduction.

00:25:27.950 --> 00:25:31.070
This is.

00:25:31.070 --> 00:25:35.910
And in fact, vertex cover,
we just showed is FPT.

00:25:35.910 --> 00:25:41.140
Independence set is not, if you
believe w1 does not equal FPT

00:25:41.140 --> 00:25:44.490
or if you believe
exponential time hypothesis.

00:25:44.490 --> 00:25:47.340
And that would contradict
this statement.

00:25:47.340 --> 00:25:50.506
So this is definitely
not a valid reduction.

00:25:50.506 --> 00:25:52.380
But those are some pretty
trivial reductions.

00:25:52.380 --> 00:25:58.460
Let's do some more
interesting things.

00:25:58.460 --> 00:26:00.810
And I want to start to
introduce the notion of w1.

00:26:25.900 --> 00:26:28.540
I won't define w1 yet.

00:26:28.540 --> 00:26:31.480
I'd like to wait a little bit
because the definition is not

00:26:31.480 --> 00:26:35.900
super intuitive, but here is a
fairly intuitive hard problem

00:26:35.900 --> 00:26:41.130
that you should be fairly
convinced is not FPT.

00:26:41.130 --> 00:26:49.141
So I'll call this k-step
non-deterministic Turing

00:26:49.141 --> 00:26:49.640
machine.

00:26:55.270 --> 00:26:58.450
The one downside is I've never
mentioned Turing machines

00:26:58.450 --> 00:26:59.140
in this class.

00:26:59.140 --> 00:27:01.170
I thought I could
get away without it,

00:27:01.170 --> 00:27:02.790
but I'm going to
have mention them.

00:27:02.790 --> 00:27:05.075
How many people
know what they are?

00:27:05.075 --> 00:27:07.710
Anyone does not?

00:27:07.710 --> 00:27:09.460
OK, a couple.

00:27:09.460 --> 00:27:11.780
Here's a Turing machine.

00:27:11.780 --> 00:27:15.590
I will give you, very briefly,
a very non-standard definition

00:27:15.590 --> 00:27:16.480
of a Turing machine.

00:27:16.480 --> 00:27:19.820
Let's think of-- so you
have this infinite tape,

00:27:19.820 --> 00:27:22.880
infinite memory, and
for our purposes,

00:27:22.880 --> 00:27:24.770
these are not just
binary symbols,

00:27:24.770 --> 00:27:28.770
you can write up to n different
symbols on each of the squares.

00:27:28.770 --> 00:27:30.600
So this is basically
your memory,

00:27:30.600 --> 00:27:33.340
but it's in the style of
old-fashioned tape drives

00:27:33.340 --> 00:27:37.160
in that you-- In order to get
to position k away from you,

00:27:37.160 --> 00:27:38.280
you have to spin k time.

00:27:38.280 --> 00:27:40.720
So you can only step
one unit at a time.

00:27:40.720 --> 00:27:45.630
And this is basically
computers that we are used to.

00:27:45.630 --> 00:27:48.740
So let's say it has a
sequence of instructions,

00:27:48.740 --> 00:27:50.570
and it has an
instruction pointer.

00:27:50.570 --> 00:27:55.270
And in general, I
want to the machine

00:27:55.270 --> 00:28:04.850
to have order n
stateS-- yeah, let's say

00:28:04.850 --> 00:28:21.647
order n lines of code and order
n options per cell of the tape.

00:28:21.647 --> 00:28:23.230
This will look
actually a little weird

00:28:23.230 --> 00:28:25.750
if you're used to
Turing machines,

00:28:25.750 --> 00:28:27.370
but I want to use
this definition.

00:28:27.370 --> 00:28:29.150
So it's basically a
regular algorithm,

00:28:29.150 --> 00:28:30.940
but you're very limited
the number-- in the amount

00:28:30.940 --> 00:28:31.690
of internal state.

00:28:31.690 --> 00:28:33.900
These are basically
registers, but you

00:28:33.900 --> 00:28:36.070
can have only a constant
number of registers

00:28:36.070 --> 00:28:37.670
that vary from one to n.

00:28:37.670 --> 00:28:40.460
One of them will be what
line of code are you on.

00:28:40.460 --> 00:28:44.390
And then you have those lines of
code are instructions like jump

00:28:44.390 --> 00:28:48.750
here, compare these things,
write a particular symbol

00:28:48.750 --> 00:28:51.640
to the current square of the
tape, move the tape left,

00:28:51.640 --> 00:28:52.510
move the tape right.

00:28:52.510 --> 00:28:55.850
So regular types of
instructions, let's

00:28:55.850 --> 00:28:59.040
say, except you have
this weird tape thing.

00:28:59.040 --> 00:29:01.225
Now, I mentioned
these parameters.

00:29:01.225 --> 00:29:02.850
And usually we think
of Turing machines

00:29:02.850 --> 00:29:05.650
of having constant size,
but here I need an n,

00:29:05.650 --> 00:29:07.090
and I need a k.

00:29:07.090 --> 00:29:07.840
Question?

00:29:07.840 --> 00:29:09.100
AUDIENCE: I was going
to ask what n was.

00:29:09.100 --> 00:29:09.683
PROFESSOR: OK.

00:29:09.683 --> 00:29:11.062
So n is the input.

00:29:11.062 --> 00:29:12.520
I mean basically
the Turing machine

00:29:12.520 --> 00:29:14.490
is specified by these things.

00:29:14.490 --> 00:29:18.140
You have order n
instructions, and we're

00:29:18.140 --> 00:29:21.650
guaranteed there's only order
n possible states, let's say.

00:29:21.650 --> 00:29:24.880
And now what we-- and this is
a non-deterministic machine.

00:29:24.880 --> 00:29:27.060
Now non-determinism, we
have talked about this

00:29:27.060 --> 00:29:28.600
in the context of NP.

00:29:28.600 --> 00:29:32.130
A non-deterministic
Turing machine

00:29:32.130 --> 00:29:34.640
has a funny
instruction which says

00:29:34.640 --> 00:29:38.340
non-deterministically branch
to one of n different locations

00:29:38.340 --> 00:29:40.060
in my memory.

00:29:40.060 --> 00:29:43.200
Or, let's say choose
a symbol from my-- I

00:29:43.200 --> 00:29:47.540
have this alphabet that
I'm using in my cell--

00:29:47.540 --> 00:29:51.030
choose a symbol
non-deterministically.

00:29:51.030 --> 00:29:58.780
So I have order n choices
made non-deterministically.

00:29:58.780 --> 00:30:01.330
In the usual sense of
NP, that if there's

00:30:01.330 --> 00:30:03.130
any way for the Turing
machine to output,

00:30:03.130 --> 00:30:06.910
yes-- one of the instructions
is output, yes-- then,

00:30:06.910 --> 00:30:08.849
I will find it.

00:30:08.849 --> 00:30:10.390
These are guesses,
and they're always

00:30:10.390 --> 00:30:13.140
lucky guesses, so I always
end up finding the return,

00:30:13.140 --> 00:30:14.960
yes, if there is such a path.

00:30:14.960 --> 00:30:17.890
Otherwise, all paths return, no,
and then the machine returns,

00:30:17.890 --> 00:30:19.451
no.

00:30:19.451 --> 00:30:19.950
Yeah?

00:30:19.950 --> 00:30:22.116
AUDIENCE: What does it mean
for the number of states

00:30:22.116 --> 00:30:23.680
to change as the input changes?

00:30:23.680 --> 00:30:28.320
PROFESSOR: I mean the
states are also given to you

00:30:28.320 --> 00:30:30.230
as part of the Turing machine.

00:30:30.230 --> 00:30:32.243
There's not one
turning machine-- well,

00:30:32.243 --> 00:30:34.493
there is actually one Turing
machine to rule them all,

00:30:34.493 --> 00:30:36.980
but that's not the point here.

00:30:36.980 --> 00:30:41.400
I give you a machine that has--
think of this as firmware built

00:30:41.400 --> 00:30:44.310
into the machine, and the
number of states in the machine

00:30:44.310 --> 00:30:48.040
includes which instruction
you're currently executing.

00:30:48.040 --> 00:30:49.540
So I mean this is
just saying you're

00:30:49.540 --> 00:30:53.210
given an arbitrary machine
of size-- of complexity n.

00:30:53.210 --> 00:30:54.900
And usual Turing
machine land, there

00:30:54.900 --> 00:30:58.640
are n states because I give you
a state diagram with size n.

00:30:58.640 --> 00:31:01.230
So you're given everything
about the Turing machine.

00:31:01.230 --> 00:31:03.830
So that is the input
to this problem.

00:31:03.830 --> 00:31:09.090
And the question is, can
I find a return, yes,

00:31:09.090 --> 00:31:12.190
solution that only
is k steps long.

00:31:12.190 --> 00:31:14.670
So I'm given this huge
machine, and yet I

00:31:14.670 --> 00:31:17.210
want a very short
execution of the machine.

00:31:17.210 --> 00:31:20.900
I want the running time to only
be k. k, again, is parameter.

00:31:20.900 --> 00:31:23.040
n is big. k is small.

00:31:23.040 --> 00:31:31.140
So I want to know is there a
yes path from the initial state

00:31:31.140 --> 00:31:33.295
of length k.

00:31:38.520 --> 00:31:41.470
So this is basically--
Yeah, question?

00:31:41.470 --> 00:31:44.740
AUDIENCE: Are you given the
input to the Turing machine?

00:31:44.740 --> 00:31:47.356
PROFESSOR: Yeah, let's
say it has no input.

00:31:47.356 --> 00:31:50.150
Input is all
embedded in the code.

00:31:50.150 --> 00:31:54.210
So the tape is initially blank.

00:31:54.210 --> 00:31:55.717
Good.

00:31:55.717 --> 00:31:57.800
There's a reason I need
to do Turing machines here

00:31:57.800 --> 00:32:01.500
instead of usual algorithms,
but if you define-- usually

00:32:01.500 --> 00:32:03.810
NP is defined in
terms of these things.

00:32:03.810 --> 00:32:08.970
So by analogy to NP, we expect
there are no lucky algorithms.

00:32:08.970 --> 00:32:11.502
And so we expect
that when you have

00:32:11.502 --> 00:32:13.710
non-deterministic branches,
the best thing you can do

00:32:13.710 --> 00:32:15.480
is to try all the branches.

00:32:15.480 --> 00:32:19.180
And so if I have an execution
time of k, and at each step,

00:32:19.180 --> 00:32:21.280
I can potentially make a
non-deterministic choice

00:32:21.280 --> 00:32:24.450
among n different
options, then you

00:32:24.450 --> 00:32:28.280
would expect the best
algorithm is n to the k.

00:32:28.280 --> 00:32:31.920
Try all the branches, that's
just like our vertex cover

00:32:31.920 --> 00:32:35.930
algorithm, but in the bad case
where I have branching factor n

00:32:35.930 --> 00:32:38.260
instead of branching factor 2.

00:32:38.260 --> 00:32:41.980
So presumably, there's no way to
replace a guess among n options

00:32:41.980 --> 00:32:45.670
with a guess among two options.

00:32:45.670 --> 00:32:47.680
That's an assumption.

00:32:47.680 --> 00:32:55.810
And this problem is w1 complete.

00:32:55.810 --> 00:32:58.670
I will to define
w1 a little later,

00:32:58.670 --> 00:33:00.710
but for now, just
take this as given.

00:33:00.710 --> 00:33:03.680
You could define w1
complete to mean problems

00:33:03.680 --> 00:33:06.020
that are reducible via
parameterized reductions

00:33:06.020 --> 00:33:07.720
to this problem.

00:33:07.720 --> 00:33:10.270
And then we'll get lots
of examples of problems

00:33:10.270 --> 00:33:11.920
that are as hard as this.

00:33:14.610 --> 00:33:18.220
So a modulo the
annoyance of having

00:33:18.220 --> 00:33:20.330
to define Turing
machines, I think

00:33:20.330 --> 00:33:23.780
this is a pretty
natural assumption.

00:33:23.780 --> 00:33:25.620
It's stronger than
p does not equal NP,

00:33:25.620 --> 00:33:29.200
so we would imply that.

00:33:29.200 --> 00:33:33.426
So let's do some
simple reductions--

00:33:33.426 --> 00:33:35.190
some reductions,
maybe not simple.

00:33:39.740 --> 00:33:43.240
So I mentioned this
problem independent set.

00:33:43.240 --> 00:33:46.980
That's also w1 complete.

00:33:46.980 --> 00:33:52.190
And to prove, first of
all, that it's w1 hard,

00:33:52.190 --> 00:33:56.650
I'm going to reduce this
problem to independent set.

00:33:56.650 --> 00:34:00.090
So I'm given a Turing
machine and a number k.

00:34:00.090 --> 00:34:02.530
I want to convert
it into a graph,

00:34:02.530 --> 00:34:04.210
and a number k such
that if there's

00:34:04.210 --> 00:34:06.330
an independent set of
size k in that graph,

00:34:06.330 --> 00:34:07.880
at most k in that
graph if and only

00:34:07.880 --> 00:34:11.670
if there's an accept
path of length at most

00:34:11.670 --> 00:34:14.820
k in the Turing machine.

00:34:14.820 --> 00:34:16.460
So I'm going to skip
a lot of details

00:34:16.460 --> 00:34:19.210
because I've been a little vague
about how Turing machines work,

00:34:19.210 --> 00:34:22.139
but the idea is nice.

00:34:35.739 --> 00:34:40.219
So my graph is actually
going to consist mostly

00:34:40.219 --> 00:34:44.480
of k squared cliques.

00:34:44.480 --> 00:34:47.170
The cliques are actually quite
large because this graph has

00:34:47.170 --> 00:34:51.070
size n, total size, but
there's going to be k squared

00:34:51.070 --> 00:34:53.701
clusters, which are cliques.

00:34:53.701 --> 00:34:58.850
Plus cliques, not very many
different cliques, only k

00:34:58.850 --> 00:34:59.780
squared of them.

00:34:59.780 --> 00:35:03.380
And my target independent
set is size k squared,

00:35:03.380 --> 00:35:05.830
which means if there's going
to be an independent set,

00:35:05.830 --> 00:35:09.140
I must choose exactly one
vertex from each clique.

00:35:11.870 --> 00:35:18.420
So one vertex per clique.

00:35:18.420 --> 00:35:20.537
Independence that has to
have no edges among them.

00:35:20.537 --> 00:35:22.120
So if I chose two
from a clique, there

00:35:22.120 --> 00:35:26.420
would be an edge among
them, that's disallowed.

00:35:26.420 --> 00:35:29.570
And so the cliques I'm
going to parameterize

00:35:29.570 --> 00:35:33.560
by two parameters i and
j between one and k.

00:35:33.560 --> 00:35:49.150
And the idea is that ij
represents memory cell i

00:35:49.150 --> 00:35:54.880
at time j plus the
state of the machine.

00:36:03.580 --> 00:36:09.150
So what I mean is there are
order n states in the machine,

00:36:09.150 --> 00:36:11.040
in general, that
completely characterize

00:36:11.040 --> 00:36:14.210
what the machine is about to do
and what it's thinking about,

00:36:14.210 --> 00:36:15.517
all of its internal state.

00:36:15.517 --> 00:36:17.350
And so I'm just going
to take those n states

00:36:17.350 --> 00:36:19.050
and plop them in.

00:36:19.050 --> 00:36:22.565
And each node in this clique
represents one of those states.

00:36:25.180 --> 00:36:31.220
But also I want to keep track
of what symbol is written

00:36:31.220 --> 00:36:32.940
on that square of the tape.

00:36:32.940 --> 00:36:35.684
And so, in general,
my running time

00:36:35.684 --> 00:36:37.850
is bounded by k-- also the
number of squares I write

00:36:37.850 --> 00:36:39.016
to the tape is bounded by k.

00:36:39.016 --> 00:36:41.190
Space is at most
time-- so I only

00:36:41.190 --> 00:36:46.930
have to worry about k different
cells for times 1 up to k.

00:36:49.580 --> 00:36:54.240
So there are only order n
states for this, only order n

00:36:54.240 --> 00:36:55.770
states for this.

00:36:55.770 --> 00:36:59.710
So the size of each clique is
order n squared polynomial.

00:36:59.710 --> 00:37:00.900
That's cool.

00:37:00.900 --> 00:37:02.600
It's a messy clique.

00:37:02.600 --> 00:37:04.711
I won't try to draw what
it really looks like.

00:37:04.711 --> 00:37:06.210
And then the general
approach is I'm

00:37:06.210 --> 00:37:09.770
going to add some edges
between pairs of vertices

00:37:09.770 --> 00:37:13.600
to say, well look, if this cell
was this thing at this time,

00:37:13.600 --> 00:37:15.920
and the state of the
machine was move right,

00:37:15.920 --> 00:37:19.210
then you better not change what
was on the cell at that time

00:37:19.210 --> 00:37:20.460
because nothing changed.

00:37:20.460 --> 00:37:22.310
You weren't here.

00:37:22.310 --> 00:37:25.720
So we can just forbid
that by saying,

00:37:25.720 --> 00:37:27.900
well, here is one state.

00:37:27.900 --> 00:37:30.030
These two states should
be mutually exclusive.

00:37:30.030 --> 00:37:32.560
So if I choose this vertex
in an independent set,

00:37:32.560 --> 00:37:34.160
I can't choose this vertex.

00:37:34.160 --> 00:37:36.010
So you just draw all
these connections

00:37:36.010 --> 00:37:36.950
of forbidden things.

00:37:36.950 --> 00:37:40.650
If something is true
time i, times j,

00:37:40.650 --> 00:37:43.680
then something else should
not be true at time j plus 1.

00:37:43.680 --> 00:37:47.110
You draw in all those
connections, and it works.

00:37:47.110 --> 00:37:50.510
I'll leave it at that because
the details are messy,

00:37:50.510 --> 00:37:53.260
but it gives you some idea.

00:37:53.260 --> 00:37:58.340
At this point, I can plug
a book on this topic that

00:37:58.340 --> 00:38:03.222
is coming out next year, a
little bit far in the future

00:38:03.222 --> 00:38:12.560
but it is-- I didn't write
it down here of course.

00:38:12.560 --> 00:38:13.160
Sorry.

00:38:13.160 --> 00:38:22.230
It is by these guys again,
Fomin, Kowalik, Lockshtanov,

00:38:22.230 --> 00:38:26.860
Marx, Pilipczuck
brothers, and Saurabh.

00:38:26.860 --> 00:38:31.064
And it does go through the
details, the gory details.

00:38:31.064 --> 00:38:33.230
If you're interested in
that, I could share with you

00:38:33.230 --> 00:38:35.800
that section.

00:38:35.800 --> 00:38:38.490
But unfortunately, the
book is not released yet.

00:38:38.490 --> 00:38:39.300
Cool.

00:38:39.300 --> 00:38:45.200
So that was a reduction
to independent set, which

00:38:45.200 --> 00:38:47.250
shows the independence
set is at least as

00:38:47.250 --> 00:38:51.290
hard as this problem, which
we think is hard, w1 hard.

00:38:51.290 --> 00:38:52.590
So that proves w1 hardness.

00:38:52.590 --> 00:38:56.630
But there's another question
of is independent set in w1.

00:38:56.630 --> 00:38:59.690
As you might guess from the name
w1, there's more than just w1.

00:38:59.690 --> 00:39:04.500
There's w2 and w3
and w4 and even more.

00:39:04.500 --> 00:39:07.260
And there's no one
notion of hardness

00:39:07.260 --> 00:39:09.830
like we had with NP here, so
we get a bit of complexity

00:39:09.830 --> 00:39:11.590
in that hierarchy.

00:39:11.590 --> 00:39:17.360
But in fact, independent set is
w1 hard-- sorry-- w1 complete.

00:39:17.360 --> 00:39:21.235
So we can do a reduction
from independence

00:39:21.235 --> 00:39:26.970
set to Turing machine.

00:39:26.970 --> 00:39:31.640
Let's say k-step
non-deterministic Turing

00:39:31.640 --> 00:39:32.140
machine.

00:39:39.150 --> 00:39:41.170
And the idea is pretty simple.

00:40:02.330 --> 00:40:05.080
We have k-steps, so let's
guess all the vertices.

00:40:08.126 --> 00:40:09.500
You can see here
this is where we

00:40:09.500 --> 00:40:12.070
need the ability to guess
among n different options

00:40:12.070 --> 00:40:13.301
in each guess step.

00:40:13.301 --> 00:40:15.300
So we're given-- we want
to know whether there's

00:40:15.300 --> 00:40:17.410
an independent set
of size k, so guess

00:40:17.410 --> 00:40:20.350
all those vertices of
size k and then check it.

00:40:20.350 --> 00:40:22.200
So this was our XP
algorithm, but now

00:40:22.200 --> 00:40:27.360
phrased as a non-deterministic
Turing machine algorithm.

00:40:27.360 --> 00:40:29.810
And it's a little bit subtle
what it means to check.

00:40:41.800 --> 00:40:43.550
Of course, what we
want to do is make sure

00:40:43.550 --> 00:40:48.580
there's no edge between any
pair of the chosen vertices.

00:40:48.580 --> 00:40:51.620
So what we're going to do is
when we guess these k vertices,

00:40:51.620 --> 00:40:54.420
we're going to write them one
at a time on the tape squares.

00:40:54.420 --> 00:40:56.770
So we'll use k tape squares
to store the k vertices

00:40:56.770 --> 00:40:57.930
that we've chosen.

00:40:57.930 --> 00:41:01.380
Then, we want to check
each pair of them.

00:41:01.380 --> 00:41:03.050
So this is like a
doubly nested loop

00:41:03.050 --> 00:41:06.664
for each vertex among the k,
for each vertex among the k.

00:41:06.664 --> 00:41:08.080
So what that means
is you're going

00:41:08.080 --> 00:41:11.340
to have to-- sorry,
that's a little bit

00:41:11.340 --> 00:41:18.810
annoying-- loop through
the vertices on one tape.

00:41:18.810 --> 00:41:22.050
And then the fun
part is the graph

00:41:22.050 --> 00:41:24.270
is encoded inside the machine.

00:41:24.270 --> 00:41:25.820
You could think of
there being a data

00:41:25.820 --> 00:41:29.480
section within the code that
says where all the edges are.

00:41:29.480 --> 00:41:32.290
The graph is size n, and so
you can put the entire graph

00:41:32.290 --> 00:41:34.110
into the machine.

00:41:34.110 --> 00:41:36.190
And then given two
vertices, you could

00:41:36.190 --> 00:41:39.140
check that there is indeed
no edge between them

00:41:39.140 --> 00:41:41.620
basically by using the
code as a look-up table.

00:41:41.620 --> 00:41:46.850
Say hey, is there a
vertex from i to j.

00:41:46.850 --> 00:41:50.440
If yes, then we're in
trouble, abort this option,

00:41:50.440 --> 00:41:52.270
try the other guesses.

00:41:52.270 --> 00:41:56.630
If you look up in the table
and it says there's no edge,

00:41:56.630 --> 00:41:57.470
then you keep going.

00:41:57.470 --> 00:41:59.440
So you test all the
pairs, make sure there's

00:41:59.440 --> 00:42:03.790
no edge by-- the code, as it
is, is a constant size algorithm

00:42:03.790 --> 00:42:07.110
which looks like this
plus an order n size

00:42:07.110 --> 00:42:11.560
thing, which is the
look-up table for what

00:42:11.560 --> 00:42:13.090
edges are in the graph.

00:42:13.090 --> 00:42:14.808
So that part's a
little bit weird.

00:42:14.808 --> 00:42:15.308
Yeah?

00:42:15.308 --> 00:42:19.220
AUDIENCE: So in case of the k
in k vertices aren't the same k?

00:42:19.220 --> 00:42:20.260
PROFESSOR: That's right.

00:42:20.260 --> 00:42:20.759
I'm sorry.

00:42:20.759 --> 00:42:22.200
This is k prime.

00:42:25.870 --> 00:42:26.820
Yes.

00:42:26.820 --> 00:42:30.406
So probably k prime here
is exactly order k squared.

00:42:30.406 --> 00:42:31.664
Thank you.

00:42:31.664 --> 00:42:33.210
It makes me a little happier.

00:42:33.210 --> 00:42:34.960
Because I'm doing this
doubly nested loop,

00:42:34.960 --> 00:42:37.310
I'm going to have to loop
over this thing several times.

00:42:37.310 --> 00:42:39.210
I would look at
the first vertex,

00:42:39.210 --> 00:42:41.720
memorize it by reloading
it into a register,

00:42:41.720 --> 00:42:44.272
and then loop over all the
other guys for each of them

00:42:44.272 --> 00:42:45.230
check that they're bad.

00:42:45.230 --> 00:42:48.715
So I'm going to have to go
over the tape k squared steps.

00:42:53.750 --> 00:42:56.700
And also n prime is whatever.

00:42:56.700 --> 00:42:59.750
We get a machine
that's size-- basically

00:42:59.750 --> 00:43:06.360
the size of the graph
V plus E. So the point

00:43:06.360 --> 00:43:08.670
of doing that was to
show the independence set

00:43:08.670 --> 00:43:10.990
is also w1 complete.

00:43:10.990 --> 00:43:13.580
So if you were not happy
with Turing machines,

00:43:13.580 --> 00:43:16.340
now you can completely
forget about them.

00:43:16.340 --> 00:43:18.660
They are mainly used
here as motivation

00:43:18.660 --> 00:43:21.130
for why you should expect
this problem to be hard.

00:43:21.130 --> 00:43:25.750
But it's equally difficult from
a parameterization perspective.

00:43:25.750 --> 00:43:28.080
If you ignore how
big the blow up is,

00:43:28.080 --> 00:43:30.210
it's just as hard as
an independence set.

00:43:30.210 --> 00:43:32.170
So independence set
is a problem we think

00:43:32.170 --> 00:43:34.060
there's no good algorithm.

00:43:34.060 --> 00:43:39.750
And by this reduction, and
also the reverse reduction,

00:43:39.750 --> 00:43:42.380
clique is also w1 complete.

00:43:46.210 --> 00:43:50.090
So a bunch more reductions.

00:44:01.800 --> 00:44:06.581
In general, most w1 hardness
results start from clique.

00:44:06.581 --> 00:44:09.080
So that's a good problem to
know about, or independent sets,

00:44:09.080 --> 00:44:10.680
I think.

00:44:10.680 --> 00:44:13.250
And there are some
simpler versions of clique

00:44:13.250 --> 00:44:14.800
that are also hard.

00:44:14.800 --> 00:44:20.315
So first one is clique
in regular graphs.

00:44:24.910 --> 00:44:28.870
So it's helpful to assume
all the vertices have

00:44:28.870 --> 00:44:30.680
exactly the same degree.

00:44:30.680 --> 00:44:32.750
Is it hard in three
regular graphs?

00:44:32.750 --> 00:44:33.450
No.

00:44:33.450 --> 00:44:35.830
Because three regular is
only a clique of size three.

00:44:35.830 --> 00:44:38.510
So that's pretty easy.

00:44:38.510 --> 00:44:42.620
But in s regular graphs, the
degree is going to be huge,

00:44:42.620 --> 00:44:45.410
going to be some function of n.

00:44:45.410 --> 00:44:50.190
Clique is hard, and
in general, let's say

00:44:50.190 --> 00:44:55.160
capital delta is the maximum
degree in your given graph,

00:44:55.160 --> 00:44:59.140
and we want to convert it
into a delta regular graph.

00:44:59.140 --> 00:45:02.410
So we're going to increase all
the lower degrees up to delta.

00:45:02.410 --> 00:45:08.550
What we're going to do is take
delta copies of the graph.

00:45:11.890 --> 00:45:16.390
So for every old vertex,
we get delta copies of it.

00:45:20.140 --> 00:45:23.250
And then we do this reduction.

00:45:23.250 --> 00:45:25.940
So the blue is the original--
is the graph that's

00:45:25.940 --> 00:45:27.570
been duplicated delta times.

00:45:27.570 --> 00:45:29.807
Here, delta is 5.

00:45:29.807 --> 00:45:31.390
But suppose, in the
graph, this vertex

00:45:31.390 --> 00:45:34.320
only had degree 3, some
number less than 5.

00:45:34.320 --> 00:45:36.440
Then, what we're
going to do is create

00:45:36.440 --> 00:45:41.010
delta minus d new vertices
and then add a biclique here,

00:45:41.010 --> 00:45:44.930
bipartite clique, connecting all
of those delta minus d things

00:45:44.930 --> 00:45:47.150
to all of these delta things.

00:45:47.150 --> 00:45:51.250
They're delta copies
of the original vertex.

00:45:51.250 --> 00:45:55.004
This is v over here,
v1 through v delta.

00:45:55.004 --> 00:45:56.670
And so what that means
is these vertices

00:45:56.670 --> 00:46:00.310
will have entered degree delta
because there were delta things

00:46:00.310 --> 00:46:01.900
to connect to over there.

00:46:01.900 --> 00:46:03.710
And now these things
will have degree delta

00:46:03.710 --> 00:46:05.180
because they used
to have degree d,

00:46:05.180 --> 00:46:06.800
and now they have
an additional degree

00:46:06.800 --> 00:46:09.830
delta minus d on the left.

00:46:09.830 --> 00:46:12.290
So cool, everything's
now delta regular.

00:46:12.290 --> 00:46:14.740
And I claim that this
reduction preserve cliques

00:46:14.740 --> 00:46:19.870
because if you look at
these added vertices,

00:46:19.870 --> 00:46:23.830
they do not belong
to any triangles.

00:46:23.830 --> 00:46:27.330
This thing here is an
induced bipartite graph,

00:46:27.330 --> 00:46:31.170
and so there are no
triangles, no things of size--

00:46:31.170 --> 00:46:34.600
no cycles of size 3 because
all cycles are even here,

00:46:34.600 --> 00:46:36.262
which means if
you're in a clique--

00:46:36.262 --> 00:46:37.720
if you have three
guys in a clique,

00:46:37.720 --> 00:46:42.215
then they better have
triangles everywhere.

00:46:42.215 --> 00:46:43.840
So if you put one of
these vertices in,

00:46:43.840 --> 00:46:46.390
you'll have a very small
clique, namely size 2.

00:46:46.390 --> 00:46:49.080
And there are always cliques
of size 2 in a graph.

00:46:49.080 --> 00:46:50.240
There's at least one edge.

00:46:50.240 --> 00:46:52.190
So if there was at
least one edge before,

00:46:52.190 --> 00:46:55.750
afterwards, we do not increase
any of the clique sizes.

00:46:55.750 --> 00:46:57.600
So clique size is preserved.

00:46:57.600 --> 00:47:00.090
Everything's cool.

00:47:00.090 --> 00:47:01.980
So in this situation
k prime equals

00:47:01.980 --> 00:47:04.991
k when you blow up anything,
except the graph size,

00:47:04.991 --> 00:47:05.490
of course.

00:47:05.490 --> 00:47:08.192
That got a little bigger.

00:47:08.192 --> 00:47:09.650
And now everything's
delta regular.

00:47:09.650 --> 00:47:13.320
Le me give you an example
of why this is useful.

00:47:17.857 --> 00:47:20.190
Of course, every time I say
something is true of clique,

00:47:20.190 --> 00:47:21.648
it's also true of
independence set.

00:47:21.648 --> 00:47:23.940
You can just flip things.

00:47:23.940 --> 00:47:25.590
Here, we have to be
a little careful,

00:47:25.590 --> 00:47:28.130
but when you complement
a graph, if you

00:47:28.130 --> 00:47:30.030
started with a regular
graph, your new graph

00:47:30.030 --> 00:47:31.780
will also be regular.

00:47:31.780 --> 00:47:34.835
Every vertex would
have degree n minus 1

00:47:34.835 --> 00:47:36.918
minus delta or n minus
delta, something like that.

00:47:39.650 --> 00:47:48.094
So here's another problem,
eerily similar to vertex cover,

00:47:48.094 --> 00:47:49.990
called partial vertex cover.

00:47:49.990 --> 00:47:53.770
This is also w1 complete.

00:47:53.770 --> 00:48:02.745
We want to know, can I choose
k vertices that cover l edges?

00:48:07.010 --> 00:48:09.950
So usually vertex cover, you
need to cover all the edges,

00:48:09.950 --> 00:48:12.185
and that let us do some
crazy things because whenever

00:48:12.185 --> 00:48:14.310
we looked at an edge, we
knew one of the two things

00:48:14.310 --> 00:48:15.670
was in the vertex cover.

00:48:15.670 --> 00:48:18.100
Now, we don't know that anymore.

00:48:18.100 --> 00:48:20.050
Now, it's a matter
of which vertices get

00:48:20.050 --> 00:48:22.210
the most bang for your buck.

00:48:22.210 --> 00:48:40.450
And this is easy to reduce from
delta regular independence set

00:48:40.450 --> 00:48:44.780
because-- k independence set
if we want to be explicit.

00:48:44.780 --> 00:48:47.350
We want to know is there an
independent set of size k.

00:48:47.350 --> 00:48:50.210
I just give that to
partial vertex cover

00:48:50.210 --> 00:48:55.270
with k prime equal
to delta times k.

00:48:55.270 --> 00:49:00.970
Sorry, that's l prime
is delta times k.

00:49:00.970 --> 00:49:04.080
So the idea is here I want
to choose k vertices that

00:49:04.080 --> 00:49:05.260
are independent.

00:49:05.260 --> 00:49:08.340
And if I can do that, I will be
able to choose k vertices that

00:49:08.340 --> 00:49:09.839
cover exactly
delta times k edges

00:49:09.839 --> 00:49:11.880
because if they're
independent, none of the edges

00:49:11.880 --> 00:49:13.710
will be shared among
my independence set.

00:49:13.710 --> 00:49:15.180
And this is if and only if.

00:49:15.180 --> 00:49:18.630
If I try to choose k vertices
that cover exactly delta k

00:49:18.630 --> 00:49:22.190
edges, then they
can't be adjacent.

00:49:22.190 --> 00:49:23.840
The vertices can be adjacent.

00:49:23.840 --> 00:49:26.490
So these problems become
the same under this mapping,

00:49:26.490 --> 00:49:28.000
so I didn't even blow up.

00:49:28.000 --> 00:49:29.190
My parameter here is k.

00:49:29.190 --> 00:49:30.990
I forgot to mention.

00:49:30.990 --> 00:49:34.290
I mean you could guess from the
letter, but parameterized by k,

00:49:34.290 --> 00:49:36.210
this problem is w1 complete.

00:49:36.210 --> 00:49:39.000
Parameterized by l,
this problem is FPT.

00:49:39.000 --> 00:49:41.850
So be a little careful.

00:49:41.850 --> 00:49:45.370
But here's the reduction for k.

00:49:45.370 --> 00:49:47.690
There is no reduction
for-- there's only

00:49:47.690 --> 00:49:51.530
good reductions for l.

00:49:51.530 --> 00:49:54.560
So there we are clearly using
that the graph was regular.

00:49:54.560 --> 00:49:57.264
Otherwise, it would be
at most k-- at most delta

00:49:57.264 --> 00:49:58.930
for everybody, and
then it's hard to get

00:49:58.930 --> 00:49:59.929
the actual independence.

00:49:59.929 --> 00:50:00.760
Yeah.

00:50:00.760 --> 00:50:02.468
AUDIENCE: Sorry, I'm
still a bit confused

00:50:02.468 --> 00:50:05.750
about the relationship
between w1 and XP and FPT.

00:50:05.750 --> 00:50:07.730
I don't know what w1 is.

00:50:07.730 --> 00:50:10.340
PROFESSOR: Yeah,
well we'll get there.

00:50:10.340 --> 00:50:12.760
For now, you can think
w1 is all problems they

00:50:12.760 --> 00:50:16.030
can be parameterized, reduced to
k-step non-deterministic Turing

00:50:16.030 --> 00:50:17.169
machine.

00:50:17.169 --> 00:50:18.210
That's a fine definition.

00:50:18.210 --> 00:50:20.150
Some people use that.

00:50:20.150 --> 00:50:23.485
And in general, FPT
is contained in w1.

00:50:23.485 --> 00:50:26.170
It's contained in other
things, which we will get to.

00:50:26.170 --> 00:50:30.490
w2 and so on is all
contained in XP.

00:50:30.490 --> 00:50:32.880
And these are strict if you
believe exponential time

00:50:32.880 --> 00:50:34.010
hypothesis.

00:50:34.010 --> 00:50:36.730
So if you believe there are
no sub exponential algorithms

00:50:36.730 --> 00:50:41.320
for SAT, then this problem has
no FPT algorithm with respect

00:50:41.320 --> 00:50:44.419
to k and all the
w1 complete things.

00:50:44.419 --> 00:50:46.210
I mean from a complexity
theory standpoint,

00:50:46.210 --> 00:50:48.800
it will be fun to look
at these larger classes.

00:50:48.800 --> 00:50:51.030
From is there an FPT
algorithm standpoint,

00:50:51.030 --> 00:50:53.540
all you care about
is it's not here.

00:50:53.540 --> 00:50:57.090
And any hardness,
w1 or worse, will

00:50:57.090 --> 00:51:00.260
imply there's no FPT
algorithm if you assume ETH.

00:51:00.260 --> 00:51:02.770
AUDIENCE: Do you know if any
of these inclusions are strict?

00:51:02.770 --> 00:51:04.520
PROFESSOR: Like I said,
they're all strict

00:51:04.520 --> 00:51:06.270
if you assume exponential
time hypothesis.

00:51:08.640 --> 00:51:12.420
If you prove XP is
different from FPT,

00:51:12.420 --> 00:51:17.190
then you prove P does not
equal NP, I think, pretty sure.

00:51:17.190 --> 00:51:20.640
So we're not going to
non-categorically say

00:51:20.640 --> 00:51:24.120
these things are strict, because
these are all stronger versions

00:51:24.120 --> 00:51:25.510
of P does not equal NP.

00:51:25.510 --> 00:51:28.240
But if you believe
exponential time hypothesis,

00:51:28.240 --> 00:51:30.110
then they're all strict.

00:51:30.110 --> 00:51:33.225
So that's one
standard assumption

00:51:33.225 --> 00:51:36.200
that gives us everything.

00:51:36.200 --> 00:51:39.440
Cool, let me give you another
version of clique that's hard.

00:51:57.020 --> 00:52:08.840
Mufti-colored clique if you
remember way back to three

00:52:08.840 --> 00:52:11.600
partition, we had a
variation on three partition

00:52:11.600 --> 00:52:14.520
called numerical
three-dimensional matching

00:52:14.520 --> 00:52:17.700
where you had to choose
your triples from three

00:52:17.700 --> 00:52:19.390
different sets.

00:52:19.390 --> 00:52:21.392
So this is the analog
of that, or the analog

00:52:21.392 --> 00:52:22.850
of three-dimensional
matching where

00:52:22.850 --> 00:52:25.230
you had three different
sets of vertices.

00:52:25.230 --> 00:52:29.410
For clique here, set
the vertices partition

00:52:29.410 --> 00:52:35.330
into k clusters.

00:52:35.330 --> 00:52:44.910
And the question is, is there
k clique with one vertex

00:52:44.910 --> 00:52:47.700
per cluster?

00:52:50.410 --> 00:52:53.100
And in fact, so you think of
these things as being colored,

00:52:53.100 --> 00:52:56.360
color 1, and these
are color k and so on.

00:52:56.360 --> 00:52:59.740
And in fact, we can assume
this is a proper coloring

00:52:59.740 --> 00:53:02.960
because if you're not allowed
to choose two vertices from one

00:53:02.960 --> 00:53:06.270
class, then there's no reason
to have edges between vertices

00:53:06.270 --> 00:53:07.510
of the same color.

00:53:07.510 --> 00:53:13.020
So this is in fact a
k-coloring of the graph.

00:53:13.020 --> 00:53:15.370
So we are given a k-color graph.

00:53:15.370 --> 00:53:18.740
And we want to know,
does it have a k clique?

00:53:18.740 --> 00:53:20.790
And if it has a d clique
and it's k-colored, then

00:53:20.790 --> 00:53:23.790
in particular, you will
have exactly one vertex

00:53:23.790 --> 00:53:24.780
per color class.

00:53:28.020 --> 00:53:33.205
We can prove this is hard
by a reduction from clique.

00:53:41.400 --> 00:53:44.040
Namely, if we have
a vertex v, we're

00:53:44.040 --> 00:53:46.080
going to make k copies of it.

00:53:48.699 --> 00:53:50.115
You'll see this
is a common trick.

00:53:55.260 --> 00:54:00.760
And we will color
them 1 to up to k.

00:54:00.760 --> 00:54:05.890
So good, now we have
vertices in each color class.

00:54:05.890 --> 00:54:12.810
And then if we have an edge
(v,w) convert it into edges

00:54:12.810 --> 00:54:19.420
(vi, wj) for all
i not equal to j.

00:54:19.420 --> 00:54:21.550
So pretty much the
obvious reduction.

00:54:21.550 --> 00:54:24.887
Once you said make k copies,
we'll also duplicate the edges

00:54:24.887 --> 00:54:27.470
in all versions except, because
it's supposed to be a k-color,

00:54:27.470 --> 00:54:31.190
we're not allowed
to connect (vi,wi).

00:54:31.190 --> 00:54:34.130
But otherwise, we'll just
throw in all of those edges.

00:54:34.130 --> 00:54:36.230
And the point is this,
again, doesn't really

00:54:36.230 --> 00:54:38.120
blow up your cliques
because if you

00:54:38.120 --> 00:54:40.260
have some clique
in this structure,

00:54:40.260 --> 00:54:42.480
you can just forget
about the i indices.

00:54:42.480 --> 00:54:45.400
Because you know
it's a clique, you

00:54:45.400 --> 00:54:47.440
will never choose two
verses from the same color

00:54:47.440 --> 00:54:49.735
class, two vertices
with the same index,

00:54:49.735 --> 00:54:51.860
and so you can take any
clique here and collapse it

00:54:51.860 --> 00:54:52.647
to a clique here.

00:54:52.647 --> 00:54:54.230
Conversely, if you
have a clique here,

00:54:54.230 --> 00:54:57.013
you can just assign those
vertices arbitrary numbers

00:54:57.013 --> 00:54:59.263
as long as they're all
distinct any permutation of one

00:54:59.263 --> 00:55:01.300
through k, and you'll
get a clique down here.

00:55:01.300 --> 00:55:02.360
So it's the same problem.

00:55:02.360 --> 00:55:02.860
Question?

00:55:02.860 --> 00:55:06.390
AUDIENCE: That v is different
from the v above it, right?

00:55:06.390 --> 00:55:09.850
PROFESSOR: Yeah, this is
for all v and for all vw.

00:55:13.130 --> 00:55:17.820
If there's an edge in the
graph, then we do that.

00:55:17.820 --> 00:55:21.980
And if there's a vertex
in the graph, we do that.

00:55:21.980 --> 00:55:24.270
Cool.

00:55:24.270 --> 00:55:27.712
So k prime here equals
k, no expansion.

00:55:31.090 --> 00:55:34.260
This may seem trivial,
but it's actually

00:55:34.260 --> 00:55:37.690
a fairly recent innovation to
think about multicolor clique

00:55:37.690 --> 00:55:40.250
and, in general, it
simplifies proofs.

00:55:40.250 --> 00:55:42.820
I have heard of proofs
simplifying from tens of pages

00:55:42.820 --> 00:55:44.950
to one page.

00:55:44.950 --> 00:55:47.390
We'll probably get to
some more-- to such

00:55:47.390 --> 00:55:49.420
sophisticated example soon.

00:55:49.420 --> 00:55:52.865
But I can give you
one simple example.

00:55:52.865 --> 00:55:54.240
Before I get to
a simple example,

00:55:54.240 --> 00:55:57.430
I want to show
you a fun example.

00:55:57.430 --> 00:56:02.340
Not simple, but let me tell
you what the problem is.

00:56:02.340 --> 00:56:03.805
I won't cover that proof.

00:56:13.280 --> 00:56:23.276
The problem is shortest
common super sequence.

00:56:27.000 --> 00:56:30.490
This is a problem that comes
up in computational biology.

00:56:30.490 --> 00:56:33.540
Your given k strings.

00:56:37.490 --> 00:56:43.250
Let's say alphabet sigma.

00:56:43.250 --> 00:56:52.480
And yes, you're given a number,
which I'm going to write l.

00:56:56.330 --> 00:56:59.970
You want to find
a string of length

00:56:59.970 --> 00:57:14.250
l that's a super sequence
of all input strings.

00:57:20.120 --> 00:57:23.310
So maybe you're given the
DNA sequence of human and DNA

00:57:23.310 --> 00:57:26.160
sequence of mouse,
and you want to know

00:57:26.160 --> 00:57:27.990
what is the shortest
DNA sequence that

00:57:27.990 --> 00:57:30.750
contains all of the
letters of those strings

00:57:30.750 --> 00:57:32.520
in the correct order.

00:57:32.520 --> 00:57:35.080
So this is often
called an alignment.

00:57:35.080 --> 00:57:39.200
If you have ACGG--
this is probably not

00:57:39.200 --> 00:57:41.930
valid, but whatever--
And we have

00:57:41.930 --> 00:57:44.020
some other guy like CAGGAT.

00:57:50.186 --> 00:57:51.560
So I tried to draw
those aligned.

00:57:55.790 --> 00:57:58.280
So then the common super
string here is ACAGGACT.

00:58:03.820 --> 00:58:06.170
This is a super string,
meaning I can drop letters

00:58:06.170 --> 00:58:07.550
from down here and get this.

00:58:07.550 --> 00:58:09.680
Or, I can drop letters from
down here and get this.

00:58:09.680 --> 00:58:11.650
That was for k equals 2.

00:58:11.650 --> 00:58:14.070
In general, this
problem does have

00:58:14.070 --> 00:58:17.030
an n to the k dynamic program.

00:58:17.030 --> 00:58:19.160
But the question is whether
you could get FPT in k.

00:58:19.160 --> 00:58:21.740
Could you get a small dependence
on the-- better dependence

00:58:21.740 --> 00:58:22.940
on the number of strings?

00:58:22.940 --> 00:58:25.610
And the answer is no
because this is w1 complete.

00:58:30.080 --> 00:58:31.840
And this is a
sketch of the proof.

00:58:34.769 --> 00:58:36.560
it's a reduction for
a multicolored clique.

00:58:36.560 --> 00:58:38.650
I think this the paper that
introduced multicolored clique

00:58:38.650 --> 00:58:39.160
in fact.

00:58:39.160 --> 00:58:43.110
It's the earliest one I
could find, and this is 2003.

00:58:43.110 --> 00:58:46.450
It became more
popular since 2009.

00:58:46.450 --> 00:58:48.660
So you're basically
encoding the edges

00:58:48.660 --> 00:58:52.480
by whether there's a 0 here
in between a huge set of ones.

00:58:52.480 --> 00:58:56.210
And then this is intuitively
rep-- and then for every vertex

00:58:56.210 --> 00:58:59.160
you're looking at, all
of the other vertices

00:58:59.160 --> 00:59:01.290
and encoding whether they
have an edge to them.

00:59:01.290 --> 00:59:02.665
And so if you have
a clique, that

00:59:02.665 --> 00:59:05.430
will be a pattern
that you see repeated

00:59:05.430 --> 00:59:07.172
among multiple vertices.

00:59:07.172 --> 00:59:08.880
And so you'll end up
being able to shrink

00:59:08.880 --> 00:59:13.380
your longest-- your shortest
common super sequence.

00:59:13.380 --> 00:59:14.600
But the details are messy.

00:59:14.600 --> 00:59:16.730
It's a few pages to prove it.

00:59:16.730 --> 00:59:18.255
I will skip that
and tell you why

00:59:18.255 --> 00:59:19.880
I wanted to tell you
about this problem

00:59:19.880 --> 00:59:22.680
other than its computational
biology and useful.

00:59:22.680 --> 00:59:27.340
Because it's used to prove
that flood-it is hard.

00:59:27.340 --> 00:59:29.310
Time for some fun.

00:59:29.310 --> 00:59:33.820
So here is flood-it.

00:59:37.700 --> 00:59:39.870
You have a grid of
colored squares.

00:59:39.870 --> 00:59:43.020
And in the top left,
we have colors.

00:59:43.020 --> 00:59:45.467
And this is the special square.

00:59:45.467 --> 00:59:47.550
What I can do is control
the color of that square.

00:59:47.550 --> 00:59:49.970
So for example, I
could set it to red

00:59:49.970 --> 00:59:52.400
and then anything that was
in that connected group

00:59:52.400 --> 00:59:53.010
becomes red.

00:59:53.010 --> 00:59:54.760
And now I've got a
bigger connected group.

00:59:54.760 --> 00:59:59.510
So now I can make it blue,
and then pink, and then red,

00:59:59.510 --> 01:00:03.285
and then blue, and then red.

01:00:03.285 --> 01:00:05.060
Let's see how well I can do.

01:00:05.060 --> 01:00:07.400
And the trouble is I have
a limited number of moves.

01:00:10.310 --> 01:00:18.710
So this is a model for Ebola
virus spreading or zombie

01:00:18.710 --> 01:00:24.000
infection or pick your favorite.

01:00:24.000 --> 01:00:27.145
I mean you imagine these are
different species-- green.

01:00:27.145 --> 01:00:28.990
Thank you.

01:00:28.990 --> 01:00:31.780
A lot easier to play
when I have an oracle.

01:00:31.780 --> 01:00:33.842
Yellow, that's better.

01:00:33.842 --> 01:00:35.580
I don't know.

01:00:35.580 --> 01:00:37.080
I'm actually doing
pretty well here.

01:00:39.820 --> 01:00:43.240
I think now I just
have to do them all.

01:00:43.240 --> 01:00:44.160
I made it!

01:00:44.160 --> 01:00:45.940
25 moves.

01:00:45.940 --> 01:00:48.000
Wow!

01:00:48.000 --> 01:00:48.870
Felicitations!

01:00:48.870 --> 01:00:50.161
That's the first time I've won.

01:00:50.161 --> 01:00:52.610
It's a good demo.

01:00:52.610 --> 01:00:55.660
Only played a few times,
but-- this apparently

01:00:55.660 --> 01:00:58.770
became quite popular in 2006,
a company called LabPixies,

01:00:58.770 --> 01:01:00.495
since bought by Google.

01:01:00.495 --> 01:01:01.870
But there's tons
of free versions

01:01:01.870 --> 01:01:04.290
out there you can play.

01:01:04.290 --> 01:01:11.400
It's w1 complete with respect
to number of colors and number

01:01:11.400 --> 01:01:14.010
of leaves.

01:01:14.010 --> 01:01:16.280
So I don't know if this
is a generalization

01:01:16.280 --> 01:01:19.310
or specialization, but it's a
variation of the problem that's

01:01:19.310 --> 01:01:20.644
been studied.

01:01:20.644 --> 01:01:22.060
It's the only one
I could can that

01:01:22.060 --> 01:01:26.430
talked about parametrized
complexity of flood-it.

01:01:26.430 --> 01:01:29.892
And so this is
flood-it on trees.

01:01:34.590 --> 01:01:38.930
So same set up, but instead
of being a square grid graph,

01:01:38.930 --> 01:01:40.270
I'm on a tree graph.

01:01:40.270 --> 01:01:43.020
And the root of the tree
is the one I can control,

01:01:43.020 --> 01:01:47.100
and my graph is going to
look like-- my tree is

01:01:47.100 --> 01:01:48.100
going to look like this.

01:01:52.955 --> 01:01:54.330
These are going
to be my strings.

01:01:57.130 --> 01:01:59.990
And if I want to be
able to solve this

01:01:59.990 --> 01:02:04.800
in some number of
moves, l moves,

01:02:04.800 --> 01:02:08.790
then I need to-- the
sequence of colors

01:02:08.790 --> 01:02:11.980
I do is a longest common
subsequence, super sequence,

01:02:11.980 --> 01:02:17.280
of these strings because they
all have to get to the bottom.

01:02:17.280 --> 01:02:19.530
They'll sit there and wait,
and so you can essentially

01:02:19.530 --> 01:02:21.430
drop letters.

01:02:21.430 --> 01:02:22.790
And that's cool.

01:02:22.790 --> 01:02:24.160
Happy?

01:02:24.160 --> 01:02:28.180
This is not literally
true because if I have,

01:02:28.180 --> 01:02:32.340
let's say, a zero and a zero
here, both the same color--

01:02:32.340 --> 01:02:35.870
So I'm representing
letters by colors,

01:02:35.870 --> 01:02:38.550
then when I play-- if I
eventually play here and play

01:02:38.550 --> 01:02:42.480
zero, I actually get advanced
two spaces instead of one.

01:02:42.480 --> 01:02:48.530
So for that, you need to
first map every letter a.

01:02:48.530 --> 01:02:54.180
So let's say in string
i, we map every letter

01:02:54.180 --> 01:03:00.160
to a followed by a special
character for that string.

01:03:00.160 --> 01:03:01.860
And so this blows things up.

01:03:01.860 --> 01:03:03.616
In particular, it blows up l.

01:03:03.616 --> 01:03:06.400
l grows by a factor
of the total length

01:03:06.400 --> 01:03:08.820
of all strings because of
all these things that we add.

01:03:08.820 --> 01:03:11.310
These are not
really compressible,

01:03:11.310 --> 01:03:13.732
but it means that we alternate
between regular characters

01:03:13.732 --> 01:03:14.690
and special characters.

01:03:14.690 --> 01:03:16.481
And therefore, we never
have two characters

01:03:16.481 --> 01:03:17.980
in a row that are the same.

01:03:17.980 --> 01:03:20.170
So first you take
this problem, then

01:03:20.170 --> 01:03:22.300
you reduce it to the
version of the problem that

01:03:22.300 --> 01:03:24.750
has no repeated characters.

01:03:24.750 --> 01:03:27.204
And then you can reduce that
to flood-it because that's

01:03:27.204 --> 01:03:27.995
what we care about.

01:03:27.995 --> 01:03:30.235
AUDIENCE: Sorry, what's the
parameter on the shortest

01:03:30.235 --> 01:03:30.822
common super--

01:03:30.822 --> 01:03:31.530
PROFESSOR: Right.

01:03:31.530 --> 01:03:34.160
Parameter is-- they're
actually two parameters,

01:03:34.160 --> 01:03:38.344
so this is a fun
thing, k and sigma.

01:03:38.344 --> 01:03:40.010
So two parameters,
you can think of that

01:03:40.010 --> 01:03:42.100
as being parameterized
by the sum or just

01:03:42.100 --> 01:03:44.517
by the vector k, sigma.

01:03:44.517 --> 01:03:47.100
Basically, we get to assume both
of them are relatively small.

01:03:47.100 --> 01:03:50.200
We could be exponential
in both k and sigma.

01:03:50.200 --> 01:03:52.410
If you put l in there,
it would not be hard.

01:03:52.410 --> 01:03:56.720
But if you put k and sigma
and it's still w1 complete.

01:03:56.720 --> 01:03:59.520
And so over here it's
the number of leaves

01:03:59.520 --> 01:04:02.042
in the tree is one parameter
and the number of colors

01:04:02.042 --> 01:04:03.000
is the other parameter.

01:04:03.000 --> 01:04:05.000
So we're fixed parameter
tractable with respect

01:04:05.000 --> 01:04:06.732
to that joint parameter.

01:04:06.732 --> 01:04:07.232
Yeah?

01:04:07.232 --> 01:04:09.565
AUDIENCE: Why is it obvious
that you can reduce flood-it

01:04:09.565 --> 01:04:10.862
on trees to not flood-it?

01:04:10.862 --> 01:04:12.570
PROFESSOR: It's not
obvious that the grid

01:04:12.570 --> 01:04:14.700
problem is related to trees.

01:04:14.700 --> 01:04:19.160
There are hardness results
for like 2 by n flood-it.

01:04:19.160 --> 01:04:20.410
But they're just NP hardness.

01:04:20.410 --> 01:04:23.270
I didn't see a w1 hardness
for 2 by n flood-it.

01:04:23.270 --> 01:04:26.270
So as far as I
know, 2 by n colored

01:04:26.270 --> 01:04:28.986
is hard-- is open from a
parameterized complexity

01:04:28.986 --> 01:04:29.485
standpoint.

01:04:33.890 --> 01:04:35.410
Cool.

01:04:35.410 --> 01:04:38.510
I want to do one more
reduction, and then I'll

01:04:38.510 --> 01:04:39.550
finally define w1.

01:05:01.780 --> 01:05:08.010
Dominating Set, this problem
is actually w2 complete,

01:05:08.010 --> 01:05:09.780
so this is even harder.

01:05:09.780 --> 01:05:16.190
But before we worry about w2,
let's prove that it's w1 hard.

01:05:16.190 --> 01:05:21.626
So we're going to reduce
from multicolored clique

01:05:21.626 --> 01:05:24.456
to dominating set.

01:05:24.456 --> 01:05:26.610
I have a nice figure.

01:05:26.610 --> 01:05:32.800
This is a preview of this cool
parameterized algorithms book.

01:05:32.800 --> 01:05:41.484
So I'm going to represent
each-- got it right.

01:05:45.348 --> 01:05:48.270
Yeah, funny.

01:05:48.270 --> 01:05:50.030
I'm going to
represent each vertex

01:05:50.030 --> 01:05:52.370
in the multicolored
independence set problem

01:05:52.370 --> 01:05:55.160
by a vertex in the
dominating set problem,

01:05:55.160 --> 01:05:57.660
so vertices mapped to vertices.

01:05:57.660 --> 01:05:59.670
This is the joy of
multicolored cliques.

01:05:59.670 --> 01:06:02.120
We did all this k
duplication stuff

01:06:02.120 --> 01:06:04.990
once so that we don't have to
duplicate our graphic anymore.

01:06:04.990 --> 01:06:05.990
We just take this graph.

01:06:05.990 --> 01:06:07.340
We poured it over.

01:06:07.340 --> 01:06:14.660
Now, what that tells us is that
the vi's form color classes.

01:06:14.660 --> 01:06:17.730
And what I'm going to do is
connect each color class.

01:06:17.730 --> 01:06:19.720
So this is the set of
all things of color one,

01:06:19.720 --> 01:06:21.710
set of all things of color two.

01:06:21.710 --> 01:06:24.270
First of all, I'm going to add
2 dummy vertices to each color

01:06:24.270 --> 01:06:26.030
class just to hang out there.

01:06:26.030 --> 01:06:29.040
And then I'm going to connect
everything in the color class

01:06:29.040 --> 01:06:32.310
by a clique.

01:06:32.310 --> 01:06:36.070
Before we only had edges
between color classes,

01:06:36.070 --> 01:06:39.100
but now I want to color--

01:06:39.100 --> 01:06:40.940
I'm going to also
change those edges.

01:06:40.940 --> 01:06:42.440
But the vertices
mapped to vertices,

01:06:42.440 --> 01:06:44.070
I add two vertices
of each color,

01:06:44.070 --> 01:06:46.730
and then this circle represents
that there's a clique in here,

01:06:46.730 --> 01:06:48.350
clique in here, clique in here.

01:06:48.350 --> 01:06:51.640
Now, my goal is to
find a dominating set.

01:06:51.640 --> 01:06:53.980
And so the intention
is that you need

01:06:53.980 --> 01:06:57.700
to choose one vertex
from each color class

01:06:57.700 --> 01:06:59.827
in the dominant set.

01:06:59.827 --> 01:07:02.160
And that's actually what these
dummy vertices are doing.

01:07:02.160 --> 01:07:04.880
They are only going to be
connect to things in here.

01:07:04.880 --> 01:07:06.910
They have to be
covered by somebody,

01:07:06.910 --> 01:07:09.310
so that means you have to
choose someone in this clique.

01:07:09.310 --> 01:07:11.060
You have to choose
someone in this clique.

01:07:11.060 --> 01:07:12.920
When you do, it covers
everybody in there.

01:07:12.920 --> 01:07:14.753
And there's going to
be no point of choosing

01:07:14.753 --> 01:07:16.910
these dummy vertices because
they're only connected

01:07:16.910 --> 01:07:17.630
to other things in the clique.

01:07:17.630 --> 01:07:19.340
You might as well
choose these things.

01:07:19.340 --> 01:07:25.150
Now, how do I represent an edge
in my independence set graph.

01:07:25.150 --> 01:07:29.300
If I have an edge
between say u and v,

01:07:29.300 --> 01:07:31.300
that's in this color class
and this color class.

01:07:31.300 --> 01:07:33.050
We know they're in
different color classes

01:07:33.050 --> 01:07:36.270
by multicolored
clique property--

01:07:36.270 --> 01:07:38.210
multicolored independent
set property.

01:07:38.210 --> 01:07:40.150
Sorry, same thing.

01:07:40.150 --> 01:07:44.170
So I want to not
choose both u and v.

01:07:44.170 --> 01:07:46.680
So I'm going to represent
that by adding a vertex here,

01:07:46.680 --> 01:07:49.110
which must be dominated,
and connect it

01:07:49.110 --> 01:07:52.480
to everyone except u over
here and everyone except v

01:07:52.480 --> 01:07:54.530
over here, the red patches.

01:07:54.530 --> 01:07:57.990
So there are edges from this
vertex to every red vertex

01:07:57.990 --> 01:07:59.250
here and here.

01:07:59.250 --> 01:08:02.580
So what that means is
if I don't choose u,

01:08:02.580 --> 01:08:04.940
I will cover this added vertex.

01:08:04.940 --> 01:08:09.410
If I don't choose v, I will
cover this added vertex.

01:08:09.410 --> 01:08:13.850
So I will cover it as long as
I don't choose both u and v.

01:08:13.850 --> 01:08:17.330
So it's like I should choose
at most one of these two guys,

01:08:17.330 --> 01:08:18.826
then this will be
covered for free.

01:08:18.826 --> 01:08:20.950
And it has to be covered
for free because we just--

01:08:20.950 --> 01:08:25.040
if I set k prime to be the
number of color classes, which

01:08:25.040 --> 01:08:28.189
is k, then I don't
have any flexibility.

01:08:28.189 --> 01:08:30.720
I can't choose any of
these vertices to dominate.

01:08:30.720 --> 01:08:33.729
And so that simulates
independence using dominating

01:08:33.729 --> 01:08:37.430
set, pretty simple and clean.

01:08:37.430 --> 01:08:40.040
So that gives you a
flavor of nice reductions

01:08:40.040 --> 01:08:45.430
you can do with multicolored
clique or independence set.

01:08:45.430 --> 01:08:49.250
AUDIENCE: What's
two extra vertices?

01:08:49.250 --> 01:08:52.060
PROFESSOR: Why two?

01:08:52.060 --> 01:08:53.667
AUDIENCE: The two
you added, it looks

01:08:53.667 --> 01:08:55.669
like are not adjacent so--

01:08:55.669 --> 01:08:59.069
AUDIENCE: Oh, so
we derived them.

01:08:59.069 --> 01:09:03.300
PROFESSOR: Right We want
them to be there in order

01:09:03.300 --> 01:09:05.779
to force choosing
somebody down here,

01:09:05.779 --> 01:09:07.830
but we don't actually want
to choose one of them.

01:09:07.830 --> 01:09:09.850
And so we emit that edge
so if you choose one,

01:09:09.850 --> 01:09:11.160
you don't cover the other.

01:09:11.160 --> 01:09:11.660
Thanks.

01:09:16.172 --> 01:09:17.630
I think it simplifies
the argument.

01:09:17.630 --> 01:09:19.340
It's probably not
necessary because I

01:09:19.340 --> 01:09:21.359
think you could argue
by replacement argument

01:09:21.359 --> 01:09:26.170
that you won't choose
them, but anyway.

01:09:26.170 --> 01:09:27.920
Time for some definitions.

01:09:27.920 --> 01:09:32.586
I wanted to do some fun problems
before we got to the somewhat

01:09:32.586 --> 01:09:33.335
weird definitions.

01:09:37.850 --> 01:09:38.700
The time has come.

01:09:45.270 --> 01:09:48.560
By the way, dominate set is
w2 complete, so is set cover.

01:09:48.560 --> 01:09:53.500
And most w2 hardness reductions
start from one of these two.

01:09:53.500 --> 01:09:56.910
We already know how to reduce
dominating set to set cover.

01:09:56.910 --> 01:10:00.100
You make a set for every
neighborhood set in the graph.

01:10:00.100 --> 01:10:02.170
So this reduction
is easy, and that

01:10:02.170 --> 01:10:05.580
preserves a parameter so just
from the same reduction we

01:10:05.580 --> 01:10:06.950
did two classes ago, I think.

01:10:10.180 --> 01:10:14.750
Here is a problem which
will look familiar,

01:10:14.750 --> 01:10:17.560
but under a slightly
different name.

01:10:17.560 --> 01:10:21.275
This is what I might
call circuit ones.

01:10:25.660 --> 01:10:29.190
One's meaning you want to
set some number of the inputs

01:10:29.190 --> 01:10:32.120
to be one, just like our
optimization version.

01:10:32.120 --> 01:10:34.406
In this universe, it is
called weighted circuit set.

01:10:34.406 --> 01:10:36.030
I'm not a fan of
weighted because there

01:10:36.030 --> 01:10:38.150
aren't actual weights here.

01:10:38.150 --> 01:10:39.900
The goal is to get
minimum hamming weight,

01:10:39.900 --> 01:10:42.770
meaning the minimum number
of ones in your input.

01:10:42.770 --> 01:10:48.780
So I give you a circuit-- I'll
draw a very simple circuit,

01:10:48.780 --> 01:10:51.010
and it has one output.

01:10:51.010 --> 01:10:53.490
It has some number of inputs.

01:10:53.490 --> 01:11:00.880
My goal is to set k
ones and get a one out.

01:11:00.880 --> 01:11:01.570
Did I do it?

01:11:01.570 --> 01:11:02.465
I did it!

01:11:02.465 --> 01:11:05.020
! wasn't even looking.

01:11:05.020 --> 01:11:07.041
So that's an example of
an input and an output

01:11:07.041 --> 01:11:08.040
to weighted circuit set.

01:11:08.040 --> 01:11:15.514
The parameter here, k, is the
number of ones, one inputs.

01:11:15.514 --> 01:11:16.930
So the question
is can you satisfy

01:11:16.930 --> 01:11:19.900
a circuit using only k ones.

01:11:19.900 --> 01:11:21.550
That's weighted circuit set.

01:11:21.550 --> 01:11:25.400
This problem defines
the class called WP.

01:11:25.400 --> 01:11:27.320
This is the original
definition of WP

01:11:27.320 --> 01:11:31.650
is all problems that reduce
to weighted circuit set

01:11:31.650 --> 01:11:34.020
in a parameterized
reduction sense.

01:11:34.020 --> 01:11:38.400
It's all parameterized
problems that

01:11:38.400 --> 01:11:40.180
reduce to weighted circuit set.

01:11:48.510 --> 01:11:49.870
I lost my hierarchy.

01:11:49.870 --> 01:11:52.600
But this is even bigger
than all the things.

01:11:52.600 --> 01:11:56.740
So we have FPT is
contained in w1.

01:11:56.740 --> 01:12:00.850
It's contained in w2, et cetera.

01:12:00.850 --> 01:12:05.300
And then we have WP,
and then we have XP.

01:12:05.300 --> 01:12:06.845
So it's not bigger
than everything.

01:12:06.845 --> 01:12:10.110
It's not bigger than XP.

01:12:10.110 --> 01:12:14.880
I won't prove this
here, but it is true.

01:12:14.880 --> 01:12:16.910
Now, I want to
specialize this problem.

01:12:16.910 --> 01:12:23.150
I'm going to simplify it
using a notion called weft.

01:12:23.150 --> 01:12:27.300
So first the depth
of the circuit,

01:12:27.300 --> 01:12:28.600
this is the longest path.

01:12:34.400 --> 01:12:35.500
That's a normal notion.

01:12:40.460 --> 01:12:49.130
Then, the weft of a circuit
is the maximum number

01:12:49.130 --> 01:12:54.814
of big gates on an
input to output path.

01:13:03.180 --> 01:13:10.214
Big gates means let's
say more than two inputs.

01:13:10.214 --> 01:13:12.630
In general, it's more than
some constant number of inputs,

01:13:12.630 --> 01:13:16.450
but two is fine because if you
use a bunch of two input gates,

01:13:16.450 --> 01:13:18.990
you can build a ten input gate.

01:13:18.990 --> 01:13:21.420
But there's a distinction
between constant input gates

01:13:21.420 --> 01:13:23.580
and super constant input gates.

01:13:23.580 --> 01:13:25.970
So if you count how many
super constant input gates

01:13:25.970 --> 01:13:30.440
you need in a constant
depth circuit,

01:13:30.440 --> 01:13:38.086
then we get the w
classes, w for weft.

01:13:43.920 --> 01:13:51.500
w(t) is the set of
parameterized problems

01:13:51.500 --> 01:14:05.170
that reduce to constant depth
weft-t weighted circuit set.

01:14:10.070 --> 01:14:12.160
t is not the parameter.

01:14:12.160 --> 01:14:14.740
K is still the parameter.
k is the number of ones.

01:14:14.740 --> 01:14:15.640
t is the thing.

01:14:19.620 --> 01:14:21.170
Let's do some examples.

01:14:21.170 --> 01:14:24.060
AUDIENCE: Depth has to be
larger than weft numbers?

01:14:24.060 --> 01:14:26.230
PROFESSOR: Yes, depth is
always larger than weft.

01:14:26.230 --> 01:14:28.980
So we allow-- so t
is a fixed constant.

01:14:28.980 --> 01:14:31.255
Depth could be 100
times t or something.

01:14:31.255 --> 01:14:34.484
AUDIENCE: Then, w in WP is
a different w than w(t)?

01:14:37.621 --> 01:14:39.370
PROFESSOR: They both
have to do with weft,

01:14:39.370 --> 01:14:42.240
but that's the P
is a polynomial.

01:14:42.240 --> 01:14:45.720
It corresponds to
polynomial weft, which

01:14:45.720 --> 01:14:47.825
is basically unbounded weft.

01:14:47.825 --> 01:14:49.359
w does not stand for weighted.

01:14:49.359 --> 01:14:50.775
In both cases, it
stands for weft.

01:14:54.150 --> 01:14:55.520
What is weft?

01:14:55.520 --> 01:14:57.720
Weft is the opposite of warp.

01:14:57.720 --> 01:15:00.080
When you're weaving,
you've got the warp threads

01:15:00.080 --> 01:15:02.690
and then you've got this weft
thread that goes back and forth

01:15:02.690 --> 01:15:05.260
and ties the whole
circuit together.

01:15:05.260 --> 01:15:09.310
So that's sideways,
so if you imagine

01:15:09.310 --> 01:15:13.980
that constant input gates do
not need a weft to hold them in,

01:15:13.980 --> 01:15:18.880
but super constant ones do, then
it's how many layers of wefting

01:15:18.880 --> 01:15:21.340
do you have to do to
cover all those things?

01:15:21.340 --> 01:15:25.730
That's the larges the longest
path in terms of counting

01:15:25.730 --> 01:15:29.610
the-- that's the term, OK?

01:15:29.610 --> 01:15:32.050
But it's actually fairly
useful to think about.

01:15:32.050 --> 01:15:35.550
So for example, independence
set, inputs are up here.

01:15:35.550 --> 01:15:36.940
Output is down here.

01:15:36.940 --> 01:15:39.050
I need to choose,
for each vertex,

01:15:39.050 --> 01:15:40.550
whether it's in the
independent set.

01:15:40.550 --> 01:15:44.480
And if I negate them, then
I need various constraints.

01:15:44.480 --> 01:15:47.490
This is the graph that's
being represented.

01:15:47.490 --> 01:15:49.650
I want to say either
I don't choose this

01:15:49.650 --> 01:15:52.560
or I don't choose this.

01:15:52.560 --> 01:15:55.410
And then all of those
clauses must be true.

01:15:55.410 --> 01:15:57.170
That's the and of those things.

01:15:57.170 --> 01:15:58.700
This is big.

01:15:58.700 --> 01:15:59.540
These are not big.

01:15:59.540 --> 01:16:00.670
They have two inputs.

01:16:00.670 --> 01:16:02.140
This has one input.

01:16:02.140 --> 01:16:04.410
So these are all free.

01:16:04.410 --> 01:16:07.140
And then we just do one
in terms of the depth

01:16:07.140 --> 01:16:10.380
here where there's one
level of big gates.

01:16:10.380 --> 01:16:12.900
So independent set is in w1.

01:16:12.900 --> 01:16:15.609
That's a proof that it's in w1.

01:16:15.609 --> 01:16:17.900
So that proves all the things
we've been talking about,

01:16:17.900 --> 01:16:22.040
except dominating set, is in w1.

01:16:22.040 --> 01:16:25.970
Now dominating set,
you need two levels.

01:16:25.970 --> 01:16:29.820
So we have the a constraint.

01:16:29.820 --> 01:16:32.080
So this is saying that
a should be covered.

01:16:32.080 --> 01:16:34.680
a is adjacent to a, b, and c.

01:16:34.680 --> 01:16:38.080
I mean if I choose a, b, or
c, then a will be dominated

01:16:38.080 --> 01:16:40.640
by the definition of
dominating set because a

01:16:40.640 --> 01:16:42.430
is adjacent just to b and c.

01:16:42.430 --> 01:16:44.540
But also if I choose
a, then it's dominated.

01:16:44.540 --> 01:16:46.429
So this is an or of
those three things.

01:16:46.429 --> 01:16:48.470
And a should be dominated,
b should be dominated,

01:16:48.470 --> 01:16:49.386
c should be dominated.

01:16:49.386 --> 01:16:50.851
So it's an and of
all those things.

01:16:50.851 --> 01:16:52.350
But if we don't
have bounded degree,

01:16:52.350 --> 01:16:53.933
which we can't because
this problem is

01:16:53.933 --> 01:17:00.140
easy for bounded degree, then
we have two levels of big gates.

01:17:00.140 --> 01:17:02.830
So this problem is in w2.

01:17:02.830 --> 01:17:07.470
Now, funnily enough, w1 and
w2 are the most common things

01:17:07.470 --> 01:17:08.140
you see.

01:17:08.140 --> 01:17:11.710
I've never seen
a paper about w3.

01:17:11.710 --> 01:17:14.290
Some statement, I
think, about problems

01:17:14.290 --> 01:17:18.990
that we tend to care about,
ands of ors are common.

01:17:18.990 --> 01:17:22.510
This, of course,
CNF SAT is in w2.

01:17:22.510 --> 01:17:25.210
I should say weighted
CNF SAT is in w2.

01:17:25.210 --> 01:17:28.800
It's actually w2 complete.

01:17:28.800 --> 01:17:33.390
Weighted three
set is w1 complete

01:17:33.390 --> 01:17:35.850
because things of size 3 are OK.

01:17:35.850 --> 01:17:38.210
We can split those into
two things of size 2.

01:17:38.210 --> 01:17:45.360
So in general, order one
SAT, you have five SAT.

01:17:45.360 --> 01:17:48.640
That's w1 complete.

01:17:48.640 --> 01:17:52.290
And CNF SAT is w2 complete.

01:17:52.290 --> 01:17:54.260
And I think most of the
problems we care about

01:17:54.260 --> 01:17:58.005
can be expressed as--
this should be weighted.

01:17:58.005 --> 01:18:01.820
I'm going tow rite w,
but I mean weighted.

01:18:01.820 --> 01:18:04.460
But this is a
capital W, different.

01:18:04.460 --> 01:18:06.810
That's weft.

01:18:06.810 --> 01:18:11.800
So fine that's-- and most of the
problems we care about can be

01:18:11.800 --> 01:18:13.030
represented as CNF SAT.

01:18:13.030 --> 01:18:15.800
I think that's why we
rarely get outside of w2.

01:18:15.800 --> 01:18:17.190
But there are
other things there.

01:18:17.190 --> 01:18:22.720
You could do an or of ands of
ors or an and of ors of ands.

01:18:22.720 --> 01:18:26.414
Now, we know such formulas
can be converted into CNF form

01:18:26.414 --> 01:18:27.830
and only get a
polynomial blow up.

01:18:27.830 --> 01:18:29.010
What's going on?

01:18:29.010 --> 01:18:32.040
Well, when you do
that inefficient form,

01:18:32.040 --> 01:18:33.370
you add extra variables.

01:18:33.370 --> 01:18:35.420
When you add extra
variables, you are no longer

01:18:35.420 --> 01:18:36.440
preserving the weight.

01:18:36.440 --> 01:18:39.020
It would not be a
parameterized reduction.

01:18:39.020 --> 01:18:41.937
So if you're just caring about
satisfiability, that's fine.

01:18:41.937 --> 01:18:44.020
But if you're considering
weighted satisfiability,

01:18:44.020 --> 01:18:46.340
you want to minimize
the number of ones,

01:18:46.340 --> 01:18:48.540
it totally changes when
you convert to CNF.

01:18:48.540 --> 01:18:49.295
So it matters.

01:18:52.220 --> 01:18:59.100
One more fun result, if you
look at 2-tape non-deterministic

01:18:59.100 --> 01:19:01.930
Turing machines--
the reason I had

01:19:01.930 --> 01:19:03.889
to define 1-tape
non-deterministic Turing

01:19:03.889 --> 01:19:05.430
machines is because
they're different

01:19:05.430 --> 01:19:07.820
from 2-tape non-deterministic
Turing machines.

01:19:07.820 --> 01:19:11.570
These are w2 complete.

01:19:11.570 --> 01:19:15.520
The k-step version of 2-tape.

01:19:15.520 --> 01:19:18.080
2-tapes is you have 2-tapes
that can advance independently.

01:19:18.080 --> 01:19:20.630
Or, you can think of there
are two fingers you can

01:19:20.630 --> 01:19:22.910
independently move on one tape.

01:19:22.910 --> 01:19:25.050
Those are equivalent,
and they give you w2.

01:19:25.050 --> 01:19:28.310
Three tapes, gives w2.

01:19:28.310 --> 01:19:31.337
Any constant number of
tapes, you still get w2.

01:19:31.337 --> 01:19:33.170
And this is another, I
think, natural reason

01:19:33.170 --> 01:19:35.780
why w2 comes up a lot.

01:19:35.780 --> 01:19:40.310
I have seen problems that are
hard for what's called w-star.

01:19:40.310 --> 01:19:43.420
This w-star is wt
for all fixed t.

01:19:43.420 --> 01:19:48.320
You can think of
that as w order a.

01:19:48.320 --> 01:19:49.870
So it's the same thing.

01:19:49.870 --> 01:19:52.310
So I've seen problems that
don't depend on what t is.

01:19:52.310 --> 01:19:53.910
They're hard for all of them.

01:19:53.910 --> 01:19:56.060
But I've never seen a
w3 complete problem.

01:19:56.060 --> 01:19:57.550
Maybe we can think of one.

01:19:57.550 --> 01:20:00.900
I've also seen WP come
up, and there's also

01:20:00.900 --> 01:20:07.240
a problem called W-SAT which
is if you have weighted formula

01:20:07.240 --> 01:20:10.840
sat instead of weighted circuits
sat, you get w-sat instead WP.

01:20:10.840 --> 01:20:15.610
I think it's a little weaker
because in circuit SAT you can

01:20:15.610 --> 01:20:17.664
reuse things and in
formula set, you can't.

01:20:17.664 --> 01:20:19.830
And it turns out to give
slightly different classes.

01:20:19.830 --> 01:20:21.532
So things are messier here.

01:20:21.532 --> 01:20:23.490
But if you just care
about whether a problem is

01:20:23.490 --> 01:20:28.220
fixed parameter tractable,
these are all bad.

01:20:28.220 --> 01:20:29.970
But the point of knowing
about them is you

01:20:29.970 --> 01:20:32.910
want to know if your
problem-- first you

01:20:32.910 --> 01:20:35.230
should check where it
fits in the w hierarchy

01:20:35.230 --> 01:20:37.730
by just thinking about what's
a trivial way to write it down

01:20:37.730 --> 01:20:39.190
in the CNF style thing.

01:20:39.190 --> 01:20:41.190
Figure out whether
it's w1 or w2.

01:20:41.190 --> 01:20:43.780
Then, you'll know which
problems you should start from.

01:20:43.780 --> 01:20:46.497
If it's w1, probably want to
start from a version of clique.

01:20:46.497 --> 01:20:48.080
If it's w2, you
probably want to start

01:20:48.080 --> 01:20:49.455
from a version of
dominating set.

01:20:49.455 --> 01:20:51.030
That's why we talked
about them here.

01:20:51.030 --> 01:20:52.640
And usually it's w1 or w2.

01:20:55.320 --> 01:20:56.210
Cool.

01:20:56.210 --> 01:20:58.820
Questions?

01:20:58.820 --> 01:21:00.360
We'll do more next time.

01:21:00.360 --> 01:21:02.000
This is a fun area.

01:21:02.000 --> 01:21:04.540
And next time we'll talk
about, in particular,

01:21:04.540 --> 01:21:08.400
how the exponential time
hypothesis relates to all this.

