WEBVTT
Kind: captions
Language: en

00:00:00.499 --> 00:00:01.950
The following
content is provided

00:00:01.950 --> 00:00:06.060
by MIT OpenCourseWare under
a Creative Commons license.

00:00:06.060 --> 00:00:08.230
Additional information
about our license

00:00:08.230 --> 00:00:10.490
and MIT OpenCourseWare
in general,

00:00:10.490 --> 00:00:12.160
is available at ocw.mit.edu.

00:00:17.247 --> 00:00:19.330
PROFESSOR: I thought I
would, in this last lecture

00:00:19.330 --> 00:00:23.780
before the break, speak
about one specific topic.

00:00:26.660 --> 00:00:30.050
It's often referred to
as a fast Poisson solver,

00:00:30.050 --> 00:00:33.920
so what does Poisson mean?

00:00:33.920 --> 00:00:36.350
Poisson means
Laplace's equation.

00:00:36.350 --> 00:00:39.870
So, this is the
five-point Laplacian,

00:00:39.870 --> 00:00:43.700
which could be some other
discrete Laplace matrix,

00:00:43.700 --> 00:00:46.760
but let's take the one we know.

00:00:46.760 --> 00:00:52.660
So we're in two dimensions,
and you use Poisson's name,

00:00:52.660 --> 00:00:55.030
instead of Laplace's
name, when there's

00:00:55.030 --> 00:00:57.560
a non-zero right-hand side.

00:00:57.560 --> 00:01:02.210
So otherwise, it's a Laplace
solver, but here Poisson.

00:01:02.210 --> 00:01:02.840
OK.

00:01:02.840 --> 00:01:05.170
So, remember that
right-hand side comes

00:01:05.170 --> 00:01:10.810
from maybe a right-hand side
f of x, y in the differential

00:01:10.810 --> 00:01:17.280
equation, but it also comes from
non-zero boundary conditions,

00:01:17.280 --> 00:01:21.850
because non-zero boundary
conditions, when the five

00:01:21.850 --> 00:01:26.580
points hit a boundary, that
known value was moved over

00:01:26.580 --> 00:01:29.480
to the right-hand side
and becomes part of f.

00:01:29.480 --> 00:01:35.370
So f comes from the non-zero
boundary conditions, as well as

00:01:35.370 --> 00:01:37.730
the non-zero right-hand side.

00:01:37.730 --> 00:01:39.840
OK.

00:01:39.840 --> 00:01:45.580
Important problems, but special
on a square or a rectangle

00:01:45.580 --> 00:01:50.040
or a cube or a box,
so we're speaking

00:01:50.040 --> 00:01:53.760
about special geometry.

00:01:53.760 --> 00:02:00.740
Today, as we've been doing,
I'll take the case on a square,

00:02:00.740 --> 00:02:05.410
and you will see that the
whole idea would not fly,

00:02:05.410 --> 00:02:12.060
if we were on an ellipse
or something like that.

00:02:12.060 --> 00:02:18.420
But a lot of problems on
rectangular domains do appear

00:02:18.420 --> 00:02:22.510
in applications,
or we could use --

00:02:22.510 --> 00:02:27.370
I hope you always think now
about possible preconditioners.

00:02:27.370 --> 00:02:29.850
Any time you have
something fast,

00:02:29.850 --> 00:02:33.700
it's a candidate to
be a preconditioner

00:02:33.700 --> 00:02:35.890
for a real problem.

00:02:35.890 --> 00:02:39.280
The real problem might
not be on a square,

00:02:39.280 --> 00:02:44.140
or the real problem might not
have the constant coefficient

00:02:44.140 --> 00:02:48.710
that we have in the Laplacian.

00:02:48.710 --> 00:02:52.970
In that case, you're not
solving the exact problem,

00:02:52.970 --> 00:02:56.720
but one that could be
reasonably close to it.

00:02:56.720 --> 00:02:59.730
OK.

00:02:59.730 --> 00:03:03.085
So we've discussed the
solution to this problem,

00:03:03.085 --> 00:03:04.460
and actually I
have a little more

00:03:04.460 --> 00:03:08.640
to say about the movie
that is now on the website,

00:03:08.640 --> 00:03:11.320
because I'm quite
excited about that movie.

00:03:14.320 --> 00:03:16.050
I'll come to the
movie in a second.

00:03:16.050 --> 00:03:20.830
Let me just say what
today's lecture would be.

00:03:20.830 --> 00:03:26.750
The key idea here is that the
eigenvalues and eigenvectors

00:03:26.750 --> 00:03:31.630
of this giant matrix of order
N squared by N squared --

00:03:31.630 --> 00:03:34.690
size is N squared by N squared.

00:03:34.690 --> 00:03:37.380
The eigenvalues and the
eigenvectors are known.

00:03:37.380 --> 00:03:40.270
First of all, they're known.

00:03:40.270 --> 00:03:44.340
The eigenvectors are nice
discrete sine functions;

00:03:44.340 --> 00:03:50.080
they're sines,
because I'm assuming

00:03:50.080 --> 00:03:52.350
they come to zero
at the boundary;

00:03:52.350 --> 00:03:56.410
so that's why I have a
sine, rather than a cosine.

00:03:56.410 --> 00:04:00.260
First, they're
known, and second, we

00:04:00.260 --> 00:04:05.680
can work with them very
quickly, using the FFT.

00:04:05.680 --> 00:04:10.710
So the point is that
it's quite exceptional;

00:04:10.710 --> 00:04:15.290
in fact, I don't think I
know any comparable example,

00:04:15.290 --> 00:04:18.990
in which a linear system is
solved by using the eigenvalues

00:04:18.990 --> 00:04:21.100
and eigenvectors.

00:04:21.100 --> 00:04:23.870
Usually eigenvalues
and eigenvectors,

00:04:23.870 --> 00:04:26.810
they pay off for
differential equations

00:04:26.810 --> 00:04:29.160
that are growing in time.

00:04:29.160 --> 00:04:32.170
Then it is worth computing
them, because then you can just

00:04:32.170 --> 00:04:35.320
multiply by e to the lambda*t,
and you know what's happening

00:04:35.320 --> 00:04:35.820
later.

00:04:38.600 --> 00:04:42.860
Eigenvectors, eigenvalues have
other purposes, but very, very

00:04:42.860 --> 00:04:46.230
rarely are they used to
solve a linear system.

00:04:46.230 --> 00:04:49.360
I mean, it's usually
far more work.

00:04:49.360 --> 00:04:51.800
And of course, it would
be incredibly more work

00:04:51.800 --> 00:04:54.380
if we had to find the
eigenvalues and eigenvectors,

00:04:54.380 --> 00:04:57.160
but for this problem
we know them.

00:04:57.160 --> 00:05:01.010
And it also would be incredibly
more work if the matrix

00:05:01.010 --> 00:05:05.130
of eigenvectors,
the basis matrix,

00:05:05.130 --> 00:05:09.600
the key matrix that
I'll denote by S --

00:05:09.600 --> 00:05:12.850
so the eigenvectors
go in a matrix S;

00:05:12.850 --> 00:05:17.430
the eigenvalues go in a
matrix capital lambda;

00:05:17.430 --> 00:05:19.000
so we know lambda.

00:05:19.000 --> 00:05:22.350
It's going to be a simple
matrix, just diagonal,

00:05:22.350 --> 00:05:25.520
got those N numbers -- N
squared numbers, I guess,

00:05:25.520 --> 00:05:28.160
because we're of size N squared.

00:05:28.160 --> 00:05:31.570
But these eigenvectors we know.

00:05:31.570 --> 00:05:35.870
So we know them, and we can
compute quickly with them,

00:05:35.870 --> 00:05:40.270
using the FFT, or using,
you might want me to say,

00:05:40.270 --> 00:05:45.210
fast sine transform,
discrete sine transform,

00:05:45.210 --> 00:05:48.180
rather than Fourier
transform, which

00:05:48.180 --> 00:05:52.400
we think of as doing
the complex exponential.

00:05:52.400 --> 00:05:58.890
So this is a small special
fast Fourier world.

00:05:58.890 --> 00:06:01.080
It's a special
fast Fourier world,

00:06:01.080 --> 00:06:07.690
in which the FFT and the
related sine and cosine

00:06:07.690 --> 00:06:13.290
give us a quick answer,
faster than elimination,

00:06:13.290 --> 00:06:16.240
because you all
know, it's n log n,

00:06:16.240 --> 00:06:19.970
if n is the number of
Fourier components,

00:06:19.970 --> 00:06:23.490
and that's hard to beat.

00:06:23.490 --> 00:06:28.170
Then I'll mention also,
for this same problem,

00:06:28.170 --> 00:06:33.640
there is another way in
which it can be simplified.

00:06:33.640 --> 00:06:41.230
It's not a Fourier way,
just a direct combining

00:06:41.230 --> 00:06:44.260
neighboring equations,
so that'll be number two.

00:06:44.260 --> 00:06:46.870
OK, but mostly the lecture
is about number one.

00:06:49.670 --> 00:06:53.250
The best example I know
in which you would use

00:06:53.250 --> 00:06:56.220
eigenvectors/eigenvalues
to solve

00:06:56.220 --> 00:07:00.420
an ordinary linear system,
and I'll say in a word

00:07:00.420 --> 00:07:04.750
just how you do it, and then
what these eigenvectors are.

00:07:04.750 --> 00:07:08.030
OK.

00:07:08.030 --> 00:07:09.180
Pause.

00:07:09.180 --> 00:07:12.330
Time out to say something
about the movie.

00:07:12.330 --> 00:07:18.200
So that movie that's
now on the website

00:07:18.200 --> 00:07:27.150
does sparse elimination, and
the example it takes is K2D,

00:07:27.150 --> 00:07:30.700
and you can make it 8
squared by 8 squared,

00:07:30.700 --> 00:07:34.780
10 squared by 10 squared,
20 squared by 20 squared,

00:07:34.780 --> 00:07:40.490
because it shows the order
that the nodes are eliminated

00:07:40.490 --> 00:07:47.040
and the graphs of
non-zeros in the matrix.

00:07:47.040 --> 00:07:49.560
It's a bit slow.

00:07:49.560 --> 00:07:55.900
If you do 20 by 20, you
have to go away for lunch,

00:07:55.900 --> 00:08:01.390
well maybe not lunch, but at
least coffee before it's done,

00:08:01.390 --> 00:08:05.740
but when it's done, it
counts the number of --

00:08:05.740 --> 00:08:14.920
it shows the non-zeros that
are in the elimination,

00:08:14.920 --> 00:08:17.930
in the factor L
from elimination,

00:08:17.930 --> 00:08:20.750
and it counts the
number of non-zeros,

00:08:20.750 --> 00:08:24.730
and I was just talking to
Persson about also getting it

00:08:24.730 --> 00:08:27.260
to count the number
of actual flops.

00:08:29.810 --> 00:08:33.610
Well, why am I interested?

00:08:33.610 --> 00:08:41.170
I'm interested, because I
don't know what power of N

00:08:41.170 --> 00:08:43.960
those numbers are growing with.

00:08:43.960 --> 00:08:47.350
I don't know whether the number
of non-zeros -- and I did 10,

00:08:47.350 --> 00:08:56.730
20, 30, and it looked not too
far from the power capital N

00:08:56.730 --> 00:09:05.240
cubed, but the notes say
for nested dissection,

00:09:05.240 --> 00:09:15.540
which would be another
ordering, N squared log N. So,

00:09:15.540 --> 00:09:19.670
and of course, what power we
get depends on what algorithm we

00:09:19.670 --> 00:09:23.250
use for ordering, so nested
dissection is one ordering,

00:09:23.250 --> 00:09:28.700
which hopefully we could
put into another movie.

00:09:28.700 --> 00:09:35.390
The movie now has exact
minimum degree, real MMD,

00:09:35.390 --> 00:09:41.520
which takes as the
next node the one

00:09:41.520 --> 00:09:46.670
with absolutely the minimum
degree, not just close to it.

00:09:46.670 --> 00:09:51.580
Anyway, have a
look at that movie,

00:09:51.580 --> 00:10:00.130
and if you have
any interest, see

00:10:00.130 --> 00:10:03.510
how the count increases
as n increases,

00:10:03.510 --> 00:10:07.720
and also you could
change, slightly adapt,

00:10:07.720 --> 00:10:11.630
the algorithm that
creates the ordering,

00:10:11.630 --> 00:10:15.370
creates the permutation
and see what happens there.

00:10:15.370 --> 00:10:20.540
There's a lot to do with that,
and it's a pretty fundamental

00:10:20.540 --> 00:10:21.430
problem, actually.

00:10:21.430 --> 00:10:26.930
We're talking there about what
MATLAB's backslash operation

00:10:26.930 --> 00:10:29.970
will do for this equation.

00:10:29.970 --> 00:10:33.100
So MATLAB'S backslash
operation will use elimination;

00:10:33.100 --> 00:10:35.470
it won't use fast
Poisson solvers,

00:10:35.470 --> 00:10:38.760
but now let me come to
the fast Poisson solver.

00:10:38.760 --> 00:10:39.870
OK.

00:10:39.870 --> 00:10:42.800
So I guess the main
point is I have

00:10:42.800 --> 00:10:45.910
to say what are the
eigenvalues and eigenvectors,

00:10:45.910 --> 00:10:47.270
and how do they get used.

00:10:47.270 --> 00:10:49.250
So let me say, how
do they get used.

00:10:49.250 --> 00:10:52.390
So how can I use
eigenvalues and eigenvectors

00:10:52.390 --> 00:10:55.870
to solve a problem like that.

00:10:55.870 --> 00:10:58.080
Let me just call it
K rather than K2D.

00:10:58.080 --> 00:11:02.380
So K*U equals F. OK.

00:11:04.970 --> 00:11:06.740
By eigenvectors.

00:11:09.780 --> 00:11:12.100
OK, there are three steps.

00:11:12.100 --> 00:11:12.890
Step one.

00:11:15.400 --> 00:11:20.530
Expand the right-hand
side as a combination

00:11:20.530 --> 00:11:22.070
of the eigenvectors.

00:11:22.070 --> 00:11:22.850
OK.

00:11:22.850 --> 00:11:28.840
So expand F, this
right-hand side vector,

00:11:28.840 --> 00:11:34.250
as some combination of
the first eigenvector,

00:11:34.250 --> 00:11:41.570
maybe I'm going to call it
y_1, second eigenvector,

00:11:41.570 --> 00:11:48.412
n-th eigenvector, OK, good.

00:11:48.412 --> 00:11:49.620
That means -- what do I mean?

00:11:49.620 --> 00:11:54.000
I mean you have to find those
c's, that's the job there.

00:11:54.000 --> 00:11:58.170
Find the coefficients.

00:11:58.170 --> 00:12:03.780
So that's a job, a numerical --
it's a linear system to solve

00:12:03.780 --> 00:12:05.860
and we'll see what
it amounts to.

00:12:05.860 --> 00:12:08.230
OK, but suppose
the right-hand side

00:12:08.230 --> 00:12:10.300
is a combination of
the eigenvectors,

00:12:10.300 --> 00:12:11.600
how can you use that?

00:12:11.600 --> 00:12:14.320
Well, step two is
the real quick step.

00:12:14.320 --> 00:12:30.910
Divide each c_i by the
eigenvalue lambda_i.

00:12:30.910 --> 00:12:36.580
OK, so it's by eigenvector,
so I'm assuming that K*y_i is

00:12:36.580 --> 00:12:41.890
lambda_i*y_i and that
we know these guys.

00:12:41.890 --> 00:12:44.670
So this is known.

00:12:44.670 --> 00:12:48.820
And now the question, I'm just
saying, how do we assume known?

00:12:53.340 --> 00:12:55.250
So my question now
is how do we use it?

00:12:55.250 --> 00:12:59.540
OK, step one -- the idea is
going to be write everything

00:12:59.540 --> 00:13:01.850
in terms of eigenvectors.

00:13:01.850 --> 00:13:04.490
Work with the eigenvectors,
because if I've

00:13:04.490 --> 00:13:09.990
got eigenvectors,
the step is scalar;

00:13:09.990 --> 00:13:12.610
I just divide these
numbers by those numbers,

00:13:12.610 --> 00:13:15.370
and then I've got the answer.

00:13:15.370 --> 00:13:21.230
And then construct -- the
correct answer will be U will

00:13:21.230 --> 00:13:32.680
be c_1 over lambda_1 y_1 and
c_2 over lambda_2 y_2 up to c_n

00:13:32.680 --> 00:13:41.120
over lambda_n y_n, a combination
of those same eigenvectors with

00:13:41.120 --> 00:13:43.740
the same coefficients,
just divided by lambda.

00:13:47.310 --> 00:13:50.790
But this is, of course,
another numerical job;

00:13:50.790 --> 00:13:54.540
this is like adding
up a Fourier series;

00:13:54.540 --> 00:13:56.650
this is like finding the
Fourier coefficients,

00:13:56.650 --> 00:14:00.980
this is like
reconstructing the input.

00:14:00.980 --> 00:14:04.150
Only because I've
divided by lambda_i,

00:14:04.150 --> 00:14:07.080
I'm getting the
output here is U,

00:14:07.080 --> 00:14:10.700
when the input was F.
And do you see that that

00:14:10.700 --> 00:14:13.690
is the correct answer?

00:14:13.690 --> 00:14:16.840
All I have to do is
check that K*U equals F.

00:14:16.840 --> 00:14:26.040
So check that this answer from
step three is the right answer.

00:14:26.040 --> 00:14:34.960
OK, so I multiply by K.
When I multiply y_1 by K,

00:14:34.960 --> 00:14:39.850
a factor lambda_1 appears, the
eigenvalue; it cancels that;

00:14:39.850 --> 00:14:42.480
well that's y divided,
so it would cancel,

00:14:42.480 --> 00:14:44.640
and I have c_1*y_1.

00:14:44.640 --> 00:14:49.940
When I multiply this by
K, K*y_2 is lambda_2*y_2;

00:14:49.940 --> 00:14:54.250
cancel the lambda_2's, and
you're left with c_2*y_2,

00:14:54.250 --> 00:14:55.000
and so on.

00:14:55.000 --> 00:15:02.250
So, when I multiplied by
K, I got F. That's it.

00:15:02.250 --> 00:15:09.760
So that's the whole idea
written out, but now,

00:15:09.760 --> 00:15:14.430
what actual computations go
into steps one and three?

00:15:14.430 --> 00:15:15.840
Step two is pretty simple.

00:15:15.840 --> 00:15:19.740
Well, actually this is
a good way to look it.

00:15:19.740 --> 00:15:23.790
I want to write that same
algorithm in matrix language.

00:15:23.790 --> 00:15:29.790
OK, so in matrix form.

00:15:29.790 --> 00:15:31.850
We have the matrix
of eigenvectors,

00:15:31.850 --> 00:15:37.320
and that's what I'm
calling S. And it's

00:15:37.320 --> 00:15:43.320
got the eigenvectors y_1,
y_2, y_n in its columns.

00:15:43.320 --> 00:15:52.090
And the eigenvalue matrix,
we need a name for that too,

00:15:52.090 --> 00:15:54.860
and that we decided
to call lambda,

00:15:54.860 --> 00:16:01.480
and that's got the
eigenvalues on its diagonal.

00:16:01.480 --> 00:16:08.030
So this is 18.06,
linear algebra.

00:16:08.030 --> 00:16:16.020
The matrix of eigenvectors,
if I multiply K by S,

00:16:16.020 --> 00:16:21.770
then I'm multiplying K by
all its little eigenvectors,

00:16:21.770 --> 00:16:26.280
and K times this
gives me lambda_1*y_1,

00:16:26.280 --> 00:16:29.250
K times y_2 gives
me lambda_2*y_2,

00:16:29.250 --> 00:16:38.250
K*y_n is lambda_n*y_n, and if
I look to see what this is,

00:16:38.250 --> 00:16:44.060
this is the same as y_1 to
y_n multiplied by lambda.

00:16:44.060 --> 00:16:47.070
If I just multiply on
the right by lambda,

00:16:47.070 --> 00:16:49.420
it will take lambda_1
times the first column,

00:16:49.420 --> 00:16:53.650
lambda_2 times the second,
lambda_n times the last,

00:16:53.650 --> 00:16:56.050
which is what we want,
so it's S*lambda.

00:16:58.660 --> 00:17:04.810
This is all n eigenvalues and
eigenvectors in one matrix

00:17:04.810 --> 00:17:07.330
equation, that's all that is.

00:17:07.330 --> 00:17:17.790
It's just K*S equal S*lambda_i
just says this for all i

00:17:17.790 --> 00:17:20.470
at once, all i at the same time.

00:17:20.470 --> 00:17:21.320
OK.

00:17:21.320 --> 00:17:26.000
So if I use these
matrices in describing

00:17:26.000 --> 00:17:29.040
steps one, two,
three, I'll see what's

00:17:29.040 --> 00:17:32.740
happening matrix language.

00:17:32.740 --> 00:17:34.770
OK, let me just do that.

00:17:34.770 --> 00:17:41.550
Step one: step one
is looking for F

00:17:41.550 --> 00:17:46.230
as a combination of
the columns of S.

00:17:46.230 --> 00:17:52.220
So step one is just
F equals S times c.

00:17:52.220 --> 00:18:00.110
The vector of coefficients
multiplies the columns of S

00:18:00.110 --> 00:18:07.700
and adds to give
F. Then step two,

00:18:07.700 --> 00:18:15.310
which just divides everything
by -- divides by the lambdas.

00:18:15.310 --> 00:18:20.310
Step two just creates
lambda inverse S*c.

00:18:22.850 --> 00:18:30.880
So I took what I had
-- let's see, no,

00:18:30.880 --> 00:18:34.500
the lambda inverse better
be multiplying the c.

00:18:34.500 --> 00:18:41.150
Well, actually I can do it
all in -- well step two,

00:18:41.150 --> 00:18:43.930
that's the easiest step,
I should be able to do it.

00:18:43.930 --> 00:18:52.150
c is changed to
lambda inverse c,

00:18:52.150 --> 00:18:56.030
so that c becomes
lambda inverse c, OK.

00:18:56.030 --> 00:19:01.600
And then step three
uses lambda inverse c

00:19:01.600 --> 00:19:11.190
to construct U. So step
three is: the answer

00:19:11.190 --> 00:19:14.540
U, what do I have here?

00:19:14.540 --> 00:19:16.740
I've got a combination
of these vectors,

00:19:16.740 --> 00:19:20.580
so they're the columns of S,
and what are they multiplied by?

00:19:20.580 --> 00:19:22.980
They're multiplied by
the c's over lambdas,

00:19:22.980 --> 00:19:24.610
which is what I have here.

00:19:24.610 --> 00:19:33.650
That's S lambda inverse c.

00:19:33.650 --> 00:19:36.840
Those are the three steps.

00:19:36.840 --> 00:19:42.900
And what's the work involved?

00:19:42.900 --> 00:19:47.410
Here, the work is solving a
linear system with the matrix

00:19:47.410 --> 00:19:53.670
S. Here, the work is
taking a combination

00:19:53.670 --> 00:19:59.650
of the columns of s, doing
a matrix multiplication.

00:19:59.650 --> 00:20:06.820
Those two steps are usually
full-scale matrix operations,

00:20:06.820 --> 00:20:10.610
and of course, the S --
if I just complete this,

00:20:10.610 --> 00:20:12.220
I'll see that I get
the right thing,

00:20:12.220 --> 00:20:22.910
that's S lambda inverse
and c is S inverse F.

00:20:22.910 --> 00:20:24.360
There's the answer.

00:20:24.360 --> 00:20:31.200
U is S lambda inverse -- that's
a lambda inverse there --

00:20:31.200 --> 00:20:37.091
S inverse F. That's the correct
answer in matrix language.

00:20:37.091 --> 00:20:37.590
Right.

00:20:40.390 --> 00:20:43.850
This is K inverse,
that's K inverse.

00:20:43.850 --> 00:20:54.570
K is S*lambda S inverse, and
if I take the inverse of that,

00:20:54.570 --> 00:21:03.830
I get S lambda inverse S
inverse to multiply F. Well,

00:21:03.830 --> 00:21:07.230
I doubt if you're much
impressed by this lower board,

00:21:07.230 --> 00:21:13.800
because the upper board was
the same thing written out.

00:21:13.800 --> 00:21:21.390
It took some indication of
what the separate pieces were,

00:21:21.390 --> 00:21:23.420
but it's pretty clear.

00:21:23.420 --> 00:21:33.910
OK, now the million dollar
question is, is it fast?

00:21:33.910 --> 00:21:37.930
And the answer is,
almost certainly no.

00:21:37.930 --> 00:21:45.690
But for the particular matrix
S, which by good fortune,

00:21:45.690 --> 00:21:50.720
S could also stand
for sine, this matrix

00:21:50.720 --> 00:21:57.130
of eigenvectors for this
particular problem are sines.

00:21:57.130 --> 00:22:00.230
These are the discrete sines,
so this is the discrete sine

00:22:00.230 --> 00:22:01.600
transform.

00:22:01.600 --> 00:22:05.630
That's we're doing, we're doing
the discrete sine transform,

00:22:05.630 --> 00:22:08.430
because those discrete
sine vectors are

00:22:08.430 --> 00:22:17.590
the eigenvectors of K. OK, now
let me say what that means.

00:22:17.590 --> 00:22:21.180
First I'm thinking of K in 1D.

00:22:21.180 --> 00:22:28.960
So this is my 2's and
minus 1's and minus 1's.

00:22:28.960 --> 00:22:31.540
Its eigenvectors
are discrete sines,

00:22:31.540 --> 00:22:38.530
if I multiply that
by sine k*h -- well,

00:22:38.530 --> 00:22:44.070
let me just take the first
one, sine h, sine 2h,

00:22:44.070 --> 00:22:52.640
sine n minus 1 h, that will
turn out to be an eigenvector.

00:22:56.620 --> 00:23:02.630
So this is K*y, K*y_1,
the first eigenvector.

00:23:02.630 --> 00:23:05.270
The eigenvectors are
-- let me draw them.

00:23:05.270 --> 00:23:13.960
The eigenvectors for that second
difference matrix K are --

00:23:13.960 --> 00:23:18.620
here's the interval, 0 to 1,
I chop it up in steps of h,

00:23:18.620 --> 00:23:26.700
and I plot the sine, which
starts at zero and ends

00:23:26.700 --> 00:23:28.960
at zero, because those are
the boundary conditions,

00:23:28.960 --> 00:23:34.500
and here is sine h, sine 2h,
sine 3h, sine 4h, sine 5h,

00:23:34.500 --> 00:23:39.880
so for the five by five case --
maybe I should just be using n

00:23:39.880 --> 00:23:45.890
here, or maybe I should
even be using capital N,

00:23:45.890 --> 00:23:49.220
so capital N is 5
in this example.

00:23:53.770 --> 00:23:56.250
Good.

00:23:56.250 --> 00:23:59.670
What's on the other side
of that equals sign?

00:23:59.670 --> 00:24:06.940
Some eigenvalue times the
same vector, sine h, sine 2h,

00:24:06.940 --> 00:24:11.890
down to sine N*h, and
that eigenvalue -- oh,

00:24:11.890 --> 00:24:16.230
let me just write
lambda 1 for it.

00:24:16.230 --> 00:24:18.270
We know what it is.

00:24:18.270 --> 00:24:24.900
The fact that this is an
eigenvector is just trig;

00:24:24.900 --> 00:24:29.670
you know, I multiply minus 1
of that plus 2 of that minus 1

00:24:29.670 --> 00:24:35.050
of sine 3h, and I use
a little trig identity.

00:24:35.050 --> 00:24:38.280
So minus that, 2 of
that, minus that,

00:24:38.280 --> 00:24:41.250
turns out that to be
a multiple of sine 2h.

00:24:41.250 --> 00:24:47.770
Well, the 2 sine 2h give us a
2, and then the minus sine h

00:24:47.770 --> 00:24:54.650
and the minus sine 3h
combine into sine 2h times,

00:24:54.650 --> 00:25:00.210
I think, it's a cosine
of h or something,

00:25:00.210 --> 00:25:07.290
it's that eigenvector
that's near zero,

00:25:07.290 --> 00:25:10.150
because the cosine
of h is near 1.

00:25:10.150 --> 00:25:11.320
Does that look familiar?

00:25:11.320 --> 00:25:16.610
That a combination
of sine h and sine 3h

00:25:16.610 --> 00:25:21.760
should give us twice sine
2h times some cosine,

00:25:21.760 --> 00:25:28.320
yep, Elementary trig identity.

00:25:28.320 --> 00:25:31.000
OK, so those are eigenvectors.

00:25:31.000 --> 00:25:36.230
That's the first one, the
next one would have h times --

00:25:36.230 --> 00:25:42.260
the k-th one would have h times
k instead of just h itself,

00:25:42.260 --> 00:25:47.200
it would take jumps
of every k -- sine.

00:25:47.200 --> 00:25:52.560
and then a cosine h*k
would show up there,

00:25:52.560 --> 00:25:56.290
and this would still
be an eigenvector.

00:25:56.290 --> 00:25:59.240
OK.

00:25:59.240 --> 00:26:06.820
I'm making a little bit
explicit these vectors,

00:26:06.820 --> 00:26:10.570
but the main point
is they're sines,

00:26:10.570 --> 00:26:14.570
they're discrete sines
at equally spaced points.

00:26:14.570 --> 00:26:22.600
That's what the real version
of the FFT just lives on.

00:26:22.600 --> 00:26:25.020
And it would also live
on discrete cosines;

00:26:25.020 --> 00:26:26.710
if we had different
boundary conditions,

00:26:26.710 --> 00:26:28.280
we could do those, too.

00:26:28.280 --> 00:26:34.170
So this isn't the only -- these
zero boundary conditions are

00:26:34.170 --> 00:26:40.240
associated with the
name of Dirichlet,

00:26:40.240 --> 00:26:44.350
where zero slopes are associated
with the name of Neumann,

00:26:44.350 --> 00:26:49.830
and both -- this one gives
sines, Neumann gives cosines,

00:26:49.830 --> 00:26:52.160
the FFT deals with both.

00:26:52.160 --> 00:26:53.600
OK.

00:26:53.600 --> 00:27:00.840
So, that's the fast solution,
and it would take N squared --

00:27:00.840 --> 00:27:02.890
well I have to go to 2D.

00:27:02.890 --> 00:27:05.330
sorry, I guess I
have a little more

00:27:05.330 --> 00:27:12.020
to say because I have to get
from this one-dimensional

00:27:12.020 --> 00:27:15.510
second difference to the
five-point two-dimensional

00:27:15.510 --> 00:27:17.530
second difference,
and that's what's

00:27:17.530 --> 00:27:21.080
going to happen over here.

00:27:21.080 --> 00:27:28.400
I wrote up some stuff about
the Kronecker operation, which

00:27:28.400 --> 00:27:33.000
is the nifty way for
these special problems

00:27:33.000 --> 00:27:36.740
to go from 1D to 2D.

00:27:36.740 --> 00:27:42.880
You remember the deal,
that K2D, our 2D matrix,

00:27:42.880 --> 00:27:51.380
was this Kronecker product
of K and I, that gave us

00:27:51.380 --> 00:27:54.410
second differences
in one direction,

00:27:54.410 --> 00:27:59.040
and then we have to add in the
Kronecker product of I and K

00:27:59.040 --> 00:28:02.150
to get second differences
in the other direction.

00:28:02.150 --> 00:28:06.845
And then we better print --
because that matrix is going

00:28:06.845 --> 00:28:12.680
to be large, I don't
want to print it.

00:28:12.680 --> 00:28:13.660
Yeah.

00:28:13.660 --> 00:28:15.080
What's the point?

00:28:15.080 --> 00:28:19.970
The point is that if I
know the eigenvectors of k,

00:28:19.970 --> 00:28:24.130
then I can find the -- if
I know the 1D eigenvectors,

00:28:24.130 --> 00:28:27.390
I can find the 2D eigenvectors,
and you don't have to know

00:28:27.390 --> 00:28:29.380
Kronecker products to do that.

00:28:29.380 --> 00:28:32.780
All you have to do is just
make a sensible guess,

00:28:32.780 --> 00:28:42.170
so the eigenvectors in 2D, are
-- so they have a double index,

00:28:42.170 --> 00:28:57.430
k and l, and their components
are sines in one direction

00:28:57.430 --> 00:29:00.160
times sines in the
other direction.

00:29:00.160 --> 00:29:02.410
So what are those sines?

00:29:02.410 --> 00:29:06.810
There's a k*h, I guess, l*h.

00:29:14.080 --> 00:29:18.860
Those are the first components,
I guess I have to tell you what

00:29:18.860 --> 00:29:26.640
all the components are: k --
the seventh component in the x

00:29:26.640 --> 00:29:34.840
direction, there'd be a factor
7 -- so k*m*h sine l*n*h.

00:29:34.840 --> 00:29:41.810
This is the (m, n)
component of y_(k, l).

00:29:48.720 --> 00:29:50.680
It's just what we had in 1D.

00:29:50.680 --> 00:29:56.510
In 1D there was no l, the
components were just sine

00:29:56.510 --> 00:29:58.130
k*m*h.

00:29:58.130 --> 00:30:05.320
Now we've got two, one
in the x direction --

00:30:05.320 --> 00:30:13.400
These are the analogs of the
-- the continuous case would be

00:30:13.400 --> 00:30:21.120
sine k*pi*x times sine l*pi*y.

00:30:24.740 --> 00:30:29.380
Those are the eigenvectors
as eigenfunctions,

00:30:29.380 --> 00:30:31.000
functions of x and y.

00:30:31.000 --> 00:30:35.300
And the point is
that, once again,

00:30:35.300 --> 00:30:37.930
with these beautiful
matrices, I can

00:30:37.930 --> 00:30:43.120
sample these at the
equally spaced points,

00:30:43.120 --> 00:30:47.940
and I get discrete
sines that the FFT is

00:30:47.940 --> 00:30:49.620
ready to go with, OK.

00:30:54.670 --> 00:31:04.770
I'm giving this much detail
partly because the continuous

00:31:04.770 --> 00:31:09.730
case, of course, our operators
d second by the dx squared and d

00:31:09.730 --> 00:31:17.820
second by dy squared, and
the whole idea of separating

00:31:17.820 --> 00:31:24.700
variables, of looking
for solutions u --

00:31:24.700 --> 00:31:28.380
here is the eigenvalue problem,
the continuous eigenvalue

00:31:28.380 --> 00:31:28.930
problem.

00:31:28.930 --> 00:31:32.300
The Laplacian of u,
maybe I do a minus,

00:31:32.300 --> 00:31:35.670
the Laplacian of
u equal lambda*u,

00:31:35.670 --> 00:31:38.760
and that's a partial
differential equation,

00:31:38.760 --> 00:31:45.540
usually it's not easy to
solve, but if I'm on a square,

00:31:45.540 --> 00:31:48.700
and I have zero
boundary conditions,

00:31:48.700 --> 00:31:52.430
then I've solved it, by
separation of variables,

00:31:52.430 --> 00:31:55.600
a function of x times
a function of y.

00:31:55.600 --> 00:31:57.760
And that function of
x times function of y

00:31:57.760 --> 00:32:00.110
is exactly what
Kronecker product

00:32:00.110 --> 00:32:03.480
is doing for matrices, yep.

00:32:08.730 --> 00:32:15.960
I thought maybe this is good to
know when the problem is easy,

00:32:15.960 --> 00:32:23.750
and as I say, the possibility of
using the easy case on a square

00:32:23.750 --> 00:32:29.560
for preconditioning a not
so easy case is attractive.

00:32:29.560 --> 00:32:31.920
All right, so
that's what I wanted

00:32:31.920 --> 00:32:38.820
to say about number one,
that's the main suggestion,

00:32:38.820 --> 00:32:46.400
and again, the point was just to
take these three simple steps,

00:32:46.400 --> 00:32:51.730
provided we know and
like the eigenvectors.

00:32:51.730 --> 00:32:55.300
Here we know them and
we like them very much,

00:32:55.300 --> 00:32:57.580
because they're
those discrete sines.

00:32:57.580 --> 00:33:02.810
OK, now just to
finish comes, what's

00:33:02.810 --> 00:33:06.480
up with odd-even reduction.

00:33:06.480 --> 00:33:11.040
I'll use the same
1D problem first.

00:33:11.040 --> 00:33:14.440
It works great in --
that's not good English,

00:33:14.440 --> 00:33:20.680
but it works very well in
1D, odd-even reduction,

00:33:20.680 --> 00:33:23.980
you'll see it, you'll
see, oh boy, simple idea.

00:33:23.980 --> 00:33:27.430
But of course, don't forget
that ordinary elimination

00:33:27.430 --> 00:33:33.210
is a breeze with a tri-diagonal
matrix, so nothing I could do

00:33:33.210 --> 00:33:35.240
could be faster than that.

00:33:35.240 --> 00:33:37.300
But let's see what you can do.

00:33:37.300 --> 00:33:40.250
I just want to write
down the -- OK,

00:33:40.250 --> 00:33:48.150
keep your eye on this matrix,
so I'm going to write out

00:33:48.150 --> 00:33:49.350
the equations.

00:33:49.350 --> 00:33:59.080
So it'll be minus U_(i-2)
plus 2*U_(i-1) minus U_i,

00:33:59.080 --> 00:34:03.840
that would be F_(i-1); that
would be equation number i

00:34:03.840 --> 00:34:04.960
minus 1, right?

00:34:04.960 --> 00:34:09.900
With a minus 1, 2,
minus 1, centered there.

00:34:09.900 --> 00:34:17.200
And then the next one will be
a minus U_(i-1) plus 2*U_i --

00:34:17.200 --> 00:34:27.270
I better move this guy over
further -- minus U_(i+1),

00:34:27.270 --> 00:34:31.340
that will be F_i, right?

00:34:31.340 --> 00:34:34.180
That's equation number i,
and now I just want to look

00:34:34.180 --> 00:34:36.450
at equation number
-- the next equation,

00:34:36.450 --> 00:34:44.850
minus U_i plus 2*U_(i+1)
minus U_(i+2) is F_(i+1).

00:34:49.350 --> 00:34:55.390
So I've written down three
consecutive equations,

00:34:55.390 --> 00:35:04.240
three consecutive rows
from my simple matrix:

00:35:04.240 --> 00:35:09.420
a row, the next row,
and the next row.

00:35:09.420 --> 00:35:12.350
So the right-hand sides
are coming in order,

00:35:12.350 --> 00:35:17.520
and the diagonals are there,
and if I look at that,

00:35:17.520 --> 00:35:18.510
do I get any idea?

00:35:21.400 --> 00:35:29.030
Well, there is an idea here.

00:35:31.900 --> 00:35:39.330
I'd like to remove these
guys, the U_(i-1)'s and the

00:35:39.330 --> 00:35:43.710
U_(i+1)'s, so that's where
this word odd-even reduction is

00:35:43.710 --> 00:35:44.210
coming in.

00:35:44.210 --> 00:35:46.820
I'm going to reduce
this system by keeping

00:35:46.820 --> 00:35:53.750
only every second unknown
and eliminating those.

00:35:53.750 --> 00:35:55.480
How to do that?

00:35:55.480 --> 00:35:57.260
Well, you can see
how to eliminate.

00:35:57.260 --> 00:36:01.050
If I multiply this
equation by 2.

00:36:01.050 --> 00:36:03.090
If I multiply that
middle equation

00:36:03.090 --> 00:36:07.380
by 2, that becomes a 4,
this becomes a minus 2,

00:36:07.380 --> 00:36:10.120
this becomes a 2, and
now what shall I do?

00:36:12.950 --> 00:36:14.720
Add.

00:36:14.720 --> 00:36:20.510
If I add the equations
together, I get minus U_(i-2) --

00:36:20.510 --> 00:36:25.270
these cancel, that was the
point -- plus 4 minus a couple,

00:36:25.270 --> 00:36:30.550
so that's two u_i's,
minus this guy,

00:36:30.550 --> 00:36:44.200
u_(i-2) is that sum F_(i-1),
two F_i's, and F_(i+1).

00:36:47.270 --> 00:36:52.680
Well, sorry it's squeezed down
here, but this is the point.

00:36:52.680 --> 00:36:56.800
The main point is look at this.

00:36:56.800 --> 00:37:00.050
What do we have?

00:37:00.050 --> 00:37:06.880
We've got a typical
equation, but now we've

00:37:06.880 --> 00:37:09.720
removed half the unknowns.

00:37:09.720 --> 00:37:12.030
The problem is now half-sized.

00:37:12.030 --> 00:37:16.450
We've only got the
even-numbered unknowns

00:37:16.450 --> 00:37:24.590
at a small cost in updating
the right-hand side,

00:37:24.590 --> 00:37:27.280
and the problem's cut in half.

00:37:27.280 --> 00:37:30.250
So that's this
odd-even reduction,

00:37:30.250 --> 00:37:35.400
and it cuts a problem in
half, and everybody knows

00:37:35.400 --> 00:37:38.820
right away what to do next.

00:37:38.820 --> 00:37:43.670
The one mantra of computer
science, "Do it again."

00:37:43.670 --> 00:37:48.760
So that is the same
problem on the even,

00:37:48.760 --> 00:37:51.910
we do it again,
so I should really

00:37:51.910 --> 00:37:57.440
call it cyclic reduction, we
just cycle with this reduction,

00:37:57.440 --> 00:38:01.140
and in the end, we
have a 2 by 2 problem.

00:38:04.700 --> 00:38:10.200
That seems pretty smart,
pretty successful move,

00:38:10.200 --> 00:38:12.730
and I guess if we do
an operation count,

00:38:12.730 --> 00:38:14.970
well, I haven't
thought that through.

00:38:14.970 --> 00:38:18.530
What does the operation
count look like?

00:38:18.530 --> 00:38:21.860
It must be pretty quick, right?

00:38:21.860 --> 00:38:25.200
Well, undoubtedly we're
solving this linear system

00:38:25.200 --> 00:38:27.620
in O of N steps.

00:38:27.620 --> 00:38:32.330
Well, no big surprise to be able
to deal with that matrix in O

00:38:32.330 --> 00:38:37.740
of N steps, because elimination
would take O of N steps,

00:38:37.740 --> 00:38:44.390
it's size N, but it's bandwidth
is 1, so 2N or something steps

00:38:44.390 --> 00:38:46.670
would do it, and
maybe, I don't know

00:38:46.670 --> 00:38:50.090
how many steps we have here.

00:38:50.090 --> 00:38:54.020
I guess, when we cut it
in half, that required

00:38:54.020 --> 00:38:56.370
us to do that much.

00:38:56.370 --> 00:39:03.920
It's order N, it's order N.

00:39:03.920 --> 00:39:10.580
So the key question
is, can we do it 2D?

00:39:10.580 --> 00:39:13.790
Can we do the same thing in 2D?

00:39:13.790 --> 00:39:20.330
So I want to follow that
plan in two dimensions

00:39:20.330 --> 00:39:25.960
where now U will be a
whole row at a time,

00:39:25.960 --> 00:39:30.700
so I'm doing block 2D, block 2D.

00:39:30.700 --> 00:39:34.430
So, can I write down the
equations in block 2D,

00:39:34.430 --> 00:39:36.030
for whole rows?

00:39:36.030 --> 00:39:42.590
So U_i is the vector -- So
now this is the 2D problem,

00:39:42.590 --> 00:39:45.450
so it'll be minus
the identity --

00:39:45.450 --> 00:39:48.860
where instead of instead of 1,
I have to write the identity --

00:39:48.860 --> 00:39:56.800
U_(i-2) and 2K, right?

00:39:56.800 --> 00:40:03.650
Oh no, what is the middle
-- what's on the --

00:40:03.650 --> 00:40:08.040
so multiplying U_i, U_(i-1).

00:40:08.040 --> 00:40:09.540
I wanted to just
say the same thing,

00:40:09.540 --> 00:40:15.300
but I have to write down the --
this is going to be N squared

00:40:15.300 --> 00:40:19.720
equations N at a time,
a whole row at a time,

00:40:19.720 --> 00:40:23.050
and what's on the
diagonal of K2D?

00:40:23.050 --> 00:40:27.310
Not 2K.

00:40:27.310 --> 00:40:32.190
It's 2I, is it 2I plus K?

00:40:32.190 --> 00:40:32.800
Yeah.

00:40:32.800 --> 00:40:36.150
Yeah, K plus 2I.

00:40:36.150 --> 00:40:42.860
Isn't that what we have on
the diagonal of the K2D one?

00:40:42.860 --> 00:40:53.960
Times U_(i-1) minus
I*U_i is equal to some --

00:40:53.960 --> 00:40:55.670
that's a whole row at a time.

00:40:55.670 --> 00:41:02.320
These are all vectors with
N components now, right?

00:41:02.320 --> 00:41:06.500
The minus i, the 2I, and
the minus I are the second

00:41:06.500 --> 00:41:10.420
differences of one of
rows, they're difference is

00:41:10.420 --> 00:41:14.560
in the vertical direction,
and this K*U_i the second

00:41:14.560 --> 00:41:15.840
difference is along the row.

00:41:19.340 --> 00:41:23.160
OK, so same equation
at the next row.

00:41:23.160 --> 00:41:34.880
So the next row is minus
I*U_(i-1), K plus 2 U_i --

00:41:34.880 --> 00:41:37.020
because that's now
the diagonal block --

00:41:37.020 --> 00:41:42.760
minus I*U_(i+1) is F_(i+1).

00:41:42.760 --> 00:41:46.640
And it'll just take me a
second to write this one.

00:41:46.640 --> 00:41:57.690
This is K plus 2I U_(i+1),
minus I*U_(i+2) is F_(i+2).

00:41:57.690 --> 00:41:58.190
OK.

00:42:01.290 --> 00:42:08.980
Exactly the same, but now
a row at a time in 2D.

00:42:08.980 --> 00:42:11.620
So the same idea is
going to work, right?

00:42:11.620 --> 00:42:13.280
What do I do?

00:42:13.280 --> 00:42:20.990
I want to cancel this, so I
multiply that row by K plus 2I.

00:42:20.990 --> 00:42:23.060
Before I multiplied
it by 2, but now

00:42:23.060 --> 00:42:26.300
I have to multiply
it by K plus 2I.

00:42:26.300 --> 00:42:29.700
Times the row, the whole row.

00:42:32.350 --> 00:42:36.480
So when I do that this
cancels, so I have minus this,

00:42:36.480 --> 00:42:40.240
this guy wasn't affected.

00:42:40.240 --> 00:42:44.100
And this will
cancel, the K plus 2I

00:42:44.100 --> 00:42:46.460
will cancel this
one, just as before.

00:42:46.460 --> 00:42:53.640
This will not be
affected, U_(i+2),

00:42:53.640 --> 00:43:07.220
and I have F_i and K plus
2I F_(i+1) and F_(i+2).

00:43:10.830 --> 00:43:14.640
That should have been i minus
1, and this should have been i,

00:43:14.640 --> 00:43:16.220
and this should
have been i plus 1,

00:43:16.220 --> 00:43:24.630
sorry, mis-labeled the
F's, but no big deal.

00:43:24.630 --> 00:43:27.650
The point is the left
side, and the point

00:43:27.650 --> 00:43:30.160
is what's in that space.

00:43:32.860 --> 00:43:34.080
What goes in that space?

00:43:34.080 --> 00:43:41.140
Well, it's minus I, K
plus 2I, squared, minus I,

00:43:41.140 --> 00:43:49.670
so this is K plus 2I squared,
which before was so easy,

00:43:49.670 --> 00:43:53.850
and then the minus I, and
the minus I is the minus 2I,

00:43:53.850 --> 00:43:55.390
is multiplying the U_i.

00:43:55.390 --> 00:44:00.490
Yeah, that was minus I.

00:44:00.490 --> 00:44:09.640
OK, this is my matrix
from odd-even reduction.

00:44:09.640 --> 00:44:12.620
It's just natural
to try the idea,

00:44:12.620 --> 00:44:18.540
and the idea works,
but not so perfectly,

00:44:18.540 --> 00:44:26.330
because previously in 1D, that
just gave us the answer 2;

00:44:26.330 --> 00:44:28.180
it was 4 minus 2.

00:44:28.180 --> 00:44:34.150
But now in 2D, we have a
matrix, not surprising,

00:44:34.150 --> 00:44:36.460
but what we don't
like is the fact

00:44:36.460 --> 00:44:40.690
that the bandwidth is
growing. k was tri-diagonal,

00:44:40.690 --> 00:44:44.290
but when we square it, it
will have five diagonals,

00:44:44.290 --> 00:44:49.430
and when we repeat the odd-even
cycles, when we do it again,

00:44:49.430 --> 00:44:56.070
those five diagonals will be
nine diagonals, and onwards.

00:44:56.070 --> 00:45:04.380
So, I'm getting
half-sized problems.

00:45:04.380 --> 00:45:11.400
All the odd-numbered rows
in my square are eliminated,

00:45:11.400 --> 00:45:13.970
this just involves the
even-numbered rows,

00:45:13.970 --> 00:45:21.500
but the matrix is not
tri-diagonal anymore,

00:45:21.500 --> 00:45:24.640
it's growing in bandwidth.

00:45:24.640 --> 00:45:28.480
So, and then you have
to keep track of it,

00:45:28.480 --> 00:45:37.650
so the final conclusion is that
this is a pretty good idea,

00:45:37.650 --> 00:45:41.800
but it's not quite
as good as this one.

00:45:41.800 --> 00:45:45.330
It's not as good as
the FFT-based idea.

00:45:45.330 --> 00:45:50.230
Also, if you look to see
what is most efficient --

00:45:50.230 --> 00:45:52.540
see the eigenvectors
are still here,

00:45:52.540 --> 00:45:59.690
so I could do three steps of
this and then go to Fourier,

00:45:59.690 --> 00:46:03.360
and that probably
is about right.

00:46:03.360 --> 00:46:09.100
So, if you really wanted to
polish off a fast Poisson

00:46:09.100 --> 00:46:13.760
solver, you could do
maybe two steps or three

00:46:13.760 --> 00:46:18.390
of odd-even cyclic
reduction, but then

00:46:18.390 --> 00:46:27.010
your matrix is getting messy and
you switch to the fast Poisson

00:46:27.010 --> 00:46:27.510
solver.

00:46:27.510 --> 00:46:30.060
So it's not quite
Poisson anymore,

00:46:30.060 --> 00:46:32.970
because it's has
a messier matrix,

00:46:32.970 --> 00:46:38.440
but it still has the
same eigenvectors.

00:46:38.440 --> 00:46:41.880
As long as we stay
with the matrix K,

00:46:41.880 --> 00:46:48.410
we know its eigenvectors, and
we know that they're sines

00:46:48.410 --> 00:46:51.170
and that they're
quick to work with.

00:46:51.170 --> 00:46:52.300
OK.

00:46:52.300 --> 00:46:53.510
Anyway, there you go.

00:46:53.510 --> 00:47:00.780
That's a fast algorithm for
the lecture before the spring

00:47:00.780 --> 00:47:02.590
break.

00:47:02.590 --> 00:47:08.210
So after the break is, first
of all, discussion of projects.

00:47:08.210 --> 00:47:13.070
If your project could
include a page --

00:47:13.070 --> 00:47:19.240
and you could maybe email
the whole project to Mr. Cho.

00:47:19.240 --> 00:47:27.100
Maybe also, could you email
to me a sort of summary page

00:47:27.100 --> 00:47:31.040
that tells me what you did, so
I'll save the summary pages,

00:47:31.040 --> 00:47:37.540
and I'll have for Mr. Cho the
complete project with printout

00:47:37.540 --> 00:47:42.030
and graph, as far
as appropriate,

00:47:42.030 --> 00:47:44.490
and so we'll spend
some time on that,

00:47:44.490 --> 00:47:53.850
and then move to the big topic
of the rest of the course,

00:47:53.850 --> 00:48:00.450
which is solving optimization
problems, minimization,

00:48:00.450 --> 00:48:05.660
maximization in many variables.

00:48:05.660 --> 00:48:14.830
OK, so have a good spring
break and see you a week

00:48:14.830 --> 00:48:16.120
from Wednesday.

00:48:16.120 --> 00:48:17.370
Good.

