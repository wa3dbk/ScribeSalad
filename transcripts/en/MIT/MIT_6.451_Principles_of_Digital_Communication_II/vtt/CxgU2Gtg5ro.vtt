WEBVTT
Kind: captions
Language: en

00:00:01.620 --> 00:00:04.730
PROFESSOR: Today we finish
up chapter 13.

00:00:04.730 --> 00:00:07.700
I think we are finished, but
I'll take questions.

00:00:07.700 --> 00:00:12.520
We get into chapter 14, which
starts to get into coding for

00:00:12.520 --> 00:00:15.440
bandwidth-limited channels.

00:00:15.440 --> 00:00:18.910
Here we're coding directly in
terms of Euclidean space

00:00:18.910 --> 00:00:21.200
rather than in terms
of Hamming space.

00:00:21.200 --> 00:00:24.150
And there's this wonderful
quote from Neil Sloane.

00:00:24.150 --> 00:00:27.110
"Euclidean space coding is to
Hamming space coding as

00:00:27.110 --> 00:00:31.660
classical music is to
rock'n'roll." Meaning that, in

00:00:31.660 --> 00:00:34.740
the Euclidean space, things
are continuous, whereas we

00:00:34.740 --> 00:00:36.950
were discrete back
in Hamming space.

00:00:36.950 --> 00:00:42.110
Nonetheless, there are strong
connections between the two,

00:00:42.110 --> 00:00:46.420
which in two lectures I'll
basically only have a chance

00:00:46.420 --> 00:00:48.830
to hint at.

00:00:48.830 --> 00:00:52.860
You were supposed to hand in
problem set nine last Friday,

00:00:52.860 --> 00:00:56.090
or today if you haven't
already.

00:00:56.090 --> 00:00:59.220
Today, we'll hand out the
solutions for nine, and then

00:00:59.220 --> 00:01:02.060
chapter 14 and its problems,
which are problem set 10.

00:01:02.060 --> 00:01:04.610
These are not due.

00:01:04.610 --> 00:01:08.060
As always, I recommend
you do them.

00:01:08.060 --> 00:01:11.650
The solutions will be handed
out Wednesday.

00:01:11.650 --> 00:01:16.000
However, on the exam, the exam
will not cover chapter 14.

00:01:16.000 --> 00:01:21.040
So everything we do this
week is gravy.

00:01:21.040 --> 00:01:25.180
The exam is a week from tomorrow
in the morning.

00:01:25.180 --> 00:01:28.960
Ashish will administer it
because I won't be here.

00:01:28.960 --> 00:01:32.620
For this exam, it's closed book,
except you're allowed to

00:01:32.620 --> 00:01:36.120
make five pages of notes,
as you did three

00:01:36.120 --> 00:01:38.010
pages on the midterm.

00:01:38.010 --> 00:01:42.660
And I do recommend you bring a
calculator, because for one of

00:01:42.660 --> 00:01:46.490
the problems the calculator
will be helpful.

00:01:46.490 --> 00:01:49.900
And I was told that erasable
memory should be cleared on

00:01:49.900 --> 00:01:55.410
the calculator, so point of
honor if you do that.

00:01:55.410 --> 00:02:02.710
Any questions about the
exam or chapter 13?

00:02:02.710 --> 00:02:03.100
Yes?

00:02:03.100 --> 00:02:06.010
AUDIENCE: Are you including the
midterm [UNINTELLIGIBLE]?

00:02:06.010 --> 00:02:08.750
PROFESSOR: Yeah, oh, everything
in the course up to

00:02:08.750 --> 00:02:11.730
chapter 13.

00:02:11.730 --> 00:02:14.660
The rule is everything that
we covered in class.

00:02:14.660 --> 00:02:16.330
If we didn't cover it in
class, then you're not

00:02:16.330 --> 00:02:19.520
responsible for it.

00:02:19.520 --> 00:02:20.770
Other questions?

00:02:23.140 --> 00:02:27.020
Any last questions
on chapter 13?

00:02:27.020 --> 00:02:33.590
This is kind of the finale
for binary codes.

00:02:33.590 --> 00:02:37.990
We found that, at least on the
binary erasure channel, we had

00:02:37.990 --> 00:02:42.460
techniques that would allow us
to design LDPC codes that

00:02:42.460 --> 00:02:46.060
would get us as close to
capacity as we liked, or at

00:02:46.060 --> 00:02:48.930
least I made that assertion,
directed you to where you

00:02:48.930 --> 00:02:50.600
could find that done
in the literature.

00:02:50.600 --> 00:02:54.840
And I further asserted that, at
least for binary symmetric

00:02:54.840 --> 00:02:57.820
input channels, like the binary
additive white Gaussian

00:02:57.820 --> 00:03:03.320
noise channels, you could do
much the same thing using

00:03:03.320 --> 00:03:07.350
techniques like density
evolution or EXIT charts.

00:03:07.350 --> 00:03:10.280
Again, the issue is to design
code, such as this iterative

00:03:10.280 --> 00:03:14.850
decoding, after many, many, many
iterations, will, with

00:03:14.850 --> 00:03:19.180
probability one, converge to the
correct solution (or with

00:03:19.180 --> 00:03:22.090
probably one minus epsilon).

00:03:22.090 --> 00:03:28.920
So that's the great conclusion
to the story for binary codes,

00:03:28.920 --> 00:03:31.800
channels in which we want
to have binary inputs.

00:03:31.800 --> 00:03:36.290
And way back in the beginning,
we said that that was

00:03:36.290 --> 00:03:40.740
perfectly sufficient, really,
for the general additive white

00:03:40.740 --> 00:03:42.630
Gaussian channel.

00:03:42.630 --> 00:03:46.560
When we were talking about
binary code rates less than

00:03:46.560 --> 00:03:51.250
1/2, or nominal spectral
efficiencies less than one bit

00:03:51.250 --> 00:03:55.690
per second per Hertz, which
are equivalent statements.

00:03:55.690 --> 00:03:59.070
So what happens if we want to go
over channels where we want

00:03:59.070 --> 00:04:03.000
to send at higher spectral
efficiencies?

00:04:03.000 --> 00:04:07.630
We want to send four bits per
second per Hertz, as we

00:04:07.630 --> 00:04:11.230
certainly do over, say,
telephone line channels, which

00:04:11.230 --> 00:04:15.190
are 3,000, or 4,000-Hertz
channels, over which we want

00:04:15.190 --> 00:04:18.610
to send tens of thousands
of bits per second.

00:04:18.610 --> 00:04:22.780
Or many radio channels will
support high numbers of bits

00:04:22.780 --> 00:04:25.560
per second per Hertz,
and so forth.

00:04:25.560 --> 00:04:30.440
Well, obviously, you can't do
that with binary codes.

00:04:30.440 --> 00:04:38.330
So, today and Wednesday in this
chapter, we very quickly,

00:04:38.330 --> 00:04:42.390
of course, go through the series
of techniques that have

00:04:42.390 --> 00:04:44.590
been developed to go over
bandwidth-limited channels

00:04:44.590 --> 00:04:46.650
such as the bandwidth-limited
additive white

00:04:46.650 --> 00:04:48.430
Gaussian noise channel.

00:04:48.430 --> 00:04:51.032
Clearly, you're going to need
a non-binary constellation,

00:04:51.032 --> 00:04:53.820
and somehow code on that
constellation.

00:04:57.330 --> 00:05:01.140
But what you'll see is that both
the techniques and much

00:05:01.140 --> 00:05:05.880
of the development are
very reminiscent

00:05:05.880 --> 00:05:07.660
of the binary case.

00:05:07.660 --> 00:05:11.290
Historically, the binary case
was always done first, and

00:05:11.290 --> 00:05:15.820
then the generalizations
to non-binary followed.

00:05:15.820 --> 00:05:17.450
And so we'll see --

00:05:17.450 --> 00:05:20.630
you can start off with very
simple, hand-designed

00:05:20.630 --> 00:05:24.160
constellations, as we were
doing back in the early

00:05:24.160 --> 00:05:26.270
chapters, four and five.

00:05:26.270 --> 00:05:29.780
We played around with little m
equals eight constellations

00:05:29.780 --> 00:05:32.790
and tried to optimize them from
the point of view of,

00:05:32.790 --> 00:05:37.070
say, minimizing power for a
fixed minimum distance between

00:05:37.070 --> 00:05:41.740
points in a certain number of
dimensions in Euclidean space.

00:05:41.740 --> 00:05:44.000
Now, we want to go up to
considerably longer

00:05:44.000 --> 00:05:45.310
constellations.

00:05:45.310 --> 00:05:50.080
So that was kind of at the level
of Hamming codes and

00:05:50.080 --> 00:05:54.760
single parity-check codes and
very elementary binary coding

00:05:54.760 --> 00:05:57.590
techniques like that.

00:05:57.590 --> 00:06:00.620
So we want more complicated
codes, let's say moderate

00:06:00.620 --> 00:06:02.200
complexity codes.

00:06:02.200 --> 00:06:08.290
We will find things that are
analogous to block codes,

00:06:08.290 --> 00:06:11.650
linear block codes,
namely lattices.

00:06:11.650 --> 00:06:14.940
These are structures in
Euclidean space that have the

00:06:14.940 --> 00:06:19.340
group property, as linear
block codes do.

00:06:19.340 --> 00:06:22.960
And then we'll find trellis
codes, which are the Euclidean

00:06:22.960 --> 00:06:26.450
space analog of convolutional
codes.

00:06:26.450 --> 00:06:31.130
And basically, our goal in the
next two lectures is to get to

00:06:31.130 --> 00:06:34.890
the level of a union-bound
estimate analysis of the

00:06:34.890 --> 00:06:39.620
performance of these kinds of
coding techniques, which, like

00:06:39.620 --> 00:06:44.460
their binary analogs, sort of
sit in a moderate complexity,

00:06:44.460 --> 00:06:48.300
pretty good performance, get up
to 6 dB coding gain, can't

00:06:48.300 --> 00:06:50.540
get to capacity.

00:06:50.540 --> 00:06:54.560
So that's as far as we'll be
able to go in chapter 14.

00:06:54.560 --> 00:06:59.500
In the last page or two of
chapter 14, I briefly mention

00:06:59.500 --> 00:07:01.130
higher performance techniques.

00:07:01.130 --> 00:07:04.160
And it's generally accepted that
you can get as close to

00:07:04.160 --> 00:07:06.910
capacity on these
bandwidth-limited channels,

00:07:06.910 --> 00:07:09.430
additive white Gaussian noise
channels as you can on the

00:07:09.430 --> 00:07:13.030
binary channels, in some cases
just by adapting binary

00:07:13.030 --> 00:07:17.190
techniques in a kind of
force-fit way, and others that

00:07:17.190 --> 00:07:19.840
are somewhat more principled,
I might say.

00:07:19.840 --> 00:07:25.290
But in any case, by adapting the
binary techniques, you can

00:07:25.290 --> 00:07:29.330
get turbo codes, low-density
parity-check codes, capacity

00:07:29.330 --> 00:07:32.540
approaching codes of various
types for the

00:07:32.540 --> 00:07:34.670
bandwidth-limited channel.

00:07:34.670 --> 00:07:39.090
But I just hint how that can
be done in one page, and I

00:07:39.090 --> 00:07:42.610
can't include it
in the course.

00:07:42.610 --> 00:07:45.870
So that's where we're going.

00:07:45.870 --> 00:07:55.510
So basically, we're trying to
design a constellation set of

00:07:55.510 --> 00:07:56.760
signal points.

00:07:59.940 --> 00:08:05.000
Constellation c or signal
set or alphabet,

00:08:05.000 --> 00:08:07.950
script c, if you like.

00:08:07.950 --> 00:08:10.320
And we're doing the kind of
things we were doing back in

00:08:10.320 --> 00:08:11.680
chapter four and five.

00:08:14.600 --> 00:08:18.510
What are some of the things that
the parameters we have--

00:08:18.510 --> 00:08:21.990
we're going to do this in
n-dimensional space.

00:08:21.990 --> 00:08:26.020
We're going to be concerned
with the size of the

00:08:26.020 --> 00:08:29.920
constellation, the log of the
size, the number of log 2 of

00:08:29.920 --> 00:08:34.169
the size, the number of
bits we can send.

00:08:34.169 --> 00:08:38.010
We call this m at one point.

00:08:38.010 --> 00:08:42.010
That will determine the number
of bits we can send per n

00:08:42.010 --> 00:08:44.510
dimensions, or we can
normalize it per two

00:08:44.510 --> 00:08:45.210
dimensions.

00:08:45.210 --> 00:08:47.260
Typically, I said, in
bandwidth-limited regime,

00:08:47.260 --> 00:08:50.500
we'll normalize everything
per two dimensions.

00:08:50.500 --> 00:08:54.370
We're going to have an average
power of the constellation,

00:08:54.370 --> 00:08:56.840
which I'm just going
to write like that.

00:08:56.840 --> 00:09:01.040
We're going to have a minimum
squared distance between

00:09:01.040 --> 00:09:04.690
points in the constellation,
which I'm going

00:09:04.690 --> 00:09:06.580
to write like that.

00:09:06.580 --> 00:09:10.190
And the idea is to optimize
the trade-off

00:09:10.190 --> 00:09:12.210
between this and this.

00:09:12.210 --> 00:09:15.970
Clearly, if I take a given
constellation and I scale it

00:09:15.970 --> 00:09:18.960
up, I'm going to get better
minimum squared distance

00:09:18.960 --> 00:09:21.480
without the cost of
increasing power.

00:09:21.480 --> 00:09:25.380
So I either fix the minimum
squared distance between

00:09:25.380 --> 00:09:31.380
points and try to minimize the
power, the average energy over

00:09:31.380 --> 00:09:39.470
the centroids of a bunch of
spheres, or vice versa.

00:09:39.470 --> 00:09:44.370
So when we do this, this is kind
of the game, we get into

00:09:44.370 --> 00:09:46.880
a classical problem called
sphere-packing.

00:09:52.230 --> 00:09:57.250
If we say, hold the minimum
squared distance constant,

00:09:57.250 --> 00:10:02.660
that means we want to have a
certain minimum distance.

00:10:02.660 --> 00:10:03.910
Here I'll make it
d_min over 2.

00:10:06.180 --> 00:10:08.980
That means that there's going to
be a sphere of radius d_min

00:10:08.980 --> 00:10:14.170
over 2 around each point in this
space, which must not be

00:10:14.170 --> 00:10:17.580
violated by the sphere
of any other point.

00:10:17.580 --> 00:10:21.190
So basically, we're trying to
pack spheres as closely

00:10:21.190 --> 00:10:23.200
together as we can.

00:10:23.200 --> 00:10:25.070
And we're maybe given
the dimension.

00:10:25.070 --> 00:10:27.400
Here I have two dimensions
to play with.

00:10:27.400 --> 00:10:30.520
We may be given the number, m.

00:10:30.520 --> 00:10:33.760
In general now, we're going
to be thinking of m

00:10:33.760 --> 00:10:35.090
becoming very large.

00:10:35.090 --> 00:10:38.290
We want to go to arbitrarily
large number of bits per

00:10:38.290 --> 00:10:39.890
second per Hertz.

00:10:39.890 --> 00:10:43.080
So think of schemes that will
continue to work as we add

00:10:43.080 --> 00:10:46.390
more and more points.

00:10:46.390 --> 00:10:51.670
And if I do that in two
dimensions, we get

00:10:51.670 --> 00:10:52.750
penny-packing.

00:10:52.750 --> 00:10:56.340
Take a bunch of seven pennies
out of your pocket, and try to

00:10:56.340 --> 00:10:58.565
squeeze them together as
tightly as you can.

00:10:58.565 --> 00:11:01.110
That's effectively what
we're trying to do.

00:11:01.110 --> 00:11:05.800
They maintain their fixed
radius, and you get something

00:11:05.800 --> 00:11:08.930
that quickly starts to look like
what this is called, a

00:11:08.930 --> 00:11:13.910
hexagonal sphere-packing,
because the centers line up on

00:11:13.910 --> 00:11:15.270
a hexagonal grid.

00:11:18.870 --> 00:11:21.910
And in fact, I forget
exactly what we did.

00:11:21.910 --> 00:11:33.110
We did some of these back
in the early chapters.

00:11:33.110 --> 00:11:37.860
And let me draw a 16-point
constellation, which I

00:11:37.860 --> 00:11:41.440
remember we did draw.

00:11:41.440 --> 00:11:46.470
Here's a constellation
consisting of 16 pennies.

00:11:46.470 --> 00:11:49.100
And subject to my drawing
ability, it's packed as

00:11:49.100 --> 00:11:54.120
closely as possible for a fixed
minimum distance between

00:11:54.120 --> 00:11:55.510
the points.

00:11:55.510 --> 00:12:01.470
And as far as anyone knows, this
constellation, consisting

00:12:01.470 --> 00:12:06.350
a set of points on a hexagonal
grid, is the most dense

00:12:06.350 --> 00:12:10.930
packing of 16 points in two
dimensions, as measured by the

00:12:10.930 --> 00:12:13.860
average power of the centers.

00:12:13.860 --> 00:12:17.800
And it's not very symmetrical,
but you can see it's based on

00:12:17.800 --> 00:12:21.380
a symmetrical packing.

00:12:21.380 --> 00:12:26.540
This hexagonal, this is
basically a subset of 16

00:12:26.540 --> 00:12:30.690
points from the hexagonal
lattice.

00:12:30.690 --> 00:12:34.210
If we extend these points
in all directions--

00:12:34.210 --> 00:12:36.020
again, I haven't done
it very well--

00:12:36.020 --> 00:12:42.040
but the hexagonal lattice
is simply the

00:12:42.040 --> 00:12:43.440
underlying lattice here.

00:12:43.440 --> 00:12:50.185
Let me just draw a set of points
that look like this.

00:12:53.786 --> 00:12:55.036
I'm going to fail again.

00:12:58.660 --> 00:13:00.560
Sorry, I'm better at rock'n'roll
than I am at

00:13:00.560 --> 00:13:03.470
classical music.

00:13:03.470 --> 00:13:07.970
Anyway, this is the lattice,
and it's called hexagonal

00:13:07.970 --> 00:13:10.960
because, if we draw the decision
regions for this

00:13:10.960 --> 00:13:16.610
lattice, if we consider this
as a set of points in a

00:13:16.610 --> 00:13:20.150
constellation and draw the
decision regions, it looks

00:13:20.150 --> 00:13:28.080
like that, this little hexagon,
defined by the six

00:13:28.080 --> 00:13:28.940
nearest neighbors.

00:13:28.940 --> 00:13:32.630
They define six sides
of a hexagon.

00:13:32.630 --> 00:13:38.120
And so each lattice point has a
little region of space that

00:13:38.120 --> 00:13:40.735
belongs to it, you can think
of as its decision region.

00:13:44.330 --> 00:13:47.940
Now, what makes this
a lattice?

00:13:47.940 --> 00:13:49.340
What's the definition
of a lattice?

00:13:52.200 --> 00:13:53.560
We'll say an n-dimensional
lattice.

00:13:57.010 --> 00:14:00.520
Very simple and elegant
definition for a lattice.

00:14:00.520 --> 00:14:03.700
It's a discrete subset.

00:14:03.700 --> 00:14:11.570
That means a set of discrete
points of Rn, sitting in

00:14:11.570 --> 00:14:15.925
Euclidean n-space, that has
the group property.

00:14:29.130 --> 00:14:31.850
And you now all remember
what that means.

00:14:31.850 --> 00:14:39.110
It means the group property,
under ordinary addition of

00:14:39.110 --> 00:14:45.330
vectors in Euclidean n-space,
addition or

00:14:45.330 --> 00:14:48.670
subtraction, of course.

00:14:48.670 --> 00:14:49.830
What are some of the
implications of

00:14:49.830 --> 00:14:51.060
having a group property?

00:14:51.060 --> 00:14:54.880
That means that the sum of any
two vectors is another vector

00:14:54.880 --> 00:14:57.470
in the lattice.

00:14:57.470 --> 00:14:59.470
Does the lattice have
to include the

00:14:59.470 --> 00:15:00.910
0, the origin point?

00:15:06.670 --> 00:15:07.980
Anybody?

00:15:07.980 --> 00:15:08.530
Oh, good.

00:15:08.530 --> 00:15:09.820
I'm going to ask this
on the final.

00:15:14.980 --> 00:15:15.270
Come on.

00:15:15.270 --> 00:15:16.260
Does it or doesn't it?

00:15:16.260 --> 00:15:17.800
AUDIENCE: It does.

00:15:17.800 --> 00:15:18.840
PROFESSOR: It does.

00:15:18.840 --> 00:15:19.580
Why?

00:15:19.580 --> 00:15:20.830
AUDIENCE: [INAUDIBLE].

00:15:25.900 --> 00:15:29.050
PROFESSOR: OK, that's
one good answer.

00:15:29.050 --> 00:15:32.050
It has to be closed under
addition and subtraction.

00:15:32.050 --> 00:15:35.766
Subtraction of the point from
itself is going to give you 0.

00:15:35.766 --> 00:15:38.770
A more mathematically trained
person would probably, say,

00:15:38.770 --> 00:15:41.560
well, it obviously has
to have an identity.

00:15:41.560 --> 00:15:43.180
Every group has an
identity element.

00:15:45.890 --> 00:15:49.330
And that means, if lambda is
in the lattice, does minus

00:15:49.330 --> 00:15:52.200
lambda have to be
in the lattice?

00:15:52.200 --> 00:15:54.520
Yeah, every element has
to have an additive

00:15:54.520 --> 00:15:58.850
inverse, and so forth.

00:15:58.850 --> 00:16:06.480
Well, so if I have a lattice
like this, that means that one

00:16:06.480 --> 00:16:10.150
of these points has to be the
origin, say this one.

00:16:15.740 --> 00:16:18.210
Now, of course, you can have a
translate of a lattice, and

00:16:18.210 --> 00:16:21.060
geometrically it has all the
same properties as the lattice

00:16:21.060 --> 00:16:25.180
itself, at any fixed t to
all the lattice points.

00:16:25.180 --> 00:16:27.730
In other words, make the
origin somewhere else.

00:16:27.730 --> 00:16:29.520
But that's not itself a group.

00:16:29.520 --> 00:16:30.800
That's the co-set of a group.

00:16:30.800 --> 00:16:33.680
It would translate as
a co-set operation.

00:16:33.680 --> 00:16:42.100
So lambda must be a group, and
lambda plus t, any lambda

00:16:42.100 --> 00:16:49.320
translate is a co-set
of lambda.

00:16:49.320 --> 00:16:51.880
So most of the things you say
about lambda are also true for

00:16:51.880 --> 00:16:55.330
lambda plus t, but in order to
fix a group, we need the

00:16:55.330 --> 00:16:58.400
origin to be part of
the points set.

00:16:58.400 --> 00:17:00.340
OK, if it's a group, what
does that mean?

00:17:00.340 --> 00:17:03.420
That means, if we take any
particular point, say this

00:17:03.420 --> 00:17:14.159
one, regard it as a vector,
then call this lambda_1 --

00:17:14.159 --> 00:17:18.000
or let me it g1, more
suggestively.

00:17:18.000 --> 00:17:23.310
Does g1 plus g1 have
to be in the group?

00:17:23.310 --> 00:17:24.119
Yes.

00:17:24.119 --> 00:17:26.760
So that forces this point
to be in there,

00:17:26.760 --> 00:17:31.310
2g1, 3g1, so forth.

00:17:31.310 --> 00:17:37.860
All of these have to be in
there, as well as minus g1,

00:17:37.860 --> 00:17:41.450
minus 2g1, and so forth.

00:17:41.450 --> 00:17:44.930
So basically, once we found a
generator, all integers times

00:17:44.930 --> 00:17:47.650
that generator have to
be in the lattice.

00:17:47.650 --> 00:17:52.170
So that means there's a
cross-section along the axis

00:17:52.170 --> 00:17:56.460
defined by g1, such that these
points are simply the

00:17:56.460 --> 00:18:02.050
integers, scaled by whatever
the norm of g1 is.

00:18:06.730 --> 00:18:11.770
And of course, that holds for
any of these neighbors here.

00:18:11.770 --> 00:18:14.190
You want to look for nearest
neighbors of 0's.

00:18:14.190 --> 00:18:21.790
Take g2 and then, of course, all
of its integer multiples

00:18:21.790 --> 00:18:22.940
have to be in the lattice.

00:18:22.940 --> 00:18:33.800
So we've got a set here that, in
some sense, spans n-space.

00:18:33.800 --> 00:18:36.290
We have now two vectors,
g1, g2, which are

00:18:36.290 --> 00:18:37.570
not linearly dependent.

00:18:37.570 --> 00:18:39.220
They're sitting in 2-space.

00:18:39.220 --> 00:18:43.080
So if we take all real linear
combinations of these two

00:18:43.080 --> 00:18:46.490
vectors, we're going to
get all of 2-space.

00:18:46.490 --> 00:18:49.460
If we take all integer linear
combinations of these two

00:18:49.460 --> 00:18:53.640
vectors, they all must be
in the lattice, right?

00:18:53.640 --> 00:18:55.860
By the group property?

00:18:55.860 --> 00:18:58.420
We've already proved that all
the integer multiples of

00:18:58.420 --> 00:19:01.000
either of them is in the
lattice, so any thing plus or

00:19:01.000 --> 00:19:03.010
minus those two.

00:19:03.010 --> 00:19:11.750
So certainly, the lattice
includes the set of all

00:19:11.750 --> 00:19:19.410
integer multiples of a_i times
gi, where i equals 1 to 2,

00:19:19.410 --> 00:19:25.410
where a1, a2 is in the
two-dimensional integer,

00:19:25.410 --> 00:19:26.660
lattice Z-squared.

00:19:32.590 --> 00:19:33.840
Could there be any
other points?

00:19:38.720 --> 00:19:41.500
There actually could, if you
made a mistake and picked your

00:19:41.500 --> 00:19:45.810
initial vector to be 2g1,
because then you wouldn't have

00:19:45.810 --> 00:19:47.290
taken account of g1.

00:19:47.290 --> 00:19:51.150
So you've got to take your two
initial vectors to be two of

00:19:51.150 --> 00:19:53.380
the minimum norm points
in the lattice.

00:19:53.380 --> 00:19:58.480
But if you do that, then you can
easily prove that this is

00:19:58.480 --> 00:19:59.680
all of the lattice points.

00:19:59.680 --> 00:20:07.545
So in fact, a two-dimensional
lattice is equal to a set of

00:20:07.545 --> 00:20:09.660
all integer linear combinations
of two

00:20:09.660 --> 00:20:12.630
generators, which we can write
as a little two-by-two

00:20:12.630 --> 00:20:18.270
generator matrix, G. Or very
briefly, we could write lambda

00:20:18.270 --> 00:20:22.260
as G times Z-squared, and we
can view it as just some

00:20:22.260 --> 00:20:26.040
linear transformation
of Z-squared.

00:20:26.040 --> 00:20:30.660
So in this hexagonal lattice, we
can view it as starting out

00:20:30.660 --> 00:20:32.820
with a square grid.

00:20:32.820 --> 00:20:37.090
Of course, Z-squared itself
is a lattice.

00:20:37.090 --> 00:20:39.840
Z-squared is a group.

00:20:39.840 --> 00:20:43.320
This is just the square lattice
in two dimensions.

00:20:43.320 --> 00:20:48.350
We have Z_n as a lattice in
n dimensions, in general.

00:20:48.350 --> 00:20:53.300
So I take Z-squared, Z_n, more
generally, and I distort it.

00:20:53.300 --> 00:20:59.620
I just run it through some
g, which is a linear

00:20:59.620 --> 00:21:06.130
transformation, and slants some
of the generator points.

00:21:06.130 --> 00:21:08.270
And that's a general way
of getting a lattice.

00:21:08.270 --> 00:21:10.330
That's not the way we
usually visualize

00:21:10.330 --> 00:21:11.400
the hexagonal lattice.

00:21:11.400 --> 00:21:14.660
We usually visualize it in such
a way as to recognize

00:21:14.660 --> 00:21:18.000
that it has six-fold symmetry.

00:21:18.000 --> 00:21:21.220
But that's certainly
a way to get it.

00:21:24.740 --> 00:21:28.426
And this has one interesting
consequence.

00:21:28.426 --> 00:21:31.530
One of the measures we want for
a lattice is the volume

00:21:31.530 --> 00:21:34.500
per lattice point.

00:21:34.500 --> 00:21:41.210
In here, the volume, you can
divide this up into volumes

00:21:41.210 --> 00:21:43.770
that are all of equal size,
and we call that

00:21:43.770 --> 00:21:45.180
the volume of A2.

00:21:45.180 --> 00:21:46.280
This lattice --

00:21:46.280 --> 00:21:50.450
hexagonal lattice, is called
A2, and it's the volume of

00:21:50.450 --> 00:21:53.200
that hexagon.

00:21:53.200 --> 00:21:57.410
1 over the volume of that
hexagon is the density of

00:21:57.410 --> 00:21:59.600
points in 2-space,
if you like.

00:21:59.600 --> 00:22:03.490
So density is 1/volume.

00:22:03.490 --> 00:22:06.930
I mean, think of every point as
having a unique volume, and

00:22:06.930 --> 00:22:12.270
if we basically consider the
cells around each lattice

00:22:12.270 --> 00:22:17.070
point, they fill up 2-space,
or the perfect tessellation

00:22:17.070 --> 00:22:19.920
tiling of 2-space.

00:22:19.920 --> 00:22:22.360
These hexagons will.

00:22:22.360 --> 00:22:25.310
One easy consequence of the
group property is that all the

00:22:25.310 --> 00:22:29.190
decision regions have to be
congruent, in fact, just

00:22:29.190 --> 00:22:32.170
translates of one another
by lattice points.

00:22:32.170 --> 00:22:35.920
The decision region around this
point is the same as the

00:22:35.920 --> 00:22:39.510
decision region about 0,
translated by a lattice point,

00:22:39.510 --> 00:22:41.360
namely the center of that
decision region.

00:22:41.360 --> 00:22:45.950
I won't prove that but it's
true and easy to prove.

00:22:45.950 --> 00:22:48.435
So that's one way of finding
what's the volume.

00:22:48.435 --> 00:22:50.080
The volume will mean
the volume of

00:22:50.080 --> 00:22:51.590
2-space per lattice point.

00:22:51.590 --> 00:22:53.700
But what's another
way to do that?

00:22:53.700 --> 00:22:57.550
Another way is just to take
this little parallelotope,

00:22:57.550 --> 00:23:03.410
whose volume is easier
to calculate.

00:23:03.410 --> 00:23:07.810
And we can see that we can
equally well tile 2-space with

00:23:07.810 --> 00:23:11.550
these little parallelograms,
just anchor --

00:23:11.550 --> 00:23:14.280
in this case, this is the one
that's anchored in the lower

00:23:14.280 --> 00:23:16.830
left corner by the origin.

00:23:16.830 --> 00:23:19.120
Then we take another one that's
anchored by this point.

00:23:19.120 --> 00:23:21.830
We take another one that's
anchored by this point,

00:23:21.830 --> 00:23:23.760
another one that's anchored
by this point.

00:23:23.760 --> 00:23:28.590
And it's clear that's another
tiling of 2-space by regions,

00:23:28.590 --> 00:23:31.880
one for each lattice point that
completely fills 2-space

00:23:31.880 --> 00:23:36.570
with overlaps only on the
boundary, which don't count.

00:23:36.570 --> 00:23:40.140
So the volume of A2 is also
the volume of that, what's

00:23:40.140 --> 00:23:42.780
called a fundamental
parallelotope.

00:23:42.780 --> 00:23:46.690
And what's the volume of that
fundamental parallelotope?

00:23:46.690 --> 00:23:52.290
Well, one way of calculating
it is to take

00:23:52.290 --> 00:23:55.420
the volume of Z-squared.

00:23:55.420 --> 00:23:59.840
We can view this as the
distortion of Z-squared, which

00:23:59.840 --> 00:24:01.090
looks like this.

00:24:03.690 --> 00:24:07.490
So if this is Z-squared, what's
the fundamental volume

00:24:07.490 --> 00:24:09.320
of Z-squared?

00:24:09.320 --> 00:24:11.640
The volume of Z-squared
per lattice point.

00:24:11.640 --> 00:24:19.150
This is (0,1,-1,1,
so forth,1,1).

00:24:19.150 --> 00:24:21.520
What's the volume of
Z-squared per--

00:24:21.520 --> 00:24:22.720
AUDIENCE: 1

00:24:22.720 --> 00:24:23.370
PROFESSOR: --1.

00:24:23.370 --> 00:24:25.176
Clearly.

00:24:25.176 --> 00:24:26.850
And we have this decision
region.

00:24:26.850 --> 00:24:30.300
This is little square side 1, or
this parallelotope is also

00:24:30.300 --> 00:24:31.930
little square side 1.

00:24:31.930 --> 00:24:33.190
It's volume is 1.

00:24:36.220 --> 00:24:40.070
General geometric principle, if
we take this and distort it

00:24:40.070 --> 00:24:47.570
by G, then what's the volume
of, say, this parallelotope

00:24:47.570 --> 00:24:48.610
going to be?

00:24:48.610 --> 00:24:54.080
This parallelotope goes by G
into this one, and its volume

00:24:54.080 --> 00:24:59.010
is the Jacobian, which is
the determinant of G.

00:24:59.010 --> 00:25:04.950
So the volume of a lattice
is the determinant of any

00:25:04.950 --> 00:25:06.730
generator matrix for
that lattice.

00:25:06.730 --> 00:25:13.330
Just a nice little side fact,
but it's an example of the

00:25:13.330 --> 00:25:15.890
kind of things you get in
Euclidean space that you don't

00:25:15.890 --> 00:25:18.980
get in Hamming space.

00:25:18.980 --> 00:25:23.110
Of course, there are various
generator matrices that you

00:25:23.110 --> 00:25:24.900
could use to generate
this lattice.

00:25:24.900 --> 00:25:29.340
But whichever one you choose,
this is going to be true.

00:25:29.340 --> 00:25:32.200
So this is an invariant among
all the generating matrices.

00:25:34.880 --> 00:25:37.504
So that's a lattice.

00:25:37.504 --> 00:25:41.490
I haven't mentioned what's the
most important thing for us

00:25:41.490 --> 00:25:42.065
about a lattice.

00:25:42.065 --> 00:25:45.610
A lattice is a group.

00:25:45.610 --> 00:25:57.340
What's the fundamental property
of a group, which I

00:25:57.340 --> 00:26:00.580
guess was embodied in
the alternate set of

00:26:00.580 --> 00:26:03.620
axioms for the group?

00:26:03.620 --> 00:26:09.480
If I take the group plus
any element of the

00:26:09.480 --> 00:26:13.300
group, what do I get?

00:26:13.300 --> 00:26:14.510
AUDIENCE: [INAUDIBLE].

00:26:14.510 --> 00:26:15.760
PROFESSOR: The group again.

00:26:19.000 --> 00:26:20.870
Let's interpret that
geometrically.

00:26:20.870 --> 00:26:22.690
One of the nice things about
this subject is you're

00:26:22.690 --> 00:26:24.010
constantly going back and forth

00:26:24.010 --> 00:26:27.100
between algebra and geometry.

00:26:27.100 --> 00:26:28.600
Geometrically, what
does that mean?

00:26:28.600 --> 00:26:32.100
That means, if we take this
lattice, say the origin is

00:26:32.100 --> 00:26:39.960
here, and we shift it, say, so
that the origin is up here,

00:26:39.960 --> 00:26:41.350
then nothing changes.

00:26:41.350 --> 00:26:45.280
Everything is invariant
to this shift.

00:26:45.280 --> 00:26:48.260
We can do this for any
lattice point.

00:26:48.260 --> 00:26:50.580
That shows you that
all the decision

00:26:50.580 --> 00:26:52.990
regions have to be congruent.

00:26:52.990 --> 00:27:04.000
It shows you that the number
of nearest neighbors, the

00:27:04.000 --> 00:27:09.840
minimum distance from any point
to its nearest neighbors

00:27:09.840 --> 00:27:12.470
is always the same for
any lattice point.

00:27:12.470 --> 00:27:14.240
Basically --

00:27:14.240 --> 00:27:18.030
it's a lattice is geometrically
uniform.

00:27:18.030 --> 00:27:21.720
That means you can stand on
any of the points and the

00:27:21.720 --> 00:27:24.630
world around you looks the same,
as if you stood on any

00:27:24.630 --> 00:27:26.310
of the other points.

00:27:26.310 --> 00:27:30.160
Think of these as stars in the
universe, and somebody

00:27:30.160 --> 00:27:33.770
blindfolds you and drops you
on one of these points.

00:27:33.770 --> 00:27:35.510
Can you tell which one of
those points you've been

00:27:35.510 --> 00:27:40.330
dropped on if nobody's written
anything on these points?

00:27:40.330 --> 00:27:41.270
No, you can't tell.

00:27:41.270 --> 00:27:44.350
The world looks the same from
whichever one you drop on.

00:27:44.350 --> 00:27:47.870
So that means the minimum square
distance from any point

00:27:47.870 --> 00:27:49.150
is the same as every other.

00:27:49.150 --> 00:27:53.040
So the number of nearest
neighbors at minimum square

00:27:53.040 --> 00:27:54.640
distance is the same.

00:27:54.640 --> 00:27:57.390
It means the total distance
profile, starting from any

00:27:57.390 --> 00:27:58.950
point, is the same.

00:27:58.950 --> 00:28:02.230
It means if we were to take one
of these points and send

00:28:02.230 --> 00:28:06.390
it over a Gaussian channel from
a very large set, and we

00:28:06.390 --> 00:28:09.250
take one of the points from the
interior and send it over

00:28:09.250 --> 00:28:12.570
an additive white Gaussian noise
channel, the probability

00:28:12.570 --> 00:28:15.030
of error is going to be the
same, regardless of which

00:28:15.030 --> 00:28:17.610
point we send it from, because
the probability of error is

00:28:17.610 --> 00:28:22.050
the probability of falling
outside your decision region.

00:28:22.050 --> 00:28:25.260
And since the decision region
always has the same shape,

00:28:25.260 --> 00:28:28.410
it's invariant to which
point you send.

00:28:28.410 --> 00:28:35.580
So this geometrical uniformity
property, which that's the

00:28:35.580 --> 00:28:38.200
general name for it and it
has all these specific

00:28:38.200 --> 00:28:45.780
consequences that I just listed,
is the principal

00:28:45.780 --> 00:28:49.140
geometrical property
of a lattice.

00:28:49.140 --> 00:28:51.650
This just follows from
its group property.

00:28:54.170 --> 00:28:57.680
You don't have to be a lattice
to have this property.

00:28:57.680 --> 00:29:01.350
For instance, the translate of
a lattice is not a group,

00:29:01.350 --> 00:29:04.700
technically, but it obviously
has the same geometrical

00:29:04.700 --> 00:29:06.360
uniformity property.

00:29:06.360 --> 00:29:09.850
And there are more elaborate
examples of things that look

00:29:09.850 --> 00:29:11.730
less like lattices that
are nonetheless

00:29:11.730 --> 00:29:12.740
geometrically uniform.

00:29:12.740 --> 00:29:14.350
On the other hand, lattices
definitely are

00:29:14.350 --> 00:29:16.130
geometrically uniform.

00:29:16.130 --> 00:29:18.640
So this seems to be a good thing
to do, because what's

00:29:18.640 --> 00:29:19.290
our objective?

00:29:19.290 --> 00:29:22.900
We're trying to have
d_min-squared be the same for

00:29:22.900 --> 00:29:24.150
every point in our
constellation.

00:29:26.990 --> 00:29:30.420
We want to tack these pennies
as close together as we can.

00:29:30.420 --> 00:29:32.860
That means we want to hold
d_min-squared constant for

00:29:32.860 --> 00:29:36.080
every point that we're
going to use.

00:29:36.080 --> 00:29:39.650
And so lattices have
that property.

00:29:39.650 --> 00:29:43.430
So that seems like a good way
to do sphere-packing.

00:29:43.430 --> 00:29:48.800
And in fact, sphere-packing, in
n-dimensions, is a very old

00:29:48.800 --> 00:29:50.050
mathematical subject.

00:29:53.130 --> 00:29:55.650
How many spheres can you
pack around a three

00:29:55.650 --> 00:29:57.835
sphere, is it 12 or 13?

00:29:57.835 --> 00:30:01.400
That was debated in the 18th
and 19th centuries, and I

00:30:01.400 --> 00:30:04.910
think it's only finally just
been resolved a few years ago.

00:30:04.910 --> 00:30:09.780
This turns out to be not
an easy problem.

00:30:09.780 --> 00:30:12.920
The densest sphere-packing in
four dimensions was found in

00:30:12.920 --> 00:30:15.230
the mid-19th century.

00:30:15.230 --> 00:30:17.060
In eight dimensions,
I think around the

00:30:17.060 --> 00:30:19.080
turn of the 20th century.

00:30:19.080 --> 00:30:22.620
In 16 dimensions, it's the
Barnes-Wall lattice, which

00:30:22.620 --> 00:30:25.635
we'll see a little later, which
was found in '54, about

00:30:25.635 --> 00:30:29.620
the same time as Reed-Muller
codes, or maybe it was '59,

00:30:29.620 --> 00:30:33.150
maybe a little later than
Reed-Muller codes.

00:30:33.150 --> 00:30:37.600
The Leech lattice was certainly
found a little

00:30:37.600 --> 00:30:40.980
earlier than that, but around
the same time as the Golay

00:30:40.980 --> 00:30:43.320
code, to which it's a
very close cousin.

00:30:43.320 --> 00:30:43.920
And so forth.

00:30:43.920 --> 00:30:47.220
So you might think that these
questions had all been

00:30:47.220 --> 00:30:50.010
settled, but in fact they're
still open for most specific

00:30:50.010 --> 00:30:53.300
dimensions, n.

00:30:53.300 --> 00:30:56.680
Conway and Sloane have the
authoritative book on this,

00:30:56.680 --> 00:30:59.690
which basically lists what
we know about the densest

00:30:59.690 --> 00:31:02.600
sphere-packings in
all dimensions.

00:31:02.600 --> 00:31:06.930
So here's our idea.

00:31:06.930 --> 00:31:10.520
Our idea is we're going to go
consult the mathematicians and

00:31:10.520 --> 00:31:13.160
find out what the densest
sphere-packings that they

00:31:13.160 --> 00:31:15.280
found are in various
dimensions.

00:31:15.280 --> 00:31:17.910
In two dimensions, it's
the hexagonal lattice.

00:31:17.910 --> 00:31:21.300
In one dimension, it's the
integer, and basically the

00:31:21.300 --> 00:31:24.900
integers are the only packing
with regularity properties in

00:31:24.900 --> 00:31:25.690
one dimension.

00:31:25.690 --> 00:31:29.380
And two dimensions, you could
consider Z-squared, or you can

00:31:29.380 --> 00:31:31.260
consider A2.

00:31:31.260 --> 00:31:33.720
These are the two obvious
candidates, the square and the

00:31:33.720 --> 00:31:35.560
hexagonal packings.

00:31:35.560 --> 00:31:38.840
In three dimensions, you get
face-centered cubic packing.

00:31:38.840 --> 00:31:41.110
And what's the BCC?

00:31:41.110 --> 00:31:43.410
The base-centered cubic
packing, I guess.

00:31:43.410 --> 00:31:44.930
AUDIENCE: Body-centered.

00:31:44.930 --> 00:31:45.490
PROFESSOR: Say again.

00:31:45.490 --> 00:31:46.700
AUDIENCE: Body center.

00:31:46.700 --> 00:31:48.250
PROFESSOR: Body-centered,
thank you.

00:31:48.250 --> 00:31:50.510
I'm sure you're right.

00:31:50.510 --> 00:31:54.570
So anyway, there's some things
that are known about this from

00:31:54.570 --> 00:31:59.250
classical mathematics and
current mathematics.

00:31:59.250 --> 00:32:02.950
So our idea is we're going to
try to find the densest

00:32:02.950 --> 00:32:08.930
possible lattice in
n dimensions.

00:32:08.930 --> 00:32:12.970
But for digital communications,
for coding, we

00:32:12.970 --> 00:32:16.480
only want a finite number
of points from that

00:32:16.480 --> 00:32:18.970
constellation.

00:32:18.970 --> 00:32:22.240
So what we're going to do is
we're going to create our

00:32:22.240 --> 00:32:26.560
constellation based
on a lattice and

00:32:26.560 --> 00:32:30.130
on a region of n-space.

00:32:30.130 --> 00:32:31.860
And the idea is simple.

00:32:31.860 --> 00:32:35.060
We're just going to intersect
the lattice.

00:32:35.060 --> 00:32:40.710
Or maybe let me allow a
translate of a lattice with

00:32:40.710 --> 00:32:43.510
the region.

00:32:43.510 --> 00:32:48.580
And make this a finite,
compact region.

00:32:48.580 --> 00:32:52.750
And this will give us a certain
finite subset of the

00:32:52.750 --> 00:32:54.000
points in the lattice.

00:32:56.620 --> 00:32:58.980
Now, when I draw a constellation
like this one,

00:32:58.980 --> 00:33:03.620
of a size 16, that's
what I've done.

00:33:03.620 --> 00:33:05.540
I've basically taken A2.

00:33:05.540 --> 00:33:07.740
Or it's actually a
translate of A2.

00:33:07.740 --> 00:33:10.800
I think the origin is
in here somewhere.

00:33:10.800 --> 00:33:14.870
And I draw a sphere
around the origin.

00:33:14.870 --> 00:33:17.330
And I take the 16 points
that happen to

00:33:17.330 --> 00:33:18.560
fall within that sphere.

00:33:18.560 --> 00:33:21.470
And I call that my
constellation.

00:33:21.470 --> 00:33:23.430
So that's the idea in general.

00:33:23.430 --> 00:33:27.050
Think of the region, r,
as a cookie cutter.

00:33:27.050 --> 00:33:30.610
We're going to take the cookie
cutter and chop out a finite

00:33:30.610 --> 00:33:35.900
number of points from this nice,
regular, infinite grid.

00:33:35.900 --> 00:33:38.930
And that will give us
a constellation.

00:33:38.930 --> 00:33:42.060
So especially for large
constellations, we'll be able

00:33:42.060 --> 00:33:45.950
to make some approximations that
will allow us to analyze

00:33:45.950 --> 00:33:48.660
that kind of constellation
very easily.

00:33:48.660 --> 00:33:52.060
And actually, the best
constellations that we know

00:33:52.060 --> 00:33:53.310
are of this type.

00:33:55.610 --> 00:33:58.810
We've taken this baseline,
m-PAM.

00:33:58.810 --> 00:34:00.340
Well, what's m-PAM?

00:34:00.340 --> 00:34:08.790
We take the lattice plus t,
let's say to be Z plus 1/2.

00:34:12.199 --> 00:34:16.980
So that means we don't take the
origin, but we take the

00:34:16.980 --> 00:34:22.199
points 1/2 minus 1/2, 3/2.

00:34:22.199 --> 00:34:26.120
They're equally spaced points,
but we've shifted them so as

00:34:26.120 --> 00:34:28.790
to be symmetric about
the origin.

00:34:28.790 --> 00:34:32.460
5/2 minus minus, so forth.

00:34:32.460 --> 00:34:37.230
So this is Z plus 1/2
is our lattice.

00:34:37.230 --> 00:34:41.219
And then if we want 4-PAM,
we take a cookie cutter

00:34:41.219 --> 00:34:44.070
consisting of this region.

00:34:44.070 --> 00:34:48.900
So this is the region going
from minus 2 to 2.

00:34:48.900 --> 00:34:51.219
It's not unreasonable to expect
that, if we take a

00:34:51.219 --> 00:34:56.670
region of length 4, we're going
to get 4 points, if,

00:34:56.670 --> 00:34:59.940
again, it's symmetric around
the corner, because each of

00:34:59.940 --> 00:35:03.800
these points takes up about one
unit of space over here.

00:35:03.800 --> 00:35:08.650
So if we made it from minus 4 to
plus 4, we'd have a region

00:35:08.650 --> 00:35:10.950
of length 8, and we'd
get 8 points.

00:35:10.950 --> 00:35:13.510
And so forth.

00:35:13.510 --> 00:35:20.000
So that's how we do m-PAM, and
then scale it if we want to.

00:35:20.000 --> 00:35:21.610
But this is the basic idea.

00:35:21.610 --> 00:35:28.040
Similarly, for m by m QAM, like
everything else, it's

00:35:28.040 --> 00:35:30.420
just doing the same thing
independently in two

00:35:30.420 --> 00:35:31.890
dimensions.

00:35:31.890 --> 00:35:39.780
Here we take our lattice to be
Z plus 1/2 squared, which is

00:35:39.780 --> 00:35:47.120
just the offset Z-squared,
offset so as to be four-way,

00:35:47.120 --> 00:35:50.200
to be roughly symmetrical
to four-way

00:35:50.200 --> 00:35:53.620
rotations, 90-degree rotations.

00:35:53.620 --> 00:35:56.190
So here is Z-squared
plus (1/2, 1/2).

00:35:59.690 --> 00:36:05.930
And then, again, here to get
16-QAM, for instance, we might

00:36:05.930 --> 00:36:12.860
take the region to be what?

00:36:12.860 --> 00:36:20.280
(-2,2)-squared which,
again, has area 16.

00:36:20.280 --> 00:36:23.370
You might think of this as just
taking the little square

00:36:23.370 --> 00:36:24.890
around each of these.

00:36:24.890 --> 00:36:27.746
Each of these squares
has area 1.

00:36:27.746 --> 00:36:31.710
So when we take the
union of all those

00:36:31.710 --> 00:36:33.160
squares, we get the region.

00:36:33.160 --> 00:36:35.480
That's another way of
characterizing this region.

00:36:35.480 --> 00:36:39.530
It's just the union of all the
decision regions of the points

00:36:39.530 --> 00:36:40.210
that we want.

00:36:40.210 --> 00:36:43.020
If you had started from the
points, you could certainly

00:36:43.020 --> 00:36:45.200
invent a region that
is the union of

00:36:45.200 --> 00:36:46.430
those decision regions.

00:36:46.430 --> 00:36:52.640
It would have an area equal to
the number of points times the

00:36:52.640 --> 00:36:57.290
volume of 2-space
per each point.

00:36:57.290 --> 00:36:59.380
That's the kind of
approximation

00:36:59.380 --> 00:37:00.800
we're going to use.

00:37:00.800 --> 00:37:05.270
The bounding region is clearly
going to have something like

00:37:05.270 --> 00:37:08.240
the number of points in the
constellation, times the

00:37:08.240 --> 00:37:10.590
fundamental volume
as its volume.

00:37:16.720 --> 00:37:18.610
So this is our idea.

00:37:18.610 --> 00:37:22.660
This is what we're going to call
a lattice constellation.

00:37:22.660 --> 00:37:25.940
And we're going to investigate
the properties of lattice

00:37:25.940 --> 00:37:29.860
constellations, not the most
general thing we could think

00:37:29.860 --> 00:37:33.270
of but good enough.

00:37:33.270 --> 00:37:34.955
Seem to have the right
sort of property.

00:37:38.490 --> 00:37:41.250
And we're going to take it
at least as far as the

00:37:41.250 --> 00:37:45.090
union-bound estimate, so we
can get a good estimate of

00:37:45.090 --> 00:37:48.010
probability of error if
we take one of these

00:37:48.010 --> 00:37:51.660
constellations, like m-by-m QAM,
and use it to transmit

00:37:51.660 --> 00:37:56.690
over a Gaussian noise channel.

00:37:56.690 --> 00:38:03.760
And furthermore, for large
constellations, meaning as we

00:38:03.760 --> 00:38:06.790
get well up into the
bandwidth-limited regions of

00:38:06.790 --> 00:38:10.730
large nominal spectral
densities, we're going to be

00:38:10.730 --> 00:38:18.880
able to separate the analysis
into lattice properties and

00:38:18.880 --> 00:38:20.130
region properties.

00:38:23.955 --> 00:38:28.600
And we'll basically be able to
just add these up to get the

00:38:28.600 --> 00:38:37.980
overall analysis, by making some
judicious approximations

00:38:37.980 --> 00:38:41.170
that become exact in the large
constellation limit.

00:38:44.180 --> 00:38:50.970
So let's now restrict ourselves
to lattices, or

00:38:50.970 --> 00:38:56.840
translates of lattices, and
start to analyze them.

00:38:56.840 --> 00:39:07.630
What are their key parameters
for our application, which is

00:39:07.630 --> 00:39:09.770
digital communications?

00:39:09.770 --> 00:39:13.090
Well, one is the minimum squared
distance between

00:39:13.090 --> 00:39:15.050
lattice points.

00:39:15.050 --> 00:39:19.060
We've seen that doesn't depend
on the particular point.

00:39:19.060 --> 00:39:20.940
So this is the minimum
squared distance

00:39:20.940 --> 00:39:24.130
from any lattice point.

00:39:24.130 --> 00:39:25.680
The number of nearest
neighbors.

00:39:31.320 --> 00:39:38.745
The volume per lattice point.

00:39:42.130 --> 00:39:45.140
And is there anything else?

00:39:50.020 --> 00:39:51.790
It seems to me there's
a fourth one.

00:40:02.360 --> 00:40:04.010
No.

00:40:04.010 --> 00:40:05.610
I guess that's all
we need to know.

00:40:11.370 --> 00:40:15.290
So I want to combine these
into a key figure

00:40:15.290 --> 00:40:17.600
of merit for coding.

00:40:17.600 --> 00:40:29.510
Again, we did this back in
chapter four for a complete

00:40:29.510 --> 00:40:30.320
constellation.

00:40:30.320 --> 00:40:34.340
We found the figure of merit by
holding the average power

00:40:34.340 --> 00:40:38.620
fixed for a fixed number of
points, m, on a fixed

00:40:38.620 --> 00:40:42.780
dimension, n, and maximizing the
minimum square distance,

00:40:42.780 --> 00:40:46.160
or, conversely, holding the
minimum squared distance fixed

00:40:46.160 --> 00:40:48.270
and minimizing the
average power.

00:40:48.270 --> 00:40:52.440
So we get some figure of merit
that was kind of normalized,

00:40:52.440 --> 00:40:55.050
and we could just optimize
that one thing.

00:40:55.050 --> 00:41:01.900
The key figure of merit was
called by 19th-century

00:41:01.900 --> 00:41:08.300
mathematician, Hermite's
parameter, but we are going to

00:41:08.300 --> 00:41:13.070
call it the nominal coding gain
because that's what it's

00:41:13.070 --> 00:41:14.320
going to turn out to be.

00:41:17.570 --> 00:41:20.455
And it's just defined
as follows.

00:41:20.455 --> 00:41:25.360
The nominal coding gain of a
lattice is just this minimum

00:41:25.360 --> 00:41:28.610
squared distance, so
it goes up as the

00:41:28.610 --> 00:41:30.470
minimum squared distance.

00:41:30.470 --> 00:41:32.840
But then we want to
normalize this.

00:41:32.840 --> 00:41:35.960
And the obvious thing to
normalize it by, we want

00:41:35.960 --> 00:41:39.070
something that's going
to scale as a

00:41:39.070 --> 00:41:41.275
two-dimensional quantity.

00:41:41.275 --> 00:41:44.970
If the lattice was scaled by
alpha, then this would go up

00:41:44.970 --> 00:41:46.550
as alpha squared.

00:41:46.550 --> 00:41:49.670
So we want something that'll
scale in the same way.

00:41:49.670 --> 00:41:53.800
And what we choose to normalize
it by is the volume

00:41:53.800 --> 00:41:59.530
of lambda per two dimensions,
if you like.

00:41:59.530 --> 00:42:02.405
So you can see immediately, this
is invariant to scaling,

00:42:02.405 --> 00:42:06.740
that the nominal coding gain of
lambda is the same as the

00:42:06.740 --> 00:42:10.610
nominal coding gain
of alpha lambda.

00:42:10.610 --> 00:42:13.890
Is alpha lambda a lattice,
by the way?

00:42:13.890 --> 00:42:17.360
If lambda's a group, and we have
any scale factor alpha

00:42:17.360 --> 00:42:22.550
greater than 0, then alpha times
lambda is also a group.

00:42:22.550 --> 00:42:24.350
Everything's just multiplied
by alpha.

00:42:24.350 --> 00:42:26.990
So alpha lambda is a lattice.

00:42:26.990 --> 00:42:29.620
And this is actually one of
the homework problems.

00:42:29.620 --> 00:42:33.010
If you scale everything by
alpha, d_min-squared goes up

00:42:33.010 --> 00:42:34.530
by alpha squared.

00:42:34.530 --> 00:42:36.970
The volume goes up by alpha
to the n, for an

00:42:36.970 --> 00:42:38.690
n-dimensional lattice.

00:42:38.690 --> 00:42:41.120
But when we take 2 over n, we're
going to get back to

00:42:41.120 --> 00:42:42.360
alpha squared.

00:42:42.360 --> 00:42:48.890
So alpha c of lambda is alpha
c of alpha lambda.

00:42:48.890 --> 00:42:51.570
So this is what you want.

00:42:51.570 --> 00:42:55.270
You want a kind of scale-free
version of a lattice, because,

00:42:55.270 --> 00:42:57.370
obviously, when we use this,
we're going to feel free to

00:42:57.370 --> 00:43:00.170
amplify it or compress
it any way we want,

00:43:00.170 --> 00:43:02.880
so we do with QAM.

00:43:02.880 --> 00:43:07.230
All right, so this is normalized
in that way.

00:43:07.230 --> 00:43:09.590
We prove in the homework
it's also

00:43:09.590 --> 00:43:10.830
invariant to other things.

00:43:10.830 --> 00:43:15.510
If you took, say, an orthogonal
transformation of

00:43:15.510 --> 00:43:20.650
this lattice, a rotation,
reflection, multiply it by a

00:43:20.650 --> 00:43:28.540
matrix that doesn't change
the scale, think of it

00:43:28.540 --> 00:43:32.270
geometrically as an orthogonal
transformation, is that going

00:43:32.270 --> 00:43:33.520
to be a lattice?

00:43:35.560 --> 00:43:37.020
It is going to be a lattice.

00:43:37.020 --> 00:43:41.583
Everything's just going to have
some h out here, times G

00:43:41.583 --> 00:43:44.080
times Z-squared, and
that's still of

00:43:44.080 --> 00:43:46.830
the form of a lattice.

00:43:46.830 --> 00:43:49.680
Or call it U for orthogonal,
unitary.

00:43:52.650 --> 00:43:56.090
So we multiply it by some
orthogonal matrix, U. It's

00:43:56.090 --> 00:43:57.980
still a lattice.

00:43:57.980 --> 00:44:01.080
If we look in here, orthogonal
transformations don't change

00:44:01.080 --> 00:44:02.330
squared distances.

00:44:05.270 --> 00:44:09.260
We're not going to change this
-- it doesn't matter.

00:44:09.260 --> 00:44:13.300
Actually, I might.

00:44:16.430 --> 00:44:17.100
No.

00:44:17.100 --> 00:44:19.090
It's a rigid rotation,
I'm sorry.

00:44:19.090 --> 00:44:20.740
Everything here stays
the same.

00:44:20.740 --> 00:44:23.390
Hexagonal lattice still is
hexagonal lattice, because

00:44:23.390 --> 00:44:28.330
it's a rigid rotation or
reflection geometrically.

00:44:28.330 --> 00:44:32.750
And the volume clearly stays
the same, so none of these

00:44:32.750 --> 00:44:33.670
things changes.

00:44:33.670 --> 00:44:37.990
And orthogonal transformation
isn't going to change this

00:44:37.990 --> 00:44:39.400
quantity either.

00:44:39.400 --> 00:44:43.680
So we can rotate this
or do whatever.

00:44:43.680 --> 00:44:46.990
And then, finally, there's an
even more interesting one,

00:44:46.990 --> 00:44:52.860
which is, if we take the
lattice, lambda to the m, this

00:44:52.860 --> 00:44:55.110
is simply the Cartesian
product.

00:44:55.110 --> 00:45:02.490
It's the set of all lambda_1,
lambda_2, lambda_m, such that

00:45:02.490 --> 00:45:06.580
each of these lambda_k
is in Lambda.

00:45:06.580 --> 00:45:08.220
So it's the set of all n-tuples

00:45:08.220 --> 00:45:10.460
of elements of Lambda.

00:45:10.460 --> 00:45:16.200
This clearly lives in dimension
mn, so it's a much

00:45:16.200 --> 00:45:18.770
higher-dimensional lattice.

00:45:18.770 --> 00:45:22.745
The simplest case is Z to the m
- it lives in m dimensions,

00:45:22.745 --> 00:45:27.190
whereas Z is down in
one dimension.

00:45:27.190 --> 00:45:29.230
So it lives in dimension mn.

00:45:29.230 --> 00:45:31.870
It's a way of constructing
high-dimensional lattices from

00:45:31.870 --> 00:45:34.560
low-dimensional lattices.

00:45:34.560 --> 00:45:37.990
It's something that, in
communications, we consider to

00:45:37.990 --> 00:45:41.180
be almost a non-event.

00:45:41.180 --> 00:45:46.940
Sending a sequence of elements
of lambda is the same as

00:45:46.940 --> 00:45:48.360
sending a sequence of elements
of lambda_m.

00:45:51.080 --> 00:45:55.340
In fact, if we just send a
sequence of things from here,

00:45:55.340 --> 00:45:58.360
and a sequence of things from
lambda, there's no difference

00:45:58.360 --> 00:46:00.010
from a receiver's
point of view.

00:46:00.010 --> 00:46:04.630
So we regard these two lattices
as equivalent, from a

00:46:04.630 --> 00:46:08.190
communications point of view.

00:46:08.190 --> 00:46:10.670
Well, what happens
to d_min-squared?

00:46:10.670 --> 00:46:15.140
d_min-squared doesn't change,
because we can always--

00:46:15.140 --> 00:46:19.335
d_min-squared is, by the way,
the weight of the smallest--

00:46:19.335 --> 00:46:20.585
should have said that.

00:46:24.310 --> 00:46:27.110
You can always do things
relative to 0.

00:46:27.110 --> 00:46:33.500
d_min-squared, therefore, is
the squared norm or lowest

00:46:33.500 --> 00:46:37.230
squared energy of any non-zero
lattice point, same as we did

00:46:37.230 --> 00:46:40.430
with Hamming codes, just
by the group property.

00:46:40.430 --> 00:46:44.930
K_min is the number
of such lowest

00:46:44.930 --> 00:46:48.340
Euclidean weight points.

00:46:48.340 --> 00:46:51.320
OK, here, what's the lowest
weight point?

00:46:51.320 --> 00:46:54.110
It would look something like
this, where this is one of the

00:46:54.110 --> 00:46:55.865
lowest weight points
in lambda.

00:46:55.865 --> 00:47:00.010
So d_min-squared
doesn't change.

00:47:00.010 --> 00:47:04.150
This changes, but we're
not considering that.

00:47:04.150 --> 00:47:05.970
The volume, what
is the volume?

00:47:05.970 --> 00:47:10.050
It turns out that it's not hard
to show that V(lambda to

00:47:10.050 --> 00:47:13.700
the m) is V(lambda) to the m.

00:47:13.700 --> 00:47:19.150
And therefore, you can show that
this is figure of merit

00:47:19.150 --> 00:47:21.740
doesn't change, even if you
take Cartesian products.

00:47:25.490 --> 00:47:31.650
So for instance, I chose that
the nominal coding gain of to

00:47:31.650 --> 00:47:38.020
the m is equal to the nominal
coding gain of Z. Which is

00:47:38.020 --> 00:47:38.600
what, by the way?

00:47:38.600 --> 00:47:40.595
I should have done this
as the first example.

00:47:43.490 --> 00:47:47.230
What's the minimum squared
distance of Z?

00:47:47.230 --> 00:47:48.250
1.

00:47:48.250 --> 00:47:50.530
What's the volume of Z?

00:47:50.530 --> 00:47:51.670
1.

00:47:51.670 --> 00:47:54.870
So this is 1, or 0 dB.

00:47:54.870 --> 00:48:00.586
We're going to measure nominal
coding gain in dB, which,

00:48:00.586 --> 00:48:04.410
well, we're getting us
into communications.

00:48:04.410 --> 00:48:07.950
What's the nominal coding gain
of the hexagonal lattice?

00:48:10.640 --> 00:48:14.420
By asking this question, I'm
basically asking, is the

00:48:14.420 --> 00:48:20.150
hexagonal lattice denser than
Z-squared in two dimensions

00:48:20.150 --> 00:48:21.920
than the square lattice
in two dimensions?

00:48:21.920 --> 00:48:25.872
And if so, quantitatively,
how much denser is it?

00:48:25.872 --> 00:48:27.800
AUDIENCE: [INAUDIBLE].

00:48:27.800 --> 00:48:32.790
PROFESSOR: OK, well, there's a
root 3 in it, but that isn't

00:48:32.790 --> 00:48:34.490
exactly the answer,
I don't think.

00:48:38.780 --> 00:48:42.110
There are various ways that you
can do it, but one of the

00:48:42.110 --> 00:48:46.910
ways is to take, as a generator
matrix, (1, 0; 1/2,

00:48:46.910 --> 00:48:50.310
root 3 over 2).

00:48:50.310 --> 00:48:53.480
You like that as a generator
matrix for the--

00:48:53.480 --> 00:48:57.390
That's taking this and this
as the two generators, and

00:48:57.390 --> 00:49:03.220
recognizing this as 1/2, and
this as root 3 over 2.

00:49:03.220 --> 00:49:07.220
That makes the length of this
equal to 1/4 plus 3/4, equals

00:49:07.220 --> 00:49:09.310
1 square root, and it's 1.

00:49:09.310 --> 00:49:12.550
So these are my two generators
of length 1 in a hexagonal

00:49:12.550 --> 00:49:18.690
lattice of minimum squared
distance 1.

00:49:18.690 --> 00:49:24.346
So based on that, I can say that
the volume of A2 is what?

00:49:24.346 --> 00:49:25.596
AUDIENCE: [INAUDIBLE].

00:49:27.820 --> 00:49:29.000
PROFESSOR: Root 3 over 2.

00:49:29.000 --> 00:49:33.490
Not hard to take the determinant
of that, right?

00:49:33.490 --> 00:49:40.310
And what's the minimum squared
distance in this scaling?

00:49:40.310 --> 00:49:43.480
What's the minimum non-zero
weight --

00:49:43.480 --> 00:49:46.610
squared norm, it's 1.

00:49:46.610 --> 00:49:53.190
So this is equal to 1 over root
3 over 2, which is equal

00:49:53.190 --> 00:49:56.060
to 2 over root 3.

00:49:56.060 --> 00:49:57.500
Which is equal to what?

00:49:57.500 --> 00:49:59.420
This is 3 dB.

00:49:59.420 --> 00:50:02.450
This is 4.8 dB, roughly.

00:50:02.450 --> 00:50:05.450
Square root is 2.4 dB.

00:50:05.450 --> 00:50:07.420
So this is about 0.6 dB.

00:50:12.850 --> 00:50:18.040
That's where it pays to
be facile with dB's.

00:50:18.040 --> 00:50:21.720
This area, by the way, if you
tried to compute the area of

00:50:21.720 --> 00:50:26.578
this little hexagon here,
what would it be?

00:50:26.578 --> 00:50:29.230
If the area of this
parallelotope is the square

00:50:29.230 --> 00:50:34.770
root of 3/2, what's the
area of this hexagon?

00:50:34.770 --> 00:50:35.840
The same, right?

00:50:35.840 --> 00:50:37.920
They're both space filling when

00:50:37.920 --> 00:50:42.810
centered on lattice points.

00:50:42.810 --> 00:50:46.290
So they have to have
the same area.

00:50:46.290 --> 00:50:48.880
So that's an easy way of
computing the area of a

00:50:48.880 --> 00:50:54.350
hexagon, or of this particular
hexagon anyway.

00:50:54.350 --> 00:50:57.020
But you would get the same
result in either way.

00:50:57.020 --> 00:51:03.630
So comparing this with
Z-squared, I now have a

00:51:03.630 --> 00:51:06.160
quantitative measure
of how much

00:51:06.160 --> 00:51:10.740
denser A2 is than Z-squared.

00:51:10.740 --> 00:51:13.870
Put in one way, if I fix the
minimum squared distance of

00:51:13.870 --> 00:51:16.440
each of them to be 1, if I'm
doing penny-packing and I say

00:51:16.440 --> 00:51:20.160
I want pennies of a certain
radius to be packed into

00:51:20.160 --> 00:51:26.530
two-dimensional space, then
this is 1 over the volume,

00:51:26.530 --> 00:51:28.450
which is basically
the density.

00:51:28.450 --> 00:51:33.140
The density of A2 is 2 over root
3, times [UNINTELLIGIBLE]

00:51:33.140 --> 00:51:35.540
as the density of points
in z squared.

00:51:35.540 --> 00:51:40.480
I get more points per unit area
with the same distance

00:51:40.480 --> 00:51:44.210
between them using A2 than
I can with Z-squared.

00:51:44.210 --> 00:51:49.510
So that's the significance
of this parameter.

00:51:49.510 --> 00:51:52.460
So it's an obvious thing for
Hermite to have come up with.

00:51:52.460 --> 00:51:55.380
It's going to turn out to be
equally fundamental for us.

00:52:00.920 --> 00:52:04.710
So that may be all we want
to do on lattices

00:52:04.710 --> 00:52:07.754
for the time being.

00:52:07.754 --> 00:52:09.530
Yeah, seems to be.

00:52:14.150 --> 00:52:18.760
So we're talking about lattice
constellations.

00:52:18.760 --> 00:52:21.110
The other thing we have
to develop is the

00:52:21.110 --> 00:52:22.360
properties of regions.

00:52:25.490 --> 00:52:32.050
So let me talk about
regions, R --

00:52:32.050 --> 00:52:36.460
I'm thinking of a compact
region in an

00:52:36.460 --> 00:52:38.397
n-dimensional space.

00:52:38.397 --> 00:52:42.160
So again, let me fix
the dimension to n.

00:52:42.160 --> 00:52:44.690
And we're going to have
a certain volume.

00:52:44.690 --> 00:52:49.570
That's obviously an important
characteristic of a region.

00:52:49.570 --> 00:52:52.520
And I'm going to assume
that this is finite.

00:52:52.520 --> 00:52:57.650
And what's the other thing
that we basically want?

00:52:57.650 --> 00:52:59.920
Well, when I pick a
constellation like this, by

00:52:59.920 --> 00:53:11.020
intersecting a lattice with a
region, eventually I'm going

00:53:11.020 --> 00:53:14.695
to let the points in this
constellation be equiprobable,

00:53:14.695 --> 00:53:18.170
as I always do in digital
communications.

00:53:18.170 --> 00:53:19.190
16 points.

00:53:19.190 --> 00:53:22.570
Say they all have probably 1/16,
the uniform probability

00:53:22.570 --> 00:53:23.970
over the discrete points.

00:53:29.750 --> 00:53:30.260
One of the approximations I am
going to make -- so what's the

00:53:30.260 --> 00:53:33.430
average power of that
constellation?

00:53:33.430 --> 00:53:41.270
We could figure it out by taking
1/16 times the energy.

00:53:41.270 --> 00:53:43.250
I should say the average
energy, I'm sorry.

00:53:43.250 --> 00:53:44.300
We're not talking power here.

00:53:44.300 --> 00:53:47.450
We're talking energy, but I
read it as P. What's the

00:53:47.450 --> 00:53:48.270
average energy?

00:53:48.270 --> 00:53:52.980
It's 1/16 times the L2
norm of each of these

00:53:52.980 --> 00:53:55.045
points, if you like that.

00:53:55.045 --> 00:53:59.940
It's simply the Euclidean
squared norm.

00:54:02.760 --> 00:54:05.210
And we get some average.

00:54:05.210 --> 00:54:08.090
But here's the key approximation
principle.

00:54:08.090 --> 00:54:11.240
I'm going to say the average
power of this constellation,

00:54:11.240 --> 00:54:14.280
especially as the constellations
get large, is

00:54:14.280 --> 00:54:16.860
simply going to be approximated
as the average

00:54:16.860 --> 00:54:20.890
energy of a uniform distribution
over this

00:54:20.890 --> 00:54:23.090
bounding region.

00:54:23.090 --> 00:54:27.670
Suppose I simply put a uniform
continuous distribution over

00:54:27.670 --> 00:54:33.080
this circle, this sphere, two
sphere that I've used to be my

00:54:33.080 --> 00:54:34.650
cookie cutter.

00:54:34.650 --> 00:54:36.950
I'm going to say that's going to
be a good approximation to

00:54:36.950 --> 00:54:41.080
the average energy of the 16
discrete points, which are, by

00:54:41.080 --> 00:54:44.560
construction, they're spread
uniformly over this region.

00:54:44.560 --> 00:54:49.600
The approximation is, I'm kind
of approximating a unit

00:54:49.600 --> 00:54:52.770
impulse of weight there
by distributed

00:54:52.770 --> 00:54:55.170
weight across its--

00:54:55.170 --> 00:54:58.620
well, actually across its
whole little region.

00:54:58.620 --> 00:55:02.280
If we have a hexagon around each
one, uniform distribution

00:55:02.280 --> 00:55:04.206
over the hexagon.

00:55:04.206 --> 00:55:08.640
And if that's a reasonable
approximation, then it's

00:55:08.640 --> 00:55:11.010
reasonable to say that the
average power of the whole

00:55:11.010 --> 00:55:14.840
constellation is just the
average power of a uniform

00:55:14.840 --> 00:55:16.175
distribution over this region.

00:55:19.100 --> 00:55:23.460
It's good exercise to try it for
some moderate size cases.

00:55:23.460 --> 00:55:26.730
And you'll find it is a very
good approximation.

00:55:26.730 --> 00:55:29.520
In any case, it's the one
we're going to use.

00:55:29.520 --> 00:55:39.260
So the other key parameter is
going to be P(R), which is, in

00:55:39.260 --> 00:55:53.665
words, the average energy of
uniform distribution over R.

00:55:53.665 --> 00:56:02.500
Sorry, script R. Now, when we
write this out, this actually

00:56:02.500 --> 00:56:05.730
gets a little formidable.

00:56:05.730 --> 00:56:09.160
So I think it's better to
remember what we're actually

00:56:09.160 --> 00:56:11.920
trying to compute.

00:56:11.920 --> 00:56:14.700
Oh, and one last thing,
we're going to

00:56:14.700 --> 00:56:16.200
normalize it per dimension.

00:56:22.480 --> 00:56:23.570
So what do I mean?

00:56:23.570 --> 00:56:28.630
What's a uniform distribution
over R?

00:56:33.670 --> 00:56:44.030
This is P(x equals 1) over
the volume of R for

00:56:44.030 --> 00:56:49.920
x in R and 0 otherwise.

00:56:56.330 --> 00:56:58.590
Uniform within, 0 outside.

00:56:58.590 --> 00:56:59.540
And what does it
have to equal?

00:56:59.540 --> 00:57:03.040
It has to be equal to 1 over
V(R), because its integral has

00:57:03.040 --> 00:57:07.260
to be equal to 1 if it's a
probability distribution.

00:57:07.260 --> 00:57:17.790
So to compute P(R), we basically
take the integral

00:57:17.790 --> 00:57:27.030
from over R, of this probability
distribution, 1

00:57:27.030 --> 00:57:30.680
over V(R), times what?

00:57:30.680 --> 00:57:32.640
The energy per dimension.

00:57:32.640 --> 00:57:43.070
The total energy of x is
x-squared, the L2 norm of x.

00:57:43.070 --> 00:57:44.910
Normalize per dimension --
we're going to put an

00:57:44.910 --> 00:57:47.080
n over there --

00:57:47.080 --> 00:57:48.330
dx.

00:57:50.620 --> 00:57:54.470
So that's an equation for what
it is I wanted to compute.

00:58:00.380 --> 00:58:02.770
I think it's much harder to
remember the terms in this

00:58:02.770 --> 00:58:09.270
than to remember this verbal
description of what we want.

00:58:09.270 --> 00:58:17.220
But you may be more literate
than I. Now, let's define --

00:58:17.220 --> 00:58:22.270
we want to normalize quantity
comparable to Hermite or

00:58:22.270 --> 00:58:24.550
coding gain over here.

00:58:24.550 --> 00:58:27.960
The normalized quantity
we're going to use --

00:58:27.960 --> 00:58:35.170
we want to take P(R) -- and what
should we normalize it by

00:58:35.170 --> 00:58:39.220
to make it scale-invariant,
let's say again?

00:58:39.220 --> 00:58:43.350
Again, if I take the region, I
scale the region by alpha and

00:58:43.350 --> 00:58:45.590
get alpha_R.

00:58:45.590 --> 00:58:48.550
What's going to happen to the
average energy of a uniform

00:58:48.550 --> 00:58:49.850
distribution over R?

00:58:53.070 --> 00:58:54.450
AUDIENCE: [INAUDIBLE].

00:58:54.450 --> 00:58:57.760
PROFESSOR: It's going to
go up as alpha squared.

00:58:57.760 --> 00:59:02.430
Energy scarce, goes as the
square of the scale factor.

00:59:02.430 --> 00:59:09.850
So again, I want to normalize
it by V(R) over 2 to the n,

00:59:09.850 --> 00:59:12.390
because this is also something
that will go up as alpha

00:59:12.390 --> 00:59:17.130
squared if I scale R by alpha.

00:59:20.960 --> 00:59:31.830
So this is what's known as the
dimension-less or normalized--

00:59:31.830 --> 00:59:33.380
it has both names--

00:59:38.060 --> 00:59:39.310
second moment.

00:59:44.190 --> 00:59:50.970
Another name for this is the
second moment per dimension of

00:59:50.970 --> 00:59:54.660
a uniform distribution over
R. This is a normalized or

00:59:54.660 --> 00:59:58.620
dimension-less second moment.

00:59:58.620 --> 01:00:02.200
I think I call it normalized in
the notes, but it's always

01:00:02.200 --> 01:00:05.100
called G in the literature.

01:00:05.100 --> 01:00:09.420
And let me just introduce
here--

01:00:09.420 --> 01:00:14.810
well, what's the normalized
second moment of an interval?

01:00:14.810 --> 01:00:19.590
Say, a symmetric interval
around the origin.

01:00:19.590 --> 01:00:22.510
G to the minus 1 to the 1, which
is like the interval

01:00:22.510 --> 01:00:26.330
that we used to pull out the
m-PAM signal structure.

01:00:26.330 --> 01:00:34.670
So let's take n to be equal to
1, R just to be the interval

01:00:34.670 --> 01:00:37.680
from -1 to +1.

01:00:37.680 --> 01:00:42.206
And V(R) equals what?

01:00:42.206 --> 01:00:44.380
It's the length of
R, which is 2.

01:00:49.640 --> 01:00:50.970
What's P(R)?

01:00:54.860 --> 01:01:02.250
The integral from -1 to 1, x
squared, dx, which is 2/3.

01:01:07.490 --> 01:01:09.716
I think so.

01:01:09.716 --> 01:01:10.966
AUDIENCE: [INAUDIBLE].

01:01:14.150 --> 01:01:15.540
PROFESSOR: Right, 1/2.

01:01:15.540 --> 01:01:16.790
Thank you.

01:01:18.660 --> 01:01:19.910
That's 1/3.

01:01:24.290 --> 01:01:28.290
All right, so what is G(R) going
to be, or G going to be?

01:01:28.290 --> 01:01:33.410
This is going to be P(R)
over V(R)-squared.

01:01:36.070 --> 01:01:40.840
And so this is going to be
1/3 over 4, or 1/12.

01:01:45.050 --> 01:01:49.480
OK, that's not as nice as
this over here, but

01:01:49.480 --> 01:01:51.940
it is what it is.

01:01:51.940 --> 01:01:56.000
There's a number that shows up
in quantization, for instance,

01:01:56.000 --> 01:01:58.890
uniform scalar quantizer, you'll
find a 1/12 in there.

01:01:58.890 --> 01:01:59.590
Why?

01:01:59.590 --> 01:02:00.840
It's because of this equation.

01:02:07.901 --> 01:02:11.830
If we ask again about the
invariances of this--

01:02:11.830 --> 01:02:15.000
this is the last homework
problem for this week--

01:02:15.000 --> 01:02:18.390
it's invariant not only to
scaling, but, again, it's

01:02:18.390 --> 01:02:20.810
invariant to orthogonal
transformations, because

01:02:20.810 --> 01:02:23.360
orthogonal transformations
won't affect

01:02:23.360 --> 01:02:24.420
either of these things.

01:02:24.420 --> 01:02:28.210
A rigid rotation, about
the origin, won't

01:02:28.210 --> 01:02:32.330
affect V(R) or P(R).

01:02:32.330 --> 01:02:35.110
And furthermore, it's invariant
to Cartesian

01:02:35.110 --> 01:02:37.045
products, again.

01:02:37.045 --> 01:02:40.920
If we just have a series of
regions, a region made up

01:02:40.920 --> 01:02:47.370
basically as the Cartesian
product, like an n-cube is a

01:02:47.370 --> 01:02:48.540
Cartesian product.

01:02:48.540 --> 01:02:52.350
We just pick each of the
dimensions independently from

01:02:52.350 --> 01:02:57.360
some set R. Then that's not
going to affect the normalized

01:02:57.360 --> 01:02:58.450
second moment either.

01:02:58.450 --> 01:03:01.600
Exercise for the student.

01:03:01.600 --> 01:03:11.110
So from that result, I can say
that G([-1 1]) to the m.

01:03:11.110 --> 01:03:12.360
Which is what?

01:03:14.640 --> 01:03:17.275
This is an m-cube of side 2.

01:03:20.570 --> 01:03:28.050
If I say each of the
coordinates, x1, x2, x3, up to

01:03:28.050 --> 01:03:35.690
xm, has got to be xk n times
[-1 1], then what I'm going to

01:03:35.690 --> 01:03:39.360
get, the region defined by that
is an m-cubed, centered

01:03:39.360 --> 01:03:41.150
on the origin of side 2.

01:03:44.260 --> 01:03:47.585
So the normalized second
moment of that

01:03:47.585 --> 01:03:48.835
is going to be 1/12.

01:03:52.650 --> 01:03:55.770
This is where the shortcut
begins to help you.

01:03:59.340 --> 01:04:02.560
I guess I'm trying to put
too much on one board.

01:04:02.560 --> 01:04:05.280
But the last thing I want
to put up on here

01:04:05.280 --> 01:04:07.320
is the shaping gain.

01:04:07.320 --> 01:04:17.760
The shape gain of a region is
defined as the shaping gain of

01:04:17.760 --> 01:04:23.120
the region is equal to 1/12.

01:04:23.120 --> 01:04:27.460
So we're kind of taking as a
baseline, the m-cube, in any

01:04:27.460 --> 01:04:31.610
number of dimensions n,
over G(the region).

01:04:36.190 --> 01:04:41.460
In other words, how much less
is the dimension-less second

01:04:41.460 --> 01:04:44.830
moment, the normalized second
moment, than what you would

01:04:44.830 --> 01:04:47.450
get for an m-cube?

01:04:47.450 --> 01:04:56.650
In every dimension except for
one, a sphere is going to be a

01:04:56.650 --> 01:05:02.360
better region, from a second
moment point of

01:05:02.360 --> 01:05:05.330
view, than an m-cube.

01:05:05.330 --> 01:05:07.460
An m-cube has corners.

01:05:07.460 --> 01:05:11.540
Really, it's pretty obvious
that, if you want to minimize

01:05:11.540 --> 01:05:14.310
the normalized second moment in
any number of dimensions,

01:05:14.310 --> 01:05:19.270
you would choose an m-sphere,
because if you have any

01:05:19.270 --> 01:05:22.440
wrinkle that comes out of an
m-sphere, anything that's not

01:05:22.440 --> 01:05:24.810
an m-sphere, you should try
to squash it back in it.

01:05:24.810 --> 01:05:26.150
And that'll make
it an m-sphere.

01:05:26.150 --> 01:05:29.430
You'll take higher energy and
make it lower energy for the

01:05:29.430 --> 01:05:35.100
same little unit of Play-Doh
or mass stuff.

01:05:35.100 --> 01:05:41.060
So a sphere will give you the
best normalized second moment

01:05:41.060 --> 01:05:44.160
in any number of dimensions n,
so you're always going to get

01:05:44.160 --> 01:05:46.830
a G(R) that's less than 1/12.

01:05:46.830 --> 01:05:48.220
That's good.

01:05:48.220 --> 01:05:52.000
And this measures the
amount of gain.

01:05:52.000 --> 01:05:55.026
Again, you can measure
this in dB.

01:05:55.026 --> 01:05:57.630
That's going to translate
directly to dB.

01:06:03.080 --> 01:06:06.190
I suppose I could do an example
for the sphere in two

01:06:06.190 --> 01:06:07.150
dimensions, but I won't.

01:06:07.150 --> 01:06:08.400
How are we doing on time?

01:06:20.080 --> 01:06:24.980
Let me see if I can do the
union-bound estimate.

01:06:28.310 --> 01:06:33.870
That would be a trick, but I
think it's doable, because

01:06:33.870 --> 01:06:35.120
this is where we're going.

01:06:38.160 --> 01:06:49.750
So the union-bound estimate,
which is something we did way

01:06:49.750 --> 01:06:58.650
back in chapter five or
something, is basically, given

01:06:58.650 --> 01:07:01.740
a constellation, which we're
going to have a lattice

01:07:01.740 --> 01:07:04.890
constellation based
on some lattice

01:07:04.890 --> 01:07:07.765
lambda and some region.

01:07:14.200 --> 01:07:19.410
So this is like our m-by-m QAM,
or it might be this guy,

01:07:19.410 --> 01:07:24.930
which is better than 16-QAM,
in some sense.

01:07:24.930 --> 01:07:26.640
In two senses, actually.

01:07:29.440 --> 01:07:31.670
The union-bound estimate,
what is it?

01:07:34.310 --> 01:07:37.605
The probability of error is
approximately equal to--

01:07:40.280 --> 01:07:41.910
let me do it per block.

01:07:41.910 --> 01:07:46.120
So I'll have the number of
nearest neighbors per block of

01:07:46.120 --> 01:07:54.650
our constellation times Q
to the square root of

01:07:54.650 --> 01:07:57.890
d_min-squared of our
constellation,

01:07:57.890 --> 01:07:58.960
over what is it?

01:07:58.960 --> 01:08:00.210
4 sigma squared?

01:08:05.930 --> 01:08:07.180
4 sigma squared.

01:08:15.600 --> 01:08:19.859
And if you remember how we got
this, we took the union-bound,

01:08:19.859 --> 01:08:24.460
we took all the pairwise error
probabilities, and from that,

01:08:24.460 --> 01:08:31.830
we get contributions from kd at
every kd contributions at

01:08:31.830 --> 01:08:33.380
distance d.

01:08:33.380 --> 01:08:35.340
We added them all up
in the union-bound.

01:08:35.340 --> 01:08:39.779
We said, well, for moderate
complexity and performance, at

01:08:39.779 --> 01:08:41.300
least this is going
to be dominated

01:08:41.300 --> 01:08:42.920
by the minimum distance.

01:08:42.920 --> 01:08:46.109
So this is the minimum
distance terms.

01:08:46.109 --> 01:08:49.310
This is the number of minimum
distance terms.

01:08:49.310 --> 01:08:50.739
And this is error
probability --

01:08:50.739 --> 01:08:53.220
the pairwise error probability
we get for each minimum

01:08:53.220 --> 01:08:54.470
distance term.

01:09:00.300 --> 01:09:07.200
Now, to compute this, I'm
going to use three

01:09:07.200 --> 01:09:12.390
approximations, but two
important ones, this

01:09:12.390 --> 01:09:15.370
continuous approximation that
I've already referred to.

01:09:18.670 --> 01:09:25.140
First, I'm going to say that
the average power of my

01:09:25.140 --> 01:09:33.470
constellation is approximately
equal to just the second

01:09:33.470 --> 01:09:46.609
moment of R. The average energy
per dimension of R.

01:09:46.609 --> 01:09:51.080
And then I'm going to say,
furthermore, how many points

01:09:51.080 --> 01:09:52.279
are there in the
constellation?

01:09:52.279 --> 01:09:54.700
This will affect what
rate I can go at.

01:09:57.300 --> 01:10:01.350
The average number of points
in the constellation --

01:10:01.350 --> 01:10:03.110
also mention this--

01:10:03.110 --> 01:10:08.470
is going to be approximately
equal to simply the volume of

01:10:08.470 --> 01:10:14.800
my bounding region, V(R), over
the volume taken up by each

01:10:14.800 --> 01:10:16.590
point in my lattice.

01:10:20.330 --> 01:10:23.590
That makes sense, doesn't it?

01:10:23.590 --> 01:10:28.580
What I want to do here to get
16 points is I take a sphere

01:10:28.580 --> 01:10:32.590
whose area is roughly 16 times
the area of one of these

01:10:32.590 --> 01:10:35.910
little hexagons or one of these
little parallelograms.

01:10:35.910 --> 01:10:39.840
And I'll get about 16 points,
because that's

01:10:39.840 --> 01:10:43.200
the density of it.

01:10:43.200 --> 01:10:48.400
And there's a final little
approximation, which is that

01:10:48.400 --> 01:10:50.710
I'm going to assume that we're
somewhere in the interior of

01:10:50.710 --> 01:10:55.450
the lattice, so that the average
number of nearest

01:10:55.450 --> 01:10:59.760
neighbors in the constellation
is simply equal to the number

01:10:59.760 --> 01:11:01.125
of nearest neighbors
in the lattice.

01:11:01.125 --> 01:11:04.990
So that's a reasonable
approximation, and that's a

01:11:04.990 --> 01:11:06.310
very obvious one.

01:11:06.310 --> 01:11:09.960
If I take any constellation
based on the hexagonal

01:11:09.960 --> 01:11:12.720
lattice, for a large enough
constellation, the average

01:11:12.720 --> 01:11:15.490
number of nearest neighbors
is going to be 6.

01:11:15.490 --> 01:11:18.110
If I take it on the square
lattice, the average number of

01:11:18.110 --> 01:11:19.990
nearest neighbors is
going to be 4.

01:11:19.990 --> 01:11:25.380
But we know we're not terribly
interested in that either.

01:11:28.730 --> 01:11:33.490
Let me do a little manipulation
here.

01:11:33.490 --> 01:11:45.000
I'm going to write this as Q of
d_min-squared of c, which

01:11:45.000 --> 01:11:49.600
clearly I'm going to also assume
that d_min-squared of c

01:11:49.600 --> 01:11:53.460
is equal to d_min-squared
of the lattice.

01:11:53.460 --> 01:11:55.940
So I'll make that d_min-squared
of the lattice.

01:11:58.810 --> 01:12:07.670
And I want to normalize that
by v of lambda, to the 2/n,

01:12:07.670 --> 01:12:12.620
whatever dimension n I'm in,
to get something that we're

01:12:12.620 --> 01:12:13.450
familiar with.

01:12:13.450 --> 01:12:17.010
So I need a V(n) over
2 to the n.

01:12:17.010 --> 01:12:23.180
And anticipating a little,
I'm going to put a V(R)

01:12:23.180 --> 01:12:25.864
over 2 to the n.

01:12:25.864 --> 01:12:32.035
And then over here, I'm going
to put V(R) over--

01:12:32.035 --> 01:12:33.450
Is this going to come
out right?--

01:12:33.450 --> 01:12:38.980
over 2 to the n, over--

01:12:38.980 --> 01:12:40.230
this seems upside-down.

01:13:00.100 --> 01:13:01.570
Ah, no, it isn't upside-down.

01:13:05.960 --> 01:13:10.560
Because I'm using this
expression here, I

01:13:10.560 --> 01:13:11.810
do want 1 over G(R).

01:13:16.340 --> 01:13:20.450
So this is 1 over G(R).

01:13:20.450 --> 01:13:29.170
If I multiply this by 1/12, now
I get 1 over G(R), so I

01:13:29.170 --> 01:13:34.440
need to put a 1/12 here.

01:13:34.440 --> 01:13:42.255
And then I have a P(R)
over 4 sigma squared.

01:13:44.810 --> 01:13:46.810
So does everyone agree I
can write it that way?

01:13:50.260 --> 01:13:52.280
This'll be good.

01:13:52.280 --> 01:13:53.570
Go out with a bang.

01:13:56.200 --> 01:13:59.220
So what are all these things?

01:13:59.220 --> 01:14:03.995
This is the coding
gain of lambda.

01:14:06.860 --> 01:14:13.090
This, according to this, is the
size of the constellation

01:14:13.090 --> 01:14:15.960
to the 2 over n,
approximately.

01:14:15.960 --> 01:14:23.900
The rate of the constellation
is log 2 times the size of

01:14:23.900 --> 01:14:32.020
lambda, R. So the rate
is equal to just

01:14:32.020 --> 01:14:33.750
log 2 of this quantity.

01:14:36.450 --> 01:14:42.680
So I can say 2 to the R. This
is 2 to the R equals this.

01:14:42.680 --> 01:14:54.780
So this would be 2 to the
2 over n, 2R over n.

01:14:54.780 --> 01:14:56.880
Is that right?

01:14:56.880 --> 01:15:00.280
Again, I'm unsure if
that's what I want.

01:15:00.280 --> 01:15:06.680
This is the shaping gain of the
region, these three things

01:15:06.680 --> 01:15:12.790
together, times 3,
times-- what's

01:15:12.790 --> 01:15:14.440
P(R) over sigma squared?

01:15:14.440 --> 01:15:15.960
That's the SNR.

01:15:15.960 --> 01:15:19.130
That's the average power
per dimension.

01:15:19.130 --> 01:15:21.130
And this is the average noise
power per dimension.

01:15:24.230 --> 01:15:26.740
I've used up these in that.

01:15:26.740 --> 01:15:28.530
So I've got a 1 over 1/12.

01:15:28.530 --> 01:15:29.890
That's 12 times that.

01:15:29.890 --> 01:15:32.270
So this is 3 times SNR.

01:15:35.340 --> 01:15:39.200
And let's see.

01:15:39.200 --> 01:15:45.657
This is 2R over n is
just 2 to the--

01:15:45.657 --> 01:15:47.525
AUDIENCE: Isn't that just 2R?

01:15:47.525 --> 01:15:49.753
Because [UNINTELLIGIBLE]
normalized by the dimension.

01:15:49.753 --> 01:15:51.003
PROFESSOR: Yeah.

01:15:58.320 --> 01:16:05.830
And furthermore, it's minus,
because it's upside-down.

01:16:05.830 --> 01:16:11.480
So it's 2 to the - 2R.

01:16:11.480 --> 01:16:15.360
Actually, what I want is the
spectral efficiency.

01:16:15.360 --> 01:16:21.330
And the spectral efficiency is
the rate per two dimensions.

01:16:21.330 --> 01:16:23.435
So this is just 2 to
the minus rho.

01:16:30.060 --> 01:16:34.030
So with a little help for
my friends, what's SNR

01:16:34.030 --> 01:16:35.280
over 2 to the rho?

01:16:39.320 --> 01:16:39.780
AUDIENCE: [INAUDIBLE].

01:16:39.780 --> 01:16:42.210
PROFESSOR: It's approximately
SNR norm.

01:16:42.210 --> 01:16:44.960
Certainly true as row
becomes large.

01:16:44.960 --> 01:16:48.320
It's actually SNR over
2 to the rho minus 1.

01:16:48.320 --> 01:16:51.020
So this is SNR norm.

01:16:53.890 --> 01:16:58.830
OK, so with a little hand waving
and a few high SNR

01:16:58.830 --> 01:17:09.960
approximations, we get that, in
summary, for the UBE, the

01:17:09.960 --> 01:17:17.930
probability of error is
approximately K_min(lambda)

01:17:17.930 --> 01:17:23.600
times Q to the square root
of the coding gain of the

01:17:23.600 --> 01:17:29.320
lattice, times the shaping
gain of the

01:17:29.320 --> 01:17:36.010
region, times 3SNR norm.

01:17:40.860 --> 01:17:42.230
And that's a wonderful
approximation.

01:17:45.590 --> 01:17:48.380
Let me just call out some of
its properties to you, and

01:17:48.380 --> 01:17:52.550
then we'll talk about
them next time.

01:17:52.550 --> 01:17:55.450
But first of all, there's
nothing here--

01:17:55.450 --> 01:17:59.790
all these terms involve either
lambda or R. So we have

01:17:59.790 --> 01:18:05.190
completely separated, in this
analysis, the effects of the

01:18:05.190 --> 01:18:07.180
region from the effects
of the lattice.

01:18:07.180 --> 01:18:12.240
We can choose the lattice and
the region independently.

01:18:12.240 --> 01:18:18.810
And they independently affect
how good a performance we get.

01:18:18.810 --> 01:18:26.260
For the baseline, which was
either m-PAM or m-by-m QAM, we

01:18:26.260 --> 01:18:33.680
have, for the baseline PAM or
QAM, we've set it up so that

01:18:33.680 --> 01:18:39.280
both the coding gain of the
lattice and the shaping gain

01:18:39.280 --> 01:18:41.330
of the region are equal to 1.

01:18:41.330 --> 01:18:45.340
So for the baseline, this
gives us what we already

01:18:45.340 --> 01:18:49.250
developed back in the
early stages.

01:18:49.250 --> 01:18:51.130
I won't worry about this --

01:18:51.130 --> 01:18:54.200
Q-hat, but it does give you
the right coefficient out

01:18:54.200 --> 01:18:58.280
here, too, depending on whether
you normalize it per

01:18:58.280 --> 01:18:59.960
two dimensions or what.

01:18:59.960 --> 01:19:04.850
It just gives you Q to the
square root of 3SNR norm,

01:19:04.850 --> 01:19:07.590
which I hope you remember
from chapter five.

01:19:07.590 --> 01:19:15.920
So this is our baseline curve
for m-PAM or m-by-m QAM.

01:19:15.920 --> 01:19:22.100
We just plotted versus SNR norm,
the probability of error

01:19:22.100 --> 01:19:25.860
per two dimensions, which
is what I recommended.

01:19:25.860 --> 01:19:28.410
And we got some curve.

01:19:28.410 --> 01:19:29.880
So this is the baseline.

01:19:33.910 --> 01:19:37.390
And now we've got everything
separated out in a nice way,

01:19:37.390 --> 01:19:40.350
so that if somebody gives us a
lattice with a certain coding

01:19:40.350 --> 01:19:45.520
gain and a region with a certain
shaping gain, those

01:19:45.520 --> 01:19:47.970
both just add up as gains.

01:19:47.970 --> 01:19:51.550
If that's all we had, forget the
coefficient, we would just

01:19:51.550 --> 01:19:55.570
move to the right here, and
it would give us effective

01:19:55.570 --> 01:19:59.800
reductions and the required SNR
norm by the coding gain of

01:19:59.800 --> 01:20:03.240
the lattice and the shaping
gain of the region.

01:20:03.240 --> 01:20:07.340
And we'd get the same curve
moved out over here.

01:20:07.340 --> 01:20:11.690
Again, as in the binary case,
typically we're going to get a

01:20:11.690 --> 01:20:15.580
larger number of near neighbors,
leading to a larger

01:20:15.580 --> 01:20:17.070
coefficient out here.

01:20:17.070 --> 01:20:19.320
We want to normalize this
per two dimensions.

01:20:19.320 --> 01:20:23.080
Per two dimensions, it's only
4 for the baseline.

01:20:23.080 --> 01:20:28.220
But in general, we're going to
get a KS of lambda, the number

01:20:28.220 --> 01:20:30.810
of nearest neighbors per two
dimensions, which is going to

01:20:30.810 --> 01:20:32.690
raise this a little.

01:20:32.690 --> 01:20:35.140
And that will lower
the effective

01:20:35.140 --> 01:20:37.650
coding gain down here.

01:20:37.650 --> 01:20:42.350
But we've basically, by these
large constellation, high SNR

01:20:42.350 --> 01:20:46.750
approximations, we've really
reduced the analysis of

01:20:46.750 --> 01:20:49.890
lattice constellations to a
very simple task, just to

01:20:49.890 --> 01:20:54.420
analyze what's the coding gain,
what's the shaping gain,

01:20:54.420 --> 01:20:56.990
number of nearest neighbors,
plot the performance curve,

01:20:56.990 --> 01:21:00.020
end of story.

01:21:00.020 --> 01:21:01.430
So we'll start there next time.

