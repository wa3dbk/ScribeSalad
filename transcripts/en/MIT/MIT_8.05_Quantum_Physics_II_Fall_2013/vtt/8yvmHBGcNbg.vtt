WEBVTT
Kind: captions
Language: en

00:00:00.100 --> 00:00:01.680
The following
content is provided

00:00:01.680 --> 00:00:03.820
under a Creative
Commons license.

00:00:03.820 --> 00:00:06.540
Your support will help MIT
OpenCourseWare continue

00:00:06.540 --> 00:00:10.140
to offer high quality
educational resources for free.

00:00:10.140 --> 00:00:12.700
To make a donation, or to
view additional materials

00:00:12.700 --> 00:00:16.498
from hundreds of MIT courses,
visit MIT OpenCourseWare

00:00:16.498 --> 00:00:17.390
at ocw.mit.edu.

00:00:28.700 --> 00:00:31.780
ARAM HARROW: So
let's get started.

00:00:31.780 --> 00:00:35.990
This week Professor
Zwiebach is away,

00:00:35.990 --> 00:00:37.770
and I'll be doing
today's lecture.

00:00:37.770 --> 00:00:42.200
And Will Detmold will
do the one on Wednesday.

00:00:42.200 --> 00:00:45.010
The normal office
hours, unfortunately,

00:00:45.010 --> 00:00:46.440
will not be held today.

00:00:46.440 --> 00:00:49.820
One of us will cover his
hours on Wednesday though.

00:00:49.820 --> 00:00:53.950
And you should also just email
either me or Professor Detmold

00:00:53.950 --> 00:00:55.450
if you want to set
up an appointment

00:00:55.450 --> 00:00:58.720
to talk to in the next few days.

00:00:58.720 --> 00:01:00.720
What I'm going to
talk about today

00:01:00.720 --> 00:01:04.629
will be more about the
linear algebra that's

00:01:04.629 --> 00:01:07.430
behind all of quantum mechanics.

00:01:07.430 --> 00:01:10.980
And, at the end of last
time-- last lecture

00:01:10.980 --> 00:01:14.740
you heard about vector
spaces from a more abstract

00:01:14.740 --> 00:01:17.030
perspective than
the usual vectors

00:01:17.030 --> 00:01:19.450
are columns of
numbers perspective.

00:01:19.450 --> 00:01:21.955
Today we're going to
look at operators,

00:01:21.955 --> 00:01:23.330
which act on vector
spaces, which

00:01:23.330 --> 00:01:27.390
are linear maps from a
vector space to itself.

00:01:27.390 --> 00:01:31.660
And they're, in a
sense, equivalent

00:01:31.660 --> 00:01:34.430
to the familiar
idea of matrices,

00:01:34.430 --> 00:01:37.940
which are squares or
rectangles of numbers.

00:01:37.940 --> 00:01:42.930
But are work in this
more abstract setting

00:01:42.930 --> 00:01:46.300
of vector spaces, which
has a number of advantages.

00:01:46.300 --> 00:01:47.830
For example, of
being able to deal

00:01:47.830 --> 00:01:51.110
with infinite dimensional
vector spaces and also

00:01:51.110 --> 00:01:54.340
of being able to talk about
basis independent properties.

00:01:54.340 --> 00:01:56.940
And so I'll tell you
all about that today.

00:01:56.940 --> 00:01:59.329
So we'll talk about how
to define operators,

00:01:59.329 --> 00:02:01.370
some examples, some of
their properties, and then

00:02:01.370 --> 00:02:07.050
finally how to relate them to
the familiar idea of matrices.

00:02:07.050 --> 00:02:09.169
I'll then talk about
eigenvectors and eigenvalues

00:02:09.169 --> 00:02:11.250
from this operator prospective.

00:02:11.250 --> 00:02:13.670
And, depending on time
today, a little bit

00:02:13.670 --> 00:02:15.300
about inner products,
which you'll

00:02:15.300 --> 00:02:16.960
hear more about the future.

00:02:16.960 --> 00:02:20.280
These numbers here correspond
to the sections of the notes

00:02:20.280 --> 00:02:23.020
that these refer to.

00:02:23.020 --> 00:02:27.890
So let me first-- this is
a little bit mathematical

00:02:27.890 --> 00:02:31.120
and perhaps dry at first.

00:02:31.120 --> 00:02:34.010
The payoff is more
distant than usual

00:02:34.010 --> 00:02:35.844
for things you'll hear
in quantum mechanics.

00:02:35.844 --> 00:02:37.301
I just want to
mention a little bit

00:02:37.301 --> 00:02:38.570
about the motivation for it.

00:02:43.930 --> 00:02:49.025
So operators, of course, are
how we define observables.

00:02:52.550 --> 00:02:56.370
And so if we want to know what
the properties of observables,

00:02:56.370 --> 00:03:05.440
of which a key example
are of Hamiltonians,

00:03:05.440 --> 00:03:08.430
then we need to know
about operators.

00:03:08.430 --> 00:03:11.766
They also, as you will
see in the future,

00:03:11.766 --> 00:03:13.265
are useful for
talking about states.

00:03:17.090 --> 00:03:21.140
Right now, states are described
as elements of a vector space,

00:03:21.140 --> 00:03:23.460
but in the future you'll
learn a different formalism

00:03:23.460 --> 00:03:27.810
in which states are also
described as dense operators.

00:03:27.810 --> 00:03:30.555
What are called density
operators or density matrices.

00:03:33.150 --> 00:03:36.560
And finally, operators are
also useful in describing

00:03:36.560 --> 00:03:38.050
symmetries of quantum systems.

00:03:41.000 --> 00:03:42.990
So already in
classical mechanics,

00:03:42.990 --> 00:03:45.480
symmetries have been very
important for understanding

00:03:45.480 --> 00:03:48.560
things like momentum
conservation and energy

00:03:48.560 --> 00:03:50.000
conservation so on.

00:03:50.000 --> 00:03:52.240
They'll be even more
important in quantum mechanics

00:03:52.240 --> 00:03:56.540
and will be understood through
the formalism of operators.

00:03:56.540 --> 00:03:59.160
So these are not things
that I will talk about today

00:03:59.160 --> 00:04:01.890
but are sort of the
motivation for understanding

00:04:01.890 --> 00:04:05.690
very well the structure
of operators now.

00:04:05.690 --> 00:04:15.900
So at the end of
the last lecture,

00:04:15.900 --> 00:04:18.810
Professor Zwiebach
defined linear maps.

00:04:18.810 --> 00:04:29.600
So this is the set of linear
maps from a vector space, v,

00:04:29.600 --> 00:04:31.600
to a vector space w.

00:04:31.600 --> 00:04:33.630
And just to remind you
what it means for a map

00:04:33.630 --> 00:04:53.050
to be linear, so T is linear if
for all pairs of vectors in v,

00:04:53.050 --> 00:04:56.260
the way T acts on their
sum is given by just T of u

00:04:56.260 --> 00:05:07.840
plus T of v. That's
the first property.

00:05:07.840 --> 00:05:19.680
And second, for all vectors
u and for all scalars a--

00:05:19.680 --> 00:05:22.680
so f is the field that
we're working over,

00:05:22.680 --> 00:05:28.751
it could be reals are
complexes-- we have that

00:05:28.751 --> 00:05:41.110
if T acts on a times u, that's
equal to a times t acting on u.

00:05:41.110 --> 00:05:44.840
So if you put these
together what this means

00:05:44.840 --> 00:05:48.260
is that t essentially
looks like multiplication.

00:05:48.260 --> 00:05:52.410
The way T acts on vectors is
precisely what you would expect

00:05:52.410 --> 00:05:55.020
from the multiplication
map, right?

00:05:55.020 --> 00:06:01.390
It has the distributive property
and it commutes with scalars.

00:06:01.390 --> 00:06:03.879
So this is sort of
informal-- I mean,

00:06:03.879 --> 00:06:06.045
the formal definition is
here, but the informal idea

00:06:06.045 --> 00:06:13.045
is that T acts like
multiplication.

00:06:16.820 --> 00:06:22.440
So if the map that squares
every entry of a vector

00:06:22.440 --> 00:06:26.080
does not act like this,
but linear operators do.

00:06:26.080 --> 00:06:29.220
And for this reason we often
neglect the parentheses.

00:06:29.220 --> 00:06:38.310
So we just write TU to mean T
of u, which is justified because

00:06:38.310 --> 00:06:40.240
of this analogy
with multiplication.

00:06:43.720 --> 00:06:51.710
So an important special case of
this is when v is equal to w.

00:06:54.280 --> 00:06:57.560
And so we just write l of
v to denote the maps from v

00:06:57.560 --> 00:06:59.820
to itself.

00:06:59.820 --> 00:07:03.240
Which you could also
write like this.

00:07:03.240 --> 00:07:11.758
And these are called
operators on v.

00:07:11.758 --> 00:07:14.200
So when we talk about
operators on a vector space,

00:07:14.200 --> 00:07:18.320
v, we mean linear maps from
that vector space to itself.

00:07:23.370 --> 00:07:27.680
So let me illustrate
this with a few examples.

00:07:36.810 --> 00:07:39.280
Starting with some of the
examples of vector spaces

00:07:39.280 --> 00:07:40.960
that you saw from last time.

00:07:40.960 --> 00:07:48.250
So one example of a
vector space is an example

00:07:48.250 --> 00:07:50.350
you've seen before but
a different notation.

00:07:50.350 --> 00:07:59.680
This is the vector space
of all real polynomials

00:07:59.680 --> 00:08:00.650
in one variable.

00:08:07.010 --> 00:08:12.550
So real polynomials
over some variable, x.

00:08:12.550 --> 00:08:15.350
And over-- this is an infinite
dimensional vector space--

00:08:15.350 --> 00:08:17.880
and we can define various
operators over it.

00:08:17.880 --> 00:08:22.770
For example, we can
define one operator, T,

00:08:22.770 --> 00:08:25.740
to be like differentiation.

00:08:25.740 --> 00:08:32.640
So what you might
write as ddx hat,

00:08:32.640 --> 00:08:38.179
and it's defined for any
polynomial, p, to map

00:08:38.179 --> 00:08:39.120
p to p prime.

00:08:42.390 --> 00:08:45.850
So this is certainly a
function from polynomials

00:08:45.850 --> 00:08:47.450
to polynomials.

00:08:47.450 --> 00:08:50.140
And you can check
that it's also linear

00:08:50.140 --> 00:08:53.150
if you multiply the
polynomial by a scalar, then

00:08:53.150 --> 00:08:55.620
the derivative multiplied
by the same scale.

00:08:55.620 --> 00:08:59.330
If I take the derivative of
a sum of two polynomials,

00:08:59.330 --> 00:09:01.450
then I get the sum
of the derivatives

00:09:01.450 --> 00:09:03.100
of those polynomials.

00:09:03.100 --> 00:09:04.960
I won't write that
down, but you can

00:09:04.960 --> 00:09:06.520
check that the
properties are true.

00:09:06.520 --> 00:09:08.070
And this is indeed
a linear operator.

00:09:11.680 --> 00:09:16.720
Another operator, which
you've seen before,

00:09:16.720 --> 00:09:18.740
is multiplication by x.

00:09:18.740 --> 00:09:27.190
So this is defined
as the map that

00:09:27.190 --> 00:09:30.550
simply multiplies
the polynomial by x.

00:09:30.550 --> 00:09:32.800
Of course, this gives
you another polynomial.

00:09:32.800 --> 00:09:36.211
And, again, you can check easily
that it satisfies these two

00:09:36.211 --> 00:09:36.710
conditions.

00:09:47.640 --> 00:09:50.990
So this gives you a
sense of why things

00:09:50.990 --> 00:09:53.630
that don't appear
to be matrix-like

00:09:53.630 --> 00:09:56.570
can still be viewed in
this operator picture.

00:09:59.250 --> 00:10:02.960
Another example,
which you'll see

00:10:02.960 --> 00:10:06.750
later shows some of the
slightly paradoxical features

00:10:06.750 --> 00:10:12.260
of infinite dimensional
vector spaces,

00:10:12.260 --> 00:10:16.630
comes from the vector space
of infinite sequences.

00:10:16.630 --> 00:10:30.100
So these are all the infinite
sequences of reals or complexes

00:10:30.100 --> 00:10:33.490
or whatever f is.

00:10:33.490 --> 00:10:42.190
One operator we can define
is the left shift operator,

00:10:42.190 --> 00:10:50.930
which is simply defined by
shifting this entire infinite

00:10:50.930 --> 00:10:54.540
sequence left by one
place and throwing away

00:10:54.540 --> 00:10:57.250
the first position.

00:10:57.250 --> 00:11:01.750
So you start with
x2, x3, and so.

00:11:01.750 --> 00:11:03.480
Still goes to
infinity so it still

00:11:03.480 --> 00:11:06.280
gives you an infinite sequence.

00:11:06.280 --> 00:11:08.840
So it is indeed a map-- that's
the first thing you should

00:11:08.840 --> 00:11:12.020
check that this is indeed
a map from v to itself--

00:11:12.020 --> 00:11:14.769
and you can also check
that it's linear,

00:11:14.769 --> 00:11:16.310
that it satisfies
these two products.

00:11:18.920 --> 00:11:21.530
Another example is right shift.

00:11:26.310 --> 00:11:32.818
And here-- Yeah?

00:11:32.818 --> 00:11:34.740
AUDIENCE: So left shift
was the first one or--

00:11:34.740 --> 00:11:36.980
ARAM HARROW: That's right.

00:11:36.980 --> 00:11:41.820
So there's no back, really.

00:11:41.820 --> 00:11:42.780
It's a good point.

00:11:42.780 --> 00:11:46.710
So you'd like to not throw
out the first one, perhaps,

00:11:46.710 --> 00:11:50.160
but there's no canonical
place to put it in.

00:11:50.160 --> 00:11:55.270
This just goes off to infinity
and just falls off the edge.

00:11:55.270 --> 00:11:58.310
It's a little bit
like differentiation.

00:11:58.310 --> 00:11:58.811
Right?

00:11:58.811 --> 00:11:59.435
AUDIENCE: Yeah.

00:11:59.435 --> 00:12:01.166
I guess it loses
some information.

00:12:01.166 --> 00:12:02.790
ARAM HARROW: It loses
some information.

00:12:02.790 --> 00:12:04.260
That's right.

00:12:04.260 --> 00:12:05.570
It's a little bit weird, right?

00:12:05.570 --> 00:12:08.080
Because how many
numbers do you have

00:12:08.080 --> 00:12:10.830
before you applied
the left shift?

00:12:10.830 --> 00:12:11.820
Infinity.

00:12:11.820 --> 00:12:14.450
How many do you have after
you applied the left shift?

00:12:14.450 --> 00:12:15.620
Infinity.

00:12:15.620 --> 00:12:17.680
But you lost some information.

00:12:17.680 --> 00:12:22.311
So you have to be a little
careful with the infinities.

00:12:22.311 --> 00:12:22.810
OK

00:12:22.810 --> 00:12:25.940
The right shift.

00:12:25.940 --> 00:12:30.500
Here it's not so
obvious what to do.

00:12:30.500 --> 00:12:36.140
We've kind of made space
for another number,

00:12:36.140 --> 00:12:38.450
and so we have to put something
in that first position.

00:12:38.450 --> 00:12:47.480
So this will be question
mark x1, x2, dot, dot, dot.

00:12:47.480 --> 00:12:50.420
Any guesses what should
go in the question mark?

00:12:50.420 --> 00:12:51.380
AUDIENCE: 0?

00:12:51.380 --> 00:12:52.180
ARAM HARROW: 0.

00:12:52.180 --> 00:12:52.680
Right.

00:12:56.410 --> 00:12:58.060
And why should that be 0?

00:12:58.060 --> 00:12:59.000
AUDIENCE: [INAUDIBLE].

00:12:59.000 --> 00:12:59.490
ARAM HARROW: What's that?

00:12:59.490 --> 00:13:00.532
AUDIENCE: So it's linear.

00:13:00.532 --> 00:13:02.406
ARAM HARROW: Otherwise
it wouldn't be linear.

00:13:02.406 --> 00:13:03.100
Right.

00:13:03.100 --> 00:13:04.600
So imagine what
happens if you apply

00:13:04.600 --> 00:13:07.920
the right shift to
the all 0 string.

00:13:07.920 --> 00:13:10.390
If you were to get
something non-zero here,

00:13:10.390 --> 00:13:14.420
then you would map to the 0
vector to a non-zero vector.

00:13:14.420 --> 00:13:18.130
But, by linearity,
that's impossible.

00:13:18.130 --> 00:13:22.340
Because I could take any vector
and multiply it by the scalar

00:13:22.340 --> 00:13:25.180
0 and I get the vector 0.

00:13:25.180 --> 00:13:28.260
And that should be
equal to the scalar

00:13:28.260 --> 00:13:32.360
0 multiplied by
the output of it.

00:13:32.360 --> 00:13:39.260
And so that means that T
should always map 0 to 0.

00:13:39.260 --> 00:13:41.900
T should always map the
vector 0 to the vector 0.

00:13:41.900 --> 00:13:45.400
And so if we want right shift
to be a linear operator,

00:13:45.400 --> 00:13:46.820
we have to put a 0 in there.

00:13:50.720 --> 00:13:56.030
And this one is strange also
because it creates more space

00:13:56.030 --> 00:13:58.330
but still preserves
all of the information.

00:14:01.330 --> 00:14:08.500
So two other small examples
of linear operators

00:14:08.500 --> 00:14:10.090
that come up very often.

00:14:10.090 --> 00:14:16.930
There's, of course,
the 0 operator,

00:14:16.930 --> 00:14:20.180
which takes any vector
to the 0 vector.

00:14:20.180 --> 00:14:22.960
Here I'm not distinguishing
between-- here

00:14:22.960 --> 00:14:25.130
the 0 means an operator,
here it means a vector.

00:14:25.130 --> 00:14:33.060
I guess I can
clarify it that way.

00:14:33.060 --> 00:14:36.380
And this is, of course, linear
and sends any vector space

00:14:36.380 --> 00:14:37.500
to itself.

00:14:37.500 --> 00:14:39.850
One important thing
is that the output

00:14:39.850 --> 00:14:42.370
doesn't have to be the
entire vector space.

00:14:42.370 --> 00:14:44.510
The fact that it sends
a vector space to itself

00:14:44.510 --> 00:14:47.040
only means that the output is
contained within the vector

00:14:47.040 --> 00:14:47.820
space.

00:14:47.820 --> 00:14:49.580
It could be something
as boring is

00:14:49.580 --> 00:14:54.630
0 that just sends all the
vectors to a single point.

00:14:54.630 --> 00:14:57.000
And finally, one other
important operator

00:14:57.000 --> 00:15:04.470
is the identity operator
that sends-- actually I

00:15:04.470 --> 00:15:08.587
won't use the arrows here.

00:15:08.587 --> 00:15:10.170
We'll get used to
the mathematical way

00:15:10.170 --> 00:15:13.220
of writing it-- that sends
any vector to itself.

00:15:32.980 --> 00:15:35.600
Those are a few
examples of operators.

00:15:35.600 --> 00:15:39.900
I guess you've seen already
kind of the more familiar

00:15:39.900 --> 00:15:44.660
matrix-type of operators,
but these show you

00:15:44.660 --> 00:15:46.670
also the range of
what is possible.

00:15:50.680 --> 00:15:58.760
So the space l of v
of all operators--

00:15:58.760 --> 00:16:01.950
I want to talk now
about its properties.

00:16:01.950 --> 00:16:08.100
So l of v is the space of all
linear maps from v to itself.

00:16:08.100 --> 00:16:11.100
So this is the space of
maps on a vector space,

00:16:11.100 --> 00:16:13.930
but itself is also
a vector space.

00:16:21.910 --> 00:16:26.550
So the set of operators
satisfies all the axioms

00:16:26.550 --> 00:16:27.600
of a vector space.

00:16:27.600 --> 00:16:31.010
It contains a 0 operator.

00:16:31.010 --> 00:16:33.680
That's this one right here.

00:16:33.680 --> 00:16:35.750
It's closed under a
linear combination.

00:16:35.750 --> 00:16:38.040
If I add together
two linear operators,

00:16:38.040 --> 00:16:39.990
I get another linear operator.

00:16:39.990 --> 00:16:41.830
It's closed under a
scalar multiplication.

00:16:41.830 --> 00:16:44.200
If I multiply a linear
operator by a scalar,

00:16:44.200 --> 00:16:48.500
I get another linear
operator, et cetera.

00:16:48.500 --> 00:16:51.150
And so everything we can
do on a vector space,

00:16:51.150 --> 00:16:53.280
like finding a
basis and so on, we

00:16:53.280 --> 00:16:58.280
can do for the space
of linear operators.

00:16:58.280 --> 00:17:02.080
However, in addition to having
the vector space structure,

00:17:02.080 --> 00:17:05.745
it has an additional structure,
which is multiplication.

00:17:17.349 --> 00:17:21.690
And here we're finally
making use of the fact

00:17:21.690 --> 00:17:26.240
that we're talking about
linear maps from a vector space

00:17:26.240 --> 00:17:27.589
to itself.

00:17:27.589 --> 00:17:31.140
If we were talking
about maps from v to w,

00:17:31.140 --> 00:17:35.060
we couldn't necessarily multiply
them by other maps from v to w,

00:17:35.060 --> 00:17:40.401
we could only multiply them by
maps from w to something else.

00:17:40.401 --> 00:17:41.900
Just like how, if
you're multiplying

00:17:41.900 --> 00:17:45.050
rectangular matrices,
the multiplication is not

00:17:45.050 --> 00:17:48.430
always defined if the
dimensions don't match up,

00:17:48.430 --> 00:17:52.730
But since these operators
are like square matrices,

00:17:52.730 --> 00:17:54.880
multiplication is
always defined,

00:17:54.880 --> 00:18:01.330
and this can be used to prove
many nice things about them.

00:18:01.330 --> 00:18:03.500
So this type of
structure being a vector

00:18:03.500 --> 00:18:06.970
space of multiplication
makes it, in many ways,

00:18:06.970 --> 00:18:11.910
like a field-- like real
numbers or complexes--

00:18:11.910 --> 00:18:14.760
but without all
of the properties.

00:18:14.760 --> 00:18:29.610
So the properties that the
multiplication does have first

00:18:29.610 --> 00:18:32.960
is that it's associative.

00:18:32.960 --> 00:18:35.640
So let's see what
this looks like.

00:18:35.640 --> 00:18:51.270
So if we have a times bc
is equal to ab times c.

00:18:51.270 --> 00:19:00.830
And the way we can
check this is just

00:19:00.830 --> 00:19:04.180
by verifying the action
of this on any vector.

00:19:04.180 --> 00:19:10.440
So an operator is defined
by its action and all

00:19:10.440 --> 00:19:13.340
of the vectors in
a vector space.

00:19:13.340 --> 00:19:23.430
So the definition
of ab can be thought

00:19:23.430 --> 00:19:31.150
of as asking how does it act
on all the possible vectors?

00:19:31.150 --> 00:19:39.600
And this is defined just in
terms of the action of a and b

00:19:39.600 --> 00:19:44.940
as you first apply b
and then you apply A.

00:19:44.940 --> 00:19:47.170
So this can be thought
of as the definition

00:19:47.170 --> 00:19:49.500
of how to multiply operators.

00:19:49.500 --> 00:19:51.970
And then from this,
you can easily

00:19:51.970 --> 00:19:56.100
check the associativity property
that in both cases, however

00:19:56.100 --> 00:20:06.880
you write it out, you
obtain A of B of C of v.

00:20:06.880 --> 00:20:09.440
I'm writing out
all the parentheses

00:20:09.440 --> 00:20:12.275
just to sort of emphasize
this is C acting on v,

00:20:12.275 --> 00:20:16.910
and then B acting on C of v, and
then A acting on all of this.

00:20:16.910 --> 00:20:20.820
The fact that this is equal--
that this is the same no matter

00:20:20.820 --> 00:20:23.260
how A, B, and C are
grouped is again

00:20:23.260 --> 00:20:27.500
part of what let's us
justify this right here,

00:20:27.500 --> 00:20:30.310
where we drop-- we just
don't use parentheses

00:20:30.310 --> 00:20:34.600
when we have operators acting.

00:20:34.600 --> 00:20:38.560
So, yes, we have the
associative property.

00:20:38.560 --> 00:20:41.460
Another property of
multiplication that operators

00:20:41.460 --> 00:20:47.050
satisfy is the existence
of an identity.

00:20:47.050 --> 00:20:50.400
That's just the
identity operator, here,

00:20:50.400 --> 00:20:52.870
which for any vector space
can always be defined.

00:20:54.951 --> 00:20:56.950
But there are other
properties of multiplication

00:20:56.950 --> 00:20:59.370
that it doesn't have.

00:20:59.370 --> 00:21:08.890
So inverses are
not always defined.

00:21:08.890 --> 00:21:10.730
They sometimes are.

00:21:10.730 --> 00:21:12.980
I can't say that a matrix
is never invertible,

00:21:12.980 --> 00:21:16.410
but for things like the
reals and the complexes,

00:21:16.410 --> 00:21:19.840
every nonzero element
has an inverse.

00:21:19.840 --> 00:21:21.380
And for matrices,
that's not true.

00:21:24.250 --> 00:21:27.850
And another property-- a
more interesting one that

00:21:27.850 --> 00:21:36.691
these lack-- is that the
multiplication is not

00:21:36.691 --> 00:21:37.190
commutative.

00:21:41.420 --> 00:21:44.340
So this is something that
you've seen for matrices.

00:21:44.340 --> 00:21:47.150
If you multiply two
matrices, the order matters,

00:21:47.150 --> 00:21:50.635
and so it's not surprising that
same is true for operators.

00:21:59.260 --> 00:22:03.110
And just to give a
quick example of that,

00:22:03.110 --> 00:22:08.280
let's look at this example
one here with polynomials.

00:22:08.280 --> 00:22:17.610
And let's consider S times
T acting on the monomial x

00:22:17.610 --> 00:22:20.500
to the n.

00:22:20.500 --> 00:22:24.400
So T is differentiation
so it sends

00:22:24.400 --> 00:22:28.410
this to n times x
to the n minus 1.

00:22:28.410 --> 00:22:33.790
So we get S times n,
x to the n minus 1.

00:22:33.790 --> 00:22:38.570
Linearity means we can move
the n past the S. S acting here

00:22:38.570 --> 00:22:46.250
multiplies by x, and so
we get n times x to the n.

00:22:46.250 --> 00:22:52.040
Whereas if we did
the other order,

00:22:52.040 --> 00:22:57.240
we get T times S acting
on x to the n, which

00:22:57.240 --> 00:22:59.740
is x to the n plus 1.

00:22:59.740 --> 00:23:10.910
When you differentiate this you
get n plus 1 times x to the n.

00:23:10.910 --> 00:23:12.600
So these numbers are
different meaning

00:23:12.600 --> 00:23:16.550
that S and T do not commute.

00:23:16.550 --> 00:23:20.320
And it's kind of cute to
measure to what extent do they

00:23:20.320 --> 00:23:22.740
not commute.

00:23:22.740 --> 00:23:25.320
This is done by the commutator.

00:23:25.320 --> 00:23:28.750
And what these equations say
is that if the commutator acts

00:23:28.750 --> 00:23:33.650
on x to the n, then
you get n plus 1 times

00:23:33.650 --> 00:23:39.130
x to the n minus n
times x to the n,

00:23:39.130 --> 00:23:40.280
which is just x to the n.

00:23:43.600 --> 00:23:49.160
And we can write this
another way as identity

00:23:49.160 --> 00:23:50.400
times x to the n.

00:23:54.280 --> 00:23:59.990
And since this is true
for any choice of n,

00:23:59.990 --> 00:24:03.420
it's true for what
turns out to be

00:24:03.420 --> 00:24:05.790
a basis for the
space of polynomials.

00:24:05.790 --> 00:24:11.540
So 1x, x squared,
x cubed, et cetera,

00:24:11.540 --> 00:24:13.960
these span the space
of polynomials.

00:24:13.960 --> 00:24:15.904
So if you know what an
operator does and all

00:24:15.904 --> 00:24:17.320
of the x to the
n's, you know what

00:24:17.320 --> 00:24:20.030
it does on all the polynomials.

00:24:20.030 --> 00:24:26.540
And so this means, actually,
that the commutator

00:24:26.540 --> 00:24:28.610
of these two is the identity.

00:24:34.670 --> 00:24:42.545
And so the significance
of this is-- well,

00:24:42.545 --> 00:24:44.670
I won't dwell on the physical
significance of this,

00:24:44.670 --> 00:24:48.860
but it's related to what you've
seen for position and momentum.

00:24:48.860 --> 00:24:51.040
And essentially the fact
that these don't commute

00:24:51.040 --> 00:24:54.520
is actually an important
feature of the theory.

00:25:00.370 --> 00:25:06.180
So these are some of
the key properties

00:25:06.180 --> 00:25:09.300
of the space of operators.

00:25:09.300 --> 00:25:10.970
I want to also now
tell you about some

00:25:10.970 --> 00:25:15.630
of the key properties
of individual operators.

00:25:15.630 --> 00:25:18.340
And basically, if
you're given an operator

00:25:18.340 --> 00:25:21.090
and want to know the
gross features of it,

00:25:21.090 --> 00:25:22.490
what should you look at?

00:25:25.070 --> 00:25:33.710
So one of these things is the
null space of an operator.

00:25:33.710 --> 00:25:45.440
So this is the set of
all v, of all vectors,

00:25:45.440 --> 00:25:46.820
that are killed by the operator.

00:25:46.820 --> 00:25:47.790
They're sent to 0.

00:25:52.740 --> 00:25:56.990
In some case-- so this will
always include the vector 0.

00:26:00.350 --> 00:26:06.950
So this always at least includes
the vector 0, but in some cases

00:26:06.950 --> 00:26:09.360
it will be a lot bigger.

00:26:09.360 --> 00:26:11.740
So for the identity
operator, the null space

00:26:11.740 --> 00:26:12.985
is only the vector 0.

00:26:12.985 --> 00:26:16.270
The only thing that gets
sent to 0 is 0 itself.

00:26:16.270 --> 00:26:20.450
Whereas, for the 0 operator,
everything gets sent to 0.

00:26:20.450 --> 00:26:24.595
So the null space is
the entire vector space.

00:26:24.595 --> 00:26:32.220
For left shift, the null space
is only 0 itself-- sorry,

00:26:32.220 --> 00:26:37.840
for right shift the null
space is only 0 itself.

00:26:37.840 --> 00:26:39.730
And what about for left shift?

00:26:39.730 --> 00:26:41.496
What's the null space here?

00:26:41.496 --> 00:26:41.995
Yeah?

00:26:41.995 --> 00:26:44.310
AUDIENCE: Some numer with a
string of 0s following it.

00:26:44.310 --> 00:26:45.101
ARAM HARROW: Right.

00:26:45.101 --> 00:26:47.310
Any sequence where
the first number

00:26:47.310 --> 00:26:51.920
is arbitrary, but everything
after the first number is 0.

00:26:51.920 --> 00:26:55.500
And so from all
of these examples

00:26:55.500 --> 00:26:58.510
you might guess that this
is a linear subspace,

00:26:58.510 --> 00:27:02.620
because in every case it's been
a vector space, and, in fact,

00:27:02.620 --> 00:27:04.220
this is correct.

00:27:04.220 --> 00:27:13.050
So this is a subspace
of v because, if there's

00:27:13.050 --> 00:27:15.820
a vector that gets sent
to 0, any multiple of it

00:27:15.820 --> 00:27:17.800
also will be sent to 0.

00:27:17.800 --> 00:27:19.760
And of the two vectors
that get sent to 0,

00:27:19.760 --> 00:27:22.830
their sum will
also be sent to 0.

00:27:22.830 --> 00:27:31.010
So the fact that it's
a linear subspace

00:27:31.010 --> 00:27:35.490
can be a helpful way of
understanding this set.

00:27:35.490 --> 00:27:40.090
And it's related to the
properties of T as a function.

00:27:40.090 --> 00:27:43.290
So for a function we often want
to know whether it's 1 to 1,

00:27:43.290 --> 00:27:47.620
or injective, or whether it's
[? onto ?] or surjective.

00:27:47.620 --> 00:27:59.940
And you can check that
if T is injective,

00:27:59.940 --> 00:28:11.010
meaning that if u is not
equal to v, then T of u

00:28:11.010 --> 00:28:14.340
is not equal to T of
v. So this property,

00:28:14.340 --> 00:28:19.790
that T maps distinct vectors
two distinct vectors,

00:28:19.790 --> 00:28:22.290
turns out to be equivalent
to the null space

00:28:22.290 --> 00:28:23.660
being only the 0 vector.

00:28:27.040 --> 00:28:27.870
So why is that?

00:28:34.900 --> 00:28:37.925
This statement here, that
whenever u is not equal to v,

00:28:37.925 --> 00:28:41.070
T of u is not equal
to T of v, another way

00:28:41.070 --> 00:28:50.166
to write that is whenever u is
not equal to v, T of u minus v

00:28:50.166 --> 00:28:51.450
is not equal to 0.

00:28:54.600 --> 00:28:58.030
And if you look
at this statement

00:28:58.030 --> 00:29:00.260
a little more carefully,
you'll realize

00:29:00.260 --> 00:29:04.580
that all we cared about on
both sides was u minus v. Here,

00:29:04.580 --> 00:29:07.010
obviously, we care
about u minus v. Here

00:29:07.010 --> 00:29:11.090
we only care if u
is not equal to v.

00:29:11.090 --> 00:29:18.910
So that's the same as saying
if u minus v is non-zero,

00:29:18.910 --> 00:29:24.630
then T of u minus v is non-zero.

00:29:24.630 --> 00:29:30.180
And this in turn is
equivalent to saying

00:29:30.180 --> 00:29:36.010
that the null space
of T is only 0.

00:29:36.010 --> 00:29:40.640
In other words, the set of
vectors that get sent to 0

00:29:40.640 --> 00:29:43.090
consists only of
the 0 vector itself.

00:29:46.710 --> 00:29:50.470
So the null space
for linear operators

00:29:50.470 --> 00:29:53.290
is how we can characterize
whether they're 1 to 1,

00:29:53.290 --> 00:29:55.160
whether they destroy
any information.

00:30:14.780 --> 00:30:19.060
The other subspace that will
be important that we will use

00:30:19.060 --> 00:30:20.585
is the range of an operator.

00:30:28.800 --> 00:30:34.970
So the range of an operator,
which we can also just

00:30:34.970 --> 00:30:44.260
write as T of v, is the set of
all points that vectors in v

00:30:44.260 --> 00:30:45.190
get mapped to.

00:30:45.190 --> 00:30:55.290
So the set of all Tv
for some vector, v.

00:30:55.290 --> 00:31:00.120
So this, too, can be
shown to be a subspace.

00:31:13.100 --> 00:31:19.310
And that's because-- it takes
a little more work to show it,

00:31:19.310 --> 00:31:23.850
but not very much-- if there's
something in the output of T,

00:31:23.850 --> 00:31:26.870
then whatever the
corresponding input is

00:31:26.870 --> 00:31:30.220
we could have multiplied
that by a scalar.

00:31:30.220 --> 00:31:32.790
And then the
corresponding output also

00:31:32.790 --> 00:31:35.010
would get multiplied
by a scalar,

00:31:35.010 --> 00:31:38.910
and so that, too,
would be in the range.

00:31:38.910 --> 00:31:41.140
And so that means that
for anything in the range,

00:31:41.140 --> 00:31:43.450
we can multiply it by
any scalar and again get

00:31:43.450 --> 00:31:45.110
something in the range.

00:31:45.110 --> 00:31:46.190
Similarly for addition.

00:31:46.190 --> 00:31:48.050
A similar argument
shows that the range

00:31:48.050 --> 00:31:49.150
is closed under addition.

00:31:49.150 --> 00:31:53.290
So indeed, it's a
linear subspace.

00:31:53.290 --> 00:31:58.180
Again, since it's a linear
subspace, it always contains 0.

00:31:58.180 --> 00:32:05.010
And depending on the operator,
may contain a lot more.

00:32:05.010 --> 00:32:08.550
So whereas the null
space determined

00:32:08.550 --> 00:32:10.870
whether T was
injective, the range

00:32:10.870 --> 00:32:13.680
determines whether
T is surjective.

00:32:13.680 --> 00:32:29.420
So the range of T equals v if
and only if T is surjective.

00:32:34.040 --> 00:32:38.620
And here this is simply the
definition of being surjective.

00:32:38.620 --> 00:32:41.590
It's not really a theorem
like it was in the case of T

00:32:41.590 --> 00:32:43.180
being injective.

00:32:43.180 --> 00:32:45.230
Here that's really what
it means to be surjective

00:32:45.230 --> 00:32:46.990
is that your output
is the entire space.

00:32:50.280 --> 00:32:53.330
So one important
property of the range

00:32:53.330 --> 00:32:57.160
of the null space whenever
v is finite dimensional

00:32:57.160 --> 00:33:08.200
is that the dimension of v
is equal to the dimension

00:33:08.200 --> 00:33:19.275
of the null space plus the
dimension of the range.

00:33:23.970 --> 00:33:29.390
And this is actually
not trivial to prove.

00:33:29.390 --> 00:33:33.118
And I'm actually not going
to prove it right now.

00:33:33.118 --> 00:33:37.020
But the intuition
of it is as follows.

00:33:37.020 --> 00:33:40.510
Imagine that v is some
n dimensional space

00:33:40.510 --> 00:33:43.430
and the null space
has dimension k.

00:33:43.430 --> 00:33:47.370
So that means you have input
of n degrees of freedom,

00:33:47.370 --> 00:33:50.320
but T kills k of n.

00:33:50.320 --> 00:33:52.320
And so k at different
degrees of freedom,

00:33:52.320 --> 00:33:54.960
no matter how you vary them,
have no effect on the output.

00:33:54.960 --> 00:33:56.860
They just get mapped to 0.

00:33:56.860 --> 00:34:00.480
And so what's left are n
minus k degrees of freedom

00:34:00.480 --> 00:34:01.950
that do affect the outcome.

00:34:01.950 --> 00:34:05.820
Where, if you vary them, it does
change the output in some way.

00:34:05.820 --> 00:34:09.350
And those correspond to
the n minus k dimensions

00:34:09.350 --> 00:34:10.924
of the range.

00:34:10.924 --> 00:34:12.340
And if you want
to get formal, you

00:34:12.340 --> 00:34:13.798
have to formalize
what I was saying

00:34:13.798 --> 00:34:16.991
about what's left is n minus k.

00:34:16.991 --> 00:34:18.949
If you talk about something
like the orthogonal

00:34:18.949 --> 00:34:23.179
complement or completing
a basis or in some way

00:34:23.179 --> 00:34:26.290
formalize that intuition.

00:34:26.290 --> 00:34:28.040
And, in fact, you can
do a little further,

00:34:28.040 --> 00:34:30.180
and you can decompose the space.

00:34:30.180 --> 00:34:31.679
So this is just
dimensions counting.

00:34:31.679 --> 00:34:35.679
You can even decompose the
space into the null space

00:34:35.679 --> 00:34:40.760
and the complement of that
and show that T is 1 to 1

00:34:40.760 --> 00:34:44.929
on the complement
of the null space.

00:34:44.929 --> 00:34:48.330
But for now, I think this is
all that we'll need for now.

00:34:50.989 --> 00:34:52.861
Any questions so far?

00:34:52.861 --> 00:34:53.360
Yeah?

00:34:53.360 --> 00:34:57.380
AUDIENCE: Why isn't the null
space part of the range?

00:34:57.380 --> 00:34:59.320
ARAM HARROW: Why isn't
it part of the range?

00:34:59.320 --> 00:35:01.280
AUDIENCE: So you're
taking T of v

00:35:01.280 --> 00:35:05.004
and null space is just the
special case when T of v

00:35:05.004 --> 00:35:06.400
is equal to 0.

00:35:06.400 --> 00:35:07.720
ARAM HARROW: Right.

00:35:07.720 --> 00:35:19.480
So the null space are all
of the-- This theorem,

00:35:19.480 --> 00:35:23.799
I guess, would be a little bit
more surprising if you realized

00:35:23.799 --> 00:35:25.340
that it works not
only for operators,

00:35:25.340 --> 00:35:27.740
but for general linear maps.

00:35:27.740 --> 00:35:32.300
And in that case, the range
is a subset space of w.

00:35:32.300 --> 00:35:34.680
Because the range
is about the output.

00:35:34.680 --> 00:35:36.740
And the null space is
a [? subset ?] space

00:35:36.740 --> 00:35:39.570
of v, which is
part of the input.

00:35:39.570 --> 00:35:42.310
And so in that case,
they're not even comparable.

00:35:42.310 --> 00:35:44.860
The vectors might just
have different lengths.

00:35:44.860 --> 00:35:51.212
And so it can never-- like
the null space in a range,

00:35:51.212 --> 00:35:53.420
in that case, would live in
totally different spaces.

00:35:58.690 --> 00:36:01.600
So let me give you a
very simple example.

00:36:01.600 --> 00:36:10.680
Let's suppose that T is
equal to 3, 0, minus 1, 4.

00:36:10.680 --> 00:36:14.690
So just a diagonal
4 by 4 matrix.

00:36:14.690 --> 00:36:22.840
Then the null space
would be the span

00:36:22.840 --> 00:36:28.440
of e2, that's the vector with
a 1 in the second position.

00:36:31.770 --> 00:36:42.680
And the range would be
the span of e1, e3, in e4.

00:36:42.680 --> 00:36:44.930
So in fact, usually it's
the opposite that happens.

00:36:44.930 --> 00:36:48.260
The null space in
the range are--

00:36:48.260 --> 00:36:51.105
in this case they're actually
orthogonal subspaces.

00:36:58.100 --> 00:37:03.040
But this picture is
actually a little bit

00:37:03.040 --> 00:37:05.180
deceptive in how nice it is.

00:37:05.180 --> 00:37:06.830
So if you look at
this, total space

00:37:06.830 --> 00:37:09.350
is 4, four dimensions,
it divides up

00:37:09.350 --> 00:37:11.360
into one dimension
that gets killed,

00:37:11.360 --> 00:37:15.674
and three dimensions where
the output still tells you

00:37:15.674 --> 00:37:17.340
something about the
input, where there's

00:37:17.340 --> 00:37:20.920
some variation of the output.

00:37:20.920 --> 00:37:29.850
But this picture makes it seem--
the simplicity of this picture

00:37:29.850 --> 00:37:30.860
does not always exist.

00:37:35.910 --> 00:37:47.340
A much more horrible
example is this matrix.

00:37:55.840 --> 00:37:57.735
So what's the null
space of this matrix?

00:38:03.940 --> 00:38:04.702
Yeah?

00:38:04.702 --> 00:38:07.118
AUDIENCE: You just don't care
about the upper [INAUDIBLE].

00:38:07.118 --> 00:38:16.902
ARAM HARROW: You don't care
about the-- informally,

00:38:16.902 --> 00:38:18.110
it's everything of this form.

00:38:18.110 --> 00:38:20.470
Everything with something
in the first position,

00:38:20.470 --> 00:38:21.705
0 in the second position.

00:38:21.705 --> 00:38:27.350
In other words,
it's the span of e1.

00:38:32.476 --> 00:38:33.350
What about the range?

00:38:38.853 --> 00:38:40.280
AUDIENCE: [INAUDIBLE].

00:38:40.280 --> 00:38:41.321
ARAM HARROW: What's that?

00:38:45.769 --> 00:38:46.269
Yeah?

00:38:46.269 --> 00:38:48.764
AUDIENCE: [INAUDIBLE].

00:38:48.764 --> 00:38:50.261
ARAM HARROW: It's actually--

00:38:50.261 --> 00:38:51.270
AUDIENCE: Isn't it e1?

00:38:51.270 --> 00:38:52.353
ARAM HARROW: It's also e1.

00:38:52.353 --> 00:38:53.446
It's the same thing.

00:38:59.410 --> 00:39:02.570
So you have this intuition
that some degrees of freedom

00:39:02.570 --> 00:39:06.010
are preserved and
some are killed.

00:39:06.010 --> 00:39:08.940
And here they look
totally different.

00:39:08.940 --> 00:39:11.600
And there they look the same.

00:39:11.600 --> 00:39:14.240
So you should be a
little bit nervous

00:39:14.240 --> 00:39:16.730
about trying to
apply that intuition.

00:39:16.730 --> 00:39:19.400
You should be reassured that
at least the theorem is still

00:39:19.400 --> 00:39:20.610
true.

00:39:20.610 --> 00:39:23.465
At least 1 plus 1 is equal to 2.

00:39:23.465 --> 00:39:27.120
We still have that.

00:39:27.120 --> 00:39:31.630
But the null space and the
range are the same thing here.

00:39:31.630 --> 00:39:36.530
And the way around
that paradox-- Yeah?

00:39:36.530 --> 00:39:38.363
AUDIENCE: So can you
just change the basis--

00:39:38.363 --> 00:39:39.766
is there always
a way of changing

00:39:39.766 --> 00:39:40.765
the basis of the matrix?

00:39:40.765 --> 00:39:43.582
In this case it
becomes [INAUDIBLE]?

00:39:43.582 --> 00:39:44.580
Or not necessarily?

00:39:44.580 --> 00:39:45.246
ARAM HARROW: No.

00:39:45.246 --> 00:39:47.460
It turns out that, even
with the change of basis,

00:39:47.460 --> 00:39:51.150
you cannot guarantee that the
null space and the range will

00:39:51.150 --> 00:39:52.850
be perpendicular.

00:39:52.850 --> 00:39:54.320
Yeah?

00:39:54.320 --> 00:40:01.250
AUDIENCE: What if you reduce
it to only measures on the--

00:40:01.250 --> 00:40:04.220
or what if you reduce the
matrix of-- [? usability ?]

00:40:04.220 --> 00:40:07.390
on only [INAUDIBLE]
on the diagonal?

00:40:07.390 --> 00:40:08.390
ARAM HARROW: Right Good.

00:40:08.390 --> 00:40:16.620
So if you do that, then-- if you
do row [? eduction ?] with two

00:40:16.620 --> 00:40:19.170
different row and
column operations,

00:40:19.170 --> 00:40:21.320
then what you've done is
you have a different input

00:40:21.320 --> 00:40:23.540
and output basis.

00:40:23.540 --> 00:40:31.160
And so that would-- then
once you kind of unpack

00:40:31.160 --> 00:40:33.770
what's going on in
terms of the basis,

00:40:33.770 --> 00:40:37.360
then it would turn out
that you could still

00:40:37.360 --> 00:40:39.294
have strange behavior like this.

00:40:39.294 --> 00:40:40.710
What your intuition
is based on is

00:40:40.710 --> 00:40:43.330
that if the matrix is
diagonal in some basis,

00:40:43.330 --> 00:40:44.770
then you don't
have this trouble.

00:40:44.770 --> 00:40:47.890
But the problem is that not all
matrices can be diagonalized.

00:40:47.890 --> 00:40:49.354
Yeah?

00:40:49.354 --> 00:40:50.818
AUDIENCE: So is it
just the trouble

00:40:50.818 --> 00:40:53.258
that the null is
what you're acting on

00:40:53.258 --> 00:40:56.136
and the range is
what results from it?

00:40:56.136 --> 00:40:57.010
ARAM HARROW: Exactly.

00:40:57.010 --> 00:41:00.470
And they could even
live in different space.

00:41:00.470 --> 00:41:06.710
And so they really just don't--
to compare them is dangerous.

00:41:06.710 --> 00:41:12.010
So it turns out that the
degrees of freedom corresponding

00:41:12.010 --> 00:41:14.820
to the range-- what
you should think about

00:41:14.820 --> 00:41:18.620
are the degrees of freedom
that get sent to the range.

00:41:18.620 --> 00:41:21.490
And in this case,
that would be e2.

00:41:21.490 --> 00:41:26.720
And so then you can say that
e1 gets sent to 0 and e2

00:41:26.720 --> 00:41:28.130
gets sent to the range.

00:41:28.130 --> 00:41:31.520
And now you really have
decomposed the input space

00:41:31.520 --> 00:41:34.730
into two orthogonal parts.

00:41:34.730 --> 00:41:37.800
And because we're talking
about a single space, the input

00:41:37.800 --> 00:41:39.990
space, it actually makes
sense to break it up

00:41:39.990 --> 00:41:41.710
into these parts.

00:41:41.710 --> 00:41:45.790
Whereas here, they look like
they're the same, but really

00:41:45.790 --> 00:41:47.400
input and output
spaces you should

00:41:47.400 --> 00:41:51.260
think of as
potentially different.

00:41:51.260 --> 00:41:57.400
So this is just a mild
warning about reading too much

00:41:57.400 --> 00:42:02.100
into this formula, even
though it's the rough idea

00:42:02.100 --> 00:42:05.170
it counting degrees of freedom
is still roughly accurate.

00:42:09.430 --> 00:42:18.300
So I want to say one more thing
about properties of operators,

00:42:18.300 --> 00:42:20.600
which is about invertibility.

00:42:20.600 --> 00:42:24.430
And maybe I'll leave
this up for now.

00:42:41.090 --> 00:42:45.780
So we say that a
linear operator, T,

00:42:45.780 --> 00:42:59.770
has a left inverse, S, if
multiplying T on the left by s

00:42:59.770 --> 00:43:02.750
will give you the identity.

00:43:02.750 --> 00:43:12.706
And T has a right
inverse, S prime,

00:43:12.706 --> 00:43:16.160
you can guess what
will happen here

00:43:16.160 --> 00:43:22.140
if multiplying T on the right
by S prime gives you identity.

00:43:22.140 --> 00:43:25.728
And what if T has both?

00:43:25.728 --> 00:43:31.620
Then in that next
case, it turns out

00:43:31.620 --> 00:43:35.447
that S and S prime
have to be the same.

00:43:35.447 --> 00:43:36.280
So here's the proof.

00:43:36.280 --> 00:43:55.140
So if both exist, then S is
equal to s times identity--

00:43:55.140 --> 00:43:57.090
by the definition
of the identity.

00:43:57.090 --> 00:44:00.225
And then we can replace
identity with TS prime.

00:44:05.370 --> 00:44:13.810
Then we can group these and
cancel them and get S prime.

00:44:13.810 --> 00:44:18.150
So if a matrix has both a
left and a right inverse,

00:44:18.150 --> 00:44:21.860
then it turns out that the left
and right inverse are the same.

00:44:21.860 --> 00:44:31.740
And in this case, we say
that T is invertible,

00:44:31.740 --> 00:44:50.330
and we define T inverse to be S.

00:44:50.330 --> 00:44:52.680
One question that
you often want to ask

00:44:52.680 --> 00:44:55.650
is when do left to
right inverses exist?

00:45:03.335 --> 00:45:05.080
Actually, maybe
I'll write it here.

00:45:15.300 --> 00:45:17.620
Intuitively, there should
exist a left inverse

00:45:17.620 --> 00:45:22.200
when, after we've
applied T, we haven't

00:45:22.200 --> 00:45:24.610
done irreparable damage.

00:45:24.610 --> 00:45:26.640
So whatever we're
left with, there's

00:45:26.640 --> 00:45:30.390
still enough information
that some linear operator

00:45:30.390 --> 00:45:35.530
can restore our original vector
and give us back the identity.

00:45:35.530 --> 00:45:39.550
And so that condition
is when-- of not doing

00:45:39.550 --> 00:45:42.140
irreparable damage, of
not losing information,

00:45:42.140 --> 00:45:45.250
is asking essentially
whether T is injective.

00:45:45.250 --> 00:45:54.350
So there exists a left inverse
if and only if T is injective.

00:46:01.540 --> 00:46:07.370
Now for a right
inverse the situation

00:46:07.370 --> 00:46:10.260
is sort of dual to this.

00:46:10.260 --> 00:46:14.470
And here what we
want-- we can multiply

00:46:14.470 --> 00:46:16.520
on the right by
whatever we like,

00:46:16.520 --> 00:46:18.890
but there won't be
anything on the left.

00:46:18.890 --> 00:46:22.920
So after the action of T,
there won't be any further room

00:46:22.920 --> 00:46:25.760
to explore the
whole vector space.

00:46:25.760 --> 00:46:29.920
So the output of T had better
cover all of the possibilities

00:46:29.920 --> 00:46:35.320
if we want to be able to achieve
identity by multiplying T

00:46:35.320 --> 00:46:37.860
by something on the right.

00:46:37.860 --> 00:46:41.145
So any guesses for
what the condition

00:46:41.145 --> 00:46:42.455
is for having a right inverse?

00:46:42.455 --> 00:46:43.330
AUDIENCE: Surjective.

00:46:43.330 --> 00:46:43.780
ARAM HARROW: Surjective.

00:46:43.780 --> 00:46:44.360
Right.

00:46:44.360 --> 00:47:00.170
So there exists a right inverse
if and only if T is surjective.

00:47:00.170 --> 00:47:05.100
Technically, I've only
proved one direction.

00:47:05.100 --> 00:47:09.410
My hand waving just now proved
that, if T is not injective,

00:47:09.410 --> 00:47:11.380
there's no way it will
have a left inverse.

00:47:11.380 --> 00:47:13.310
If it's not surjective,
there's no way

00:47:13.310 --> 00:47:14.640
it'll have a right inverse.

00:47:14.640 --> 00:47:17.660
I haven't actually proved
that, if it is injective,

00:47:17.660 --> 00:47:19.260
there is such a left inverse.

00:47:19.260 --> 00:47:22.480
And if it is surjective, there
is such a right universe.

00:47:22.480 --> 00:47:24.364
But those I think
are good exercises

00:47:24.364 --> 00:47:26.780
for you to do to make sure you
understand what's going on.

00:47:31.220 --> 00:47:34.110
This takes us part
of the way there.

00:47:34.110 --> 00:47:38.230
In some cases our lives
become much easier.

00:47:38.230 --> 00:47:45.490
In particular, if v
is finite dimensional,

00:47:45.490 --> 00:47:51.930
it turns out that all
of these are equivalent.

00:47:51.930 --> 00:48:12.870
So T is injective if and
only if T is surjective if

00:48:12.870 --> 00:48:16.950
and only if T is invertible.

00:48:31.340 --> 00:48:32.020
And why is this?

00:48:32.020 --> 00:48:34.760
Why should it be true
that T is surjective if

00:48:34.760 --> 00:48:37.456
and only if T is injective?

00:48:37.456 --> 00:48:39.205
Why should those be
equivalent statements?

00:48:44.022 --> 00:48:45.516
Yeah?

00:48:45.516 --> 00:48:48.504
AUDIENCE: This isn't really
a rigorous statement,

00:48:48.504 --> 00:48:53.484
but if the intuition of it is
a little bit that you're taking

00:48:53.484 --> 00:48:54.726
vectors in v to vectors in v.

00:48:54.726 --> 00:48:55.476
ARAM HARROW: Yeah.

00:48:55.476 --> 00:48:59.520
AUDIENCE: And so your
mapping is 1 to 1

00:48:59.520 --> 00:49:04.700
if and only if every vector
is mapped to, because then

00:49:04.700 --> 00:49:06.310
you're not leaving anything out.

00:49:06.310 --> 00:49:07.393
ARAM HARROW: That's right.

00:49:07.393 --> 00:49:10.680
In failing to be injective
and failing to be surjective

00:49:10.680 --> 00:49:12.896
both look like
losing information.

00:49:12.896 --> 00:49:14.270
Failing to be
injective means I'm

00:49:14.270 --> 00:49:17.380
sending a whole non-zero
vector and its multiples

00:49:17.380 --> 00:49:20.230
to 0, that's a degree
of freedom lost.

00:49:20.230 --> 00:49:22.470
Failing to be surjective
means once I look

00:49:22.470 --> 00:49:24.270
at all the degrees
of freedom I reach,

00:49:24.270 --> 00:49:25.720
I haven't reached everything.

00:49:25.720 --> 00:49:29.710
So they intuitively
look the same.

00:49:29.710 --> 00:49:31.720
So that's the right intuition.

00:49:31.720 --> 00:49:33.380
There's a proof,
actually, that makes

00:49:33.380 --> 00:49:36.800
use of something on a
current blackboard though.

00:49:36.800 --> 00:49:37.487
Yeah?

00:49:37.487 --> 00:49:39.112
AUDIENCE: Well, you
need the dimensions

00:49:39.112 --> 00:49:42.458
of-- so if the
[INAUDIBLE] space is 0,

00:49:42.458 --> 00:49:44.370
you need dimensions of
[? the range to p. ?]

00:49:44.370 --> 00:49:45.461
ARAM HARROW: Right.

00:49:45.461 --> 00:49:45.960
Right.

00:49:45.960 --> 00:49:48.600
So from this dimensions
formula you immediately get

00:49:48.600 --> 00:49:55.010
because if this is 0, then
this is the whole vector space.

00:49:55.010 --> 00:49:58.930
And if this is non-zero, this
is not the whole vector space.

00:49:58.930 --> 00:50:02.470
And this proof is sort
of non-illuminating

00:50:02.470 --> 00:50:04.600
if you don't know the
proof of that thing-- which

00:50:04.600 --> 00:50:05.900
I apologize for.

00:50:05.900 --> 00:50:08.275
But also, you can see
immediately from that

00:50:08.275 --> 00:50:13.050
that we've used the fact
that v is finite dimensional.

00:50:13.050 --> 00:50:17.080
And it turns out this
equivalence breaks down

00:50:17.080 --> 00:50:21.210
if the vector space is
infinite dimensional.

00:50:21.210 --> 00:50:23.050
Which is pretty weird.

00:50:23.050 --> 00:50:26.020
There's a lot of subtleties
of infinite dimensional vector

00:50:26.020 --> 00:50:30.890
spaces that it's easy to
overlook if you build up

00:50:30.890 --> 00:50:32.670
your intuition from matrices.

00:50:35.470 --> 00:50:40.580
So does anyone have an
idea of a-- so let's think

00:50:40.580 --> 00:50:43.580
of an example of a
vector of something

00:50:43.580 --> 00:50:46.060
that is on an infinite
dimensional space that's

00:50:46.060 --> 00:50:48.495
surjective but not injective.

00:50:51.090 --> 00:50:55.000
Any guesses for
such an operation?

00:50:55.000 --> 00:50:55.840
Yeah?

00:50:55.840 --> 00:50:57.070
AUDIENCE: The left shift.

00:50:57.070 --> 00:50:58.267
ARAM HARROW: Yes.

00:50:58.267 --> 00:51:00.725
You'll notice I didn't erase
this blackboard strategically.

00:51:00.725 --> 00:51:01.225
Yes.

00:51:01.225 --> 00:51:04.270
The left shift
operator is surjective.

00:51:04.270 --> 00:51:07.990
I can prepare any
vector here I like just

00:51:07.990 --> 00:51:11.750
by putting it into the x2,
x3, dot, dot, dot parts.

00:51:11.750 --> 00:51:15.190
So the range is everything,
but it's not injective

00:51:15.190 --> 00:51:17.360
because it throws away
the first register.

00:51:17.360 --> 00:51:21.690
It's maps things with
it a non-zero element

00:51:21.690 --> 00:51:24.920
in the first position and
0's everywhere else to 0.

00:51:24.920 --> 00:51:33.820
So this is surjective
not injective.

00:51:36.510 --> 00:51:39.190
On the other hand,
if you want something

00:51:39.190 --> 00:51:42.000
that's injective
and not surjective,

00:51:42.000 --> 00:51:48.480
you don't have to look
very far, the right shift

00:51:48.480 --> 00:51:54.940
is injective and not surjective.

00:51:54.940 --> 00:51:57.380
It's pretty obvious
it's not surjective.

00:51:57.380 --> 00:52:01.180
There's that 0 there which
definitely means it cannot

00:52:01.180 --> 00:52:02.462
achieve any vector.

00:52:02.462 --> 00:52:04.295
And it's not too hard
to see it's injective.

00:52:04.295 --> 00:52:06.520
It hasn't lost any information.

00:52:06.520 --> 00:52:09.260
It's like you're in the
hotel that's infinitely long

00:52:09.260 --> 00:52:14.692
and all the rooms are full and
the person at the front desk

00:52:14.692 --> 00:52:15.400
says, no problem.

00:52:15.400 --> 00:52:18.325
I'll just move everyone
down one room to the right,

00:52:18.325 --> 00:52:19.820
and you can take the first room.

00:52:19.820 --> 00:52:24.370
So that policy is
injective-- you'll

00:52:24.370 --> 00:52:28.370
always get a room to
yourself-- and made

00:52:28.370 --> 00:52:33.170
possible by having an infinite
dimensional vector space.

00:52:33.170 --> 00:52:37.230
So in infinite dimensions
we cannot say this.

00:52:37.230 --> 00:52:48.050
Instead, we can say
that T is invertible if

00:52:48.050 --> 00:52:56.720
and only if T is
injective and surjective.

00:52:59.990 --> 00:53:03.040
So this statement
is true in general

00:53:03.040 --> 00:53:06.390
for infinite dimensional,
whatever, vector spaces.

00:53:06.390 --> 00:53:10.460
And only in the nice special
case of finite dimensions

00:53:10.460 --> 00:53:11.690
do we get this equivalence.

00:53:16.012 --> 00:53:17.509
Yeah?

00:53:17.509 --> 00:53:23.996
AUDIENCE: Can the range and null
space of T a [INAUDIBLE] of T

00:53:23.996 --> 00:53:27.084
the operator again use a
vector space [INAUDIBLE]?

00:53:27.084 --> 00:53:28.500
ARAM HARROW: Yes.
the question was

00:53:28.500 --> 00:53:32.630
do the null space in a range
are they properties just of T

00:53:32.630 --> 00:53:34.200
or also of v?

00:53:34.200 --> 00:53:35.904
And definitely you
also need to know

00:53:35.904 --> 00:53:40.350
v. The way I've
been writing it, T

00:53:40.350 --> 00:53:43.940
is implicitly defined
in terms of v,

00:53:43.940 --> 00:53:45.550
which in turn is
implicitly defined

00:53:45.550 --> 00:53:47.820
in terms of the field, f.

00:53:47.820 --> 00:53:51.641
And all these things
can make a difference.

00:53:51.641 --> 00:53:52.140
Yes?

00:53:52.140 --> 00:53:55.767
AUDIENCE: So do you have to
be a bijection for it to be--

00:53:55.767 --> 00:53:56.850
ARAM HARROW: That's right.

00:53:56.850 --> 00:53:57.442
That's right.

00:53:57.442 --> 00:53:58.900
Invertible is the
same a bijection.

00:54:01.800 --> 00:54:08.105
So let me now try and
relate this to matrices.

00:54:12.732 --> 00:54:14.190
I've been saying
that operators are

00:54:14.190 --> 00:54:18.075
like the fancy mathematician's
form of matrices.

00:54:22.490 --> 00:54:24.410
If you're Arrested
Development fans,

00:54:24.410 --> 00:54:28.810
it's like magic trick
versus an illusion.

00:54:28.810 --> 00:54:32.165
But are they different or not
depends on your perspective.

00:54:34.940 --> 00:54:37.160
There are advantages to
seeing it both ways, I think.

00:54:37.160 --> 00:54:40.560
So let me tell you
how you can view

00:54:40.560 --> 00:54:44.620
an operator in a matrix form.

00:54:44.620 --> 00:54:47.180
The way to do this--
and the reason

00:54:47.180 --> 00:54:52.540
why matrices are not universally
loved by mathematicians

00:54:52.540 --> 00:54:56.730
is I haven't specified
a basis this whole time.

00:54:56.730 --> 00:54:59.390
But if I want a
matrix, all I needed

00:54:59.390 --> 00:55:01.330
was a vector space
and a function--

00:55:01.330 --> 00:55:03.710
a linear function between
two vector spaces--

00:55:03.710 --> 00:55:05.600
or, sorry, from a
vector space to itself.

00:55:05.600 --> 00:55:09.290
But if I want a matrix, I
need additional structure.

00:55:09.290 --> 00:55:13.540
And mathematicians try to
avoid that whenever possible.

00:55:13.540 --> 00:55:16.460
But if you're willing to take
this additional structure-- so

00:55:16.460 --> 00:55:26.400
if you choose a
basis v1 through vn--

00:55:26.400 --> 00:55:28.680
it turns out you
can get a simpler

00:55:28.680 --> 00:55:31.640
form of the operator that's
useful to compute with.

00:55:31.640 --> 00:55:32.930
So why is that?

00:55:32.930 --> 00:55:35.170
Well, the fact that
it's a basis that

00:55:35.170 --> 00:55:50.630
means that any v can be
written as linear combinations

00:55:50.630 --> 00:55:56.250
of these basis elements where a1
though an belong to the field.

00:55:56.250 --> 00:56:09.480
And since T is linear,
if T acts on v,

00:56:09.480 --> 00:56:12.820
we can rewrite it
in this way, and you

00:56:12.820 --> 00:56:17.570
see that the entire
action is determined

00:56:17.570 --> 00:56:22.350
by T acting on v1 through vn.

00:56:22.350 --> 00:56:24.490
So think about-- if
you wanted to represent

00:56:24.490 --> 00:56:27.330
an operator in a
computer, you'd say,

00:56:27.330 --> 00:56:29.620
well, there's an infinite
number of input vectors.

00:56:29.620 --> 00:56:32.160
And for each input vector
I have to write down

00:56:32.160 --> 00:56:33.330
the output vector.

00:56:33.330 --> 00:56:34.950
And this says, no, you don't.

00:56:34.950 --> 00:56:37.390
You only need to restore
on your computer what

00:56:37.390 --> 00:56:43.415
does T do to v1, what does
T do to v2, et cetera.

00:56:43.415 --> 00:56:44.040
So that's good.

00:56:44.040 --> 00:56:46.850
Now you only have to
write down n vectors,

00:56:46.850 --> 00:56:50.430
and since these
factors in turn can

00:56:50.430 --> 00:56:52.310
be expressed in
terms of the basis,

00:56:52.310 --> 00:56:55.590
you can express this just in
terms of a bunch of numbers.

00:56:55.590 --> 00:57:03.400
So let's further expand
Tvj in this basis.

00:57:03.400 --> 00:57:04.820
And so there's some coefficient.

00:57:04.820 --> 00:57:12.930
So it's something times v1 plus
something times v2 something

00:57:12.930 --> 00:57:14.370
times vn.

00:57:14.370 --> 00:57:19.100
And I'm going to-- these
somethings are a function of T

00:57:19.100 --> 00:57:30.120
so I'm just going to call this
T sub 1j, T sub 2j, T sub nj.

00:57:33.460 --> 00:57:44.980
And this whole thing I can write
more succinctly in this way.

00:57:44.980 --> 00:57:51.040
And now all I need
are these T's of ij,

00:57:51.040 --> 00:57:54.120
and that can completely
determine for me the action

00:57:54.120 --> 00:58:21.630
of T because this
Tv here-- so Tv

00:58:21.630 --> 00:58:27.685
we can write as a sum
over j of T times ajvj.

00:58:27.685 --> 00:58:30.420
And we can move
the aj past the T.

00:58:30.420 --> 00:58:34.550
And then if we
expand this out, we

00:58:34.550 --> 00:58:45.080
get that it's a sum over i from
1 to n, sum over j from 1 to n,

00:58:45.080 --> 00:58:45.755
of Tijajvi.

00:58:56.110 --> 00:58:59.640
And so if we act on
in general vector, v,

00:58:59.640 --> 00:59:04.640
and we know the coefficients
of v in some basis,

00:59:04.640 --> 00:59:09.690
then we can re-express it
in that basis as follows.

00:59:09.690 --> 00:59:16.257
And this output in
general can always

00:59:16.257 --> 00:59:18.215
be written in the basis
with some coefficients.

00:59:24.187 --> 00:59:25.770
So we could always
write it like this.

00:59:28.840 --> 00:59:33.630
And this formula tells you what
those coefficients should be.

00:59:33.630 --> 00:59:40.570
They say, if your input vector
has coefficients a1 through an,

00:59:40.570 --> 00:59:46.210
then your output vector has
coefficients b1 through bn,

00:59:46.210 --> 00:59:51.970
where the b sub i are
defined by this sum.

01:00:03.430 --> 01:00:11.300
And of course there's a
more-- this formula is one

01:00:11.300 --> 01:00:16.530
that you've seen
before, and it's often

01:00:16.530 --> 01:00:20.590
written in this
more familiar form.

01:00:33.380 --> 01:00:36.475
So this is now the familiar
matrix-vector multiplication.

01:00:36.475 --> 01:00:42.540
And it says that the b vector
is obtained from the a vector

01:00:42.540 --> 01:00:46.495
by multiplying it by
the matrix of these Tij.

01:00:49.790 --> 01:00:53.560
And so this T is
the matrix form--

01:00:53.560 --> 01:00:59.010
this is a matrix form
of the operator T.

01:00:59.010 --> 01:01:03.304
And you might find this
not very impressive.

01:01:03.304 --> 01:01:04.720
You say, well,
look I already knew

01:01:04.720 --> 01:01:08.440
how to multiply a
matrix by vector.

01:01:08.440 --> 01:01:13.110
But what I think is nice about
this is that the usual way

01:01:13.110 --> 01:01:15.790
you learn linear
algebra if someone says,

01:01:15.790 --> 01:01:18.130
a vector is a list of numbers.

01:01:18.130 --> 01:01:20.350
A matrix is a
rectangle of numbers.

01:01:20.350 --> 01:01:24.200
Here's are the rules for
what you do with them.

01:01:24.200 --> 01:01:25.660
If you want to
put them together,

01:01:25.660 --> 01:01:27.170
you do it in this way.

01:01:27.170 --> 01:01:30.860
Here this was not an axiom
of the theory at all.

01:01:30.860 --> 01:01:34.000
We just started with linear
maps from one vector space

01:01:34.000 --> 01:01:37.780
to another one and
the idea of a basis

01:01:37.780 --> 01:01:40.390
as something that you
can prove has to exist

01:01:40.390 --> 01:01:43.140
and you can derive
matrix multiplication.

01:01:43.140 --> 01:01:45.250
So matrix
multiplication emerges--

01:01:45.250 --> 01:01:47.970
or matrix-vector
multiplication emerges

01:01:47.970 --> 01:01:50.570
as a consequence of the theory
rather than as something

01:01:50.570 --> 01:01:52.710
that you have to put in.

01:01:52.710 --> 01:01:55.940
So that, I think, is what's
kind of cute about this

01:01:55.940 --> 01:01:58.120
even if it comes back
on the end to something

01:01:58.120 --> 01:02:02.939
that you had been taught before.

01:02:02.939 --> 01:02:03.980
Any questions about that?

01:02:09.420 --> 01:02:13.750
So this is matrix-vector
multiplication.

01:02:13.750 --> 01:02:17.385
You can similarly derive
matrix-matrix multiplication.

01:02:30.960 --> 01:02:43.230
So if we have two
operators, T and S,

01:02:43.230 --> 01:02:48.400
and we act on a
vector, v sub k--

01:02:48.400 --> 01:02:50.390
and by what I
argued before, it's

01:02:50.390 --> 01:02:53.120
enough just to know how they
act on the basis vectors.

01:02:53.120 --> 01:02:55.350
You don't need to
know-- and once you

01:02:55.350 --> 01:03:00.120
do that, you can figure out
how they act on any vector.

01:03:00.120 --> 01:03:04.400
So if we just expand out
what we wrote before,

01:03:04.400 --> 01:03:09.656
this is equal to T times
the sum over j of Sjkvj.

01:03:18.050 --> 01:03:21.660
So Svk can be
re-expressed in terms

01:03:21.660 --> 01:03:24.644
of the basis with
some coefficients.

01:03:24.644 --> 01:03:26.060
And those coefficients
will depend

01:03:26.060 --> 01:03:32.620
on the vector you start
with, k, and the part

01:03:32.620 --> 01:03:36.762
of the basis that you're
using to express it with j.

01:03:36.762 --> 01:03:39.810
Then we apply the same
thing again with T.

01:03:39.810 --> 01:03:47.940
We get-- this is sum over
i, sum over j TijSjkvi.

01:03:58.050 --> 01:04:01.690
And now, what have we done?

01:04:01.690 --> 01:04:06.800
TS is an operator and
when you act of vk

01:04:06.800 --> 01:04:10.290
it spat out something that's
a linear combination of all

01:04:10.290 --> 01:04:19.190
the basis states, v sub i,
and the coefficient of v sub i

01:04:19.190 --> 01:04:22.280
is this part in the parentheses.

01:04:22.280 --> 01:04:27.210
And so this is the
matrix element of TS.

01:04:30.120 --> 01:04:45.117
So the ik matrix element of ts
is the sum over j of Tijsjk.

01:04:48.680 --> 01:04:51.530
And so just like we derived
matrix-vector multiplication,

01:04:51.530 --> 01:04:54.045
here we can derive
matrix-matrix multiplication.

01:04:58.990 --> 01:05:02.020
And so what was originally just
sort of an axiom of the theory

01:05:02.020 --> 01:05:04.780
is now the only
possible way it could

01:05:04.780 --> 01:05:09.490
be if you want to define
operator multiplication is

01:05:09.490 --> 01:05:12.380
first one operator acts,
than the other operator acts.

01:05:15.460 --> 01:05:21.200
So in terms of this--
so this, I think,

01:05:21.200 --> 01:05:24.400
justifies why you
can think of matrices

01:05:24.400 --> 01:05:28.750
as a faithful
representation of operators.

01:05:28.750 --> 01:05:34.170
And once you've chosen
a basis, they can--

01:05:34.170 --> 01:05:37.840
the square full
of numbers becomes

01:05:37.840 --> 01:05:41.300
equivalent to the abstract
map between vector spaces.

01:05:43.820 --> 01:05:46.320
And the equivalent-- they're
so equivalent that I'm just

01:05:46.320 --> 01:05:47.950
going to write things
like equal signs.

01:05:47.950 --> 01:05:52.420
Like I'll write identity
equals a bunch of 1's

01:05:52.420 --> 01:05:54.020
down the diagonal, right?

01:05:54.020 --> 01:05:56.390
And not worry about the
fact that technically this

01:05:56.390 --> 01:05:59.090
is an operator and
this is a matrix.

01:05:59.090 --> 01:06:09.640
And similarly, the 0 matrix
equals a matrix full of 0's.

01:06:09.640 --> 01:06:14.600
Technically, we
should write-- if you

01:06:14.600 --> 01:06:17.490
want to express the
basis dependence,

01:06:17.490 --> 01:06:33.425
you can write things like
T parentheses-- sorry,

01:06:33.425 --> 01:06:35.290
let me write it like this.

01:06:41.290 --> 01:06:43.910
If you really want to be very
explicit about the basis,

01:06:43.910 --> 01:06:46.480
you could use this to
refer to the matrix.

01:06:46.480 --> 01:06:50.600
Just to really emphasize
that the matrix depends

01:06:50.600 --> 01:06:54.400
not only on the operator, but
also on your choice of basis.

01:06:54.400 --> 01:06:56.930
But we'll almost never
bothered to do this.

01:06:56.930 --> 01:06:59.440
We usually just sort of say
it in words what the basis is.

01:07:12.650 --> 01:07:16.020
So matrices are an important
calculational tool,

01:07:16.020 --> 01:07:20.520
and we ultimately want to
compute numbers of physical

01:07:20.520 --> 01:07:23.730
quantities so we cannot always
spend our lives in abstract

01:07:23.730 --> 01:07:25.660
vector spaces.

01:07:25.660 --> 01:07:29.570
But the basis dependence
is an unfortunate thing.

01:07:29.570 --> 01:07:32.170
A basis is like a choice
of coordinate systems,

01:07:32.170 --> 01:07:35.350
and you really don't want
your physics to depend on it,

01:07:35.350 --> 01:07:39.050
and you don't want quantity if
you compute to be dependent on.

01:07:39.050 --> 01:07:42.474
And so we often
want to formulate--

01:07:42.474 --> 01:07:44.890
we're interested in quantities
that are basis independent.

01:07:44.890 --> 01:07:47.140
And in fact, that's a big
point of the whole operator

01:07:47.140 --> 01:07:50.190
picture is that because
the quantities we want

01:07:50.190 --> 01:07:52.110
are ultimately
basis independent,

01:07:52.110 --> 01:07:55.720
it's nice to have language that
is itself basis independent.

01:07:55.720 --> 01:08:00.930
Terminology and theorems
that do not refer to a basis.

01:08:00.930 --> 01:08:13.120
I'll mention a few basis
independent quantities,

01:08:13.120 --> 01:08:14.960
and I won't say too
much more about them

01:08:14.960 --> 01:08:17.710
because you will prove
properties [INAUDIBLE]

01:08:17.710 --> 01:08:23.790
on your p set, but one
of them is the trace

01:08:23.790 --> 01:08:25.595
and another one is
the determinant.

01:08:28.640 --> 01:08:30.920
And when you first
look at them-- OK,

01:08:30.920 --> 01:08:35.130
you can check that each
one is basis independent,

01:08:35.130 --> 01:08:37.630
and it really looks
kind of mysterious.

01:08:37.630 --> 01:08:43.242
I mean, like, who pulled
these out of the hat?

01:08:43.242 --> 01:08:44.700
They look totally
different, right?

01:08:44.700 --> 01:08:48.060
They don't look remotely
related to each other.

01:08:48.060 --> 01:08:50.510
And are these all there is?

01:08:50.510 --> 01:08:53.319
Are there many more?

01:08:53.319 --> 01:08:57.740
And it turns out that, at least
for matrices with eigenvalues,

01:08:57.740 --> 01:09:02.210
these can be seen as members
of a much larger family.

01:09:02.210 --> 01:09:04.710
And the reason is that
the trace turns out

01:09:04.710 --> 01:09:06.941
to be the sum of
all the eigenvalues

01:09:06.941 --> 01:09:08.399
and the determinant
turns out to be

01:09:08.399 --> 01:09:10.899
the product of all
of the eigenvalues.

01:09:10.899 --> 01:09:14.529
And in general, we'll see
in a minute, that basis

01:09:14.529 --> 01:09:16.500
independent things--
actually, not in a minute.

01:09:16.500 --> 01:09:20.370
In a future lecture-- that
basis independent things

01:09:20.370 --> 01:09:22.830
are functions of eigenvalues.

01:09:22.830 --> 01:09:25.290
And furthermore, that don't
care about the ordering

01:09:25.290 --> 01:09:26.560
of the eigenvalues.

01:09:26.560 --> 01:09:29.080
So they're symmetric
functions of eigenvalues.

01:09:29.080 --> 01:09:31.240
And then it starts to make
a little bit more sense.

01:09:31.240 --> 01:09:33.769
Because if you talk about
symmetric polynomials,

01:09:33.769 --> 01:09:36.060
those are two of the most
important ones where you just

01:09:36.060 --> 01:09:39.189
add up all the things and when
you multiply all the things.

01:09:39.189 --> 01:09:40.950
And then, if you
add this perspective

01:09:40.950 --> 01:09:43.930
of symmetric polynomial
of the eigenvalue,

01:09:43.930 --> 01:09:47.830
then you can cook up other
basis independent quantities.

01:09:47.830 --> 01:09:50.252
So this is actually
not the approach

01:09:50.252 --> 01:09:51.460
you should take on the p set.

01:09:51.460 --> 01:09:52.835
The [? p set ?]
asks you to prove

01:09:52.835 --> 01:09:56.520
more directly that the
trace is basis independent,

01:09:56.520 --> 01:09:59.710
but the sort of framework
that these fit into

01:09:59.710 --> 01:10:01.750
is symmetric functions
of eigenvalues.

01:10:06.110 --> 01:10:11.340
So I want to say a little
bit about eigenvalues.

01:10:11.340 --> 01:10:13.120
Any questions about
matrices before I do?

01:10:29.950 --> 01:10:33.852
So eigenvalues--
I guess, these are

01:10:33.852 --> 01:10:35.060
basis independent quantities.

01:10:42.800 --> 01:10:45.840
Another important basis
independent quantity,

01:10:45.840 --> 01:10:49.370
or property of a matrix, is
its eigenvalue-eigenvector

01:10:49.370 --> 01:10:49.870
structure.

01:10:58.320 --> 01:11:01.410
The place where
eigenvectors come from

01:11:01.410 --> 01:11:05.190
is by considering a slightly
more general thing, which

01:11:05.190 --> 01:11:07.790
is the idea of an
invariant subspace.

01:11:07.790 --> 01:11:22.490
So we say that U is a
T invariant subspace

01:11:22.490 --> 01:11:29.310
if T of U-- this is an operator
acting on an entire subspace.

01:11:29.310 --> 01:11:32.390
So what do I mean by that?

01:11:32.390 --> 01:11:39.450
I mean the set of all TU
for vectors in the subspace.

01:11:39.450 --> 01:11:47.450
If T of U is contained in U.

01:11:47.450 --> 01:11:51.470
So I take a vector in this
subspace, act on it with T,

01:11:51.470 --> 01:11:52.940
and then I'm still
in the subspace

01:11:52.940 --> 01:11:55.680
no matter which vector I had.

01:11:55.680 --> 01:12:03.680
So some examples
that always work.

01:12:03.680 --> 01:12:06.310
The 0 subspace is invariant.

01:12:06.310 --> 01:12:10.320
T always maps it to itself.

01:12:10.320 --> 01:12:14.760
And the entire space, v, T
is a linear operator on v

01:12:14.760 --> 01:12:17.470
so by definition it
maps v to itself.

01:12:20.280 --> 01:12:22.730
These are called the
trivial examples.

01:12:22.730 --> 01:12:26.410
And usually when people talk
about non-trivial invariant

01:12:26.410 --> 01:12:29.810
subspaces they mean
not one of these two.

01:12:29.810 --> 01:12:32.610
The particular type that
we will be interested in

01:12:32.610 --> 01:12:34.520
are one dimensional ones.

01:12:43.100 --> 01:12:51.330
So this corresponds to a
direction that T fixes.

01:12:51.330 --> 01:12:57.430
So U-- this vector space
now can be written just

01:12:57.430 --> 01:13:10.710
as the span of a single vector,
U, and U being T invariant

01:13:10.710 --> 01:13:19.280
is equivalent to TU being
a mu, because they're just

01:13:19.280 --> 01:13:20.020
a single vector.

01:13:20.020 --> 01:13:22.590
So all I have to do is get
that single vector right

01:13:22.590 --> 01:13:25.170
and I'll get the
whole subspace right.

01:13:25.170 --> 01:13:41.040
And that, in turn, is equivalent
to TU being some multiple of U.

01:13:41.040 --> 01:13:47.360
And this equation
you've seen before.

01:13:47.360 --> 01:13:50.060
This is the familiar
eigenvector equation.

01:13:50.060 --> 01:13:53.734
And if it's a very,
very important equation

01:13:53.734 --> 01:13:55.400
it might be named
after a mathematician,

01:13:55.400 --> 01:13:59.420
but this one is so important
that two of the pieces of it

01:13:59.420 --> 01:14:02.710
have their own special name.

01:14:02.710 --> 01:14:08.920
So these are called-- lambda
is called an eigenvalue

01:14:08.920 --> 01:14:15.680
and U is called an eigenvector.

01:14:21.340 --> 01:14:28.500
And more or less it's true that
all of the solutions to this

01:14:28.500 --> 01:14:30.590
are called eigenvalues,
and all the solutions

01:14:30.590 --> 01:14:32.550
are called eigenvectors.

01:14:32.550 --> 01:14:38.960
There's one exception,
which is there's

01:14:38.960 --> 01:14:41.980
one kind of trivial solution
to this equation, which

01:14:41.980 --> 01:14:46.780
is when U is 0 this
equation is always true.

01:14:46.780 --> 01:14:49.820
And that's not very
interesting, but it's

01:14:49.820 --> 01:14:53.520
true for all values of lambda.

01:14:53.520 --> 01:14:56.910
And so that doesn't count
as being an eigenvalue.

01:14:56.910 --> 01:14:59.264
And you can tell a doesn't
correspond to 1D invariant

01:14:59.264 --> 01:14:59.930
subspace, right?

01:14:59.930 --> 01:15:02.970
It corresponds to a 0
dimensional subspace,

01:15:02.970 --> 01:15:04.090
which is the trivial case.

01:15:11.120 --> 01:15:23.010
So we say that lambda
is an eigenvalue of T

01:15:23.010 --> 01:15:34.270
if Tu equals lambda U for
some non-zero vector, U.

01:15:34.270 --> 01:15:36.160
So the non 0 is crucial.

01:15:42.250 --> 01:15:51.250
And then the spectrum
of T is the collection

01:15:51.250 --> 01:15:52.115
of all eigenvalues.

01:16:10.060 --> 01:16:15.000
So there's something a little
bit asymmetric about this,

01:16:15.000 --> 01:16:17.830
which is we still
say that 0 vector is

01:16:17.830 --> 01:16:24.710
an eigenvector with all
the various eigenvalues,

01:16:24.710 --> 01:16:27.510
but we had to put this
here or everything

01:16:27.510 --> 01:16:31.570
would be an eigenvalue and it
wouldn't be very interesting.

01:16:31.570 --> 01:16:34.210
So the--

01:16:38.710 --> 01:16:42.380
Oh, also I want to say this term
spectrum you'll see it other

01:16:42.380 --> 01:16:42.880
[INAUDIBLE].

01:16:42.880 --> 01:16:46.500
You'll see spectral theory
or spectral this or that,

01:16:46.500 --> 01:16:49.480
and that means essentially
making use of the eigenvalues.

01:16:49.480 --> 01:16:52.900
So people talk about
partitioning a graph using

01:16:52.900 --> 01:16:55.690
eigenvalues of the
associated matrix, that's

01:16:55.690 --> 01:16:57.910
called spectral partitioning.

01:16:57.910 --> 01:17:04.260
And so throughout math,
this term is used a lot.

01:17:06.960 --> 01:17:12.550
So I have only
about three minutes

01:17:12.550 --> 01:17:16.340
left to tell-- so
I think I will not

01:17:16.340 --> 01:17:21.920
finish the eigenvalue discussion
but will just show you

01:17:21.920 --> 01:17:26.280
a few examples of
how it's not always

01:17:26.280 --> 01:17:29.810
as nice as you might expect.

01:17:29.810 --> 01:17:40.910
So one example
that I'll consider

01:17:40.910 --> 01:17:50.270
is the vector space will be
the reals, 3D real space,

01:17:50.270 --> 01:17:56.620
and the operator, T, will
be rotation about the z-axis

01:17:56.620 --> 01:17:59.500
by some small angle.

01:17:59.500 --> 01:18:08.345
Let's call it a theta
rotation about the z-axis.

01:18:10.860 --> 01:18:13.899
Turns out, if you write
this in matrix form,

01:18:13.899 --> 01:18:14.690
it looks like this.

01:18:14.690 --> 01:18:24.840
Cosine theta minus sine theta
0 sine theta cosine theta 0, 0,

01:18:24.840 --> 01:18:27.640
0, 0, 1.

01:18:27.640 --> 01:18:30.560
That 1 is because it
leaves the z-axis alone

01:18:30.560 --> 01:18:33.620
and then x and y get rotated.

01:18:33.620 --> 01:18:35.520
You can tell if theta
is 0 it does nothing

01:18:35.520 --> 01:18:36.966
so that's reassuring.

01:18:36.966 --> 01:18:38.590
And if theta does a
little bit, then it

01:18:38.590 --> 01:18:42.170
starts mixing the
x and y components.

01:18:42.170 --> 01:18:46.120
So that is the rotation matrix.

01:18:46.120 --> 01:18:51.125
So what is an
eigenvalue-- and anyone

01:18:51.125 --> 01:18:53.670
say what an eigenvalue
is of this matrix?

01:18:53.670 --> 01:18:54.170
AUDIENCE: 1.

01:18:54.170 --> 01:18:54.650
ARAM HARROW: 1.

01:18:54.650 --> 01:18:54.840
Good.

01:18:54.840 --> 01:18:56.070
And what's the eigenvector?

01:18:56.070 --> 01:18:57.450
AUDIENCE: The z basis vector.

01:18:57.450 --> 01:18:58.520
ARAM HARROW: The z basis vector.

01:18:58.520 --> 01:18:59.019
Right.

01:18:59.019 --> 01:19:02.290
So it fixes a z
basis vector so this

01:19:02.290 --> 01:19:06.430
is an eigenvector
with eigenvalue 1.

01:19:06.430 --> 01:19:09.940
Does it have any
other eigenvectors?

01:19:09.940 --> 01:19:10.538
Yeah?

01:19:10.538 --> 01:19:13.190
AUDIENCE: If you go to the
complex plane, then yes.

01:19:13.190 --> 01:19:15.880
ARAM HARROW: If you are talking
about complex numbers, then

01:19:15.880 --> 01:19:19.130
yes, it has complex eigenvalues.

01:19:19.130 --> 01:19:22.900
But if we're talking
about a real vector space,

01:19:22.900 --> 01:19:26.040
then it doesn't.

01:19:26.040 --> 01:19:30.910
And so this just has one
eigenvalue and one eigenvector.

01:19:30.910 --> 01:19:36.600
And if we were to get rid
of the third dimension--

01:19:36.600 --> 01:19:39.800
so if we just had T-- and
let's be even simpler,

01:19:39.800 --> 01:19:41.595
let's just take theta
to be pi over 2.

01:19:46.370 --> 01:19:51.540
So let's just take a 90
degree rotation in the plane.

01:19:54.090 --> 01:19:57.620
Now T has no eigenvalues.

01:19:57.620 --> 01:20:02.800
There are no vectors other
than 0 that it sends to itself.

01:20:02.800 --> 01:20:11.020
And so this is a
slightly unfortunate note

01:20:11.020 --> 01:20:11.996
to end the lecture on.

01:20:11.996 --> 01:20:13.870
You think, well, these
eigenvalues are great,

01:20:13.870 --> 01:20:16.140
but maybe they exist,
maybe they don't.

01:20:16.140 --> 01:20:19.250
And you'll see next
time part of the reason

01:20:19.250 --> 01:20:21.680
why we use complex numbers,
even though it looks

01:20:21.680 --> 01:20:26.200
like real space isn't complex,
is because any polynomial can

01:20:26.200 --> 01:20:28.300
be completely factored
in complex numbers,

01:20:28.300 --> 01:20:32.660
and every matrix has
a complex egeinvalue.

01:20:32.660 --> 01:20:34.880
OK, I'll stop here.

