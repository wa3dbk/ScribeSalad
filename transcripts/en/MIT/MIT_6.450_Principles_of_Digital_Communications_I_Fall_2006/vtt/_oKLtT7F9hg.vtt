WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.350
The following content is
provided under a Creative

00:00:02.350 --> 00:00:03.630
Commons license.

00:00:03.630 --> 00:00:06.600
Your support will help MIT
OpenCourseWare continue to

00:00:06.600 --> 00:00:10.030
offer high quality educational
resources for free.

00:00:10.030 --> 00:00:13.060
To make a donation, or to view
additional materials from

00:00:13.060 --> 00:00:15.561
hundreds of MIT courses, visit
MIT OpenCourseWare at

00:00:15.561 --> 00:00:21.180
ocw.mit.edu

00:00:21.180 --> 00:00:22.950
PROFESSOR: Review what
random processes are

00:00:22.950 --> 00:00:25.530
a little bit now.

00:00:25.530 --> 00:00:31.260
And remember, in what we're
doing here, we have pointed

00:00:31.260 --> 00:00:36.860
out that random processes
in general, have a mean.

00:00:36.860 --> 00:00:38.420
The mean is not important.

00:00:38.420 --> 00:00:41.150
The mean is not important for
random variables or random

00:00:41.150 --> 00:00:42.500
vectors either.

00:00:42.500 --> 00:00:45.180
It's just something you add
on when you're all done.

00:00:45.180 --> 00:00:48.500
So the best way to study random
variables, random

00:00:48.500 --> 00:00:52.040
vectors, and random processes,
particularly when you're

00:00:52.040 --> 00:00:57.600
dealing with things which are
based on base things being

00:00:57.600 --> 00:01:01.450
Gaussian, is to forget about the
means, do everything you

00:01:01.450 --> 00:01:04.830
want to with the fluctuations,
and then put the means in when

00:01:04.830 --> 00:01:06.280
you're all done.

00:01:06.280 --> 00:01:09.910
Which is why the notes do most
of what they do, in terms of

00:01:09.910 --> 00:01:13.130
zero mean, random variables,
random vectors, and random

00:01:13.130 --> 00:01:19.940
processes, and simply put in
what the value of the mean is

00:01:19.940 --> 00:01:21.640
at the end.

00:01:21.640 --> 00:01:24.470
OK so some of this has
the mean put in.

00:01:24.470 --> 00:01:27.560
Some of it doesn't.

00:01:27.560 --> 00:01:30.520
So to start out with, a random
process is defined by its

00:01:30.520 --> 00:01:34.490
joint distribution at each
finite set of epochs.

00:01:34.490 --> 00:01:37.210
OK, that's where we started
with all of this.

00:01:37.210 --> 00:01:41.570
How do you define a
random waveform?

00:01:41.570 --> 00:01:43.870
I mean, you really have to
come to grips with that

00:01:43.870 --> 00:01:48.870
question before you
can see how you

00:01:48.870 --> 00:01:50.740
might answer the question.

00:01:50.740 --> 00:01:54.910
If you think it's obvious how
to define a random process,

00:01:54.910 --> 00:01:57.130
then you really have to go
back and think about it.

00:01:57.130 --> 00:02:00.810
Because a random process
is a random waveform.

00:02:00.810 --> 00:02:05.890
It's an uncountably infinite
number of random variables.

00:02:05.890 --> 00:02:10.790
So that defining what it is,
is not a trivial problem.

00:02:10.790 --> 00:02:13.720
And people have generally agreed
that the way to define

00:02:13.720 --> 00:02:18.130
it, be the test for whether you
have defined it or not, is

00:02:18.130 --> 00:02:22.460
whether you can find the joint
distribution at each finite

00:02:22.460 --> 00:02:24.660
set of epochs.

00:02:24.660 --> 00:02:29.160
Fortunately most processes of
interest can be defined in a

00:02:29.160 --> 00:02:30.950
much, much simpler
way in terms of

00:02:30.950 --> 00:02:32.850
an orthonormal expansion.

00:02:32.850 --> 00:02:37.150
When you to find it in terms of
an orthonormal expansion,

00:02:37.150 --> 00:02:39.910
you have the orthonormal
expansion which is the sum

00:02:39.910 --> 00:02:46.690
over k, of a set of random
variables, Z sub 1, Z sub 2

00:02:46.690 --> 00:02:51.820
and so forth Z sub k, times a
set of orthonormal functions.

00:02:51.820 --> 00:02:55.660
So all the variation on t
is stuck into the set of

00:02:55.660 --> 00:02:57.210
orthonormal functions.

00:02:57.210 --> 00:02:59.550
All the randomness
is stuck into the

00:02:59.550 --> 00:03:01.350
sequence of random variables.

00:03:01.350 --> 00:03:04.680
So at this point, you have a
sequence of random variables,

00:03:04.680 --> 00:03:06.090
rather than a waveform.

00:03:06.090 --> 00:03:10.870
Why is it so important to have
something countable instead of

00:03:10.870 --> 00:03:12.680
something uncountable?

00:03:12.680 --> 00:03:16.370
I mean, mathematicians
understand, in a deep

00:03:16.370 --> 00:03:20.090
mathematical sense, why
that's so important.

00:03:20.090 --> 00:03:24.750
For engineers the argument is
a little bit different.

00:03:24.750 --> 00:03:28.100
I think for engineers the
argument is, no matter how

00:03:28.100 --> 00:03:32.770
small the interval of a waveform
you're looking at,

00:03:32.770 --> 00:03:36.440
even if you shrink it, no matter
what you do with it,

00:03:36.440 --> 00:03:40.800
you can't approximate it in any
way and get rid of that

00:03:40.800 --> 00:03:44.090
uncountably infinite number
of random variables.

00:03:44.090 --> 00:03:46.830
As soon as you represent it
in terms of an orthonormal

00:03:46.830 --> 00:03:53.520
expansion though, at that point
you have represented it

00:03:53.520 --> 00:03:56.050
as a countable sum.

00:03:56.050 --> 00:03:59.770
As soon as you represent it as
a countable sum, you can

00:03:59.770 --> 00:04:04.340
approximate it by knocking
off the tail of that sum.

00:04:04.340 --> 00:04:07.420
OK, and at that point you have
a finite number of random

00:04:07.420 --> 00:04:09.270
variables, instead
of an infinite

00:04:09.270 --> 00:04:10.830
number of random variables.

00:04:10.830 --> 00:04:12.730
We all know how to deal
with a finite

00:04:12.730 --> 00:04:14.330
set of random variables.

00:04:14.330 --> 00:04:18.790
You've been doing that since
you were in 6.041 OK.

00:04:18.790 --> 00:04:22.600
So if you have a countable set
of random variables, the way

00:04:22.600 --> 00:04:25.250
to deal with them is
always the same.

00:04:28.050 --> 00:04:31.000
It's to hope that they're
defined in such a way that

00:04:31.000 --> 00:04:33.780
when you look at enough of them,
all the rest of them are

00:04:33.780 --> 00:04:35.570
unimportant.

00:04:35.570 --> 00:04:37.940
That sort of is the underlying
meaning of

00:04:37.940 --> 00:04:40.730
what countable means.

00:04:40.730 --> 00:04:44.880
You can arrange them
in a sequence, yes.

00:04:44.880 --> 00:04:49.760
But like when you count in
school, after you name the

00:04:49.760 --> 00:04:53.170
first hundred numbers you get
tired of it, and you realize

00:04:53.170 --> 00:04:56.000
that you understand the whole
thing at that point.

00:04:56.000 --> 00:04:59.270
OK in other words, you don't
have to count up to infinity

00:04:59.270 --> 00:05:02.590
to understand what it
means to have a

00:05:02.590 --> 00:05:04.600
countable set of integers.

00:05:04.600 --> 00:05:06.150
You all understand
the integers.

00:05:06.150 --> 00:05:08.090
You understand them
intuitively.

00:05:08.090 --> 00:05:11.090
And you understand that no
matter how big an integer you

00:05:11.090 --> 00:05:13.940
choose, somebody else can always
choose one bigger than

00:05:13.940 --> 00:05:14.850
you've chosen.

00:05:14.850 --> 00:05:17.730
But you also understand that
that's not important.

00:05:17.730 --> 00:05:18.670
OK?

00:05:18.670 --> 00:05:24.320
So this is the way we usually
define a stochastic process, a

00:05:24.320 --> 00:05:27.170
random process to
start off with.

00:05:27.170 --> 00:05:30.500
The other thing that we have
to do, is given a random

00:05:30.500 --> 00:05:34.140
process like this, we very often
want to go from this

00:05:34.140 --> 00:05:38.160
random process to other random
processes which are defined in

00:05:38.160 --> 00:05:39.970
terms of this random process.

00:05:39.970 --> 00:05:43.610
So almost always we start out
with a random process which is

00:05:43.610 --> 00:05:45.180
defined this way.

00:05:45.180 --> 00:05:48.320
And then we move from there to
various other processes which

00:05:48.320 --> 00:05:50.420
we define in terms of this.

00:05:50.420 --> 00:05:54.400
So we never really have to deal
with this uncountably

00:05:54.400 --> 00:05:56.570
infinite number of
random variables.

00:05:56.570 --> 00:05:58.490
If we really had to
deal with that, we

00:05:58.490 --> 00:06:01.170
would be in deep trouble.

00:06:01.170 --> 00:06:04.040
Because the only way we could
deal with it, would be to take

00:06:04.040 --> 00:06:06.600
five courses in measure
theory.

00:06:06.600 --> 00:06:09.290
And after you take five courses
in measure theory,

00:06:09.290 --> 00:06:11.100
it's not enough.

00:06:11.100 --> 00:06:16.180
Because at that point, you're
living in measure theory.

00:06:16.180 --> 00:06:20.210
And most of the mathematicians
that I know can't come back

00:06:20.210 --> 00:06:25.530
from measure theory to talk
about real problems.

00:06:25.530 --> 00:06:28.360
So that every time they write
a paper, which looks like

00:06:28.360 --> 00:06:33.110
beautiful mathematics, it's very
difficult to interpret

00:06:33.110 --> 00:06:35.650
whether it means anything
about real problems.

00:06:35.650 --> 00:06:38.990
Because that's where
things get hard.

00:06:38.990 --> 00:06:42.510
OK so the point is, if you
define random processes in

00:06:42.510 --> 00:06:46.350
this way, then you always have
some sort of grounding about

00:06:46.350 --> 00:06:47.750
what you're talking about.

00:06:47.750 --> 00:06:49.270
Because you have a countable
number of

00:06:49.270 --> 00:06:50.870
random variables here.

00:06:50.870 --> 00:06:52.920
You know that if you're dealing
with anything that

00:06:52.920 --> 00:06:56.480
makes any sense, only a
finite number of them

00:06:56.480 --> 00:06:57.890
are going to be important.

00:06:57.890 --> 00:07:01.520
The thing you don't know is how
many of them you need to

00:07:01.520 --> 00:07:03.250
be important.

00:07:03.250 --> 00:07:06.930
OK, so that's why we take
an infinite sum here.

00:07:06.930 --> 00:07:09.500
Because you don't know ahead of
time, how many you'd need

00:07:09.500 --> 00:07:10.780
to deal with.

00:07:10.780 --> 00:07:14.880
OK, so we then started to
talk about stationarity.

00:07:17.410 --> 00:07:21.680
The process Z of t is stationary
if Z of t sub 1 up

00:07:21.680 --> 00:07:27.280
to Z of t sub k and Z of t sub
1 plus tau up to Z of t sub k

00:07:27.280 --> 00:07:30.750
plus tau have the same
distribution.

00:07:30.750 --> 00:07:34.810
OK, so you take any finite set
of random variables in this

00:07:34.810 --> 00:07:39.000
process, you shift them all to
some other place, and you ask

00:07:39.000 --> 00:07:41.380
whether they have the
same distribution.

00:07:41.380 --> 00:07:43.760
How do you answer
that question?

00:07:43.760 --> 00:07:45.010
God only knows.

00:07:47.680 --> 00:07:50.860
I mean, what you need to be able
to answer that question

00:07:50.860 --> 00:07:55.320
is some easy way of finding
those joint probabilities.

00:07:55.320 --> 00:07:57.080
And that's why you deal
with examples.

00:07:57.080 --> 00:07:58.750
That's why we deal with jointly

00:07:58.750 --> 00:08:00.600
Gaussian random variables.

00:08:00.600 --> 00:08:03.260
Because once we're dealing with
jointly Gaussian random

00:08:03.260 --> 00:08:07.640
variables, we can write down
those joint distributions just

00:08:07.640 --> 00:08:11.960
in terms of covariance
matrices.

00:08:11.960 --> 00:08:15.770
Well it means, of course, if you
want to deal with things

00:08:15.770 --> 00:08:19.050
that are not a zero mean.

00:08:19.050 --> 00:08:21.700
And after you learn how to deal
with covariance matrices,

00:08:21.700 --> 00:08:23.340
at least you're dealing
with a function of a

00:08:23.340 --> 00:08:24.670
finite number of variables.

00:08:24.670 --> 00:08:26.890
So it's not so bad.

00:08:26.890 --> 00:08:32.480
OK so the argument is, this is
what you need to know if

00:08:32.480 --> 00:08:34.270
you're going to call
it stationary.

00:08:34.270 --> 00:08:37.370
But we have easier ways
of testing for that.

00:08:37.370 --> 00:08:40.920
And in fact this Wide Sense
Stationary we said, if the

00:08:40.920 --> 00:08:47.040
covariance matrix, mainly the
expected value of Z of t sub 1

00:08:47.040 --> 00:08:50.890
times the expected value of Z
of t sub 2 is equal to some

00:08:50.890 --> 00:08:54.010
function of just t sub
1 minus t sub 2.

00:08:54.010 --> 00:08:58.040
In other words, where the
expected value of this value

00:08:58.040 --> 00:09:01.090
times this value is the
same, if you shift it

00:09:01.090 --> 00:09:03.030
over by some amount.

00:09:03.030 --> 00:09:05.010
OK, you see the difference
between?

00:09:05.010 --> 00:09:07.970
I mean the difference between
these two things is one, that

00:09:07.970 --> 00:09:10.950
in the definition for
stationarity, you need an

00:09:10.950 --> 00:09:14.570
arbitrarily large set of
random variables here.

00:09:14.570 --> 00:09:18.340
You shift them over to some
other point here.

00:09:18.340 --> 00:09:20.790
And you need the same joint
distribution over this

00:09:20.790 --> 00:09:23.260
arbitrarily large set
of random variables.

00:09:23.260 --> 00:09:26.780
When you get into dealing with
the covariance matrix, all you

00:09:26.780 --> 00:09:29.460
need for Wide Sense Stationarity
is you only have

00:09:29.460 --> 00:09:32.120
to deal with two random
variables here, and the shift

00:09:32.120 --> 00:09:34.830
of those two random
variables here.

00:09:34.830 --> 00:09:39.440
Which really comes out to a
problem involving just two

00:09:39.440 --> 00:09:42.830
random variables t sub 1, t sub
2, and the fact that it's

00:09:42.830 --> 00:09:47.180
the same no matter where you
shift the t sub 1 to.

00:09:47.180 --> 00:09:50.000
It's only a function of the
difference between t

00:09:50.000 --> 00:09:53.120
sub 1 and t sub 2.

00:09:53.120 --> 00:09:55.400
So it's a whole lot easier.

00:09:55.400 --> 00:09:59.120
And then we pointed out that
since jointly Gaussian random

00:09:59.120 --> 00:10:02.170
variables, and since the
Gaussian random process, it's

00:10:02.170 --> 00:10:06.800
completely determined by its
covariance function, a zero

00:10:06.800 --> 00:10:09.740
mean Gaussian process.

00:10:09.740 --> 00:10:12.260
If you'll forgive me, I'm not
going to keep coming back and

00:10:12.260 --> 00:10:12.840
saying that.

00:10:12.840 --> 00:10:15.950
I will just be thinking
solely today in terms

00:10:15.950 --> 00:10:17.880
of zero mean processes.

00:10:17.880 --> 00:10:21.090
And you should think solely in
terms of zero mean processes.

00:10:21.090 --> 00:10:23.640
You can sort out for yourselves
whether you'd need

00:10:23.640 --> 00:10:25.090
a mean or not.

00:10:25.090 --> 00:10:28.160
That's sort of a trivial
addition to all of this.

00:10:28.160 --> 00:10:32.930
OK, so it's Wide Sense
Stationary.

00:10:32.930 --> 00:10:34.880
Well we already said that.

00:10:38.320 --> 00:10:41.820
Well, here I put in the mean.

00:10:41.820 --> 00:10:44.990
OK, Wide Sense Stationary
implies that it's stationary

00:10:44.990 --> 00:10:46.790
for Gaussian processes.

00:10:46.790 --> 00:10:50.650
OK, so in other words, this
messy looking question of

00:10:50.650 --> 00:10:53.990
asking whether a process is
stationary or not, with all of

00:10:53.990 --> 00:10:56.390
these random variables here
and all of these random

00:10:56.390 --> 00:11:01.920
variables here, becomes
trivialized for the case of a

00:11:01.920 --> 00:11:05.703
Gaussian random process, where
all you need to worry about is

00:11:05.703 --> 00:11:07.150
the covariance function.

00:11:07.150 --> 00:11:11.140
Because that's the thing that
specifies the whole process.

00:11:11.140 --> 00:11:12.390
OK.

00:11:15.020 --> 00:11:17.540
That's part of what
we did last time.

00:11:20.450 --> 00:11:23.810
We set an important
example of this.

00:11:23.810 --> 00:11:26.280
And you don't quite know how
important this is yet.

00:11:26.280 --> 00:11:31.060
But this is really important
because all of the Gaussian

00:11:31.060 --> 00:11:34.710
processes you want to talk about
can be formed in terms

00:11:34.710 --> 00:11:36.840
of this process.

00:11:36.840 --> 00:11:38.990
So it's nice in that way.

00:11:38.990 --> 00:11:42.860
And the process is Z
of t is the sum of

00:11:42.860 --> 00:11:44.340
an orthonormal expansion.

00:11:44.340 --> 00:11:49.080
But the orthonormal functions
now are just the time shifted

00:11:49.080 --> 00:11:50.250
sinc functions.

00:11:50.250 --> 00:11:54.530
We know the time shifted sinc
functions are orthogonal.

00:11:54.530 --> 00:11:57.460
So we just have this set
of random variables.

00:11:57.460 --> 00:12:00.990
And we say OK, what we're going
to assume here is that

00:12:00.990 --> 00:12:07.530
these random variables, well
here I put in the mean.

00:12:07.530 --> 00:12:09.090
Let's forget about the mean.

00:12:09.090 --> 00:12:13.830
The expected value of V sub k
times V sub i, namely any two

00:12:13.830 --> 00:12:19.000
of these random variables
expected value of the pair, is

00:12:19.000 --> 00:12:21.840
equal to zero if K
is unequal to i.

00:12:21.840 --> 00:12:25.620
OK, in other words, the random
variables are in some sense

00:12:25.620 --> 00:12:26.990
orthogonal to each other.

00:12:26.990 --> 00:12:30.700
But let's save the word
orthogonal for functions, and

00:12:30.700 --> 00:12:33.450
use the word correlated
or uncorrelated

00:12:33.450 --> 00:12:34.890
here for random variables.

00:12:34.890 --> 00:12:37.940
So these random variables
are uncorrelated.

00:12:37.940 --> 00:12:41.080
So we're dealing with an
expansion where the functions

00:12:41.080 --> 00:12:44.620
are orthogonal, and where
the random variables are

00:12:44.620 --> 00:12:46.350
uncorrelated.

00:12:46.350 --> 00:12:49.150
And now, what we're really
interested in, is making these

00:12:49.150 --> 00:12:52.060
random variables Gaussian.

00:12:52.060 --> 00:12:57.140
And then we have a Gaussian
random process with

00:12:57.140 --> 00:12:58.740
this whole sum here.

00:12:58.740 --> 00:13:03.940
We're interested in making these
have the same variance

00:13:03.940 --> 00:13:05.220
in all cases.

00:13:05.220 --> 00:13:12.080
And therefore what we have is
process which is stationary.

00:13:12.080 --> 00:13:16.870
And its covariance function is
just sigma squared times this

00:13:16.870 --> 00:13:18.460
sinc function here.

00:13:18.460 --> 00:13:22.620
There's an error in the notes,
in lecture 16, about this.

00:13:22.620 --> 00:13:25.570
It will be corrected
on the web.

00:13:25.570 --> 00:13:28.690
This is in the notes.

00:13:31.200 --> 00:13:34.990
This quantity here
is left out.

00:13:34.990 --> 00:13:39.430
So you can put that back
in if you want to.

00:13:39.430 --> 00:13:44.570
Or you can get the new notes
off the web after a

00:13:44.570 --> 00:13:45.590
few hours or so.

00:13:45.590 --> 00:13:47.790
This is not put on
the web yet.

00:13:47.790 --> 00:13:49.200
I just noticed yesterday.

00:13:49.200 --> 00:13:52.130
In fact I noticed
it this morning.

00:13:52.130 --> 00:13:55.820
OK, the sample functions of a
Wide Sense Stationary non-zero

00:13:55.820 --> 00:13:58.280
process are not L sub 2.

00:13:58.280 --> 00:14:01.670
When I talk about a non-zero
process I'm not talking about

00:14:01.670 --> 00:14:05.810
zero mean, I'm saying this
peculiar random process which

00:14:05.810 --> 00:14:08.100
is zero everywhere.

00:14:08.100 --> 00:14:10.890
In other words, a random process
which really doesn't

00:14:10.890 --> 00:14:13.490
deserve to be called
a random process.

00:14:13.490 --> 00:14:15.370
But unfortunately by
the definitions,

00:14:15.370 --> 00:14:17.180
it is a random process.

00:14:17.180 --> 00:14:21.160
Because with probability one,
Z of t is equal zero

00:14:21.160 --> 00:14:22.860
everywhere.

00:14:22.860 --> 00:14:25.130
And that just means you have
a constant, which is zero

00:14:25.130 --> 00:14:25.590
everywhere.

00:14:25.590 --> 00:14:27.610
So you're not interested
in it.

00:14:27.610 --> 00:14:30.780
OK, if you take the sample
functions of any non-trivial

00:14:30.780 --> 00:14:33.790
Wide Sense Stationary
random process,

00:14:33.790 --> 00:14:36.430
they dribble on forever.

00:14:36.430 --> 00:14:39.270
And they have the same--
no matter how far

00:14:39.270 --> 00:14:42.140
you go out in time--

00:14:42.140 --> 00:14:46.470
the V sub k's which are the
samples of this process,

00:14:46.470 --> 00:14:51.880
sample random variables of
the process, weigh out.

00:14:51.880 --> 00:14:54.790
These things all have
the same variance.

00:14:54.790 --> 00:14:57.010
So that this is stationary
because it just

00:14:57.010 --> 00:14:59.610
keeps going on forever.

00:14:59.610 --> 00:15:03.510
OK and as we said last time,
that runs into violent

00:15:03.510 --> 00:15:10.040
conflict with our whole idea
here of trying to understand

00:15:10.040 --> 00:15:12.320
what L sub 2 functions
are all about.

00:15:12.320 --> 00:15:15.520
Because what we would like to
be able to do is stick the L

00:15:15.520 --> 00:15:19.500
sub 2 functions as sample values
of these processes,

00:15:19.500 --> 00:15:23.080
have the waveforms that
we send the L sub 2.

00:15:23.080 --> 00:15:25.270
When you add up two L
sub 2 functions, you

00:15:25.270 --> 00:15:27.180
get an L sub 2 function.

00:15:27.180 --> 00:15:30.050
And therefore, all the sample
values of all the things that

00:15:30.050 --> 00:15:35.160
happen with probability one
are all L sub 2 functions.

00:15:35.160 --> 00:15:39.480
As soon as you do the most
natural and simple thing with

00:15:39.480 --> 00:15:45.270
random processes, you wind up
with something which cannot be

00:15:45.270 --> 00:15:47.110
L sub 2 anymore.

00:15:47.110 --> 00:15:51.830
But it's not L sub 2 in
only a trivial way.

00:15:51.830 --> 00:15:55.140
OK in other words, it's
not L sub 2 because

00:15:55.140 --> 00:15:57.210
it keeps going forever.

00:15:57.210 --> 00:16:00.980
It's not L sub 2 for the same
reason that sine x is not L

00:16:00.980 --> 00:16:04.450
sub 2, or 1 is not L sub 2.

00:16:04.450 --> 00:16:07.670
OK, and we've decided not to
look at those functions as

00:16:07.670 --> 00:16:09.790
reasonable functions for
the waveforms that

00:16:09.790 --> 00:16:12.460
we transmit or receive.

00:16:12.460 --> 00:16:16.570
But for random processes, maybe
they're not so bad.

00:16:16.570 --> 00:16:18.120
OK.

00:16:18.120 --> 00:16:19.650
But we don't know yet.

00:16:25.580 --> 00:16:27.730
We started to talk about
something called effectively

00:16:27.730 --> 00:16:30.530
Wide Sense Stationary,
which the notes talk

00:16:30.530 --> 00:16:32.720
about quite a bit.

00:16:32.720 --> 00:16:36.200
Which to say, OK we will assume
that the process is

00:16:36.200 --> 00:16:40.840
stationary over some very
wide time interval.

00:16:40.840 --> 00:16:43.470
We don't know how wide that time
interval is, but we'll

00:16:43.470 --> 00:16:44.530
assume that it's finite.

00:16:44.530 --> 00:16:47.220
In other words, we're going to
take this random process,

00:16:47.220 --> 00:16:51.420
whatever it is, you can define
it, a nice random

00:16:51.420 --> 00:16:53.270
process this way.

00:16:53.270 --> 00:16:55.800
And then we're going to get our
scissors, and we're going

00:16:55.800 --> 00:16:57.940
to cut it off over here
and we're going to

00:16:57.940 --> 00:16:59.360
cut it off over here.

00:16:59.360 --> 00:17:03.600
So it's then, time-limited over
this very broad band.

00:17:03.600 --> 00:17:05.820
Why do we want to do this?

00:17:05.820 --> 00:17:11.460
Because again, it's like the
integers which are countable.

00:17:11.460 --> 00:17:15.210
You don't know how far out you
have to go before you're not

00:17:15.210 --> 00:17:17.550
interested in something
anymore.

00:17:17.550 --> 00:17:20.750
But you know if you go out
far enough, you can't be

00:17:20.750 --> 00:17:23.000
interested in it anymore.

00:17:23.000 --> 00:17:24.590
Because otherwise you
can't take limits.

00:17:24.590 --> 00:17:28.590
You can't do anything
interesting with sequences.

00:17:28.590 --> 00:17:30.450
OK, so here the idea is.

00:17:30.450 --> 00:17:33.420
If you go out far enough,
you don't care.

00:17:33.420 --> 00:17:38.880
This has to be the way that you
model things because any

00:17:38.880 --> 00:17:45.190
piece of equipment that you ever
design is going to start

00:17:45.190 --> 00:17:48.820
being used at a certain time,
and it's going to stop being

00:17:48.820 --> 00:17:51.260
used at a certain time.

00:17:51.260 --> 00:17:53.880
And about halfway after when
it stops being used, the

00:17:53.880 --> 00:17:56.720
people who make it will
stop supporting it.

00:17:56.720 --> 00:17:59.710
And that's to hurry on the time
at which you have to buy

00:17:59.710 --> 00:18:01.280
something new.

00:18:01.280 --> 00:18:04.650
I bought a new computer
about six months ago.

00:18:04.650 --> 00:18:07.810
And I find that Microsoft is
not supporting any of the

00:18:07.810 --> 00:18:12.060
stuff that it loaded into this
damn computer anymore.

00:18:12.060 --> 00:18:19.330
Six months old, so you see why
I'm angry about this matter,

00:18:19.330 --> 00:18:20.890
of things not being supported.

00:18:20.890 --> 00:18:24.210
So in a sense they're not
stationary after that.

00:18:24.210 --> 00:18:27.470
So you can only ask for things
to be stationary over a period

00:18:27.470 --> 00:18:28.970
of six months or so.

00:18:28.970 --> 00:18:31.030
OK.

00:18:31.030 --> 00:18:34.680
But for electronic times, when
you're sending data at

00:18:34.680 --> 00:18:37.270
kilobits per second or megabits
per second, or

00:18:37.270 --> 00:18:41.160
gigabits per second as people
like to do now, that's an

00:18:41.160 --> 00:18:42.200
awful long time.

00:18:42.200 --> 00:18:46.530
You can send an awful lot
of bits in that time.

00:18:46.530 --> 00:18:50.290
So stationary means stationary
for a long time.

00:18:50.290 --> 00:18:52.480
The notes do a lot of
analysis of this.

00:18:52.480 --> 00:18:54.410
I'm going to do a little
of that analysis

00:18:54.410 --> 00:18:56.210
here in class again.

00:18:56.210 --> 00:18:59.900
Not because it's so important
to understand the details of

00:18:59.900 --> 00:19:03.890
it, but to understand what it
really means to have a process

00:19:03.890 --> 00:19:07.090
be stationary, and to understand
that doesn't really

00:19:07.090 --> 00:19:11.410
destroy any of the L sub 2
theory that we built up.

00:19:11.410 --> 00:19:12.500
OK.

00:19:12.500 --> 00:19:15.720
So the covariance function
is L sub 2 in

00:19:15.720 --> 00:19:18.880
cases of physical relevance.

00:19:18.880 --> 00:19:20.990
That's the thing.

00:19:20.990 --> 00:19:23.210
That's one of the
things we need.

00:19:23.210 --> 00:19:28.770
We want the sample functions
also to be L sub 2 in cases of

00:19:28.770 --> 00:19:30.550
physical relevance.

00:19:30.550 --> 00:19:33.290
If you have a function here,
this is just a function of

00:19:33.290 --> 00:19:34.490
time now at this point.

00:19:34.490 --> 00:19:36.710
There's nothing random
about this.

00:19:36.710 --> 00:19:39.910
It's just a statistic of
this random process.

00:19:39.910 --> 00:19:42.220
But it's a nice well-defined
function.

00:19:42.220 --> 00:19:45.560
We're talking about real
random processes here.

00:19:45.560 --> 00:19:49.020
So what can you say about
this function?

00:19:49.020 --> 00:19:50.550
Is it symmetric in time?

00:19:54.330 --> 00:19:55.830
How many people think it
must be symmetric?

00:19:59.040 --> 00:20:00.980
I see a number of people.

00:20:00.980 --> 00:20:02.300
How many people think
it's not symmetric?

00:20:05.350 --> 00:20:06.480
Well it is symmetric.

00:20:06.480 --> 00:20:13.656
It has to be symmetric because
its expected value of Z of t

00:20:13.656 --> 00:20:17.370
sub 1 times Z of t sub 2, and if
you flip the roles of those

00:20:17.370 --> 00:20:19.790
two, you have to get
the same answer.

00:20:19.790 --> 00:20:21.910
So it is symmetric.

00:20:21.910 --> 00:20:23.160
It is real.

00:20:25.380 --> 00:20:28.070
We're going to talk about the
Fourier transform of this

00:20:28.070 --> 00:20:32.920
before trying to give any
relevance or physical meaning

00:20:32.920 --> 00:20:35.800
to this thing called
spectral density.

00:20:35.800 --> 00:20:39.640
We'll just say this is the
Fourier transform of the

00:20:39.640 --> 00:20:45.290
covariance function for a Wide
Sense Stationary process.

00:20:45.290 --> 00:20:51.380
So we have some kind of
Fourier transform.

00:20:51.380 --> 00:20:53.560
We'll assume this is L
sub 2 and everything.

00:20:53.560 --> 00:20:55.980
We won't worry about
any of that.

00:20:55.980 --> 00:20:59.750
So there is some function
here which makes sense.

00:20:59.750 --> 00:21:04.660
Now if k is both real and
symmetric, what can you say

00:21:04.660 --> 00:21:06.360
about its Fourier transform?

00:21:09.020 --> 00:21:12.330
If you have a real function,
what property does this

00:21:12.330 --> 00:21:13.940
Fourier transform have?

00:21:17.160 --> 00:21:18.450
AUDIENCE: [UNINTELLIGIBLE]

00:21:18.450 --> 00:21:21.460
PROFESSOR: It's conjugate
symmetric, yes.

00:21:21.460 --> 00:21:28.860
And furthermore, if k is
symmetric itself, then you can

00:21:28.860 --> 00:21:32.990
go back from this thinking you
have a symmetric transform,

00:21:32.990 --> 00:21:35.830
and realize that this
has to be real.

00:21:35.830 --> 00:21:36.350
OK.

00:21:36.350 --> 00:21:43.890
So spectral densities of real
processes are always both real

00:21:43.890 --> 00:21:45.170
and symmetric.

00:21:45.170 --> 00:21:48.810
We will define what we mean by
spectral density later for

00:21:48.810 --> 00:21:50.810
complex processes.

00:21:50.810 --> 00:21:54.750
And spectral densities
are always real.

00:21:54.750 --> 00:22:00.450
They aren't always symmetric if
you have a complex process.

00:22:00.450 --> 00:22:03.010
But don't worry about
that now.

00:22:03.010 --> 00:22:05.440
There are enough peculiarities
that come in when we start

00:22:05.440 --> 00:22:08.550
dealing with complex processes,
that I don't want

00:22:08.550 --> 00:22:10.020
to worry about it at all.

00:22:10.020 --> 00:22:13.160
So spectral density is
real and symmetric.

00:22:13.160 --> 00:22:16.850
And so far it's just
a definition.

00:22:16.850 --> 00:22:21.640
OK, but the thing we found about
this sinc process, is

00:22:21.640 --> 00:22:26.580
that the sinc process in fact,
is a stationary process.

00:22:26.580 --> 00:22:30.240
It's a Wide Sense Stationary
process for whatever variables

00:22:30.240 --> 00:22:31.580
you want to put in here.

00:22:31.580 --> 00:22:35.980
And if these variables are IID
and they're Gaussian and zero

00:22:35.980 --> 00:22:41.230
mean, then this process is a
zero mean, stationary Gaussian

00:22:41.230 --> 00:22:42.540
random process.

00:22:42.540 --> 00:22:45.520
And it has a spectral density.

00:22:45.520 --> 00:22:48.750
This turns out to be
a sinc function.

00:22:48.750 --> 00:22:53.420
The Fourier transform of it
is a rectangular function.

00:22:53.420 --> 00:22:57.390
So this process has a spectral
density which is constant out

00:22:57.390 --> 00:23:01.090
to a certain point, and then
drops off to zero.

00:23:01.090 --> 00:23:03.850
So it's nice in that sense.

00:23:03.850 --> 00:23:08.850
Because you can make this be
flat as far as you want to,

00:23:08.850 --> 00:23:11.410
and then chop it off wherever
you want to.

00:23:11.410 --> 00:23:14.290
And we'll see that when we start
putting a process like

00:23:14.290 --> 00:23:18.760
this through filters, very
nice things happen.

00:23:18.760 --> 00:23:21.510
OK.

00:23:21.510 --> 00:23:25.150
So we're familiar, relatively
familiar, and conversant with

00:23:25.150 --> 00:23:30.120
at least one Gaussian
random process.

00:23:30.120 --> 00:23:32.750
OK, we talked about linear
functionals last

00:23:32.750 --> 00:23:35.840
time before the quiz.

00:23:35.840 --> 00:23:38.360
And we said a linear functional
is a random

00:23:38.360 --> 00:23:43.750
variable, V, which is simply
this integral here.

00:23:43.750 --> 00:23:46.970
We've talked about this integral
a lot where Z of t is

00:23:46.970 --> 00:23:50.810
a function, and g of
t is a function.

00:23:50.810 --> 00:23:54.850
If the sample values of Z of
t are L sub 2, then this

00:23:54.850 --> 00:23:58.240
integral is very well-defined.

00:23:58.240 --> 00:24:03.180
It turns out, also, that if g
of t is L sub 2, and these

00:24:03.180 --> 00:24:06.640
sample values are bounded, then
all of this works out

00:24:06.640 --> 00:24:08.420
very nicely also.

00:24:08.420 --> 00:24:10.990
And we'll find other ways to
look at this as we move on.

00:24:10.990 --> 00:24:16.560
OK but what this means is that
for all sample points in this

00:24:16.560 --> 00:24:19.340
entire sample space, in other
words we're dealing with a

00:24:19.340 --> 00:24:22.790
probability space where we
have a bunch of processes

00:24:22.790 --> 00:24:25.780
running around, we have
data coming in.

00:24:25.780 --> 00:24:27.820
We're transmitting the data.

00:24:27.820 --> 00:24:29.820
We have data that
gets received.

00:24:29.820 --> 00:24:31.970
We're doing crazy
things with it.

00:24:31.970 --> 00:24:33.730
All of this stuff is random.

00:24:33.730 --> 00:24:37.450
We might have other processes
doing something else.

00:24:37.450 --> 00:24:42.380
Big complicated sample space
here for all of the elements

00:24:42.380 --> 00:24:49.530
in it, the sample value of the
random variable, V, is just

00:24:49.530 --> 00:24:53.350
the integral of the sample value
of the process here.

00:24:53.350 --> 00:24:55.550
In other words for
each omega, this

00:24:55.550 --> 00:24:59.340
process is simply a waveform.

00:24:59.340 --> 00:25:03.840
So it's this waveform times the
function g of t, which we

00:25:03.840 --> 00:25:05.180
think of like that.

00:25:05.180 --> 00:25:12.610
So if g of t is time limited in
L sub 2, if g of t is time

00:25:12.610 --> 00:25:17.350
limited as far as these sample
functions are concerned, we

00:25:17.350 --> 00:25:20.250
don't give a fig about
what Z of t is

00:25:20.250 --> 00:25:22.990
outside of that interval.

00:25:22.990 --> 00:25:28.120
And if we're going to only deal
with linear operations on

00:25:28.120 --> 00:25:31.310
this process where those
linear operations are

00:25:31.310 --> 00:25:35.540
constrained to some interval,
then we don't care what the

00:25:35.540 --> 00:25:38.550
process does outside
of that interval.

00:25:38.550 --> 00:25:41.300
And since we don't care what
the process does outside of

00:25:41.300 --> 00:25:46.940
that interval, we can simply
define the process in terms of

00:25:46.940 --> 00:25:50.320
what it's doing from some large
negative time to some

00:25:50.320 --> 00:25:51.620
large positive time.

00:25:51.620 --> 00:25:55.990
And that's all we care about.

00:25:55.990 --> 00:25:59.040
And so we wanted to find
effective stationarity, as

00:25:59.040 --> 00:26:00.860
something which obeys
the law of

00:26:00.860 --> 00:26:03.280
stationarity over that interval.

00:26:03.280 --> 00:26:07.260
And we don't care what it does
outside of that interval, OK?

00:26:07.260 --> 00:26:10.000
In other words, if Microsoft
stops supporting things after

00:26:10.000 --> 00:26:12.900
six months, we don't
care about it.

00:26:12.900 --> 00:26:15.160
Because everything we're going
to do we're going to get done

00:26:15.160 --> 00:26:16.970
within six months.

00:26:16.970 --> 00:26:19.020
And if we don't get it done
within six months we're going

00:26:19.020 --> 00:26:20.530
to have a terrible crash.

00:26:20.530 --> 00:26:21.900
And we're going to throw
everything away

00:26:21.900 --> 00:26:23.060
and start over again.

00:26:23.060 --> 00:26:25.530
So it doesn't make any
difference at that point.

00:26:30.130 --> 00:26:35.080
OK, so we have a process now
we'll assume is effectively

00:26:35.080 --> 00:26:39.370
stationary within
two time limits.

00:26:39.370 --> 00:26:43.400
And our definition is if the
covariance function, which is

00:26:43.400 --> 00:26:46.740
a function of two values, t
and tau, namely it's the

00:26:46.740 --> 00:26:52.360
expected value of Z of t times
expected value of Z of tau.

00:26:52.360 --> 00:26:55.590
And our definition of effective
stationarity is that

00:26:55.590 --> 00:27:00.120
this is equal to a function of
t minus tau, whenever t and

00:27:00.120 --> 00:27:02.910
tau are in this box.

00:27:02.910 --> 00:27:06.030
OK, and we drew figures last
time for what that meant.

00:27:06.030 --> 00:27:10.060
We drew this box, you know,
bounded by T sub 0 over 2.

00:27:10.060 --> 00:27:13.390
And what this effective
stationarity means, is that on

00:27:13.390 --> 00:27:17.570
all these diagonal lines, the
covariance function is

00:27:17.570 --> 00:27:20.390
constant on those
diagonal lines.

00:27:20.390 --> 00:27:25.380
And that gives us the same
effect whenever we're passing

00:27:25.380 --> 00:27:33.850
this process any time we're
calculating the inner product

00:27:33.850 --> 00:27:37.900
of a sample value of the process
with some function,

00:27:37.900 --> 00:27:40.680
which is contained within
those limits.

00:27:40.680 --> 00:27:44.540
All we need to know about is
just what the process is doing

00:27:44.540 --> 00:27:48.720
within minus T sub 0 over 2
to plus T sub 0 over 2.

00:27:48.720 --> 00:27:50.720
Nothing else matters.

00:27:50.720 --> 00:27:53.970
And therefore, something is
effectively stationary within

00:27:53.970 --> 00:27:57.820
those limits means that we get
the same answer here, whether

00:27:57.820 --> 00:28:00.830
or not it's stationary.

00:28:00.830 --> 00:28:02.110
Why is that important?

00:28:05.320 --> 00:28:10.160
It's important because the
things that you can do, the

00:28:10.160 --> 00:28:14.170
simple things that you can do
with stationary processes are

00:28:14.170 --> 00:28:18.790
so simple that you'd like to
remember them, and not

00:28:18.790 --> 00:28:23.030
remember all these formulas
with capital T sub 0's

00:28:23.030 --> 00:28:25.110
floating around in them.

00:28:25.110 --> 00:28:29.520
Because having a capital T sub
0 in it is just as crazy as

00:28:29.520 --> 00:28:33.200
assuming that it's stationary,
because you never know how

00:28:33.200 --> 00:28:35.350
long it's going to be until
Microsoft stops

00:28:35.350 --> 00:28:38.190
supporting its software.

00:28:38.190 --> 00:28:39.550
I mean, if you knew
you'd never buy

00:28:39.550 --> 00:28:41.570
their products, right?

00:28:41.570 --> 00:28:43.960
So you can't possibly know.

00:28:43.960 --> 00:28:47.140
So you assume it's going
to go on forever.

00:28:47.140 --> 00:28:51.190
But then, when we try to ask
what does that mean, we say

00:28:51.190 --> 00:28:54.310
what it really means is over
some finite limits, which are

00:28:54.310 --> 00:28:56.840
large compared to anything we're
going to deal with that

00:28:56.840 --> 00:28:58.730
at all of these results hold.

00:28:58.730 --> 00:29:01.360
So eventually we're trying to
get to the point where we can

00:29:01.360 --> 00:29:05.040
leave the T sub 0 out of it.

00:29:05.040 --> 00:29:08.250
But we're going through this
so we can say, OK there's

00:29:08.250 --> 00:29:12.650
nothing magical that happens in
the limit as T sub 0 goes

00:29:12.650 --> 00:29:13.900
to infinite.

00:29:15.810 --> 00:29:20.320
Mainly we're trying to really
derive the fact that all these

00:29:20.320 --> 00:29:22.530
results can be established
over a

00:29:22.530 --> 00:29:24.560
finite interval of time.

00:29:24.560 --> 00:29:28.060
And therefore you don't care.

00:29:28.060 --> 00:29:28.460
OK.

00:29:28.460 --> 00:29:37.120
So if this covariance function,
single variable

00:29:37.120 --> 00:29:40.880
covariance function, is
less than infinity,

00:29:40.880 --> 00:29:43.130
what does that mean?

00:29:43.130 --> 00:29:46.510
What is a single variance,
single value, covariance

00:29:46.510 --> 00:29:48.980
function of zero mean?

00:29:48.980 --> 00:29:55.120
It's the expected value of Z of
T, times Z of T. In other

00:29:55.120 --> 00:29:59.790
words, it's the variance of
the process at T for any T

00:29:59.790 --> 00:30:04.860
within minus T sub 0 to plus T
sub 0, or T sub 0 over 2 to

00:30:04.860 --> 00:30:06.700
plus T sub 0 over 2.

00:30:06.700 --> 00:30:07.950
OK?

00:30:09.960 --> 00:30:14.590
So if that variance is finite,
then you can just integrate

00:30:14.590 --> 00:30:17.830
over the sample values of
the process, and you

00:30:17.830 --> 00:30:20.050
get something finite.

00:30:20.050 --> 00:30:24.200
In other words this is what
you need for the sample

00:30:24.200 --> 00:30:29.370
functions to be L sub 2
with probability one.

00:30:29.370 --> 00:30:33.491
As soon as you have this, then
you're in business with all of

00:30:33.491 --> 00:30:35.780
your L sub 2 theory.

00:30:35.780 --> 00:30:38.500
And what does your L
sub 2 theory say?

00:30:38.500 --> 00:30:42.080
It really says you can ignore
L sub 2 theory.

00:30:42.080 --> 00:30:43.740
OK, that was the nice
thing about it.

00:30:43.740 --> 00:30:44.930
That was why we did it.

00:30:44.930 --> 00:30:48.780
OK in other words, the whole
thing we're doing in this

00:30:48.780 --> 00:30:54.040
course is we're going through
complicated things sometimes

00:30:54.040 --> 00:30:57.310
so that you can know
how far you can go

00:30:57.310 --> 00:30:59.680
with the simple things.

00:30:59.680 --> 00:31:01.560
And we always end up with
the simple things

00:31:01.560 --> 00:31:03.990
when we're all done.

00:31:03.990 --> 00:31:06.680
OK, and that's the
whole principle.

00:31:06.680 --> 00:31:08.970
I mean we could just do the
simple things like most

00:31:08.970 --> 00:31:10.940
courses do.

00:31:10.940 --> 00:31:13.840
Except then, you would never
know when it applies and when

00:31:13.840 --> 00:31:14.800
it doesn't apply.

00:31:14.800 --> 00:31:16.690
So, OK.

00:31:19.840 --> 00:31:21.090
So there we are.

00:31:23.600 --> 00:31:28.180
Let's talk about linear
filtering of processes.

00:31:28.180 --> 00:31:32.740
The notes don't do quite
enough of this.

00:31:32.740 --> 00:31:36.125
So when I get to the place where
you need it I will-- and

00:31:36.125 --> 00:31:38.420
some of it's on this slide--

00:31:38.420 --> 00:31:40.780
I'll tell you.

00:31:40.780 --> 00:31:46.570
We're taking a random process
and we're passing the sample

00:31:46.570 --> 00:31:49.770
waveform of that random process
through a linear

00:31:49.770 --> 00:31:53.620
filter, a linear time and
variant filter, so some other

00:31:53.620 --> 00:31:56.360
random process comes out.

00:31:56.360 --> 00:32:01.810
OK, and now the output at some
time tau, is just the

00:32:01.810 --> 00:32:07.480
convolution of the input Z of t
times this filter response h

00:32:07.480 --> 00:32:09.110
of tau minus t.

00:32:09.110 --> 00:32:11.970
And you can interpret this
in terms of these sample

00:32:11.970 --> 00:32:15.190
functions the way
we did before.

00:32:15.190 --> 00:32:18.600
But now we already sort
of understand that.

00:32:18.600 --> 00:32:21.330
So we're just interpreting in
terms of if there's a random

00:32:21.330 --> 00:32:26.780
variable V, which is the value
of the process at time at

00:32:26.780 --> 00:32:29.540
epoch tau, which is
given in this way.

00:32:32.570 --> 00:32:35.690
It's a random variable when you
pass the process through

00:32:35.690 --> 00:32:37.310
the filter.

00:32:37.310 --> 00:32:41.430
OK, if Z of t is effectively
Wide Sense Stationary with L

00:32:41.430 --> 00:32:46.650
sub 2 sample functions, and if h
of t is non-zero only within

00:32:46.650 --> 00:32:51.290
finite limits, minus A to plus
A, what's going to happen?

00:32:51.290 --> 00:32:55.540
When you pass the sample
waveforms through a linear

00:32:55.540 --> 00:33:00.560
filter, which is bounded between
minus A and plus A,

00:33:00.560 --> 00:33:08.470
that linear filter cannot do
anything with the inputs.

00:33:08.470 --> 00:33:12.260
Namely the output that comes out
of there at some time, T,

00:33:12.260 --> 00:33:15.510
can't depend on the input
anymore than A

00:33:15.510 --> 00:33:18.900
away from that output.

00:33:18.900 --> 00:33:23.040
OK in other words, let me
draw a picture of that.

00:33:27.040 --> 00:33:32.430
Here's some time where we're
observing V of t.

00:33:32.430 --> 00:33:41.420
And V of t can depend on Z of t
only in this region here, Z

00:33:41.420 --> 00:33:49.970
of t minus A up to Z of t plus
A. OK that's what the

00:33:49.970 --> 00:33:51.030
convolution says.

00:33:51.030 --> 00:33:54.250
That's what those filter
equations say.

00:33:54.250 --> 00:33:58.860
It says that this output, here
at this time, depends on the

00:33:58.860 --> 00:34:02.110
input only over these
finite limits.

00:34:02.110 --> 00:34:05.410
And again remember we don't care
about realizability here.

00:34:05.410 --> 00:34:08.550
Because the timing at the
receiver is always different

00:34:08.550 --> 00:34:11.740
from the timing at
the transmitter.

00:34:11.740 --> 00:34:20.640
OK so what that says then
is, what does it say?

00:34:24.260 --> 00:34:30.380
It says that we know that Z of
t is Wide Sense Stationary

00:34:30.380 --> 00:34:36.240
over these big limits minus T
sub 0 to t sub 0 from six

00:34:36.240 --> 00:34:39.130
months ago until six
months from now.

00:34:39.130 --> 00:34:43.700
And this linear filter is
non-zero only over one

00:34:43.700 --> 00:34:45.810
millisecond.

00:34:45.810 --> 00:34:50.070
It says that the process which
comes out, is going to be Wide

00:34:50.070 --> 00:34:53.520
Sense Stationary over minus
six months plus one

00:34:53.520 --> 00:34:59.400
millisecond to six months
minus one millisecond.

00:34:59.400 --> 00:35:00.660
And that's just common sense.

00:35:00.660 --> 00:35:04.400
Because the filter isn't moving
the process any more

00:35:04.400 --> 00:35:05.650
than that little bit.

00:35:09.200 --> 00:35:10.420
OK.

00:35:10.420 --> 00:35:14.340
So V of t then, is going to be
Wide Sense Stationary in L sub

00:35:14.340 --> 00:35:18.800
2 within those slightly
smaller limits.

00:35:18.800 --> 00:35:23.880
The covariance function is going
to be the same thing

00:35:23.880 --> 00:35:26.880
that it was before if you had
a completely stationary

00:35:26.880 --> 00:35:31.010
process, if you only worry about
what's going on within

00:35:31.010 --> 00:35:35.900
those limits, T sub 0 minus A to
T sub zero plus A. You can

00:35:35.900 --> 00:35:41.430
take that big mess there, and
view it more simply as a

00:35:41.430 --> 00:35:42.810
convolution.

00:35:42.810 --> 00:35:47.250
Let's see, this part of it is a
convolution of h of t with K

00:35:47.250 --> 00:35:49.470
tilde, which is the covariance,
which is a

00:35:49.470 --> 00:35:51.640
function of one variable.

00:35:51.640 --> 00:35:54.770
And then it's convolved with--

00:35:54.770 --> 00:36:00.170
and here I put in the complex
conjugate of h, because it's

00:36:00.170 --> 00:36:02.720
easier to do it here.

00:36:02.720 --> 00:36:06.730
Because at some point, we want
to start dealing with complex

00:36:06.730 --> 00:36:08.150
random processes.

00:36:08.150 --> 00:36:10.940
And for filters it's
easy to do that.

00:36:10.940 --> 00:36:13.070
For linear functionals
it's a little harder.

00:36:13.070 --> 00:36:17.790
So I want to stick to
real processes.

00:36:17.790 --> 00:36:21.170
If we're dealing with filtering,
I can simply define

00:36:21.170 --> 00:36:27.710
this covariance function as the
expected value of Z of t

00:36:27.710 --> 00:36:32.410
times Z complex conjugate
of tau.

00:36:32.410 --> 00:36:36.200
And when you do that, we're
taking this integral, which is

00:36:36.200 --> 00:36:39.700
the convolution of h
of t with K tilde.

00:36:39.700 --> 00:36:42.730
And then we're convolving
it with h complex

00:36:42.730 --> 00:36:44.980
conjugate of minus t.

00:36:44.980 --> 00:36:48.490
Because the t gets turned
around in there.

00:36:48.490 --> 00:36:51.340
And what that says when you take
the Fourier transform of

00:36:51.340 --> 00:36:53.270
it, at least what
I hope it says--

00:36:53.270 --> 00:36:57.030
I'm not very good at taking
Fourier transforms.

00:36:57.030 --> 00:37:01.230
And you people, some of you
are very good at it.

00:37:01.230 --> 00:37:07.030
We get the spectral density of
the process, Z of t times the

00:37:07.030 --> 00:37:09.400
magnitude of h hat of f.

00:37:09.400 --> 00:37:12.010
Now this is the formula, which
is not in the notes

00:37:12.010 --> 00:37:14.040
I just found out.

00:37:14.040 --> 00:37:15.560
And it's a very important
formula.

00:37:15.560 --> 00:37:19.490
So you ought to write it down.

00:37:19.490 --> 00:37:22.890
It says what the spectral
density is on the output of

00:37:22.890 --> 00:37:26.550
the filter, if you know the
spectral density of the input

00:37:26.550 --> 00:37:29.060
of the filter.

00:37:29.060 --> 00:37:33.520
And it's a kind of a neat,
simple formula for it.

00:37:33.520 --> 00:37:38.580
It says, you pass a
random process.

00:37:38.580 --> 00:37:42.820
I mean, it's like the formula
that you have when you take a

00:37:42.820 --> 00:37:46.350
waveform and you pass it through
a linear filter.

00:37:46.350 --> 00:37:49.170
And you know it's kind of easier
to look at that in the

00:37:49.170 --> 00:37:52.690
frequency domain where you
multiply, than it is to look

00:37:52.690 --> 00:37:55.430
at it in a time domain where
you have to convolve.

00:37:55.430 --> 00:38:00.010
Here things become even simpler
because all we have to

00:38:00.010 --> 00:38:02.860
do is take the spectral density,
which is now a

00:38:02.860 --> 00:38:06.610
function of frequency, multiply
it by this magnitude

00:38:06.610 --> 00:38:08.870
squared, and suddenly
we get the spectral

00:38:08.870 --> 00:38:13.000
density at the output.

00:38:13.000 --> 00:38:17.760
Now what happens when you take
this nice sinc filter, this

00:38:17.760 --> 00:38:22.890
sinc Gaussian process that we
have, which is flat over as

00:38:22.890 --> 00:38:26.690
large a bandwidth as you
want to make it.

00:38:26.690 --> 00:38:30.790
And you pass that process
through a linear filter.

00:38:30.790 --> 00:38:32.040
What do you get out?

00:38:35.310 --> 00:38:39.040
You get a process out which has
any spectral density that

00:38:39.040 --> 00:38:41.790
you want within those
wide limits.

00:38:44.690 --> 00:38:50.220
OK, so just by understanding the
sinc Gaussian process and

00:38:50.220 --> 00:38:54.700
by understanding this result,
you can create a process which

00:38:54.700 --> 00:38:57.030
has any old spectral density
that you want.

00:39:00.490 --> 00:39:03.470
And if you can create any old
spectral density that you

00:39:03.470 --> 00:39:08.710
want, you know you can create
any old covariance function

00:39:08.710 --> 00:39:09.770
that you want.

00:39:09.770 --> 00:39:12.450
And all of this is good for
Wide Sense Stationarity so

00:39:12.450 --> 00:39:16.710
long as your filter doesn't
extend for too far.

00:39:16.710 --> 00:39:20.310
Which says all of our theory
works starting at some

00:39:20.310 --> 00:39:23.810
negative time going to some
positive time, taking filters

00:39:23.810 --> 00:39:28.030
that only exist for a relatively
small time.

00:39:28.030 --> 00:39:32.650
If it's Wide Sense Stationary
and it's Gaussian, then

00:39:32.650 --> 00:39:34.210
everything works beautifully.

00:39:36.780 --> 00:39:39.370
You can simply create
any old spectral

00:39:39.370 --> 00:39:41.310
density that you want.

00:39:41.310 --> 00:39:45.270
If you have some waveform that's
coming in, you can

00:39:45.270 --> 00:39:49.110
filter it and make the output
process look whatever you want

00:39:49.110 --> 00:39:51.290
to make it look like, so
long as you have the

00:39:51.290 --> 00:39:53.540
stationarity property.

00:39:53.540 --> 00:39:57.480
Now you know the secret of
why everybody deals with

00:39:57.480 --> 00:39:59.610
stationary processes.

00:39:59.610 --> 00:40:02.380
It's because of this formula.

00:40:02.380 --> 00:40:04.160
It's an incredibly
simple formula.

00:40:04.160 --> 00:40:06.800
It's an incredibly
simple idea.

00:40:06.800 --> 00:40:08.220
You see now we know
something else.

00:40:08.220 --> 00:40:12.300
We know that it also applies
over finite,

00:40:12.300 --> 00:40:13.600
but large time intervals.

00:40:18.360 --> 00:40:18.680
OK.

00:40:18.680 --> 00:40:24.980
So just to spell out the
conclusions once more, a Wide

00:40:24.980 --> 00:40:27.540
Sense Stationary process
is Wide Sense

00:40:27.540 --> 00:40:30.380
Stationary after filtering.

00:40:30.380 --> 00:40:33.520
So if you start out with Wide
Sense Stationary, it's Wide

00:40:33.520 --> 00:40:36.000
Sense Stationary when
you get done.

00:40:36.000 --> 00:40:39.750
If it's effectively Wide Sense
Stationary, it's effectively

00:40:39.750 --> 00:40:44.090
stationary with a reduced
interval of effective

00:40:44.090 --> 00:40:46.670
stationarity after you filter.

00:40:46.670 --> 00:40:49.580
OK in other words, if you start
out with a process which

00:40:49.580 --> 00:40:53.840
is effectively stationary from
minus T sub 0 to plus T sub 0,

00:40:53.840 --> 00:40:56.870
then after you filter it with
any filter whose impulse

00:40:56.870 --> 00:41:00.550
response is limited in time,
you get something which is

00:41:00.550 --> 00:41:05.150
effectively stationary within
that six months minus one

00:41:05.150 --> 00:41:06.660
millisecond period of time.

00:41:06.660 --> 00:41:06.890
Yes?

00:41:06.890 --> 00:41:09.270
AUDIENCE: [UNINTELLIGIBLE]

00:41:09.270 --> 00:41:09.700
PROFESSOR: What?

00:41:09.700 --> 00:41:12.750
AUDIENCE: Don't you
differentiate [UNINTELLIGIBLE]

00:41:12.750 --> 00:41:17.820
PROFESSOR: Effectively Wide
Sense Stationary, thank you.

00:41:22.630 --> 00:41:26.430
I try to differentiate between
them, but sometimes I'm

00:41:26.430 --> 00:41:29.380
thinking about Gaussian
processes, where it doesn't

00:41:29.380 --> 00:41:32.530
make any difference.

00:41:32.530 --> 00:41:38.990
Is effectively Wide Sense
Stationary with reduced

00:41:38.990 --> 00:41:40.320
interval after filtering.

00:41:40.320 --> 00:41:43.230
OK, good.

00:41:43.230 --> 00:41:46.860
The internal covariance, in
other words, the covariance

00:41:46.860 --> 00:41:50.040
function within these intervals,
the spectral

00:41:50.040 --> 00:41:54.170
density, and the joint
probability density aren't

00:41:54.170 --> 00:41:56.910
affected by this interval of
effective stationarity.

00:41:56.910 --> 00:41:59.440
In other words, so long as you
stay in this region of

00:41:59.440 --> 00:42:04.230
interest, you don't care
what T sub 0 is.

00:42:04.230 --> 00:42:06.620
And since you don't care
what T sub 0 is, you

00:42:06.620 --> 00:42:08.570
forget about it.

00:42:08.570 --> 00:42:11.290
And for now on we will
forget about it.

00:42:11.290 --> 00:42:15.800
But we know that whatever
we're dealing with, it's

00:42:15.800 --> 00:42:19.700
effectively stationary within
some time period, which is

00:42:19.700 --> 00:42:23.040
what makes the functions
L sub 2.

00:42:23.040 --> 00:42:26.130
We don't care how big that is.

00:42:26.130 --> 00:42:28.560
So we don't have to
specify T sub 0.

00:42:28.560 --> 00:42:30.530
We don't have to
worry about it.

00:42:30.530 --> 00:42:33.600
We just assume it's big
enough and move on.

00:42:33.600 --> 00:42:36.380
And then if it turns out it's
not big enough, if your system

00:42:36.380 --> 00:42:40.610
crashes when one of Microsoft's
infernal errors

00:42:40.610 --> 00:42:43.450
come up, then you worry
about it then.

00:42:43.450 --> 00:42:45.180
But you don't worry about
it before that.

00:42:51.020 --> 00:42:54.060
OK so now we can really define
white noise and understand

00:42:54.060 --> 00:42:57.260
what white noise is.

00:42:57.260 --> 00:43:03.550
White noise is noise that is
effectively Wide Sense

00:43:03.550 --> 00:43:11.810
Stationary over a large enough
interval to include all time

00:43:11.810 --> 00:43:14.080
intervals of interest.

00:43:14.080 --> 00:43:18.015
Also on the other part of
white noise, is that the

00:43:18.015 --> 00:43:21.710
spectral density is constant
in f over all

00:43:21.710 --> 00:43:24.050
frequencies of interest.

00:43:24.050 --> 00:43:28.940
OK in other words, white noise,
really you have to

00:43:28.940 --> 00:43:32.880
define it within terms of what
you're interested in.

00:43:32.880 --> 00:43:37.010
And that's important
to understand.

00:43:37.010 --> 00:43:40.150
If you're dealing with wireless
channels, you

00:43:40.150 --> 00:43:45.750
sometimes want to assume that
the noise you're seeing is

00:43:45.750 --> 00:43:47.990
actually white noise.

00:43:47.990 --> 00:43:50.440
But sometimes you do things
like jumping from one

00:43:50.440 --> 00:43:53.820
frequency band to another
frequency band to transmit.

00:43:53.820 --> 00:43:56.920
And when you jump from one
frequency band to another

00:43:56.920 --> 00:43:59.820
frequency band, the noise
might in fact be quite

00:43:59.820 --> 00:44:03.450
different in this band, than
it is in this band.

00:44:03.450 --> 00:44:05.160
But you're going to stay
in these bands for a

00:44:05.160 --> 00:44:07.210
relatively long time.

00:44:07.210 --> 00:44:11.460
So as far as any kind of
analysis goes which is dealing

00:44:11.460 --> 00:44:14.680
with those intervals, you
want to assume that

00:44:14.680 --> 00:44:17.330
the noise is white.

00:44:17.330 --> 00:44:21.640
As far as any overall analysis
that looks at all of the

00:44:21.640 --> 00:44:25.850
intervals at the same time, in
other words, which is dealing

00:44:25.850 --> 00:44:29.680
with some very long-term
phenomenon, you can't assume

00:44:29.680 --> 00:44:31.050
that the noise is white.

00:44:31.050 --> 00:44:35.880
OK in other words, white noise
is a modeling tool.

00:44:35.880 --> 00:44:39.470
It's not something that occurs
in the real world.

00:44:39.470 --> 00:44:42.600
But it's a simple
modeling tool.

00:44:42.600 --> 00:44:43.920
Because what do you
do with it?

00:44:43.920 --> 00:44:47.160
You start out by assuming
that the noise is white.

00:44:47.160 --> 00:44:49.320
And then you start to get some
understanding of what the

00:44:49.320 --> 00:44:51.090
problem is about.

00:44:51.090 --> 00:44:53.640
And after you understand what
the problem is about, you come

00:44:53.640 --> 00:44:57.440
back and say, is it really
important for me that the

00:44:57.440 --> 00:45:00.880
noise is going to have a
constant spectral density over

00:45:00.880 --> 00:45:02.520
this interval of interest?

00:45:02.520 --> 00:45:06.350
And at that point you
say, yes or no.

00:45:06.350 --> 00:45:06.850
OK.

00:45:06.850 --> 00:45:10.100
It's important to always be
aware that this doesn't apply

00:45:10.100 --> 00:45:12.140
for times and frequencies
outside

00:45:12.140 --> 00:45:14.000
the interval of interest.

00:45:14.000 --> 00:45:17.470
And if the process is also
Gaussian, it's called white

00:45:17.470 --> 00:45:18.500
Gaussian noise.

00:45:18.500 --> 00:45:22.850
So white Gaussian noise is
noise which has a flat

00:45:22.850 --> 00:45:27.250
spectral density over this
effective stationarity period

00:45:27.250 --> 00:45:30.040
that we're interested in, over
some very large period of

00:45:30.040 --> 00:45:32.280
time, minus T sub 0
to plus T sub 0.

00:45:32.280 --> 00:45:36.850
And over some very large
frequency interval, minus W up

00:45:36.850 --> 00:45:42.300
to plus W or even better over
some limited frequency band in

00:45:42.300 --> 00:45:45.750
positive and negative
frequencies, the spectral

00:45:45.750 --> 00:45:47.610
density is constant.

00:45:47.610 --> 00:45:51.740
You can't define the spectral
density until after you look

00:45:51.740 --> 00:45:54.920
at this effective period
of stationarity.

00:45:54.920 --> 00:45:57.590
But at that point, you can
then define the spectral

00:45:57.590 --> 00:46:01.630
density and see whether it looks
constant over the period

00:46:01.630 --> 00:46:02.730
you're interested in.

00:46:02.730 --> 00:46:06.590
You can either see whether it
is, or if you're an academic

00:46:06.590 --> 00:46:09.700
and you write papers, you
assume that it is.

00:46:09.700 --> 00:46:11.990
OK.

00:46:11.990 --> 00:46:14.650
The nice thing about being an
academic is you can assume

00:46:14.650 --> 00:46:15.900
whatever you want to assume.

00:46:20.620 --> 00:46:22.670
OK let's go back to linear
functionals.

00:46:22.670 --> 00:46:24.450
The stuff about linear
functionals is

00:46:24.450 --> 00:46:25.490
in fact in the notes.

00:46:25.490 --> 00:46:28.250
This will start to give us an
idea of what spectral density

00:46:28.250 --> 00:46:31.340
really means.

00:46:31.340 --> 00:46:37.440
So if I have a Wide Sense
Stationary process, a linear

00:46:37.440 --> 00:46:39.360
functional, well this
is what a linear

00:46:39.360 --> 00:46:41.910
functional is in general.

00:46:41.910 --> 00:46:46.810
But for a Wide Sense Stationary
process, we have

00:46:46.810 --> 00:46:50.780
the single variance, single
variable covariance function.

00:46:50.780 --> 00:46:54.970
The expected value of the
product of two random

00:46:54.970 --> 00:46:59.230
variables of this sort is
just this integral here.

00:46:59.230 --> 00:47:00.430
This is what we did before.

00:47:00.430 --> 00:47:03.860
If you don't remember
this formula, good.

00:47:03.860 --> 00:47:07.693
It's just what you get when you
take the definition of V

00:47:07.693 --> 00:47:12.400
sub i, which is the integral of
g sub i of t with Z of t.

00:47:12.400 --> 00:47:15.970
And then you multiply
it by V sub j.

00:47:15.970 --> 00:47:18.480
And you take Z of
t and Z of tau.

00:47:18.480 --> 00:47:20.700
And you take the expected
value of the two.

00:47:20.700 --> 00:47:22.160
And then you integrate
the whole thing.

00:47:22.160 --> 00:47:26.370
And it sort of comes out to be
that, if I've done it right.

00:47:26.370 --> 00:47:30.290
You can express this in this
terms, namely as the integral

00:47:30.290 --> 00:47:35.000
of g sub i of t times a
convolution of this function

00:47:35.000 --> 00:47:36.840
times this function.

00:47:36.840 --> 00:47:39.450
You can then use Parseval's
identity.

00:47:39.450 --> 00:47:43.030
Because this convolution gives
you a function of t, OK.

00:47:43.030 --> 00:47:46.560
When you integrate this over
tau, this whole thing is just

00:47:46.560 --> 00:47:48.960
a function of t.

00:47:48.960 --> 00:47:49.690
OK.

00:47:49.690 --> 00:47:52.970
So what we're dealing with is
the integral of a function of

00:47:52.970 --> 00:47:56.180
t times a function of t.

00:47:56.180 --> 00:48:00.240
And Parseval's relation says
that the integral of a

00:48:00.240 --> 00:48:03.690
function of time, times a
function of time is equal to

00:48:03.690 --> 00:48:07.020
the integral of the
Fourier transform

00:48:07.020 --> 00:48:08.550
times the Fourier transform.

00:48:08.550 --> 00:48:08.860
OK.

00:48:08.860 --> 00:48:10.950
You all know this.

00:48:10.950 --> 00:48:14.300
This is almost the same as a
trick we used with linear

00:48:14.300 --> 00:48:17.580
filters, just slightly
different.

00:48:17.580 --> 00:48:21.480
So that says that this expected
value is equal to

00:48:21.480 --> 00:48:24.380
this product here.

00:48:24.380 --> 00:48:27.820
And here I've used the complex
conjugate for the Fourier

00:48:27.820 --> 00:48:32.130
transform of g sub j, because I
really need it there if g is

00:48:32.130 --> 00:48:35.240
not a symmetric function.

00:48:35.240 --> 00:48:39.060
OK so if these Fourier
transforms, g sub i of t and g

00:48:39.060 --> 00:48:43.540
sub j of t are non-overlapping
in frequency, what does this

00:48:43.540 --> 00:48:48.240
integral go to?

00:48:48.240 --> 00:48:51.390
And this is absolutely wild.

00:48:51.390 --> 00:48:53.610
I mean if this doesn't surprise
you, you really ought

00:48:53.610 --> 00:48:55.730
to go back and think
about what it is

00:48:55.730 --> 00:48:56.670
that's going on here.

00:48:56.670 --> 00:49:00.360
Because it's very astonishing.

00:49:00.360 --> 00:49:04.060
It's saying that stationarity
says something you would never

00:49:04.060 --> 00:49:07.150
guess in a million years
that it says.

00:49:07.150 --> 00:49:10.800
OK it says, that if this and
this do not overlap in

00:49:10.800 --> 00:49:13.860
frequency, this integral is 0.

00:49:13.860 --> 00:49:17.060
OK in other words, if you take
a linear functional over one

00:49:17.060 --> 00:49:22.350
band of frequencies, it is
uncorrelated with every random

00:49:22.350 --> 00:49:26.060
variable that you can take in
any other band of frequencies.

00:49:26.060 --> 00:49:30.460
So long as the function is Wide
Sense Stationary, this

00:49:30.460 --> 00:49:33.250
uncorrelated effect
takes place.

00:49:33.250 --> 00:49:35.750
If you're dealing with the
Gaussian random process, it's

00:49:35.750 --> 00:49:37.240
also stationery.

00:49:37.240 --> 00:49:39.790
Uncorrelated means
independent.

00:49:39.790 --> 00:49:45.140
And what happens then, is that
expected value of V sub i and

00:49:45.140 --> 00:49:46.970
V sub j is zero.

00:49:46.970 --> 00:49:49.720
V sub i and V sub j are
statistically independent

00:49:49.720 --> 00:49:51.040
random variables.

00:49:51.040 --> 00:49:54.550
And it says that whatever is
happening in one frequency

00:49:54.550 --> 00:50:01.130
band of a stationary Gaussian
process is completely

00:50:01.130 --> 00:50:03.750
independent of what's happening
in any other band.

00:50:06.300 --> 00:50:09.160
Now if any of you can give me
an intuitive reason for why

00:50:09.160 --> 00:50:12.850
that is, I would love
to hear it.

00:50:12.850 --> 00:50:16.600
Because I mean it's
one of those--

00:50:16.600 --> 00:50:19.290
well after you think about it
for a long time, it starts to

00:50:19.290 --> 00:50:22.090
sort of make sense.

00:50:22.090 --> 00:50:24.820
If you read the appendix in the
notes, the appendix in the

00:50:24.820 --> 00:50:28.440
notes sort of interprets
it a little bit more.

00:50:28.440 --> 00:50:32.830
If you take this process, if you
limit it in time and you

00:50:32.830 --> 00:50:37.150
expand it in a Fourier series,
what's going to happen to

00:50:37.150 --> 00:50:40.800
those coefficients in
the Fourier series?

00:50:40.800 --> 00:50:46.340
If the process is stationary,
each one of them, if you think

00:50:46.340 --> 00:50:49.640
of them as being complex
coefficients, the phase in

00:50:49.640 --> 00:50:58.220
each one has to be random and
uniformly distributed between

00:50:58.220 --> 00:51:01.050
zero and 2 Pi.

00:51:01.050 --> 00:51:07.260
In other words, if you have a
phase on any sinusoid, which

00:51:07.260 --> 00:51:14.140
is either deterministic or
anything other than uniform,

00:51:14.140 --> 00:51:17.980
then that little slice of the
process cannot be Wide Sense

00:51:17.980 --> 00:51:19.450
Stationary.

00:51:19.450 --> 00:51:22.130
And there's nothing in any other
frequencies that can

00:51:22.130 --> 00:51:25.650
happen to make the process
stationary again.

00:51:25.650 --> 00:51:28.840
OK in other words, when you
expand it in a Fourier series,

00:51:28.840 --> 00:51:33.600
you have to get coefficients
at every little frequency

00:51:33.600 --> 00:51:38.810
slice, which have uniform
face to them.

00:51:38.810 --> 00:51:41.540
In other words, the Gaussian
random variables that you're

00:51:41.540 --> 00:51:45.990
looking at, are things that
are called proper complex

00:51:45.990 --> 00:51:50.330
Gaussian random variables which
have the same variance

00:51:50.330 --> 00:51:53.400
for the real part as for
the imaginary part.

00:51:53.400 --> 00:51:55.920
When you look at them
as a probability

00:51:55.920 --> 00:51:58.800
density, you find circles.

00:51:58.800 --> 00:52:01.410
In other words, you find
circularly symmetric random

00:52:01.410 --> 00:52:04.480
variables at every little
frequency interval.

00:52:04.480 --> 00:52:07.510
And that's somehow, what
stationarity is

00:52:07.510 --> 00:52:09.590
implying for us.

00:52:09.590 --> 00:52:11.090
So it's a pretty important
thing.

00:52:13.740 --> 00:52:15.700
And it doesn't just apply
to white noise.

00:52:15.700 --> 00:52:19.480
It applies to any old process
with any old spectral density,

00:52:19.480 --> 00:52:25.770
so long as we've assumed that
it's Wide Sense Stationary.

00:52:25.770 --> 00:52:26.160
OK.

00:52:26.160 --> 00:52:30.090
So, different frequency bands
are uncorrelated for Gaussian

00:52:30.090 --> 00:52:32.420
Wide Sense Stationary
processes.

00:52:32.420 --> 00:52:34.380
Different frequency bands
are completely

00:52:34.380 --> 00:52:37.510
independent of each other.

00:52:37.510 --> 00:52:42.320
OK, as soon as you have
stationary noise, looking at

00:52:42.320 --> 00:52:45.750
one frequency band to try to get
any idea of what's going

00:52:45.750 --> 00:52:50.780
on in another frequency band,
is an absolute loser.

00:52:50.780 --> 00:52:53.930
You can't tell anything from one
band about what's going on

00:52:53.930 --> 00:52:55.060
in another band.

00:52:55.060 --> 00:52:58.650
So long as you're only looking
at Gaussian noise If you're

00:52:58.650 --> 00:53:01.090
sending data which is correlated
between the two

00:53:01.090 --> 00:53:03.310
bands, then of course
it's worthwhile to

00:53:03.310 --> 00:53:04.690
look at both bands.

00:53:04.690 --> 00:53:07.160
But if you're sending data
that's limited to one

00:53:07.160 --> 00:53:13.980
frequency band, then at that
point, there's no reason to

00:53:13.980 --> 00:53:15.310
look anywhere else.

00:53:15.310 --> 00:53:16.680
We're going to find
that out when we

00:53:16.680 --> 00:53:18.060
start studying detection.

00:53:18.060 --> 00:53:21.550
It'll be a major feature of
understanding detection at

00:53:21.550 --> 00:53:22.850
that point.

00:53:22.850 --> 00:53:24.700
OK, and one of the things you
want to understand with

00:53:24.700 --> 00:53:27.940
detection, is what things can
you ignore and what things

00:53:27.940 --> 00:53:28.940
can't you ignore.

00:53:28.940 --> 00:53:32.170
And this is telling us something
that can be ignored.

00:53:32.170 --> 00:53:34.640
It's also starting to
tell us what this

00:53:34.640 --> 00:53:38.600
spectral density means.

00:53:38.600 --> 00:53:46.010
But I'll interpret that in
the next slide I hope.

00:53:46.010 --> 00:53:49.050
OK if you take this set of
functions g sub j of t, in the

00:53:49.050 --> 00:53:54.600
notes I now convert that set
of functions to V sub j's

00:53:54.600 --> 00:53:57.710
instead of g sub j's.

00:53:57.710 --> 00:54:03.810
If I start out with that set
of functions these--

00:54:03.810 --> 00:54:07.440
oh dear, this is a
random variable--

00:54:13.010 --> 00:54:17.580
V sub j is the component of the
process in the expansion,

00:54:17.580 --> 00:54:19.230
in that orthonormal set.

00:54:19.230 --> 00:54:22.600
So in other words, you can take
this process, you can

00:54:22.600 --> 00:54:27.150
expand it into an orthonormal
expansion using random

00:54:27.150 --> 00:54:31.400
variables and these orthonormal
functions.

00:54:31.400 --> 00:54:36.150
If g sub j of t is a narrow
band, and S sub Z of f is

00:54:36.150 --> 00:54:38.930
constant within that band-- in
other words we don't need

00:54:38.930 --> 00:54:39.860
white noise here.

00:54:39.860 --> 00:54:43.410
All we need is a spectral
density

00:54:43.410 --> 00:54:45.250
which changes smoothly.

00:54:45.250 --> 00:54:47.680
And if it changes smoothly,
we can look at a

00:54:47.680 --> 00:54:50.560
small enough interval.

00:54:50.560 --> 00:54:52.160
And then what happens?

00:54:52.160 --> 00:54:56.130
When we take the expected value
of V sub j squared, what

00:54:56.130 --> 00:55:00.780
we get is this times
this times this--

00:55:00.780 --> 00:55:05.790
OK the product of these three
terms and since this--

00:55:12.320 --> 00:55:14.150
that's supposed to
be a j there.

00:55:17.010 --> 00:55:20.580
OK so what's happening is these
are narrow band, this is

00:55:20.580 --> 00:55:23.110
just a constant within
that narrow band.

00:55:23.110 --> 00:55:31.250
So we can pull this out and
then all we have is the

00:55:31.250 --> 00:55:38.340
integral g sub i of f times g
sub i complex conjugate of f,

00:55:38.340 --> 00:55:42.460
which since these functions are
orthonormal, is just one.

00:55:42.460 --> 00:55:49.390
So what we find is that random
variable there is simply the

00:55:49.390 --> 00:55:52.920
value of the spectral density.

00:55:52.920 --> 00:55:57.350
Now we don't use that to figure
out what these random

00:55:57.350 --> 00:55:58.800
variables are all about.

00:55:58.800 --> 00:56:00.960
We use this to understand
what the spectral

00:56:00.960 --> 00:56:02.640
density is all about.

00:56:02.640 --> 00:56:05.850
Because the spectral density
is really the energy per

00:56:05.850 --> 00:56:09.290
degree of freedom in the
process at frequency f.

00:56:09.290 --> 00:56:12.670
In other words, if we use
orthonormal functions which

00:56:12.670 --> 00:56:16.900
are tightly constrained in
frequency, they just pick out

00:56:16.900 --> 00:56:20.440
the value of the spectral
density at that frequency.

00:56:20.440 --> 00:56:25.700
S sub Z of f sub 0 is just sort
of a central frequency in

00:56:25.700 --> 00:56:30.260
this band, or integrating
over here, OK.

00:56:30.260 --> 00:56:33.920
So the interpretation of the
spectral density is that it's

00:56:33.920 --> 00:56:37.340
the energy per degree of
freedom in any kind of

00:56:37.340 --> 00:56:40.920
orthonormal expansion you want
to use, so long as those

00:56:40.920 --> 00:56:45.660
functions are tightly limited
in frequency.

00:56:45.660 --> 00:56:47.700
So that's what spectral
density means.

00:56:47.700 --> 00:56:52.540
It tells you how much energy
there is in the noise, at all

00:56:52.540 --> 00:56:55.170
these different frequencies.

00:56:55.170 --> 00:56:57.900
It tells you, for example at
this point, that if you start

00:56:57.900 --> 00:57:00.930
out with a sinc Gaussian process
like we were talking

00:57:00.930 --> 00:57:04.960
about before, pass it through
a filter, what the filter is

00:57:04.960 --> 00:57:07.800
going to do is change the
spectral density at all the

00:57:07.800 --> 00:57:09.260
different frequencies.

00:57:09.260 --> 00:57:12.650
And the interpretation of that
which now makes perfect sense

00:57:12.650 --> 00:57:15.050
when you're going through a
filter, is the amount of

00:57:15.050 --> 00:57:18.340
energy in each of those
frequency bands is going to be

00:57:18.340 --> 00:57:22.970
changed exactly by the amount
that you filtered it.

00:57:22.970 --> 00:57:28.200
OK so we have a nice
thing there.

00:57:28.200 --> 00:57:30.720
S sub Z of f is the energy per
degree of freedom in the

00:57:30.720 --> 00:57:33.490
process at frequency f.

00:57:33.490 --> 00:57:36.770
For a Gaussian Wide Sense
Stationary process, this is

00:57:36.770 --> 00:57:38.810
the energy per degree
of freedom at f.

00:57:38.810 --> 00:57:41.330
And the noise at all different
frequencies is independent.

00:57:41.330 --> 00:57:43.090
That's what we were
saying before.

00:57:43.090 --> 00:57:47.160
And after filtering, this
independence is maintained.

00:57:47.160 --> 00:57:51.210
In other words, you pass a
Gaussian random process

00:57:51.210 --> 00:57:55.200
through a filter and you still
have independence between all

00:57:55.200 --> 00:57:56.560
the different frequency bands.

00:57:56.560 --> 00:57:59.460
It's an absolutely remarkable
property.

00:58:03.830 --> 00:58:09.620
Just this property of the
process acting the same at

00:58:09.620 --> 00:58:13.170
each time as it acts at each
other time, that this leads to

00:58:13.170 --> 00:58:15.910
this amazing property of things
being independent from

00:58:15.910 --> 00:58:21.040
one frequency to another is
really worth thinking about.

00:58:24.360 --> 00:58:27.130
And it's one of the things
that people use when they

00:58:27.130 --> 00:58:29.250
actually design systems.

00:58:29.250 --> 00:58:31.110
I mean it's almost natural
to use if you

00:58:31.110 --> 00:58:32.480
don't think about it.

00:58:32.480 --> 00:58:36.220
If you start thinking about it
then it becomes even more

00:58:36.220 --> 00:58:38.640
worthwhile.

00:58:38.640 --> 00:58:42.390
OK should I start detection?

00:58:42.390 --> 00:58:46.620
Let me just start to say a few
things about detection, then I

00:58:46.620 --> 00:58:49.680
want to stop in time to
pass the quiz back.

00:58:53.240 --> 00:59:00.180
OK let's talk about where
detection fits in with our

00:59:00.180 --> 00:59:02.200
master plan of what
we've been doing.

00:59:05.060 --> 00:59:10.000
We started the course talking
about what was over on this

00:59:10.000 --> 00:59:13.490
side of the channel business,
namely all the source coding

00:59:13.490 --> 00:59:16.000
and all of those things.

00:59:16.000 --> 00:59:20.020
We then started to talk about
how do you do signal encoding.

00:59:20.020 --> 00:59:23.920
In other words, how do you map
from binary digits coming in,

00:59:23.920 --> 00:59:28.820
into signals.

00:59:28.820 --> 00:59:31.870
Well we never talked
about this problem.

00:59:31.870 --> 00:59:34.190
We only talked about this
problem, when you

00:59:34.190 --> 00:59:36.170
didn't have any noise.

00:59:36.170 --> 00:59:39.430
We carefully avoided dealing
with this problem at all.

00:59:39.430 --> 00:59:44.640
Then we talked about
baseband modulation

00:59:44.640 --> 00:59:46.810
for PAM and for QAM.

00:59:46.810 --> 00:59:50.720
For PAM it was real, for
QAM it was complex.

00:59:50.720 --> 00:59:53.900
Then we talked about how you go
from baseband frequencies

00:59:53.900 --> 00:59:56.970
up to passband frequencies.

00:59:56.970 --> 01:00:01.960
Now we've been talking about
what happens when you add

01:00:01.960 --> 01:00:06.620
white Gaussian noise, or any
other noise at passband.

01:00:06.620 --> 01:00:09.370
Then you go from passband
down to baseband.

01:00:09.370 --> 01:00:13.310
When I do this, we're going to
have to come back at some

01:00:13.310 --> 01:00:17.010
point and deal with the question
of what happens when

01:00:17.010 --> 01:00:20.380
you take passband white Gaussian
noise and convert it

01:00:20.380 --> 01:00:22.210
down to baseband.

01:00:22.210 --> 01:00:24.770
Because some kind of funny
things happen there.

01:00:24.770 --> 01:00:28.630
And we have to deal with it
with a little bit of care.

01:00:28.630 --> 01:00:31.190
But let's forget about that
for the time being, and

01:00:31.190 --> 01:00:34.040
suppose everything works
out all right.

01:00:34.040 --> 01:00:37.580
Then we go through our
baseband demodulator.

01:00:37.580 --> 01:00:41.900
Finally something comes out of
the baseband demodulator,

01:00:41.900 --> 01:00:46.040
which we hope is the same
as what came out

01:00:46.040 --> 01:00:47.590
of the signal encoder.

01:00:47.590 --> 01:00:51.040
But what we're trying to do at
this point now, is when the

01:00:51.040 --> 01:00:54.820
noise is here to say, this is
in some sense, going to be

01:00:54.820 --> 01:00:58.150
what we put in plus noise.

01:00:58.150 --> 01:01:01.460
And for the time being, in terms
of looking at detection,

01:01:01.460 --> 01:01:02.740
we will just assume that.

01:01:02.740 --> 01:01:06.690
We will assume that this thing
down here is what went in,

01:01:06.690 --> 01:01:07.760
plus noise.

01:01:07.760 --> 01:01:10.980
And assume that all this process
of passing white

01:01:10.980 --> 01:01:17.150
Gaussian noise through this junk
gives us just a signal

01:01:17.150 --> 01:01:19.870
plus noise.

01:01:19.870 --> 01:01:24.790
OK, so what a detector does is
it observes a sample value of

01:01:24.790 --> 01:01:31.165
this random variable V, or it
observes a vector, or observes

01:01:31.165 --> 01:01:32.500
a random process.

01:01:32.500 --> 01:01:34.840
It observes whatever it's
going to observe.

01:01:34.840 --> 01:01:37.960
And it guesses the value of
another random variable.

01:01:37.960 --> 01:01:40.650
And we'll call the other random
variable H. We had been

01:01:40.650 --> 01:01:43.910
calling it other things
for the input.

01:01:43.910 --> 01:01:48.200
It's nice to call it H because
statisticians always call this

01:01:48.200 --> 01:01:50.820
a hypothesis.

01:01:50.820 --> 01:01:53.960
And I think it's easier to think
of what's going on if

01:01:53.960 --> 01:01:55.930
you think of it as a

01:01:55.930 --> 01:01:59.270
hypothesis type random variable.

01:01:59.270 --> 01:02:03.420
Namely the detector
has to guess.

01:02:03.420 --> 01:02:05.960
If we have a binary
input, it has to

01:02:05.960 --> 01:02:08.480
guess at a binary output.

01:02:08.480 --> 01:02:12.850
It has to flip a coin somehow
and say, I think that what

01:02:12.850 --> 01:02:18.120
came in is a zero, or I think
what came in is a one.

01:02:18.120 --> 01:02:23.400
The synonyms for detection are
hypothesis testing, decision

01:02:23.400 --> 01:02:26.650
making, and decoding.

01:02:26.650 --> 01:02:28.450
They all mean exactly
the same thing.

01:02:28.450 --> 01:02:31.360
There's no difference whatsoever
between them,

01:02:31.360 --> 01:02:34.930
except the fields that they
happen to deal with.

01:02:34.930 --> 01:02:38.520
And each field talks about
it in a different sense.

01:02:38.520 --> 01:02:42.580
The word detection really came
mostly from the radar field

01:02:42.580 --> 01:02:45.170
where people were trying to
determine whether a target was

01:02:45.170 --> 01:02:48.030
there or not.

01:02:48.030 --> 01:02:50.210
And hypothesis testing
has been around for

01:02:50.210 --> 01:02:51.300
a great deal longer.

01:02:51.300 --> 01:02:56.250
Because all scientists from time
immemorial have had to

01:02:56.250 --> 01:02:59.400
deal with the question of have
you collect a bunch of data

01:02:59.400 --> 01:03:00.720
about something.

01:03:00.720 --> 01:03:02.140
And it's always noisy data.

01:03:02.140 --> 01:03:05.210
There's always a bunch of
junk that comes into it.

01:03:05.210 --> 01:03:09.050
And you have to try to make some
conclusions out of that.

01:03:09.050 --> 01:03:12.670
And making conclusions out of
it is easier if you know

01:03:12.670 --> 01:03:15.760
there's only a finite
set of alternatives

01:03:15.760 --> 01:03:16.750
which might be true.

01:03:16.750 --> 01:03:19.640
In a presidential election,
after you know who the

01:03:19.640 --> 01:03:23.610
candidates are, it's
a binary decision.

01:03:23.610 --> 01:03:25.840
The people who try to guess who
people are going to vote

01:03:25.840 --> 01:03:30.230
for, are really doing binary
decision making.

01:03:30.230 --> 01:03:34.030
They don't have very good
probability models.

01:03:34.030 --> 01:03:37.240
And it certainly isn't
a stationary process.

01:03:37.240 --> 01:03:40.800
But in fact, it is a detection
problem, the same as the

01:03:40.800 --> 01:03:42.430
detection problem
we have here.

01:03:42.430 --> 01:03:47.180
So all these problems really
fall into the same category.

01:03:47.180 --> 01:03:50.260
We're fortunate here at having
one of the cleanest

01:03:50.260 --> 01:03:53.080
probabilistic descriptions
for detection that

01:03:53.080 --> 01:03:54.640
you will ever find.

01:03:54.640 --> 01:03:57.810
So if you want to study decision
making, you're far

01:03:57.810 --> 01:04:01.860
better off trying to learn about
it in this context and

01:04:01.860 --> 01:04:05.570
then using it in all these other
contexts, than you are

01:04:05.570 --> 01:04:06.890
doing something else.

01:04:06.890 --> 01:04:11.300
If you go back and look at old
books about this, you will be

01:04:11.300 --> 01:04:16.670
amazed at the philosophical
discussions that people would

01:04:16.670 --> 01:04:22.870
go through about what makes
sense for hypothesis testing,

01:04:22.870 --> 01:04:25.390
and what doesn't make sense.

01:04:25.390 --> 01:04:28.960
And all of these arguments were
based on a fundamental

01:04:28.960 --> 01:04:32.590
misapprehension that scientists
had until

01:04:32.590 --> 01:04:35.080
relatively recently.

01:04:35.080 --> 01:04:38.660
And the misapprehension that
they had, was that they

01:04:38.660 --> 01:04:42.970
refused to accept models as
taking an intermediate

01:04:42.970 --> 01:04:47.710
position between physical
reality, and then you have

01:04:47.710 --> 01:04:51.230
models, and then you have doing
something with a model.

01:04:51.230 --> 01:04:53.430
And you do something with the
model instead of doing

01:04:53.430 --> 01:04:56.510
something with the
physical reality.

01:04:56.510 --> 01:04:59.040
And people didn't understand
that originally.

01:04:59.040 --> 01:05:04.150
So they got the analysis of the
model terribly confused

01:05:04.150 --> 01:05:07.630
with trying to understand what
the physical reality was.

01:05:07.630 --> 01:05:12.530
In other words, statisticians
and probabilists were both

01:05:12.530 --> 01:05:14.100
doing the same thing.

01:05:14.100 --> 01:05:17.130
Both of them were trying to
simultaneously determine what

01:05:17.130 --> 01:05:20.090
a reasonable model was,
and to determine what

01:05:20.090 --> 01:05:21.380
to do with a model.

01:05:21.380 --> 01:05:24.390
And because of that, they can
never decide on anything.

01:05:24.390 --> 01:05:28.910
Because they can never do what
we've been doing, which says

01:05:28.910 --> 01:05:31.990
take a toy model, analyze
the toy model.

01:05:31.990 --> 01:05:35.190
Figure out something from it
about the physical reality.

01:05:35.190 --> 01:05:38.140
Then go back and look at reality
and see what you need

01:05:38.140 --> 01:05:42.920
to know about reality in order
to make these decisions, which

01:05:42.920 --> 01:05:45.880
is the way people do
it now I think.

01:05:45.880 --> 01:05:48.500
Particularly decision theory
is done that way now.

01:05:48.500 --> 01:05:53.240
Because now we usually assume
both that we know Apriori

01:05:53.240 --> 01:05:57.790
probabilities, it's called,
for these inputs here.

01:05:57.790 --> 01:06:01.080
For the binary communication
problem, we usually want to

01:06:01.080 --> 01:06:06.730
assume that these inputs are
equiprobable, in other words,

01:06:06.730 --> 01:06:08.890
if P sub 0 is equal to
a half, and P sub 1

01:06:08.890 --> 01:06:09.840
is equal to a half.

01:06:09.840 --> 01:06:11.890
And that's the Apriori
probability of

01:06:11.890 --> 01:06:13.140
what's coming in.

01:06:15.980 --> 01:06:20.840
And we then want to assume some
probabilistic model on v.

01:06:20.840 --> 01:06:24.680
Usually what we do, is we figure
out what v is in terms

01:06:24.680 --> 01:06:27.530
of a conditional probability
density, conditional

01:06:27.530 --> 01:06:32.020
probability density of v,
conditional on either of these

01:06:32.020 --> 01:06:33.270
two outputs.

01:06:33.270 --> 01:06:37.520
You see for this model here,
this is just made

01:06:37.520 --> 01:06:39.680
in heaven for us.

01:06:39.680 --> 01:06:43.430
Because this thing is
an input plus a

01:06:43.430 --> 01:06:45.700
Gaussian random variable.

01:06:45.700 --> 01:06:48.970
So if we look at the conditional
probability

01:06:48.970 --> 01:06:53.300
density of the output, which
is signal plus noise,

01:06:53.300 --> 01:06:58.050
conditional on the signal is
just a Gaussian random

01:06:58.050 --> 01:07:01.180
variable shifted over
by the input.

01:07:01.180 --> 01:07:01.750
OK.

01:07:01.750 --> 01:07:03.740
So we have these two
probabilities, the input

01:07:03.740 --> 01:07:07.170
probability, this thing, which
statisticians call a

01:07:07.170 --> 01:07:10.140
likelihood, which is the
probability of this

01:07:10.140 --> 01:07:11.610
conditional on this.

01:07:11.610 --> 01:07:16.490
And in terms of that, we try
to make the right decision.

01:07:16.490 --> 01:07:19.210
Now, I think I'm going to stop
now so we can pass back the

01:07:19.210 --> 01:07:22.280
quizzes and so on.

01:07:22.280 --> 01:07:26.530
Before Wednesday, if you don't
read the notes before

01:07:26.530 --> 01:07:32.050
Wednesday, spend a little bit of
time thinking about how you

01:07:32.050 --> 01:07:35.250
want to make the optimal
decision for this problem as

01:07:35.250 --> 01:07:37.260
we've laid it out now.

01:07:37.260 --> 01:07:41.340
What's surprising is that it
really is a trivial problem.

01:07:41.340 --> 01:07:43.740
Or you can read the notes and
accept the fact that it's a

01:07:43.740 --> 01:07:46.440
trivial problem.

01:07:46.440 --> 01:07:48.480
But there's nothing
hard about it.

01:07:48.480 --> 01:07:52.930
The only interesting things in
detection theory are finding

01:07:52.930 --> 01:07:56.400
easy ways to actually solve
this problem which is

01:07:56.400 --> 01:07:58.920
conceptional trivial.

01:07:58.920 --> 01:08:00.170
OK.

