WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.750
The following content is
provided under a Creative

00:00:02.750 --> 00:00:03.650
Commons license.

00:00:03.650 --> 00:00:06.600
Your support will help MIT
OpenCourseWare continue to

00:00:06.600 --> 00:00:09.485
offer high quality educational
resources for free.

00:00:09.485 --> 00:00:12.780
To make a donation or to view
additional materials from

00:00:12.780 --> 00:00:16.990
hundreds of MIT courses, visit
MIT OpenCourseWare at

00:00:16.990 --> 00:00:18.240
ocw.mit.edu.

00:00:22.850 --> 00:00:25.110
PROFESSOR: I want to spend a
little bit of time today

00:00:25.110 --> 00:00:29.860
finishing up what we were
saying last time, about

00:00:29.860 --> 00:00:32.910
orthonormal expansions.

00:00:32.910 --> 00:00:39.600
Just, essentially, to get a
better understanding of what

00:00:39.600 --> 00:00:41.740
it means to go from finite

00:00:41.740 --> 00:00:45.030
dimension to infinite dimension.

00:00:45.030 --> 00:00:48.280
For the most part, you can just
ignore the question of

00:00:48.280 --> 00:00:52.780
being at an infinite dimensional
space, and

00:00:52.780 --> 00:00:55.610
everything is pretty much the
same as it is in finite

00:00:55.610 --> 00:00:56.780
dimensions.

00:00:56.780 --> 00:00:59.500
But there are a few things
you have to be a

00:00:59.500 --> 00:01:01.700
little careful about.

00:01:01.700 --> 00:01:07.020
So, let's suppose that we have
a set of orthonormal vectors.

00:01:07.020 --> 00:01:10.310
Like the sinc vectors for
example, that we were using

00:01:10.310 --> 00:01:13.490
for the sinc functions that we
were using, to talk about the

00:01:13.490 --> 00:01:16.040
Fourier series.

00:01:16.040 --> 00:01:26.830
And the thing that we said last
time at the end of the

00:01:26.830 --> 00:01:32.710
hour, we quoted this theorem
which was almost obvious, I

00:01:32.710 --> 00:01:36.410
think, in terms of the other
things that we did.

00:01:36.410 --> 00:01:40.220
Which said that if you start
out with some vector v, you

00:01:40.220 --> 00:01:47.010
can project it successively onto
the set of orthonormal

00:01:47.010 --> 00:01:51.350
functions in such a way that you
get closer and closer to

00:01:51.350 --> 00:01:53.560
the vector that you're
looking for.

00:01:53.560 --> 00:01:58.530
And the theorem that we had was
that the projection on the

00:01:58.530 --> 00:02:04.950
whole space, which we call u,
in fact is the same as the

00:02:04.950 --> 00:02:10.510
limit of this sum of orthonormal
functions as we

00:02:10.510 --> 00:02:15.780
take the limit adding more and
more functions into this sum.

00:02:15.780 --> 00:02:18.790
So this was sort of the same
kind of thing that we were

00:02:18.790 --> 00:02:21.900
doing when we were going --

00:02:27.570 --> 00:02:30.230
it's the same sort of thing that
we did when we were doing

00:02:30.230 --> 00:02:34.070
the projection theorem for
finite dimensions.

00:02:34.070 --> 00:02:38.110
You remember the thing that we
did there was to first project

00:02:38.110 --> 00:02:43.440
this function onto a
single waveform.

00:02:43.440 --> 00:02:47.550
Then we found the part of the
waveform that was orthogonal

00:02:47.550 --> 00:02:49.130
to the first waveform.

00:02:49.130 --> 00:02:52.150
That gave us our second
waveform.

00:02:52.150 --> 00:02:57.820
And the thing that we're doing
here, which is sort of

00:02:57.820 --> 00:03:02.000
different, is we're starting
out with the orthogonal

00:03:02.000 --> 00:03:05.830
sequence to start with.

00:03:05.830 --> 00:03:09.720
Anyway, the thing that we showed
was that the vector we

00:03:09.720 --> 00:03:15.600
start with, minus this
approximation, which is in the

00:03:15.600 --> 00:03:19.320
space generated by all of these
p sub n's, that this

00:03:19.320 --> 00:03:22.600
difference has to be orthogonal
to each p sub n.

00:03:22.600 --> 00:03:26.150
Because that was a way
we constructed it.

00:03:26.150 --> 00:03:31.780
So this difference is -- well,
the limit goes to zero, but

00:03:31.780 --> 00:03:34.770
also the difference is
orthogonal to each phi sub x.

00:03:34.770 --> 00:03:38.880
This is the thing which is new,
which we didn't get from

00:03:38.880 --> 00:03:41.130
looking at the Fourier series.

00:03:41.130 --> 00:03:44.030
We could have gotten it from
looking at the Fourier series,

00:03:44.030 --> 00:03:48.640
but this is the thing which is
general for every orthonormal

00:03:48.640 --> 00:03:50.770
expansion in the world.

00:03:50.770 --> 00:03:55.440
Not just a Fourier series,
except all of them.

00:03:55.440 --> 00:03:59.600
And then we say that an inner
product space has accountably

00:03:59.600 --> 00:04:02.230
infinite dimension.

00:04:02.230 --> 00:04:06.070
If accountably infinite set of
orthonormal vectors exist,

00:04:06.070 --> 00:04:08.240
such that only the
zero vectors are

00:04:08.240 --> 00:04:10.520
orthogonal to each other.

00:04:10.520 --> 00:04:14.530
This comes back to one of the
issues that we faced when we

00:04:14.530 --> 00:04:16.380
were going through the
Fourier series.

00:04:16.380 --> 00:04:23.850
Namely, we showed all this nice
stuff, which was really

00:04:23.850 --> 00:04:28.660
about approximating a waveform
by the Fourier series.

00:04:28.660 --> 00:04:31.630
The thing that we never showed,
and the thing which is

00:04:31.630 --> 00:04:35.700
messy and difficult, is that
if you take a time-limited

00:04:35.700 --> 00:04:39.590
function and expand it in a
Fourier series, how do you

00:04:39.590 --> 00:04:42.450
know that when you get all done,
you're actually going to

00:04:42.450 --> 00:04:45.220
get the function back again?

00:04:45.220 --> 00:04:48.610
This is the part of it which
we never talked about.

00:04:48.610 --> 00:04:51.540
We talked about how you generate
all of the Fourier

00:04:51.540 --> 00:04:55.180
coefficients, and all
of that was fine.

00:04:55.180 --> 00:04:58.480
And we showed that when you
were all done, this

00:04:58.480 --> 00:05:03.210
representation function that you
had was in fact orthogonal

00:05:03.210 --> 00:05:04.770
the thing that was left over.

00:05:04.770 --> 00:05:06.790
But you never showed that
the thing that was left

00:05:06.790 --> 00:05:09.680
over went to 0.

00:05:09.680 --> 00:05:12.820
And the thing this is saying
is you don't have to worry

00:05:12.820 --> 00:05:14.220
about that too much.

00:05:14.220 --> 00:05:17.470
Because the thing which is left
over has to be orthogonal

00:05:17.470 --> 00:05:19.560
to everything you
started with.

00:05:19.560 --> 00:05:24.030
And this sum here has
to go to some limit.

00:05:24.030 --> 00:05:27.230
What is this limit in the case
of the Fourier series?

00:05:27.230 --> 00:05:31.030
Suppose we start out with an
arbitrary L2 function.

00:05:31.030 --> 00:05:35.260
We now try to expand that
arbitrary L2 function in a

00:05:35.260 --> 00:05:39.940
Fourier series from minus t
over 2 to plus t over 2.

00:05:39.940 --> 00:05:42.430
And we can do that.

00:05:42.430 --> 00:05:48.380
When we're all done, this series
here is in fact going

00:05:48.380 --> 00:05:52.080
to be the part of the function
which lies between minus t

00:05:52.080 --> 00:05:54.890
over 2 and plus t over 2.

00:05:54.890 --> 00:06:01.420
And this difference here -- well
v minus u, the difference

00:06:01.420 --> 00:06:05.500
between the original vector and
this representation vector

00:06:05.500 --> 00:06:08.220
is going to be this part of
the function which lies

00:06:08.220 --> 00:06:11.550
outside of minus t over
2 to plus t over 2.

00:06:15.700 --> 00:06:18.580
So the next thing that we did
then was to show that the

00:06:18.580 --> 00:06:23.080
truncated sinusoids and the sinc
weighted sinusoids both

00:06:23.080 --> 00:06:25.250
in fact span L2.

00:06:25.250 --> 00:06:32.350
In other words, any function in
L2, after you represent it

00:06:32.350 --> 00:06:36.450
in terms of that series, what
you have left over has to be

00:06:36.450 --> 00:06:37.770
orthogonal.

00:06:37.770 --> 00:06:42.250
In other words, if you take this
entire set of truncated

00:06:42.250 --> 00:06:48.060
sinusoids, every non-zero
function is

00:06:48.060 --> 00:06:50.820
orthogonal to all of that.

00:06:50.820 --> 00:06:56.130
Every non-zero function in
this L2 equivalent sense.

00:06:56.130 --> 00:07:01.580
So in fact, the thing you get
out of this is this part of

00:07:01.580 --> 00:07:05.460
looking at functions which
we always ignored.

00:07:05.460 --> 00:07:07.890
There's another issue that we
ignored, when we were looking

00:07:07.890 --> 00:07:12.100
at functions, which
comes out of this.

00:07:12.100 --> 00:07:17.020
Since v, by assumption
is an L2 vector.

00:07:17.020 --> 00:07:20.450
In other words, it's an L2
function aside from this L2

00:07:20.450 --> 00:07:25.940
equivalent stuff, and phi sub
m is an L2 vector also, this

00:07:25.940 --> 00:07:28.980
inner product has to be
finite, by the Schwarz

00:07:28.980 --> 00:07:31.280
inequality.

00:07:31.280 --> 00:07:33.440
How did we get around this when
we were dealing with the

00:07:33.440 --> 00:07:34.195
Fourier series?

00:07:34.195 --> 00:07:36.080
Does anybody remember?

00:07:36.080 --> 00:07:37.400
Of course you don't remember.

00:07:37.400 --> 00:07:40.120
I hardly remembered that.

00:07:40.120 --> 00:07:45.500
The way we got around it
was by saying that this

00:07:45.500 --> 00:07:50.230
orthonormal function which was
in a truncated sinusoid.

00:07:50.230 --> 00:07:54.590
It was truncated to minus t over
2 and plus t over 2, and

00:07:54.590 --> 00:07:59.810
the waveform was just
e to the i 2 pi f t.

00:07:59.810 --> 00:08:02.405
The magnitude of that function
was always less

00:08:02.405 --> 00:08:03.655
than or equal to 1.

00:08:06.470 --> 00:08:09.330
And because the magnitude was
always less than or equal to

00:08:09.330 --> 00:08:14.130
1, you could just take the
integral of v of t times phi

00:08:14.130 --> 00:08:17.750
sub n of t, and you could show
directly that that integral

00:08:17.750 --> 00:08:19.400
always existed.

00:08:19.400 --> 00:08:22.660
Because of the special property
of the sinusoids.

00:08:22.660 --> 00:08:25.160
Here what we're saying is,
you don't have to worry

00:08:25.160 --> 00:08:27.160
about that any more.

00:08:27.160 --> 00:08:32.110
You can use arbitrary L2
functions as the components of

00:08:32.110 --> 00:08:36.350
an L2 orthonormal expansion.

00:08:36.350 --> 00:08:37.960
And it still works.

00:08:37.960 --> 00:08:41.480
So every one of these things
has to be finite also.

00:08:41.480 --> 00:08:44.040
So, in fact we're buying
something out of this.

00:08:44.040 --> 00:08:48.690
At the end of the notes on
Lectures Eight to Ten, you

00:08:48.690 --> 00:08:52.140
will notice something that I'm
certainly not going to hold

00:08:52.140 --> 00:08:55.140
you responsible for.

00:08:55.140 --> 00:08:59.350
Something called the prolate
spiroidal expansion.

00:08:59.350 --> 00:09:02.940
And it's just given there
primarily as one more example

00:09:02.940 --> 00:09:05.600
of an orthonormal expansion.

00:09:05.600 --> 00:09:08.950
It's a very interesting
orthonormal expansion because

00:09:08.950 --> 00:09:15.760
it has the property that if you
start out asking how much

00:09:15.760 --> 00:09:20.170
energy can I concentrate within
a fixed time interval,

00:09:20.170 --> 00:09:22.540
and a fixed bandwidth
interval.

00:09:22.540 --> 00:09:25.610
Which is one of the things which
make this whole subject

00:09:25.610 --> 00:09:29.880
a little bit fishy
in a whole and

00:09:29.880 --> 00:09:31.530
certainly very, very messy.

00:09:31.530 --> 00:09:34.790
If I start out with a time
limited function and go

00:09:34.790 --> 00:09:38.970
through the Fourier series,
these truncated sinusoids

00:09:38.970 --> 00:09:44.050
spill their energy out of
the band enormously.

00:09:44.050 --> 00:09:47.840
In fact, one of the problems at
the end of Lectures Eight

00:09:47.840 --> 00:09:52.500
to Ten carries you through the
process of just how much

00:09:52.500 --> 00:09:55.990
energy you can have outside
of band by using

00:09:55.990 --> 00:09:57.780
these truncated sinusoids.

00:09:57.780 --> 00:10:02.260
Because truncated sinusoids,
when you look at them in a

00:10:02.260 --> 00:10:05.160
Fourier series, have a lot
of energy outside of

00:10:05.160 --> 00:10:07.910
where it should be.

00:10:07.910 --> 00:10:14.300
So that this particular set of
prolate spiroidal functions

00:10:14.300 --> 00:10:18.010
gives you the answer to the
following question.

00:10:18.010 --> 00:10:22.540
I would like to find that
function, which is limited to

00:10:22.540 --> 00:10:27.410
minus t over 2 plus t over 2,
which has the largest amount

00:10:27.410 --> 00:10:31.220
of energy, largest fraction of
its energy, within the band

00:10:31.220 --> 00:10:33.870
minus w to plus w.

00:10:33.870 --> 00:10:35.530
What is that function?

00:10:35.530 --> 00:10:38.850
Well, that function happens to
be the zero order prolate

00:10:38.850 --> 00:10:41.460
spheroidal function.

00:10:41.460 --> 00:10:42.970
It's the nice property
that it has.

00:10:42.970 --> 00:10:46.330
And it's a nice function, which
almost looks like a

00:10:46.330 --> 00:10:47.590
rectangular function.

00:10:47.590 --> 00:10:50.370
But it just trails off to 0.

00:10:50.370 --> 00:10:54.480
And I say, OK, if I want to
construct an orthonormal

00:10:54.480 --> 00:10:59.920
expansion, and I want to find
another function which is

00:10:59.920 --> 00:11:03.870
orthogonal to that function
and has the next biggest

00:11:03.870 --> 00:11:08.880
amount fraction of its energy,
inside of this -- strictly

00:11:08.880 --> 00:11:12.700
inside of this time limit, and
as much of the energy within

00:11:12.700 --> 00:11:14.640
the frequency limit
as possible,

00:11:14.640 --> 00:11:16.820
what's that next function?

00:11:16.820 --> 00:11:18.590
Well, you solve that problem.

00:11:18.590 --> 00:11:20.830
If you're very good at
interval equations.

00:11:20.830 --> 00:11:23.010
And I certainly couldn't
solve it.

00:11:23.010 --> 00:11:25.480
But, anyway, it has
been solved.

00:11:25.480 --> 00:11:28.610
I mean, physicists for a long
time have worried about that

00:11:28.610 --> 00:11:30.200
particular kind of question.

00:11:30.200 --> 00:11:32.610
Because it comes up in
physics all the time.

00:11:32.610 --> 00:11:35.600
If you time-limit something
and you take the Fourier

00:11:35.600 --> 00:11:39.390
integral, how can you also come
as close to frequency

00:11:39.390 --> 00:11:41.140
limiting as possible?

00:11:41.140 --> 00:11:45.430
So this particular prolate
spheroidal set of orthogonal

00:11:45.430 --> 00:11:50.190
functions, in fact, exactly
solves the problem of how do

00:11:50.190 --> 00:11:53.390
you generate a set of
orthonormal functions which in

00:11:53.390 --> 00:11:56.680
fact have as much energy
as possible both

00:11:56.680 --> 00:11:59.900
frequency-limited and
time-limited?

00:11:59.900 --> 00:12:04.930
And what you find when you do
that is, when you take 2 w t

00:12:04.930 --> 00:12:11.310
of them, at that point the
energy in these orthonormal

00:12:11.310 --> 00:12:14.670
functions, the part of it that's
inside the band, really

00:12:14.670 --> 00:12:17.540
starts to cut off very,
very sharply,

00:12:17.540 --> 00:12:19.940
when w and t are large.

00:12:19.940 --> 00:12:23.710
So that in fact you get 2 w t of
these functions, which have

00:12:23.710 --> 00:12:26.890
almost all of their energy
in this band.

00:12:26.890 --> 00:12:30.690
And everything else has almost
no energy in the band.

00:12:30.690 --> 00:12:34.740
So if you ever get interested
in the question of how many

00:12:34.740 --> 00:12:39.020
waveforms really are there,
which are concentrated in time

00:12:39.020 --> 00:12:43.410
and frequency, that's the
answer to the problem.

00:12:43.410 --> 00:12:46.380
And don't bother to read it now,
but just remember that if

00:12:46.380 --> 00:12:48.860
you ever get interested in that
problem, which I'm sure

00:12:48.860 --> 00:12:51.610
you will at some point or
other, that's where the

00:12:51.610 --> 00:12:53.470
solution lies.

00:12:53.470 --> 00:12:55.430
And I hope I gave a reference
to it there.

00:12:55.430 --> 00:12:56.680
I think I did.

00:13:00.260 --> 00:13:03.930
Anyway, we have these functions
which span L2.

00:13:03.930 --> 00:13:05.940
And there's at least one
other orthonormal

00:13:05.940 --> 00:13:08.560
expansion that we have.

00:13:08.560 --> 00:13:13.260
Which is both time- and
frequency-limited.

00:13:13.260 --> 00:13:17.920
And either at the end of today
or at the beginning of Monday,

00:13:17.920 --> 00:13:21.890
we're going to find another
particularly important

00:13:21.890 --> 00:13:25.180
sequence of orthonormal
functions.

00:13:25.180 --> 00:13:28.470
Which we actually use when
we're transmitting data.

00:13:31.050 --> 00:13:33.560
So to give an example of what
we were just talking about,

00:13:33.560 --> 00:13:37.430
the Fourier series functions
span the space of functions

00:13:37.430 --> 00:13:40.480
over minus t over
2 to t over 2.

00:13:40.480 --> 00:13:44.560
And when normalized, these
functions become 1 over the

00:13:44.560 --> 00:13:48.340
square root of t times what
we started with before.

00:13:48.340 --> 00:13:51.410
Namely, a sinusoid truncated.

00:13:51.410 --> 00:13:54.530
Before we were dealing with
the orthogonal functions

00:13:54.530 --> 00:13:57.210
without the 1 over square
root of t in it.

00:13:57.210 --> 00:14:00.210
If you want to make them
orthonormal, you get this

00:14:00.210 --> 00:14:03.430
square root of 1 over t, because
when you take the

00:14:03.430 --> 00:14:07.120
energy in this function,
you get t.

00:14:07.120 --> 00:14:11.030
If you don't believe me, set k
equal to 0 and look at that.

00:14:11.030 --> 00:14:13.720
And even I can integrate that.

00:14:13.720 --> 00:14:16.830
And when you integrate 1 from
minus t over 2 to t

00:14:16.830 --> 00:14:19.360
over 2, you get 1.

00:14:19.360 --> 00:14:22.670
So, then you have to multiply
by the square

00:14:22.670 --> 00:14:24.010
root of 1 over t.

00:14:24.010 --> 00:14:29.930
When you view the Fourier series
functions in this way,

00:14:29.930 --> 00:14:32.700
it's nice because they're
orthonormal.

00:14:32.700 --> 00:14:35.790
It's not nice because the
square root of 1 over t

00:14:35.790 --> 00:14:37.250
appears everyplace.

00:14:37.250 --> 00:14:40.930
But the nice thing about it is
that then when you expand in

00:14:40.930 --> 00:14:44.800
the Fourier series, you don't
find this t anywhere.

00:14:44.800 --> 00:14:50.900
Namely, v is equal to the limit
of these approximations

00:14:50.900 --> 00:14:54.970
where the n'th approximation is
just the sum from minus m

00:14:54.970 --> 00:15:00.440
to m of alpha sub k. phi sub k
and alpha sub k is just this

00:15:00.440 --> 00:15:02.640
inner product.

00:15:02.640 --> 00:15:05.420
So, again, you got something
nicer by looking at these

00:15:05.420 --> 00:15:09.840
things in terms of vectors You
get nice statements about how

00:15:09.840 --> 00:15:14.740
these things converge, and you
also get a very compact way of

00:15:14.740 --> 00:15:18.200
writing out what the
expressions are.

00:15:18.200 --> 00:15:22.330
You also get something a little
bit fishy, which a lot

00:15:22.330 --> 00:15:25.790
of people in the communication
field, especially ones who do

00:15:25.790 --> 00:15:30.230
theoretical work, run
into problems with.

00:15:30.230 --> 00:15:34.570
And the problem is the
following: when you start

00:15:34.570 --> 00:15:37.890
dealing all the time with
vectors, and you forget about

00:15:37.890 --> 00:15:42.620
the underlying functions and the
underlying integrals, you

00:15:42.620 --> 00:15:45.760
start to think that the
subject is simpler

00:15:45.760 --> 00:15:47.330
than it really is.

00:15:47.330 --> 00:15:50.180
Because you forget about all
the limiting issues.

00:15:50.180 --> 00:15:53.260
And when you forget about all
the limiting issues, it's fine

00:15:53.260 --> 00:15:55.680
almost all the time.

00:15:55.680 --> 00:15:58.070
But every once in a while,
you get trapped.

00:15:58.070 --> 00:16:03.130
And when you get trapped, you
then have to go back behind

00:16:03.130 --> 00:16:05.890
all of the vector stuff and you
have to start looking at

00:16:05.890 --> 00:16:08.800
these integrals again and it
gets rather frustrating.

00:16:08.800 --> 00:16:10.750
So you ought to keep both
of these things in mind.

00:16:15.490 --> 00:16:22.440
Let's go on, let's get back from
mathematics into worrying

00:16:22.440 --> 00:16:26.960
about the question of how
do you send data over

00:16:26.960 --> 00:16:29.950
communication channels.

00:16:29.950 --> 00:16:34.020
And this is just a picture that
we saw starting on Day

00:16:34.020 --> 00:16:40.830
One of this course, which says
the usual way of doing this

00:16:40.830 --> 00:16:48.210
is, you start out with --

00:16:48.210 --> 00:16:49.860
you start out with the source.

00:16:49.860 --> 00:16:54.050
You break the source down into
binary data, and then you take

00:16:54.050 --> 00:16:57.340
the binary data and you transmit
it over a channel.

00:16:57.340 --> 00:16:59.640
And this is the picture of
what you get when you're

00:16:59.640 --> 00:17:03.550
trying to transmit binary
data over a channel.

00:17:03.550 --> 00:17:08.060
And here, we've broken down the
encoder, as we called it,

00:17:08.060 --> 00:17:09.410
into two pieces.

00:17:09.410 --> 00:17:13.080
One of which we call the
discrete encoder and one of

00:17:13.080 --> 00:17:16.260
which we call modulation.

00:17:16.260 --> 00:17:18.860
Now, this is a little
bit fishy also.

00:17:18.860 --> 00:17:22.810
Because there are a lot of
people who now talk about

00:17:22.810 --> 00:17:27.240
coded modulation, where it
turns out to be nice to

00:17:27.240 --> 00:17:32.620
combine this discrete encoder
with the modulation function.

00:17:32.620 --> 00:17:38.160
And when you actually build
modern-day full encoder,

00:17:38.160 --> 00:17:41.640
namely the whole thing from
binary digits to what goes out

00:17:41.640 --> 00:17:44.990
on the channel, you very
often combine these

00:17:44.990 --> 00:17:47.240
two functions together.

00:17:47.240 --> 00:17:50.320
What is it that allows
people to do that?

00:17:50.320 --> 00:17:53.320
The fact that they first studied
how to do it when they

00:17:53.320 --> 00:17:57.530
separate the problems into
two separate problems.

00:17:57.530 --> 00:18:01.180
And when you separate the two
problems, what we wind up with

00:18:01.180 --> 00:18:03.450
is binary digits coming in.

00:18:03.450 --> 00:18:08.940
You massage those binary digits
strictly digitally.

00:18:08.940 --> 00:18:12.970
And what comes out is a
sequence of symbols.

00:18:12.970 --> 00:18:16.430
And, usually, the sequence of
symbols comes out at a slower

00:18:16.430 --> 00:18:18.690
rate, and the binary
digits come in.

00:18:18.690 --> 00:18:22.590
For example, if you take two
binary digits and you convert

00:18:22.590 --> 00:18:27.820
them into one symbol, from a
symbol alphabet with force of

00:18:27.820 --> 00:18:33.670
size four, then for every
two in you get one out.

00:18:33.670 --> 00:18:38.990
If you take three binary digits
coming in, then you

00:18:38.990 --> 00:18:42.090
need a symbol alphabet
of size eight here.

00:18:42.090 --> 00:18:50.220
If you move down in rate from
four binary digits to one

00:18:50.220 --> 00:18:55.020
symbol, then of course you need
an alphabet of size 16.

00:18:55.020 --> 00:18:58.340
So the size of the alphabet
here is going up

00:18:58.340 --> 00:19:03.330
exponentially, as the great
advantage that you get is

00:19:03.330 --> 00:19:05.190
going down.

00:19:05.190 --> 00:19:08.880
We will talk about that
more as we move on.

00:19:08.880 --> 00:19:12.120
You should sort of keep in mind
that there's a strange

00:19:12.120 --> 00:19:18.090
relationship between size of
alphabet and the rate at which

00:19:18.090 --> 00:19:19.840
you can transmit.

00:19:19.840 --> 00:19:23.910
As you try to transmit at higher
and higher rates here,

00:19:23.910 --> 00:19:27.660
the size of your alphabet goes
up exponentially, with the

00:19:27.660 --> 00:19:31.000
rate gain that you get.

00:19:31.000 --> 00:19:34.900
In fact, when you look at
Shannon's famous formula of

00:19:34.900 --> 00:19:38.690
how fast you can communicate
on a channel, you find a

00:19:38.690 --> 00:19:43.270
logarithm of a signal
to noise ratio.

00:19:43.270 --> 00:19:46.110
Of 1 plus a signal
to noise ratio.

00:19:46.110 --> 00:19:49.400
When you look at that signal to
noise ratio and you look at

00:19:49.400 --> 00:19:53.070
this alphabet size in here, you
can almost see just from

00:19:53.070 --> 00:19:56.360
that why in fact that
logarithm in

00:19:56.360 --> 00:19:58.230
these capacity formulas.

00:19:58.230 --> 00:19:59.860
And we'll come back
to talk about that

00:19:59.860 --> 00:20:02.110
again more later also.

00:20:02.110 --> 00:20:06.040
But, anyway, what we're going
to do at this point is to

00:20:06.040 --> 00:20:10.660
separate this into the problem
of discrete encoding and

00:20:10.660 --> 00:20:11.690
modulation.

00:20:11.690 --> 00:20:14.870
And in modulation, you start
out with some arbitrary

00:20:14.870 --> 00:20:16.360
alphabet of symbols.

00:20:16.360 --> 00:20:21.450
You turn the symbols in that
alphabet into waveforms.

00:20:21.450 --> 00:20:23.340
You transmit the waveforms.

00:20:23.340 --> 00:20:26.730
You then get the symbol back,
and then you put it into a

00:20:26.730 --> 00:20:30.960
digital encoder, which gets
the binary digits back.

00:20:30.960 --> 00:20:34.270
Now, what order are we going
to study these in?

00:20:34.270 --> 00:20:38.140
Well, we're going to study
the modulation first.

00:20:38.140 --> 00:20:41.710
And we're not going to study
this at all, essentially,

00:20:41.710 --> 00:20:45.260
except for a few very, very
simple-minded examples.

00:20:45.260 --> 00:20:50.220
6.451, which is the course that
follows after this, which

00:20:50.220 --> 00:20:54.480
might or might not be taught
in the Spring term, and

00:20:54.480 --> 00:20:57.150
incidentally those of you who
want it to be taught should

00:20:57.150 --> 00:21:01.100
send me an email saying you
really need to take this next

00:21:01.100 --> 00:21:04.110
term for some reason or other.

00:21:04.110 --> 00:21:08.160
Because otherwise it will be
given in Spring of the

00:21:08.160 --> 00:21:11.640
following year instead of
Spring of this year.

00:21:11.640 --> 00:21:15.070
There's another side to that
issue, which is a wireless

00:21:15.070 --> 00:21:19.580
course is going to be given in
the Spring of this year.

00:21:19.580 --> 00:21:23.140
And if you want to take a
wireless course and postpone

00:21:23.140 --> 00:21:26.770
taking the coding course until
the following year, then in

00:21:26.770 --> 00:21:31.140
fact it might be better to
do the postponing job.

00:21:31.140 --> 00:21:34.290
I would recommend that those of
you who are interested in

00:21:34.290 --> 00:21:35.890
wireless take the course now.

00:21:35.890 --> 00:21:39.220
Because if you're looking for
research problems in a

00:21:39.220 --> 00:21:44.040
communication area, wireless is
probably the hottest area

00:21:44.040 --> 00:21:45.900
around, and the area
where the most

00:21:45.900 --> 00:21:47.780
interesting problems occur.

00:21:47.780 --> 00:21:49.710
So, you have to make
that tradeoff.

00:21:49.710 --> 00:21:51.470
If you want to take both
of the courses, great.

00:21:51.470 --> 00:21:53.690
Send me an email and
say you want to

00:21:53.690 --> 00:21:54.850
take the coding course.

00:21:54.850 --> 00:21:57.830
But anyway, there'll be very
little coding in this course,

00:21:57.830 --> 00:22:00.540
very little discrete coding.

00:22:00.540 --> 00:22:03.400
And mostly we're going to look
at simple ways of taking

00:22:03.400 --> 00:22:07.870
simple symbol sequences, turning
them into waveforms

00:22:07.870 --> 00:22:12.416
and then transmitting
them on channels.

00:22:12.416 --> 00:22:16.211
AUDIENCE: So, basically, when
you do the source coding, you

00:22:16.211 --> 00:22:19.910
have to [UNINTELLIGIBLE]

00:22:19.910 --> 00:22:24.240
binary --

00:22:27.390 --> 00:22:30.990
PROFESSOR: Binary to
symbol to waveform.

00:22:30.990 --> 00:22:32.240
Yes.

00:22:33.938 --> 00:22:34.970
AUDIENCE: [UNINTELLIGIBLE]

00:22:34.970 --> 00:22:37.280
PROFESSOR: OK, why don't I go
from waveforms directly to

00:22:37.280 --> 00:22:40.750
waveforms instead of going from
waveforms to symbols to

00:22:40.750 --> 00:22:44.950
binary digits, and then I just
have to go back up again.

00:22:44.950 --> 00:22:47.080
A bunch of reasons.

00:22:47.080 --> 00:22:49.400
One of the reasons is that some
of the stuff that you

00:22:49.400 --> 00:22:53.940
transmit is already digital
to start with.

00:22:53.940 --> 00:22:56.530
When you look at what goes
over a network today, for

00:22:56.530 --> 00:22:59.500
example, it's carrying
digital data, it's

00:22:59.500 --> 00:23:01.130
carrying analog data.

00:23:01.130 --> 00:23:02.790
It's carrying images.

00:23:02.790 --> 00:23:05.800
It's carrying everything
you can imagine.

00:23:05.800 --> 00:23:11.670
If you want to design a
modulation system which goes

00:23:11.670 --> 00:23:16.800
directly from analog data to
channel data, and you have a

00:23:16.800 --> 00:23:21.280
hundred different kinds of
analog source data, and you

00:23:21.280 --> 00:23:25.500
have a hundred different kinds
of channels, then you're going

00:23:25.500 --> 00:23:32.070
to have to build one encoder
for every combination of

00:23:32.070 --> 00:23:33.740
source and channel.

00:23:33.740 --> 00:23:37.220
In other words, you've got to
built ten thousand devices.

00:23:37.220 --> 00:23:42.470
If your interest is in keeping
engineers employed, which I

00:23:42.470 --> 00:23:46.620
think is a very good interest,
that's a very good

00:23:46.620 --> 00:23:48.300
philosophy to take.

00:23:48.300 --> 00:23:51.910
But, unfortunately, most people
who build communication

00:23:51.910 --> 00:23:56.270
equipment say, we would really
rather have just a hundred --

00:23:56.270 --> 00:23:58.640
well, two hundred different
devices.

00:23:58.640 --> 00:24:02.430
One hundred devices which turn
all these different sources

00:24:02.430 --> 00:24:06.190
into binary digits, and another
hundred devices which

00:24:06.190 --> 00:24:09.420
turn the binary digits into
something that can be

00:24:09.420 --> 00:24:12.620
transmitted over any channel
you want to.

00:24:12.620 --> 00:24:14.530
I mean, this is just a
general example of

00:24:14.530 --> 00:24:16.430
what people call layering.

00:24:16.430 --> 00:24:20.810
You want to turn systems into
systems with a bunch of layers

00:24:20.810 --> 00:24:24.840
in them, where each layer is
standardized in some way.

00:24:24.840 --> 00:24:29.310
And only has to take care of
a few particular functions.

00:24:29.310 --> 00:24:31.530
And, in fact, that's what
we're doing here also.

00:24:31.530 --> 00:24:35.990
Because we're dealing with
one layer, which is

00:24:35.990 --> 00:24:38.530
doing discrete encoding.

00:24:38.530 --> 00:24:42.190
Which in fact is sort of
generating, for the most part,

00:24:42.190 --> 00:24:45.680
binary digits out of the
discrete encoder.

00:24:45.680 --> 00:24:49.220
Where, as you go through all of
this stuff and you come out

00:24:49.220 --> 00:24:52.540
with binary digits, some of
which are wrong, you can do

00:24:52.540 --> 00:24:56.150
the decoding and get the correct
binary digits out.

00:24:56.150 --> 00:24:59.880
If you look at an awful lot of
coding theory, you'll find out

00:24:59.880 --> 00:25:02.890
it doesn't pay any attention at
all to what's going on in

00:25:02.890 --> 00:25:05.160
the channel.

00:25:05.160 --> 00:25:07.880
Lots of people who've studied
coding all of their lives, and

00:25:07.880 --> 00:25:11.030
decoding all of their lives,
live in this mathematical

00:25:11.030 --> 00:25:13.190
theory of abstract algebra.

00:25:13.190 --> 00:25:16.170
And they have no idea of
what channels are.

00:25:16.170 --> 00:25:20.010
And they survive because of
this layering principle.

00:25:20.010 --> 00:25:22.260
So if you want to employ
engineers, it's nice to have

00:25:22.260 --> 00:25:24.310
layering also.

00:25:24.310 --> 00:25:27.650
Because engineers don't have
to know as much then.

00:25:27.650 --> 00:25:29.550
Of course, you people
should know it all.

00:25:29.550 --> 00:25:32.540
So because, then you
can do anything.

00:25:32.540 --> 00:25:36.510
And you can be part of those
very rare people who can put

00:25:36.510 --> 00:25:38.640
it all together.

00:25:38.640 --> 00:25:42.350
I was just at the 70th birthday
party of Irwin Jacobs

00:25:42.350 --> 00:25:44.550
this past week.

00:25:44.550 --> 00:25:50.360
And Irwin Jacobs is the CEO of
Qualcomm, which is the company

00:25:50.360 --> 00:25:56.590
that started to build CDMA
wireless systems.

00:25:56.590 --> 00:26:02.690
For a long time, the CDMA
systems were thought of as

00:26:02.690 --> 00:26:09.500
probably better than TDMA and
FDMA, but much more expensive.

00:26:09.500 --> 00:26:13.380
And eventually, we're in the
situation where all the new

00:26:13.380 --> 00:26:16.790
systems being designed
are all using CMDA.

00:26:16.790 --> 00:26:19.350
As a result of this,
Irwin Jacobs is a

00:26:19.350 --> 00:26:21.320
very wealthy person.

00:26:21.320 --> 00:26:26.540
We went to a symphony Friday
night, out in San Diego, which

00:26:26.540 --> 00:26:28.730
was given specifically
for him.

00:26:28.730 --> 00:26:31.690
Partly because he'd just
given $110 million to

00:26:31.690 --> 00:26:32.940
the San Diego Symphony.

00:26:35.400 --> 00:26:38.370
So you can figure from that
that he's fairly wealthy.

00:26:38.370 --> 00:26:40.750
Well the point of all of that
is, he started out as a

00:26:40.750 --> 00:26:43.220
faculty member here.

00:26:43.220 --> 00:26:48.390
He was one of the authors of
Rosencraft and Jacobs, which

00:26:48.390 --> 00:26:53.700
is sort of the Bible of digital
communications systems

00:26:53.700 --> 00:26:54.800
from the `60s.

00:26:54.800 --> 00:26:58.050
And people still use it, it's
still an excellent book.

00:26:58.050 --> 00:27:00.970
And in doing that, he really
learned the communication

00:27:00.970 --> 00:27:05.960
trade from soup to --

00:27:05.960 --> 00:27:09.800
I guess, soup to nuts is
the way we put it now.

00:27:09.800 --> 00:27:11.630
And he could do the
whole thing.

00:27:11.630 --> 00:27:13.730
And because he could do the
whole thing, because he

00:27:13.730 --> 00:27:17.460
understood coding, because he
understood modulation, because

00:27:17.460 --> 00:27:21.000
he understood channels, this is
why he could design systems

00:27:21.000 --> 00:27:23.400
that really work.

00:27:23.400 --> 00:27:27.320
I was talking to the Chief
Technology Officer out there,

00:27:27.320 --> 00:27:29.690
and the Chief Technology Officer
said his job was

00:27:29.690 --> 00:27:34.330
really very easy because Irwin
did all of that himself.

00:27:34.330 --> 00:27:36.730
And he still does it.

00:27:36.730 --> 00:27:39.580
So it really makes sense to know
something about all of

00:27:39.580 --> 00:27:41.040
these pieces.

00:27:41.040 --> 00:27:42.970
If you want to become
a billionaire.

00:27:42.970 --> 00:27:47.510
And if you want to become a
billionaire without cheating.

00:27:47.510 --> 00:27:50.780
Now, many people become
billionaires by cheating, and

00:27:50.780 --> 00:27:53.160
you've heard of many
of them but.

00:27:53.160 --> 00:27:54.170
Well, anyway.

00:27:54.170 --> 00:27:55.860
Enough of that.

00:27:55.860 --> 00:27:59.780
I mean, none of us really want
to be billionaires anyway, --

00:27:59.780 --> 00:28:01.750
I mean, there's nothing you
can do with more than a

00:28:01.750 --> 00:28:04.240
billion dollars, right?

00:28:04.240 --> 00:28:05.490
It's all wasted.

00:28:09.600 --> 00:28:11.960
Let's move on.

00:28:11.960 --> 00:28:14.740
We've gotten rid of digital
coding, saying we're not going

00:28:14.740 --> 00:28:15.990
to deal with that here.

00:28:19.460 --> 00:28:22.110
Let me take the PAM out of here
because I don't want to

00:28:22.110 --> 00:28:23.400
even say what it is yet.

00:28:26.950 --> 00:28:30.840
If we take this box I called
modulation before, which was

00:28:30.840 --> 00:28:37.280
one of the two main pieces of
a channel encoder, we can

00:28:37.280 --> 00:28:40.080
break it down into two pieces.

00:28:40.080 --> 00:28:43.120
Namely, mainly we're going to
be layering things again.

00:28:43.120 --> 00:28:46.090
We start out with this symbol
sequence, which is what comes

00:28:46.090 --> 00:28:50.500
out of the encoder, which is
usually just a short sequence

00:28:50.500 --> 00:28:52.590
of binary digits.

00:28:52.590 --> 00:28:57.850
So the symbols can be thought
of as two binary digits in a

00:28:57.850 --> 00:29:01.260
sequence, or four binary digits,
or six tuples of

00:29:01.260 --> 00:29:03.690
binary digits, or
what have you.

00:29:03.690 --> 00:29:06.420
And all of those are common.

00:29:06.420 --> 00:29:09.920
There's then a signal
consolation that turns symbols

00:29:09.920 --> 00:29:11.390
into signals.

00:29:11.390 --> 00:29:17.770
Now, notation in the field is
totally non-uniform, because

00:29:17.770 --> 00:29:24.290
most people use symbols and
sequences and waveforms and

00:29:24.290 --> 00:29:29.810
binary n-tuples, all totally
synonymously.

00:29:29.810 --> 00:29:33.930
And the distinction we'll try
to make here is that symbols

00:29:33.930 --> 00:29:39.790
are things like binary digits,
that don't have any numerical

00:29:39.790 --> 00:29:41.190
meaning to them.

00:29:41.190 --> 00:29:45.480
I mean, a 1 and a 0, the only
thing important there is that

00:29:45.480 --> 00:29:48.150
1 and 0 are different
from each other.

00:29:48.150 --> 00:29:54.030
All computer scientists call
1 and 0's Alices and Bobs.

00:29:54.030 --> 00:29:56.130
Lots of other people
call them plus-1's.

00:29:56.130 --> 00:29:57.310
and minus 1's.

00:29:57.310 --> 00:29:59.640
Doesn't make any difference
what you'd call them, it's

00:29:59.640 --> 00:30:03.830
just an alphabet with
two values in it.

00:30:03.830 --> 00:30:06.130
When we talk about signals,
we're talking

00:30:06.130 --> 00:30:08.650
about numerical values.

00:30:08.650 --> 00:30:12.780
So in fact, you could have a
signal constellation with two

00:30:12.780 --> 00:30:16.320
elements in it, where what
you're doing is mapping 1 and

00:30:16.320 --> 00:30:19.490
0 into plus 1 and minus 1.

00:30:19.490 --> 00:30:22.850
Now, that's a pretty silly
kind of situation, but it

00:30:22.850 --> 00:30:27.320
still has the value of saying,
you're turning things where

00:30:27.320 --> 00:30:32.000
there's just an alphabeticized
2 into something where you're

00:30:32.000 --> 00:30:34.820
saying, these are numerical
values and you're interested

00:30:34.820 --> 00:30:37.280
in the difference between
the numerical values.

00:30:37.280 --> 00:30:41.630
And the difference is measured
in the ordinary way of

00:30:41.630 --> 00:30:45.310
measuring difference for
numerical values.

00:30:45.310 --> 00:30:48.650
Or, these can be vectors also.

00:30:48.650 --> 00:30:51.300
But vectors in an inner product
space where, again,

00:30:51.300 --> 00:30:52.650
you have length.

00:30:52.650 --> 00:30:53.530
And you have distance.

00:30:53.530 --> 00:30:55.520
So you have numerical
values here.

00:30:55.520 --> 00:30:57.890
You don't have numerical
values here.

00:30:57.890 --> 00:31:00.830
So we're going to talk about
how you do this.

00:31:00.830 --> 00:31:04.710
And then, for the most part,
we're going to define

00:31:04.710 --> 00:31:08.200
modulation as what happens when
you go from the signal

00:31:08.200 --> 00:31:11.790
sequence to the waveform.

00:31:11.790 --> 00:31:16.100
You might realize that our
notation is lousy here.

00:31:16.100 --> 00:31:19.070
Because I'm calling modulation
this whole thing.

00:31:19.070 --> 00:31:23.050
And I'm also calling modulation
just this thing.

00:31:23.050 --> 00:31:26.250
And I'm doing that because there
doesn't seem to be any

00:31:26.250 --> 00:31:29.810
other reasonable word for either
one of these things.

00:31:29.810 --> 00:31:32.120
So, and I'm going to
later split this

00:31:32.120 --> 00:31:34.430
into two pieces also.

00:31:34.430 --> 00:31:35.730
But that will come later.

00:31:35.730 --> 00:31:37.870
So, anyway, we're going
from symbols

00:31:37.870 --> 00:31:41.080
to signals to waveforms.

00:31:41.080 --> 00:31:44.090
Which might look remarkably
similar to

00:31:44.090 --> 00:31:45.590
what we did with sources.

00:31:45.590 --> 00:31:47.420
But we just did it backwards
with sources.

00:31:47.420 --> 00:31:52.580
We went from waveforms to
signals to symbols there.

00:31:52.580 --> 00:31:56.030
And here we're doing
the same thing.

00:31:56.030 --> 00:32:01.120
The modulator often converts a
signal sequence to a baseband

00:32:01.120 --> 00:32:05.000
waveform, and then converts
the baseband waveform to a

00:32:05.000 --> 00:32:07.110
passband waveform.

00:32:07.110 --> 00:32:12.220
And, just historically, people
used to think of modulation as

00:32:12.220 --> 00:32:16.060
the process of taking something
at baseband and

00:32:16.060 --> 00:32:19.820
converting it up to
some passband.

00:32:19.820 --> 00:32:23.670
Now it's recognized that the
interesting problem is not,

00:32:23.670 --> 00:32:26.120
how do you go from baseband
to passband.

00:32:26.120 --> 00:32:29.610
Which is just multiplying by
cosine wave, not much more to

00:32:29.610 --> 00:32:32.270
it than that.

00:32:32.270 --> 00:32:36.320
Well, it's a little more
to it, but not much.

00:32:36.320 --> 00:32:42.320
And the interesting problem is,
how do you convert signals

00:32:42.320 --> 00:32:44.180
into waveforms.

00:32:44.180 --> 00:32:46.930
Which is why we went, one of
the reasons we went through

00:32:46.930 --> 00:32:52.040
all of this stuff about L2
waveforms and orthonormal

00:32:52.040 --> 00:32:55.810
expansions and all
of this stuff.

00:32:55.810 --> 00:32:59.360
So, for the time being, we're
going to look at modulation

00:32:59.360 --> 00:33:03.400
and demodulation, without
worrying about what bandwidth

00:33:03.400 --> 00:33:04.720
any of this occurs at.

00:33:04.720 --> 00:33:06.750
So we'll just look at
it at baseband.

00:33:13.140 --> 00:33:16.520
So the simplest example of all
of this, so simple that it

00:33:16.520 --> 00:33:21.460
almost looks like it's silly, is
to map a sequence of binary

00:33:21.460 --> 00:33:26.260
digits into a sequence of
signals from the constellation

00:33:26.260 --> 00:33:28.480
1 and minus 1.

00:33:28.480 --> 00:33:35.000
So all you're doing there is
mapping 0 into 1 and mapping 1

00:33:35.000 --> 00:33:35.960
into minus 1.

00:33:35.960 --> 00:33:39.290
In other words, the 0 and 1
binary digits are mapped into

00:33:39.290 --> 00:33:40.480
1 and minus 1.

00:33:40.480 --> 00:33:43.580
Why do you do it this way, which
is a little confusing,

00:33:43.580 --> 00:33:47.600
mapping 0 into 1 instead
of mapping 1 into 1?

00:33:47.600 --> 00:33:51.750
Well, primarily, so you can
look at multiplication of

00:33:51.750 --> 00:33:56.830
signals in the same way as you
look at modular to addition of

00:33:56.830 --> 00:33:58.400
binary digits.

00:33:58.400 --> 00:34:00.030
It just turns out to
be a little more

00:34:00.030 --> 00:34:01.990
convenient that way.

00:34:01.990 --> 00:34:03.870
And it doesn't make a whole
lot of difference.

00:34:03.870 --> 00:34:08.800
The point is, we're going from
0 1's to signals which are 1

00:34:08.800 --> 00:34:10.650
and minus 1.

00:34:10.650 --> 00:34:15.630
Then this sequence of signals
is mapped into a waveform

00:34:15.630 --> 00:34:19.820
which is the sum of u of k times
the sinc function t over

00:34:19.820 --> 00:34:22.300
capital T minus k.

00:34:22.300 --> 00:34:25.040
In other words, you're thinking
of each one of these

00:34:25.040 --> 00:34:31.780
signals, now, think of it as
being a sum of impulses,

00:34:31.780 --> 00:34:33.220
delayed impulses.

00:34:33.220 --> 00:34:37.600
And then think of taking a
little sinc, sinc t over t and

00:34:37.600 --> 00:34:39.710
putting it around each one
of those impulses.

00:34:39.710 --> 00:34:42.060
In other words, think of
passing each of those

00:34:42.060 --> 00:34:46.900
impulses, which is weighted by
one of these values, u sub k,

00:34:46.900 --> 00:34:55.150
into a linear filter whose
response is sine of x over x.

00:34:55.150 --> 00:34:55.550
Anybody

00:34:55.550 --> 00:34:57.520
see anything a little bit
fishy about that?

00:35:01.510 --> 00:35:03.520
Have you ever built a filter
whose response

00:35:03.520 --> 00:35:05.978
was sine x over x?

00:35:05.978 --> 00:35:08.400
AUDIENCE: It's hard to.

00:35:08.400 --> 00:35:10.890
PROFESSOR: It's hard to, why?

00:35:10.890 --> 00:35:13.850
AUDIENCE: [UNINTELLIGIBLE]

00:35:13.850 --> 00:35:16.170
PROFESSOR: Because it's
not causal, yes.

00:35:16.170 --> 00:35:18.830
We're going to talk about
that more later today.

00:35:18.830 --> 00:35:24.340
Communication engineers hardly
ever talk about causality.

00:35:24.340 --> 00:35:26.045
They hardly ever talk about
whether something

00:35:26.045 --> 00:35:28.940
is realized or not.

00:35:28.940 --> 00:35:33.600
And the reason I want to say
this a number of times is that

00:35:33.600 --> 00:35:36.170
one of the parts of
any receiver is a

00:35:36.170 --> 00:35:38.370
timing recovery circuit.

00:35:38.370 --> 00:35:42.140
And what the timing recovery
circuit does is, it tries to

00:35:42.140 --> 00:35:45.070
figure out what the transmitter
timing is, in

00:35:45.070 --> 00:35:47.640
terms of what it's receiving.

00:35:47.640 --> 00:35:50.160
And when you're figuring out
what the transmitter timing is

00:35:50.160 --> 00:35:53.910
in terms of what you're
receiving, the most convenient

00:35:53.910 --> 00:35:57.740
way of doing that, if you're
sending a pulse or something,

00:35:57.740 --> 00:36:02.140
you would like the receiver
timing to peak up at the same

00:36:02.140 --> 00:36:05.310
time that the transmitter timing
is peaking up, in terms

00:36:05.310 --> 00:36:05.960
of the pulse.

00:36:05.960 --> 00:36:08.650
In other words, you want to
get rid of the propagation

00:36:08.650 --> 00:36:11.240
delay and just ignore that.

00:36:11.240 --> 00:36:14.430
The receiver timing is going
to be delayed from the

00:36:14.430 --> 00:36:19.970
transmitter timing exactly
by the propagation delay.

00:36:19.970 --> 00:36:23.970
Now, if we always do that,
causality becomes totally

00:36:23.970 --> 00:36:25.220
unimportant.

00:36:27.720 --> 00:36:31.070
Now, one of the problems with a
sinc function is, it starts

00:36:31.070 --> 00:36:33.830
at time minus infinity.

00:36:33.830 --> 00:36:37.770
And even if you delay the
receiver timing by an infinite

00:36:37.770 --> 00:36:41.440
amount of time you can never use
the communications system

00:36:41.440 --> 00:36:45.910
until it's time to tear it down
and bring in a new one.

00:36:45.910 --> 00:36:47.880
Which, unfortunately, is what
happened to the third

00:36:47.880 --> 00:36:49.590
generation wireless systems.

00:36:49.590 --> 00:36:52.870
By the time, people figured
out how to build them,

00:36:52.870 --> 00:36:56.940
everybody was saying, ah,
old-fashioned stuff, we're

00:36:56.940 --> 00:36:59.580
going to go directly onto
to fourth generation.

00:36:59.580 --> 00:37:02.430
And they might go directly
from fourth generation to

00:37:02.430 --> 00:37:03.300
fifth generation.

00:37:03.300 --> 00:37:04.650
Who knows.

00:37:04.650 --> 00:37:09.900
Anyway, you can't implement
these even with the receiver

00:37:09.900 --> 00:37:13.360
timing different than the
transmitter timing.

00:37:13.360 --> 00:37:17.430
But you can always approximate
things with enough delay

00:37:17.430 --> 00:37:19.610
between transmitter
and receiver.

00:37:19.610 --> 00:37:22.840
So that you can approximate
any filter you want to,

00:37:22.840 --> 00:37:26.580
without worrying about
causality at all.

00:37:26.580 --> 00:37:29.520
And it's hard enough designing
good filters without worrying

00:37:29.520 --> 00:37:33.530
about casuality that you don't
want to bring that into your

00:37:33.530 --> 00:37:36.420
picture at the beginning.

00:37:36.420 --> 00:37:41.030
So communication engineers
usually say that those timing

00:37:41.030 --> 00:37:43.850
issues are not important.

00:37:43.850 --> 00:37:48.060
And a little bit of delay is
not going to hurt anything.

00:37:48.060 --> 00:37:53.250
All of Shannon's theory was as
successful as it has been, and

00:37:53.250 --> 00:37:56.530
transformed the whole
communication industry,

00:37:56.530 --> 00:38:00.020
because he never talked
about delay at all.

00:38:00.020 --> 00:38:02.190
He just ignored the question
of delay from

00:38:02.190 --> 00:38:04.840
beginning to end.

00:38:04.840 --> 00:38:09.000
And when you look at his
capacity formulas, they are

00:38:09.000 --> 00:38:12.300
assuming that you have as much
delay as you need between

00:38:12.300 --> 00:38:14.050
transmitter and receiver.

00:38:14.050 --> 00:38:17.210
And, assuming that that isn't
important, and in terms of the

00:38:17.210 --> 00:38:22.330
propagation delays, and the
filtering delays in most

00:38:22.330 --> 00:38:25.090
modern communication systems,
those delays

00:38:25.090 --> 00:38:26.500
are not really important.

00:38:26.500 --> 00:38:29.300
The propagation delays
you can't avoid.

00:38:29.300 --> 00:38:31.130
So you might as well
ignore them.

00:38:31.130 --> 00:38:34.650
Which is why you build timing
recovery circuits.

00:38:34.650 --> 00:38:38.840
And the filtering delays are
usually unimportant.

00:38:38.840 --> 00:38:40.880
Relative to all of the
other delays that

00:38:40.880 --> 00:38:42.700
come into these system.

00:38:42.700 --> 00:38:44.840
So for the most part,
we're just going to

00:38:44.840 --> 00:38:47.240
ignore those questions.

00:38:47.240 --> 00:38:52.130
If you have no noise, no delay,
and no attenuation, the

00:38:52.130 --> 00:38:55.180
received waveform is going
to be the same as

00:38:55.180 --> 00:38:57.390
the transmitted waveform.

00:38:57.390 --> 00:39:00.300
And then what you would like
to do is to sample that and

00:39:00.300 --> 00:39:02.370
convert it back to binary.

00:39:02.370 --> 00:39:04.430
So, that's what the receiving
side of this

00:39:04.430 --> 00:39:07.140
trivial system is doing.

00:39:07.140 --> 00:39:09.060
Now, why can you ignore
attenuation?

00:39:11.700 --> 00:39:14.260
Anybody have any idea why
we might want to ignore

00:39:14.260 --> 00:39:15.510
attenuation?

00:39:20.380 --> 00:39:23.020
Well, it's like all of
these other things.

00:39:23.020 --> 00:39:27.340
You don't ignore it, but the
question you ask is, can I

00:39:27.340 --> 00:39:30.500
separate the issue of
attentuation from the other

00:39:30.500 --> 00:39:32.570
issues that I want
to look at now.

00:39:32.570 --> 00:39:36.220
In other words, can I layer
this problem in such a way

00:39:36.220 --> 00:39:39.830
that we can deal with our
problems one by one.

00:39:39.830 --> 00:39:42.570
And the problem of attentuation
is something that

00:39:42.570 --> 00:39:47.590
communication engineers have had
to deal with from day one.

00:39:47.590 --> 00:39:51.690
In dealing with it from day one,
they have dealt with all

00:39:51.690 --> 00:39:56.060
of the different ways of losing
power that you have.

00:39:56.060 --> 00:40:01.370
Which includes attenuation in
the actual medium between

00:40:01.370 --> 00:40:03.180
transmitter and receiver.

00:40:03.180 --> 00:40:06.840
It deals with the attenuation
you get in the receiver by

00:40:06.840 --> 00:40:10.400
building filters and by all the
other things you do there.

00:40:10.400 --> 00:40:13.760
When you get all done with that,
there's only one thing

00:40:13.760 --> 00:40:15.010
that's important.

00:40:18.430 --> 00:40:23.620
Because you can deal digitally
with very, very small signals.

00:40:23.620 --> 00:40:25.960
And the only thing that's
important is what

00:40:25.960 --> 00:40:27.050
happens to the noise.

00:40:27.050 --> 00:40:29.600
You get noise in the
communication medium.

00:40:29.600 --> 00:40:32.520
That comes into the receiver.

00:40:32.520 --> 00:40:36.410
And now, any time you amplify
what you receive, they're

00:40:36.410 --> 00:40:39.730
going to be amplifying the noise
as well as amplifying

00:40:39.730 --> 00:40:41.490
the signal.

00:40:41.490 --> 00:40:45.130
An easy way to deal with that is
to assume that there isn't

00:40:45.130 --> 00:40:48.410
any attentuation in what you're
calling the signal,

00:40:48.410 --> 00:40:50.830
because you're going to amplify
that to a reasonable

00:40:50.830 --> 00:40:54.040
value to operate on it anyway.

00:40:54.040 --> 00:40:57.350
So in fact you're amplifying
the noise.

00:40:57.350 --> 00:41:00.380
So the only thing you're
interested in is, what's the

00:41:00.380 --> 00:41:04.580
ratio between the signal power
and the noise power.

00:41:07.490 --> 00:41:10.980
So, those issues, we can deal
with completely separately

00:41:10.980 --> 00:41:12.920
from these issues of what
kind of waveforms

00:41:12.920 --> 00:41:14.460
do we want to choose.

00:41:14.460 --> 00:41:19.390
What should we choose in place
of this sinc function, which

00:41:19.390 --> 00:41:24.500
is impractical because it causes
too much delay and it's

00:41:24.500 --> 00:41:27.770
also hard to build
the filters.

00:41:27.770 --> 00:41:30.530
So we'd like to deal with that
question separately from the

00:41:30.530 --> 00:41:32.010
question of attenuation.

00:41:32.010 --> 00:41:34.350
There's a short section in the
notes, which I'm not going to

00:41:34.350 --> 00:41:38.150
go into, because you can read
it just as easily as I can

00:41:38.150 --> 00:41:41.950
talk about it, about dB.

00:41:41.950 --> 00:41:47.550
And why people use dB to talk
about all of these questions

00:41:47.550 --> 00:41:49.750
of energy losses.

00:41:49.750 --> 00:41:53.120
And why, in fact, there's a
whole set of engineers whose

00:41:53.120 --> 00:41:56.030
function is to deal
with link budgets.

00:41:56.030 --> 00:41:58.810
In other words, when you build
a communications system,

00:41:58.810 --> 00:42:02.710
you're losing power everywhere
along the way.

00:42:02.710 --> 00:42:04.570
You're losing it
in the median.

00:42:04.570 --> 00:42:07.710
You're losing it by the way
you build the antennas.

00:42:07.710 --> 00:42:11.320
You're losing some here, you're
losing some there.

00:42:11.320 --> 00:42:14.920
And what these people do is,
they look at all of these

00:42:14.920 --> 00:42:16.550
attenuations together.

00:42:16.550 --> 00:42:18.530
And they multiply together.

00:42:18.530 --> 00:42:22.270
So, in fact, it's easier to take
the logarithm of all of

00:42:22.270 --> 00:42:23.520
these terms.

00:42:23.520 --> 00:42:26.590
And when you take the logarithm,
that's where you

00:42:26.590 --> 00:42:28.570
get decibels from.

00:42:28.570 --> 00:42:31.830
Because these people, instead of
taking natural logs, which

00:42:31.830 --> 00:42:35.820
would seem like a much more
reasonable thing, always take

00:42:35.820 --> 00:42:40.610
the logarithm to the base 10
and then divide by 10.

00:42:40.610 --> 00:42:45.420
And what the section in the
notes says is that that's a

00:42:45.420 --> 00:42:50.520
practice which has grown up
because it's very easy to do

00:42:50.520 --> 00:42:52.920
mental arithmetic with that.

00:42:52.920 --> 00:42:59.380
The logarithm to the base 10 of
the number 2 which is one

00:42:59.380 --> 00:43:02.400
of the biggest numbers
that ever appears.

00:43:02.400 --> 00:43:06.050
2's are more important
than thousands, OK?

00:43:06.050 --> 00:43:16.930
And the logarithm to the
base 2 of 10 is 0.3.

00:43:16.930 --> 00:43:20.450
And when you divide it
by 10 you get 3 dB.

00:43:20.450 --> 00:43:23.720
So a factor of 2
is called 3 dB.

00:43:23.720 --> 00:43:28.160
This means that a factor of 4,
which is 2 squared, is 6 dB.

00:43:28.160 --> 00:43:32.200
Factor of 8 is 9 dB,
and so forth.

00:43:32.200 --> 00:43:36.010
And this gives this table of
all these numbers which all

00:43:36.010 --> 00:43:38.990
communication engineers
memorize.

00:43:38.990 --> 00:43:42.180
The other reason for it is that
if you're talking to a

00:43:42.180 --> 00:43:45.570
communication engineer, he will
recognize you as a member

00:43:45.570 --> 00:43:48.790
of his fraternity if
you let the word dB

00:43:48.790 --> 00:43:51.290
slip several times.

00:43:51.290 --> 00:43:52.520
Don't even have to know
what it means.

00:43:52.520 --> 00:43:54.980
You just talk about dB.

00:43:54.980 --> 00:43:59.800
And you say, my income is 3
dB lower than your income.

00:43:59.800 --> 00:44:01.050
And that makes him very happy.

00:44:03.910 --> 00:44:04.580
So.

00:44:04.580 --> 00:44:07.360
Anyway.

00:44:07.360 --> 00:44:10.230
That's all we need for
this simple example.

00:44:10.230 --> 00:44:16.050
We now want to go into more
complicated things.

00:44:16.050 --> 00:44:19.570
OK pulse amplitude
and modulation.

00:44:19.570 --> 00:44:26.110
This is one of the major ways of
turning bits into signals,

00:44:26.110 --> 00:44:30.170
and then turning the signals
into waveforms again.

00:44:30.170 --> 00:44:33.430
Again, I'm doing this first
not because it's the most

00:44:33.430 --> 00:44:36.460
important scheme to talk about,
but because we want to

00:44:36.460 --> 00:44:39.630
understand these things
one by one.

00:44:39.630 --> 00:44:43.330
And when we understand PAM,
we'll then talk about QAM.

00:44:43.330 --> 00:44:44.800
And you'll understand that.

00:44:44.800 --> 00:44:48.050
And then we can go on
to look at other

00:44:48.050 --> 00:44:50.040
variations of these things.

00:44:50.040 --> 00:44:54.030
So the signals in PAM
are one dimensional.

00:44:54.030 --> 00:44:57.470
The constellation, the only
thing it can be, since it's

00:44:57.470 --> 00:45:00.700
one dimensional, one
real dimension, is

00:45:00.700 --> 00:45:03.570
a set of real numbers.

00:45:03.570 --> 00:45:09.600
So you're going to modulate
these real numbers.

00:45:09.600 --> 00:45:11.230
And that, we're going
to talk about later.

00:45:11.230 --> 00:45:13.120
How do you find this
function here?

00:45:13.120 --> 00:45:15.580
We're just going to take these
real numbers, which are coming

00:45:15.580 --> 00:45:18.030
into the transmitter
one by one out

00:45:18.030 --> 00:45:19.790
of the digital encoder.

00:45:19.790 --> 00:45:21.030
You're going to take
these numbers.

00:45:21.030 --> 00:45:24.130
You're going to view them as
being multiplied by delayed

00:45:24.130 --> 00:45:27.560
impulses, and then pass
through a filter.

00:45:27.560 --> 00:45:31.290
And the filter's response is
just, impulse response is

00:45:31.290 --> 00:45:32.400
just, p of t.

00:45:32.400 --> 00:45:35.060
In other words, what we're doing
is saying, we don't like

00:45:35.060 --> 00:45:37.430
the sinc function.

00:45:37.430 --> 00:45:40.640
Therefore, simplest thing to
do is replace the sinc

00:45:40.640 --> 00:45:43.130
function by something
we do like.

00:45:43.130 --> 00:45:46.380
So we're going to replace the
sinc function by some filter

00:45:46.380 --> 00:45:49.440
characteristic, which we like.

00:45:49.440 --> 00:45:54.230
And that's the way to modify
this previous example into

00:45:54.230 --> 00:45:56.160
something that makes
better sense.

00:45:56.160 --> 00:45:59.240
So we're doing two things here
when we're talking about PAM.

00:45:59.240 --> 00:46:03.020
One, we're talking about
generalizing this binary

00:46:03.020 --> 00:46:04.540
signal set.

00:46:04.540 --> 00:46:08.510
And, two, we're talking about
generalizing the sinc function

00:46:08.510 --> 00:46:12.920
into some arbitrary
impulse response.

00:46:12.920 --> 00:46:18.640
So a standard PAM signal set
uses equispaced signals

00:46:18.640 --> 00:46:22.260
symmetric around 0.

00:46:22.260 --> 00:46:24.250
And if you look at the picture,
it makes it clear

00:46:24.250 --> 00:46:26.070
what this means.

00:46:26.070 --> 00:46:28.840
It's the same thing that we were
using all along when we

00:46:28.840 --> 00:46:31.450
were talking about
quantization.

00:46:31.450 --> 00:46:34.750
If you're looking at one
dimensional quantization,

00:46:34.750 --> 00:46:37.510
that's a very natural
way to choose the

00:46:37.510 --> 00:46:39.560
representation points.

00:46:39.560 --> 00:46:42.450
Here, we're doing everything
in the opposite way.

00:46:42.450 --> 00:46:46.370
So we're starting out
with these points.

00:46:46.370 --> 00:46:50.130
Well, we're starting out with
eight symbols and turning them

00:46:50.130 --> 00:46:51.980
into eight signals.

00:46:51.980 --> 00:46:54.620
And when you take eight symbols
and turn them into

00:46:54.620 --> 00:46:59.660
eight signals, perfectly natural
thing to do is to make

00:46:59.660 --> 00:47:03.480
each of these signals equispaced
from each other,

00:47:03.480 --> 00:47:05.130
and center them on the origin.

00:47:09.100 --> 00:47:12.450
This is not something we
have to see later.

00:47:12.450 --> 00:47:17.340
I think you can see that if
these symbols all are used

00:47:17.340 --> 00:47:22.750
with equal probability, and
you're trying to reduce the

00:47:22.750 --> 00:47:28.680
amount of energy that the
signals use, which will then

00:47:28.680 --> 00:47:31.970
go through into reducing the
amount of energy in the

00:47:31.970 --> 00:47:37.470
waveforms that we're
transmitting, it's nice to

00:47:37.470 --> 00:47:39.760
have them centered around 0.

00:47:39.760 --> 00:47:42.460
Because if they have a mean,
that mean is just going to

00:47:42.460 --> 00:47:47.850
contribute directly to the
expected mean square value of

00:47:47.850 --> 00:47:49.790
the signal that you're using.

00:47:49.790 --> 00:47:51.870
So why not center it around 0.

00:47:54.730 --> 00:47:56.800
Later on we'll see many
reasons for not

00:47:56.800 --> 00:47:58.260
centering it around 0.

00:47:58.260 --> 00:48:01.350
But for now we're not going to
worry about any of those, and

00:48:01.350 --> 00:48:03.780
we're going to center
it around 0.

00:48:03.780 --> 00:48:05.700
And the other thing is,
why do you want them

00:48:05.700 --> 00:48:07.270
to be equally spaced?

00:48:07.270 --> 00:48:09.320
Well, I'll talk about that
in just a second.

00:48:13.730 --> 00:48:19.520
Anyway, the signal energy in
these equally spaced signals,

00:48:19.520 --> 00:48:23.280
you can calculate it to be d
squared times m squared minus

00:48:23.280 --> 00:48:26.100
1 divided by 12.

00:48:26.100 --> 00:48:28.630
I think we sort of did this when
we were worrying about

00:48:28.630 --> 00:48:30.600
quantization.

00:48:30.600 --> 00:48:38.870
There's a problem at the end
of lectures 11 and 12 which

00:48:38.870 --> 00:48:42.800
guides you by the hand in
how to calculate this.

00:48:42.800 --> 00:48:45.710
Or you can sit down and just
calculate it by hand, it's not

00:48:45.710 --> 00:48:46.620
hard to do.

00:48:46.620 --> 00:48:55.620
But, anyway, that's the mean
square value of these signals.

00:48:55.620 --> 00:48:58.760
If you assume that they're
equiprobable.

00:48:58.760 --> 00:49:06.090
Now, if you take m to be 2 to
the b, now what's b? b is the

00:49:06.090 --> 00:49:13.360
number of binary digits which
come into this signal former

00:49:13.360 --> 00:49:18.450
when you produce
one signal out.

00:49:18.450 --> 00:49:23.660
Namely, if you bring in b binary
digits, you need an

00:49:23.660 --> 00:49:27.730
alphabet of size 2 to the b.

00:49:27.730 --> 00:49:30.745
If you have an alphabet of size
2 to b, then you're going

00:49:30.745 --> 00:49:33.850
to need m equals 2 to the
b different signals.

00:49:33.850 --> 00:49:40.340
So, usually when you have a
standard PAM system, that

00:49:40.340 --> 00:49:46.360
number there, 8 PAM,
means 8 signals.

00:49:46.360 --> 00:49:51.570
8 is usually going to be
replaced by 4, or 16, or 32,

00:49:51.570 --> 00:49:54.360
or 64, or what have you.

00:49:54.360 --> 00:49:56.140
And, usually, not
anything else.

00:49:56.140 --> 00:49:58.810
Because you're usually going to
deal with something which

00:49:58.810 --> 00:50:00.180
is a power of 2.

00:50:00.180 --> 00:50:04.710
Because the logarithm of this to
the base 2 is the number of

00:50:04.710 --> 00:50:08.730
bits which are coming into the
signal former for each signal

00:50:08.730 --> 00:50:09.980
that comes out.

00:50:16.830 --> 00:50:22.520
This goes up very rapidly
as m squared goes up.

00:50:22.520 --> 00:50:26.790
In other words, you try to
transmit data faster by

00:50:26.790 --> 00:50:32.280
bringing more and more bits in
per signal that you transmit,

00:50:32.280 --> 00:50:35.870
it's a losing proposition
very, very quickly.

00:50:35.870 --> 00:50:38.350
It's this business of a
logarithm which comes into

00:50:38.350 --> 00:50:41.300
everything here.

00:50:41.300 --> 00:50:43.010
We're going to talk
about noise later.

00:50:43.010 --> 00:50:45.540
We're not going to talk
about it now.

00:50:45.540 --> 00:50:50.230
But, we have to recognize the
existence of noise enough to

00:50:50.230 --> 00:50:54.730
realize that when you look at
this diagram here, when you

00:50:54.730 --> 00:50:58.960
look at generating a waveform
around this, or a waveform

00:50:58.960 --> 00:51:03.790
around this, however you receive
these things, noise is

00:51:03.790 --> 00:51:07.940
going to corrupt what you
receive here by a little bit.

00:51:07.940 --> 00:51:10.440
Usually it's Gaussian,
which means it tails

00:51:10.440 --> 00:51:12.920
off very, very quickly.

00:51:12.920 --> 00:51:14.750
With larger amplitudes.

00:51:14.750 --> 00:51:20.350
And what that means is, when
you send a three, the most

00:51:20.350 --> 00:51:24.490
likely thing to happen is
that you're going to

00:51:24.490 --> 00:51:26.770
detect a three again.

00:51:26.770 --> 00:51:28.950
The next most likely thing
is you'll detect

00:51:28.950 --> 00:51:31.440
either a four or a two.

00:51:31.440 --> 00:51:33.670
In other words, what's important
here is this

00:51:33.670 --> 00:51:34.550
distance here.

00:51:34.550 --> 00:51:36.260
And hardly anything else.

00:51:36.260 --> 00:51:40.690
If you send these signals with
equal probability, and the

00:51:40.690 --> 00:51:44.000
noise is additive, the noise
does the same thing no matter

00:51:44.000 --> 00:51:46.710
where you are along here.

00:51:46.710 --> 00:51:51.100
Which says that this standard
PAM set is almost the only

00:51:51.100 --> 00:51:53.170
thing you want to look at.

00:51:53.170 --> 00:51:56.180
So long as you assume that the
noise is going to be additive.

00:51:56.180 --> 00:51:58.950
And the noise is going to affect
everything along this

00:51:58.950 --> 00:52:00.490
line equally.

00:52:00.490 --> 00:52:03.950
It says that you just want to
make the spacing between

00:52:03.950 --> 00:52:09.110
points big enough that it will
pretty much avoid the noise.

00:52:09.110 --> 00:52:12.830
So, the point of all of
that is that d is

00:52:12.830 --> 00:52:14.910
fixed, ahead of time.

00:52:14.910 --> 00:52:16.420
You can't play with that.

00:52:16.420 --> 00:52:17.710
You can play with m.

00:52:17.710 --> 00:52:19.840
When you play with m,
you're playing a

00:52:19.840 --> 00:52:22.510
losing game with energy.

00:52:22.510 --> 00:52:25.590
So that's why standard PAM
is the thing which

00:52:25.590 --> 00:52:28.020
is used with PAM.

00:52:28.020 --> 00:52:29.530
Not much you can do about it.

00:52:32.330 --> 00:52:38.380
And say here that the noise is
independent of the signal.

00:52:38.380 --> 00:52:40.020
We will talk about this later.

00:52:40.020 --> 00:52:42.150
The noise being additive.

00:52:42.150 --> 00:52:45.770
The noise being independent of
the signal are both saying

00:52:45.770 --> 00:52:47.300
almost the same thing.

00:52:47.300 --> 00:52:50.410
Which will be obvious to
you in a little while.

00:52:50.410 --> 00:52:52.540
But not until we start
talking about noise.

00:52:52.540 --> 00:52:55.250
We don't want to start talking
about noise now.

00:52:55.250 --> 00:52:58.260
So, for now, we deal with the
noise just by saying that

00:52:58.260 --> 00:53:01.600
every signal must be separated
by some minimum amount.

00:53:09.280 --> 00:53:11.740
Again, what we said about
delay and attenuation.

00:53:11.740 --> 00:53:14.940
Let me say it again, because
it's important enough that you

00:53:14.940 --> 00:53:16.270
have to understand it.

00:53:19.300 --> 00:53:22.960
After you go through two or
three problem sets, you will

00:53:22.960 --> 00:53:24.320
not understand it again.

00:53:24.320 --> 00:53:28.360
Because you'll get so used to
dealing the receive signal as

00:53:28.360 --> 00:53:32.020
the same as the transmitted
signal that you'll forget the

00:53:32.020 --> 00:53:34.670
weirdness in that.

00:53:34.670 --> 00:53:37.150
I mean, people become accustomed
to extraordinarily

00:53:37.150 --> 00:53:40.180
weird things very,
very easily.

00:53:40.180 --> 00:53:42.300
And especially when you're
taking a course and trying to

00:53:42.300 --> 00:53:45.090
get the problems done, you just
take things which are

00:53:45.090 --> 00:53:48.270
totally ridiculous and accept
them if it lets you get

00:53:48.270 --> 00:53:49.320
through the problem set.

00:53:49.320 --> 00:53:53.420
So I want to tell you right up
front that there is some

00:53:53.420 --> 00:53:55.180
weirdness associated here.

00:53:55.180 --> 00:53:57.450
It is something you have
to think about.

00:53:57.450 --> 00:54:00.820
After you think about it once,
you then accept this is a

00:54:00.820 --> 00:54:02.140
layering decision.

00:54:02.140 --> 00:54:05.330
You ignore delay, since the
timing recovery locks the

00:54:05.330 --> 00:54:08.210
receiver clock to the
transmitter clock plus the

00:54:08.210 --> 00:54:09.730
propagation delay.

00:54:09.730 --> 00:54:12.510
And in fact, it can lock the
receive clock to any place

00:54:12.510 --> 00:54:15.300
that wants to lock it to.

00:54:15.300 --> 00:54:18.070
So we're going to lock it in
such a way that the receive

00:54:18.070 --> 00:54:21.950
signal looks like the
transmitted signal.

00:54:21.950 --> 00:54:26.090
And the attenuation is really
part of the link budget.

00:54:26.090 --> 00:54:30.470
We can separate that from all
the things we're going to do.

00:54:30.470 --> 00:54:33.500
I mean, if we don't separate
that, you have to go into

00:54:33.500 --> 00:54:35.380
antenna design.

00:54:35.380 --> 00:54:36.550
And all this other stuff.

00:54:36.550 --> 00:54:39.060
And who wants to do that?

00:54:39.060 --> 00:54:41.660
I mean, we have enough
to do in this course.

00:54:41.660 --> 00:54:44.990
It's pretty full anyway.

00:54:44.990 --> 00:54:48.970
So we're just going to scale the
signal and noise together.

00:54:48.970 --> 00:54:51.030
And that's a separate issue.

00:54:54.370 --> 00:54:57.170
So now we want to look at the
thing which is called PAM

00:54:57.170 --> 00:54:58.010
modulation.

00:54:58.010 --> 00:55:00.840
In other words, in this one
slide we separated the

00:55:00.840 --> 00:55:07.040
question of choosing the signal
constellation, which

00:55:07.040 --> 00:55:10.000
we've now solved by saying, we
want to use signals that are

00:55:10.000 --> 00:55:12.160
equally spaced.

00:55:12.160 --> 00:55:13.450
So that's an easy one.

00:55:13.450 --> 00:55:17.160
From the question of, how do
you choose the filter.

00:55:17.160 --> 00:55:21.640
So the PAM modulation is going
to go by taking a sequence of

00:55:21.640 --> 00:55:26.800
signals, mapping it into a
waveform, which is this

00:55:26.800 --> 00:55:27.970
expansion here.

00:55:27.970 --> 00:55:31.420
We're not assuming that these
functions are orthogonal to

00:55:31.420 --> 00:55:33.500
each other.

00:55:33.500 --> 00:55:38.030
Although later, we will find
out that they should be.

00:55:38.030 --> 00:55:43.810
But for now, p of t is just
some arbitrary waveform.

00:55:43.810 --> 00:55:49.520
And we will try to figure out
how to choose this waveform in

00:55:49.520 --> 00:55:54.350
such a way as to replace the
sinc waveform with something

00:55:54.350 --> 00:55:58.770
which is better in terms of
delay and almost as good in

00:55:58.770 --> 00:56:00.620
all other ways.

00:56:00.620 --> 00:56:05.080
We're not going to worry about
the fact that p of t has to be

00:56:05.080 --> 00:56:05.930
realizable.

00:56:05.930 --> 00:56:08.630
Because with our arbitrary
time reference at the

00:56:08.630 --> 00:56:10.930
receiver, it doesn't have
to be realizable.

00:56:10.930 --> 00:56:12.630
It doesn't have to be causal.

00:56:15.150 --> 00:56:19.220
So, p of t could be
sinc of t over t.

00:56:19.220 --> 00:56:23.160
Which would give us a nice
baseband limited function.

00:56:23.160 --> 00:56:24.680
But it could be anything
else at all.

00:56:28.080 --> 00:56:31.260
As we said before, sinc
t over t a dies out

00:56:31.260 --> 00:56:33.960
impractically slowly.

00:56:33.960 --> 00:56:38.830
And it requires infinite delay
at the transmitter.

00:56:38.830 --> 00:56:42.740
You can't even send these
signals with a

00:56:42.740 --> 00:56:46.040
sinc t over t signal.

00:56:46.040 --> 00:56:50.040
Because to send them, you have
to start out at the beginning

00:56:50.040 --> 00:56:52.360
of this little bit
of wiggling.

00:56:52.360 --> 00:56:54.950
Now, you say, OK I'm going
to truncate that

00:56:54.950 --> 00:56:56.430
when it's very small.

00:56:56.430 --> 00:56:58.620
And I don't worry about that.

00:56:58.620 --> 00:57:01.740
The point of what we're starting
to look at now,

00:57:01.740 --> 00:57:04.210
though, is we're saying, OK,
you truncate it, you do all

00:57:04.210 --> 00:57:05.660
these practical things.

00:57:05.660 --> 00:57:09.170
But it turns out that this
problem of choosing this

00:57:09.170 --> 00:57:12.240
filter response has
a very elegant

00:57:12.240 --> 00:57:14.240
and a very nice solution.

00:57:14.240 --> 00:57:18.220
And when we put noise in, it
fits in perfectly with the

00:57:18.220 --> 00:57:24.270
idea of also choosing this
filter in a particular way.

00:57:24.270 --> 00:57:26.620
Now, we've talked about many
problems here which were

00:57:26.620 --> 00:57:30.630
solved almost immediately after
Shannon came out with

00:57:30.630 --> 00:57:34.880
his way of looking at
communication in 1948.

00:57:34.880 --> 00:57:38.280
Guess when this problem
was solved?

00:57:38.280 --> 00:57:41.960
It was solved 20 years earlier
than that by a guy by the name

00:57:41.960 --> 00:57:44.340
of Nyquist, who was at
Bell Labs back when

00:57:44.340 --> 00:57:46.610
Bell Labs meant something.

00:57:49.150 --> 00:57:51.000
I mean, in `28 it was
a great place.

00:57:51.000 --> 00:57:56.150
It was a great place until
seven or eight years ago.

00:57:56.150 --> 00:57:59.630
Nyquist is important
in feedback theory.

00:57:59.630 --> 00:58:02.880
He's done some of the most
important things there.

00:58:02.880 --> 00:58:06.360
His Nyquist criterion in dealing
with how do you choose

00:58:06.360 --> 00:58:10.450
these filters to avoid
intersymbol interference is

00:58:10.450 --> 00:58:11.640
fairly simple.

00:58:11.640 --> 00:58:14.700
But it's a very, very
nice result.

00:58:14.700 --> 00:58:17.000
So we're going to
talk about it.

00:58:17.000 --> 00:58:20.070
And then we will use that to
say, ok, we don't have to

00:58:20.070 --> 00:58:23.230
worry about intersymbol
interference any more, so all

00:58:23.230 --> 00:58:24.620
we have to worry
about is noise.

00:58:24.620 --> 00:58:27.430
So we're getting rid of these
problems one by one.

00:58:31.280 --> 00:58:34.820
Our main problem is to choose
this filter, so that we get

00:58:34.820 --> 00:58:38.070
some kind of reasonable
compromise between time delay

00:58:38.070 --> 00:58:39.830
and bandwidth.

00:58:39.830 --> 00:58:44.990
That's -- that's basically the
problem that we're facing.

00:58:44.990 --> 00:58:47.610
And Nyquist's solution
to this was to say,

00:58:47.610 --> 00:58:49.330
forget about that also.

00:58:49.330 --> 00:58:53.270
Let's look at what set of
filters work, and what set of

00:58:53.270 --> 00:58:55.320
filters don't work.

00:58:55.320 --> 00:58:57.510
And then you can take your
choice among this set of

00:58:57.510 --> 00:58:58.760
folders that work.

00:59:05.900 --> 00:59:14.630
So our problem is, how do you
take the waveform u of t,

00:59:14.630 --> 00:59:21.330
which you receive, and find
these samples out of it?

00:59:21.330 --> 00:59:24.600
Now, we already know that if
you use sinc functions, all

00:59:24.600 --> 00:59:27.920
you have to do is sample this
and you get these u sub k's

00:59:27.920 --> 00:59:29.040
back again.

00:59:29.040 --> 00:59:32.810
But if this thing is some
absolutely wild waveform,

00:59:32.810 --> 00:59:35.090
maybe that's not what you get.

00:59:35.090 --> 00:59:42.250
So we say, OK, how do we
retrieve these signals from

00:59:42.250 --> 00:59:45.650
the waveform that was
transmitted and therefore from

00:59:45.650 --> 00:59:47.610
the waveform that
was received.

00:59:47.610 --> 00:59:50.520
We're separating this from
the noise question.

00:59:50.520 --> 00:59:53.630
Even if there's no noise at all,
we still have a question

00:59:53.630 --> 00:59:56.490
and how do you find
those input values

00:59:56.490 --> 00:59:59.600
directly from the function.

00:59:59.600 --> 01:00:01.940
What we're going to do is to
assume that the receiver

01:00:01.940 --> 01:00:06.200
filters u of t, with a linear
time invariant filter, with

01:00:06.200 --> 01:00:08.970
impulse response q of t.

01:00:08.970 --> 01:00:11.530
Now, since we've said that
p of t doesn't have to be

01:00:11.530 --> 01:00:15.800
causal, we might as well say
that q of t doesn't have to be

01:00:15.800 --> 01:00:16.370
causal either.

01:00:16.370 --> 01:00:21.150
Because we've already thrown
these details of

01:00:21.150 --> 01:00:24.080
delay to the winds.

01:00:24.080 --> 01:00:27.310
So the filtered waveform, then
is going to be r of t.

01:00:27.310 --> 01:00:32.910
Will be the integral of what
was transmitted, convolved

01:00:32.910 --> 01:00:34.380
with q of t.

01:00:34.380 --> 01:00:37.330
So this is what you receive.

01:00:37.330 --> 01:00:40.300
And then what we're going to do
is, we're going to sample

01:00:40.300 --> 01:00:42.810
this waveform.

01:00:42.810 --> 01:00:47.370
So Nyquist said, let's restrict
ourselves to looking

01:00:47.370 --> 01:00:51.570
at receivers which first filter
by some filter we're

01:00:51.570 --> 01:00:55.200
going to decide on,
and then sample.

01:00:55.200 --> 01:00:56.600
Why do you want to do that?

01:00:56.600 --> 01:00:58.950
Well, an interesting question.

01:00:58.950 --> 01:01:01.830
And Nyquist said, that's
what we're going to do.

01:01:01.830 --> 01:01:06.060
And since Nyquist was famous,
that's what we're going to do.

01:01:06.060 --> 01:01:11.500
One of the problems in the
homework this week is to show

01:01:11.500 --> 01:01:15.160
that if you relax this a little
bit and you say, well,

01:01:15.160 --> 01:01:18.710
I don't want to do what Nyquist
said, what I want to

01:01:18.710 --> 01:01:22.690
do is to look at an arbitrary
linear receiver which takes

01:01:22.690 --> 01:01:25.760
this received waveform, goes
through any old linear

01:01:25.760 --> 01:01:29.960
operations that I want
to go through, and

01:01:29.960 --> 01:01:31.600
solves for what --

01:01:34.170 --> 01:01:37.820
and from that, tries to pick
out these coefficients.

01:01:37.820 --> 01:01:41.560
And the question is, can you
do anything more than what

01:01:41.560 --> 01:01:42.880
Nyquist did.

01:01:42.880 --> 01:01:45.250
And the answer is, no.

01:01:45.250 --> 01:01:49.460
If you look at it in another
way, you will find that in

01:01:49.460 --> 01:01:53.980
fact, if you know what the
signal constellation is, you

01:01:53.980 --> 01:01:55.790
can look at non-linear
receivers.

01:01:55.790 --> 01:01:59.700
Which in the absence of noise
will let you pick out these

01:01:59.700 --> 01:02:03.270
coefficients in a much
broader context than

01:02:03.270 --> 01:02:05.170
what Nyquist said.

01:02:05.170 --> 01:02:07.090
And why don't we do that?

01:02:07.090 --> 01:02:10.660
Well, because when noise comes
in, that doesn't work at all.

01:02:10.660 --> 01:02:13.930
That's a lousy solution.

01:02:13.930 --> 01:02:17.320
So what we're going to do is
say, OK, Mr. Nyquist, we'll

01:02:17.320 --> 01:02:19.400
play your silly game.

01:02:19.400 --> 01:02:22.920
We will have this filter
at the transmitter.

01:02:22.920 --> 01:02:25.680
We'll have this filter
at the receiver.

01:02:25.680 --> 01:02:28.690
We'll have the sampler and we'll
look at what conditions

01:02:28.690 --> 01:02:32.360
we need in order to make
the whole thing work.

01:02:32.360 --> 01:02:34.980
And then we will fit it in with
noise and everything.

01:02:34.980 --> 01:02:37.070
It will fit in, in
a very nice way.

01:02:37.070 --> 01:02:40.500
So we have a nice layered
solution when we do that.

01:02:40.500 --> 01:02:42.910
And we will find --

01:02:42.910 --> 01:02:44.930
I mean, Nyquist had
some of Shannon's

01:02:44.930 --> 01:02:47.030
genes in him, I think.

01:02:47.030 --> 01:02:50.640
Because what we find when we're
all finished with this

01:02:50.640 --> 01:02:54.600
is that by avoiding -- by being
able to receive these

01:02:54.600 --> 01:02:58.710
coefficients perfectly, which
we'll refer to as avoiding

01:02:58.710 --> 01:03:03.530
intersymbol interference, it
doesn't hurt us at all as far

01:03:03.530 --> 01:03:05.190
as taking care of the noise.

01:03:05.190 --> 01:03:07.410
In other words, you can have
your cake and you can eat it

01:03:07.410 --> 01:03:10.080
too as far as intersymbol
interference

01:03:10.080 --> 01:03:11.330
and noise is concerned.

01:03:18.790 --> 01:03:23.260
We wind up with a received
waveform, which is the

01:03:23.260 --> 01:03:27.290
integral of the transmitted
waveform times some filter.

01:03:27.290 --> 01:03:29.350
And we don't know how to
choose this filter.

01:03:29.350 --> 01:03:32.830
But let's just -- this
is a filter.

01:03:32.830 --> 01:03:37.340
This can be represented now in
terms of u of t is equal to

01:03:37.340 --> 01:03:41.630
this transmitted waveform in
terms of this other filter

01:03:41.630 --> 01:03:42.760
that we don't understand.

01:03:42.760 --> 01:03:46.270
So we now have two filters
that we don't understand.

01:03:46.270 --> 01:03:49.000
And we have this
integral here.

01:03:49.000 --> 01:03:50.890
Well, we can take this sum.

01:03:50.890 --> 01:03:54.380
We can bring this sum outside
of the integral and have the

01:03:54.380 --> 01:03:59.010
sum of u sub k times, just some
composite filter g of t

01:03:59.010 --> 01:04:01.840
where g of t is the
convolution of p

01:04:01.840 --> 01:04:03.510
of t and q of t.

01:04:03.510 --> 01:04:06.860
Now, when you look at the notes,
the notes are fairly

01:04:06.860 --> 01:04:10.850
careful in dealing with all
these questions of L2 and

01:04:10.850 --> 01:04:12.860
convergence and all of
this stuff that we've

01:04:12.860 --> 01:04:14.860
been talking about.

01:04:14.860 --> 01:04:18.780
Again, when you're trying to
understand something for the

01:04:18.780 --> 01:04:22.970
first time, ignore all those
mathematical issues.

01:04:22.970 --> 01:04:25.120
Try to find out what's
going on.

01:04:25.120 --> 01:04:28.170
When you get an intuitive sense
of what's going on, go

01:04:28.170 --> 01:04:30.150
back and look at the
mathematics then.

01:04:30.150 --> 01:04:34.230
But don't fuss about the
mathematics at this point.

01:04:34.230 --> 01:04:38.840
OK, so r of t, then, is just
going to be the sum over k of

01:04:38.840 --> 01:04:45.440
these sample values that are
coming in, times this

01:04:45.440 --> 01:04:50.040
composite filter, which is the
convolution of the transmit

01:04:50.040 --> 01:04:53.550
filter and the recieve filter.

01:04:53.550 --> 01:04:55.770
Now, this shouldn't
be surprising.

01:04:55.770 --> 01:05:00.320
If you think of u of t as
being formed by taking a

01:05:00.320 --> 01:05:04.200
sequence of impulses, and then
passing that sequence of

01:05:04.200 --> 01:05:08.530
impulses through a filter p of
t, and then passing the output

01:05:08.530 --> 01:05:12.670
through a filter q of t, all
you're doing is passing this

01:05:12.670 --> 01:05:16.005
sequence of impulses through
the convolution of p

01:05:16.005 --> 01:05:19.130
of t and q of t.

01:05:19.130 --> 01:05:22.780
In other words, in terms of
this received waveform, it

01:05:22.780 --> 01:05:25.750
couldn't care less what
filtering you do at the

01:05:25.750 --> 01:05:29.490
transmitter and what filtering
you do at the receiver.

01:05:29.490 --> 01:05:34.290
It's all one big filter as far
as the receiver is concerned.

01:05:34.290 --> 01:05:37.300
When we study noise, what
happens with the transmitter

01:05:37.300 --> 01:05:39.200
and what happens with
the receiver will

01:05:39.200 --> 01:05:40.870
become important again.

01:05:40.870 --> 01:05:45.420
But, so far, none of this
makes any difference.

01:05:45.420 --> 01:05:50.070
And this is all we need
to worry about.

01:05:50.070 --> 01:05:55.040
Then, Nyquist said, a waveform
g of t is ideal Nyquist.

01:05:55.040 --> 01:05:56.430
He didn't call it ideal
Nyquist, he

01:05:56.430 --> 01:05:58.080
was very modest person.

01:05:58.080 --> 01:06:01.850
But since then, people call it
ideal Nyquist because he was

01:06:01.850 --> 01:06:05.230
the guy that sorted
it all out.

01:06:05.230 --> 01:06:08.700
It's ideal Nyquist with period
t, and I usually leave out the

01:06:08.700 --> 01:06:12.880
period t because that's usually
understood, if g of k

01:06:12.880 --> 01:06:16.780
t is equal to delta of k.

01:06:16.780 --> 01:06:25.380
In other words, if g of 0 is
equal to 1 and g of k times t,

01:06:25.380 --> 01:06:29.470
for every non-zero integer
k is equal to 0.

01:06:29.470 --> 01:06:32.880
In other words, if g has the
same property that the sine

01:06:32.880 --> 01:06:36.000
function has, sine
function is 1.

01:06:36.000 --> 01:06:37.020
It's 0.

01:06:37.020 --> 01:06:41.770
And it's 0 at every integer
point beyond that.

01:06:41.770 --> 01:06:46.520
And Nyquist said, well, gee, all
we need to do is make this

01:06:46.520 --> 01:06:48.750
filter has a property.

01:06:48.750 --> 01:06:52.010
And when you look at this, it's
fairly obvious that that

01:06:52.010 --> 01:06:55.200
works, right?

01:06:55.200 --> 01:06:57.260
I mean, we want a
sample RFT --

01:06:57.260 --> 01:07:03.150
say, at j times capital T. Or,
if j times capital T, it's

01:07:03.150 --> 01:07:15.760
going to be the sum here of u
sub k times g of j t r of t.

01:07:15.760 --> 01:07:29.150
r of j t is equal to sum
over k of u k times g

01:07:29.150 --> 01:07:33.860
of j t minus k t.

01:07:40.200 --> 01:07:45.680
If the waveform is ideal
Nyquist, then this quantity is

01:07:45.680 --> 01:07:50.800
0 for all integer k except
when k is equal to j.

01:07:50.800 --> 01:07:57.090
So, this is just equal
to u sub j.

01:07:57.090 --> 01:08:02.570
And conversely, if this waveform
is not ideal Nyquist,

01:08:02.570 --> 01:08:08.430
you have the problem that you
can pick two values. u sub k

01:08:08.430 --> 01:08:10.890
and u sub j, in such
a way that they

01:08:10.890 --> 01:08:13.140
interfere with each other.

01:08:13.140 --> 01:08:16.880
In other words, that they add up
at some sample point to the

01:08:16.880 --> 01:08:17.610
wrong value.

01:08:17.610 --> 01:08:22.150
One of them is going to come
in and clobber the other.

01:08:22.150 --> 01:08:26.590
So, this is a necessary and
sufficient condition for

01:08:26.590 --> 01:08:29.540
avoiding intersymbol
interference.

01:08:29.540 --> 01:08:32.130
So long as you're not smart
enough to look at what those

01:08:32.130 --> 01:08:33.950
actual values are.

01:08:33.950 --> 01:08:36.010
In other words, so long as
you're only going to use a

01:08:36.010 --> 01:08:40.620
linear receiver, which is
what that amounts to.

01:08:40.620 --> 01:08:44.560
While ignoring noise, r of t is
determined by g of t, p of

01:08:44.560 --> 01:08:47.050
t and q of t otherwise
irrelevant.

01:08:47.050 --> 01:08:49.320
That's what we said.

01:08:49.320 --> 01:08:50.280
We said this.

01:08:50.280 --> 01:08:54.600
If t of t is ideal Nyquist and
r of k t equals u of k

01:08:54.600 --> 01:08:55.540
for all k and z.

01:08:55.540 --> 01:08:59.910
If f of t is not ideal Nyquist,
and r f k t unequal

01:08:59.910 --> 01:09:05.320
to u k for some k and some
choice of the sequence.

01:09:05.320 --> 01:09:07.740
Now, so far there's
no big deal here.

01:09:10.350 --> 01:09:12.070
This is pretty easy
to figure out.

01:09:12.070 --> 01:09:16.040
You don't need rocket science
to say, once I pose the

01:09:16.040 --> 01:09:18.990
problem this why, which
is where part of

01:09:18.990 --> 01:09:20.870
Nyquist's genius came from.

01:09:20.870 --> 01:09:22.920
The hard thing is always to
post the right problem.

01:09:22.920 --> 01:09:25.160
It's not to solve it.

01:09:25.160 --> 01:09:30.140
Those of you who want to do Ph.D
theses, believe me, 80%

01:09:30.140 --> 01:09:33.080
of the problem is finding
the problem.

01:09:33.080 --> 01:09:35.670
20% of the problem
is doing it.

01:09:35.670 --> 01:09:40.890
If you do a really outstanding
thesis, 99% is finding the

01:09:40.890 --> 01:09:45.430
problem and 1% of
it is doing it.

01:09:45.430 --> 01:09:46.620
And I believe that.

01:09:46.620 --> 01:09:47.870
I'm not exaggerating.

01:09:52.890 --> 01:09:56.710
An ideal Nyquist g of t implies
that no intersymbol

01:09:56.710 --> 01:09:59.500
interference occurs at
the above receiver.

01:09:59.500 --> 01:10:03.400
In other words, you have a
receiver that actually works.

01:10:03.400 --> 01:10:05.850
We're going to see that choosing
g of t to be ideal

01:10:05.850 --> 01:10:09.440
Nyquist fits in nicely when
looking at the real problem,

01:10:09.440 --> 01:10:12.160
which is coping with both
noise and intersymbol

01:10:12.160 --> 01:10:13.880
interference.

01:10:13.880 --> 01:10:18.180
And we've also seen that if g of
t is sync of t over capital

01:10:18.180 --> 01:10:19.770
T, that works.

01:10:19.770 --> 01:10:22.390
It has no intersymbol
interference because that's,

01:10:22.390 --> 01:10:28.380
one, at t equals 0, and it's 0
at every other sample point.

01:10:28.380 --> 01:10:32.150
We don't like that, because
it has too much delay.

01:10:32.150 --> 01:10:34.490
We want to make g of
t strictly baseband

01:10:34.490 --> 01:10:37.370
limited to 1 over 2t.

01:10:37.370 --> 01:10:39.660
And this turns out to be
the only solution.

01:10:39.660 --> 01:10:42.390
And we'll see that in
a little while.

01:10:42.390 --> 01:10:45.860
In other words, if you want to
do something which has better

01:10:45.860 --> 01:10:50.190
delay characteristics than the
sinc function, your only way

01:10:50.190 --> 01:10:52.860
of doing it is spilling out
into slightly higher

01:10:52.860 --> 01:10:54.170
frequencies.

01:10:54.170 --> 01:10:58.360
So, what Nyquist really wanted
to find out, although you

01:10:58.360 --> 01:11:01.480
won't find that out by reading
his paper because it's all

01:11:01.480 --> 01:11:05.300
this nice mathematical proof,
is how much do you have to

01:11:05.300 --> 01:11:11.130
expand the bandwidth in order to
get nice delay things which

01:11:11.130 --> 01:11:12.380
do the same thing.

01:11:17.830 --> 01:11:20.150
Well, we think about
it a little bit.

01:11:20.150 --> 01:11:23.140
And we have an advantage that
Nyquist didn't have.

01:11:23.140 --> 01:11:26.460
Because we understand
about aliasing.

01:11:26.460 --> 01:11:28.340
Nyquist didn't understand
about aliasing.

01:11:28.340 --> 01:11:30.855
Aliasing hadn't been
done at that time.

01:11:30.855 --> 01:11:33.270
It was probably done
as a result of

01:11:33.270 --> 01:11:36.710
what Nyquist had done.

01:11:36.710 --> 01:11:40.700
And if we look at the
reconstruction from the

01:11:40.700 --> 01:11:46.940
samples, g of k t of g, we get
this function s of t, which is

01:11:46.940 --> 01:11:51.180
the sum of all the samples
times sinc of t

01:11:51.180 --> 01:11:53.100
over t, minus k.

01:11:53.100 --> 01:11:55.980
That's for an arbitrary
filter.

01:11:55.980 --> 01:12:00.500
This baseband reconstruction
by definition, when we were

01:12:00.500 --> 01:12:04.360
talking about alising, is
just this function.

01:12:04.360 --> 01:12:07.880
We've said that g of t is ideal
Nyquist, if and only if

01:12:07.880 --> 01:12:12.180
s of t is equal to sinc
of t over capital t.

01:12:12.180 --> 01:12:14.190
Why is that?

01:12:14.190 --> 01:12:17.140
You look at this
function here.

01:12:17.140 --> 01:12:19.540
You look at s of 0.

01:12:19.540 --> 01:12:21.970
And what do you get?

01:12:21.970 --> 01:12:25.940
You get the sum of g of
k t times sinc of t

01:12:25.940 --> 01:12:28.500
over t minus k.

01:12:28.500 --> 01:12:32.190
If we have an ideal Nyquist
filter, then all of these

01:12:32.190 --> 01:12:36.250
terms are 0 except when
k is equal to 0.

01:12:43.120 --> 01:12:48.830
s of t, in general, if you have
a ideal Nyquist filter,

01:12:48.830 --> 01:12:50.760
you only have one
term in here.

01:12:50.760 --> 01:12:54.720
So s of t is just equal
to sinc of t over t.

01:12:54.720 --> 01:12:58.390
Because all those other
terms go away.

01:12:58.390 --> 01:13:01.950
If we take the Fourier transform
of s of t equals

01:13:01.950 --> 01:13:06.820
sinc t over capital T, the
Fourier transform is s s tilde

01:13:06.820 --> 01:13:10.340
of f is equal to t times
the rectangle

01:13:10.340 --> 01:13:13.070
function of f times t.

01:13:13.070 --> 01:13:19.070
A sinc function of a Fourier
transform is a rectangle.

01:13:19.070 --> 01:13:23.350
And the aliasing theorem then
says that this Fourier

01:13:23.350 --> 01:13:28.480
transform of this low pass
representation has to be equal

01:13:28.480 --> 01:13:32.330
to this sum of different
frequency terms.

01:13:32.330 --> 01:13:34.410
That's what aliasing says.

01:13:34.410 --> 01:13:38.750
It says that this baseband
representation is aliased into

01:13:38.750 --> 01:13:41.030
by all of these other
frequency bands.

01:13:41.030 --> 01:13:44.340
And each of them come in and
add to what you get in

01:13:44.340 --> 01:13:45.480
frequency here.

01:13:45.480 --> 01:13:48.740
Remember that diagram that we
drew where we took this

01:13:48.740 --> 01:13:55.080
arbitrary frequency function and
we picked up what was in

01:13:55.080 --> 01:13:58.240
each band, then we stuck it into
the center band and then

01:13:58.240 --> 01:14:01.010
we added all these things up?

01:14:01.010 --> 01:14:03.630
That's what the aliasing
thing says.

01:14:03.630 --> 01:14:08.380
So it says that g of t is ideal
Nyquist if and only if

01:14:08.380 --> 01:14:12.210
this sum is equal to that.

01:14:12.210 --> 01:14:15.220
And that's the Nyquist
criteria.

01:14:15.220 --> 01:14:16.470
That's what Nyquist did.

01:14:21.340 --> 01:14:25.190
But he did it long before
anybody had heard of aliasing.

01:14:38.260 --> 01:14:43.390
There's a slightly easier way to
look at aliasing criterion,

01:14:43.390 --> 01:14:46.810
which now becomes the
Nyquist criterion.

01:14:46.810 --> 01:14:53.100
When what you're interested in
is a waveform g of t, which is

01:14:53.100 --> 01:14:56.580
almost band-limited
but not quite.

01:14:56.580 --> 01:14:59.870
So what we're going to assume
is that it's limited to, at

01:14:59.870 --> 01:15:05.940
most, twice this -- this w here
is 1 over 2t, which is

01:15:05.940 --> 01:15:08.900
called the Nyquist bandwidth.

01:15:08.900 --> 01:15:12.860
Everything is called
Nyquist here, so.

01:15:12.860 --> 01:15:17.850
This value here is the minimum
bandwidth you could be using.

01:15:17.850 --> 01:15:19.810
It's 1 over capital 2t.

01:15:19.810 --> 01:15:22.620
It's what you would get if you
were using the sinc function.

01:15:22.620 --> 01:15:25.830
If you were using the sinc
function, what you would get

01:15:25.830 --> 01:15:28.420
is something which is
a rectangle here.

01:15:28.420 --> 01:15:30.730
Cut off, right at this point.

01:15:30.730 --> 01:15:34.260
And cut off right
at this point.

01:15:34.260 --> 01:15:41.440
Nyquist is saying, suppose it's
limited to at most 2w.

01:15:41.440 --> 01:15:44.650
In other words, suppose you
have a slopover into other

01:15:44.650 --> 01:15:50.280
frequencies but, at most into
the next frequency band and no

01:15:50.280 --> 01:15:51.870
more than that.

01:15:51.870 --> 01:15:57.330
Then, if you look at this thing,
which is spilling out,

01:15:57.330 --> 01:16:00.740
and we take the same picture we
were looking at before, we

01:16:00.740 --> 01:16:02.230
take this quantity.

01:16:02.230 --> 01:16:04.550
Bring it back down here.

01:16:04.550 --> 01:16:07.630
We take this quantity,
bring it up here.

01:16:07.630 --> 01:16:09.630
And what do we get?

01:16:09.630 --> 01:16:12.860
This, added into here.

01:16:12.860 --> 01:16:17.140
This thing adds up
right there.

01:16:22.210 --> 01:16:27.110
In other words -- well, let's
take this one here.

01:16:30.740 --> 01:16:35.160
This thing here gets translated
over to here.

01:16:38.850 --> 01:16:42.150
And added to this.

01:16:42.150 --> 01:16:53.840
This is assuming that g hat of
f is real, and we're ignoring

01:16:53.840 --> 01:16:55.260
the complex part of it.

01:16:55.260 --> 01:16:57.830
So this gets added to this.

01:16:57.830 --> 01:17:03.090
This is just enough to turn
this into something, which

01:17:03.090 --> 01:17:07.140
goes across here
and down here.

01:17:09.850 --> 01:17:10.520
Down here.

01:17:10.520 --> 01:17:12.730
My finger is not perfect.

01:17:12.730 --> 01:17:16.990
But, anyway, when you add this
to this, you get this ideal

01:17:16.990 --> 01:17:18.790
rectangular shape.

01:17:18.790 --> 01:17:22.130
What this is saying, in terms
of just this upper side band

01:17:22.130 --> 01:17:26.320
here, this is going to be the
same as this, from symmetry.

01:17:26.320 --> 01:17:29.940
So it's saying, if you take
what's on the positive side of

01:17:29.940 --> 01:17:35.790
w, and you rotate it around this
way, you rotate it around

01:17:35.790 --> 01:17:40.340
up here, if it's just enough to
fill that in, then you've

01:17:40.340 --> 01:17:43.410
satisfied the Nyquist
criteria.

01:17:43.410 --> 01:17:46.250
In other words, you want band
edge symmetry here.

01:17:46.250 --> 01:17:50.340
You want this to be symmetrical
to this.

01:17:50.340 --> 01:17:54.030
In that rotated 180
degree sense.

01:17:54.030 --> 01:17:56.820
So it says that anything which
has this band edged symmetry

01:17:56.820 --> 01:18:01.430
condition satisfies the property
of no intersymbol

01:18:01.430 --> 01:18:03.820
interference.

01:18:03.820 --> 01:18:06.220
So this makes the problem
easy for us.

01:18:06.220 --> 01:18:09.020
It says, we would like to have
a rectangular function.

01:18:09.020 --> 01:18:11.960
That has too much delay, because
in particular the

01:18:11.960 --> 01:18:15.240
inverse Fourier transform of
that, because we have a

01:18:15.240 --> 01:18:20.080
discontinuity, can drop off,
at most, as 1 over t.

01:18:20.080 --> 01:18:22.890
Which is pretty slow.

01:18:22.890 --> 01:18:26.710
We've gotten rid of
the discontinuity.

01:18:26.710 --> 01:18:29.980
We've also gotten rid of the
slope discontinuity, the way I

01:18:29.980 --> 01:18:31.910
drew the figure here.

01:18:31.910 --> 01:18:34.510
And, therefore, we can wind up
with a function which decays

01:18:34.510 --> 01:18:36.970
as 1 over t cubed.

01:18:36.970 --> 01:18:38.940
Which is a whole lot better
than 1 over t.

01:18:42.160 --> 01:18:47.930
And, excuse me for going
a little over, but.

01:18:51.710 --> 01:18:54.210
The things people build in
practice usually have

01:18:54.210 --> 01:18:57.500
something called a raised
cosine filter.

01:18:57.500 --> 01:19:00.920
Which is this messy expression
here, for

01:19:00.920 --> 01:19:02.690
the frequency response.

01:19:02.690 --> 01:19:04.780
But it really isn't that bad.

01:19:04.780 --> 01:19:08.120
In fact, it's just
what this is.

01:19:12.070 --> 01:19:18.090
This frequency response is t
over most of the frequency

01:19:18.090 --> 01:19:21.560
interval, up to 1 over 2t.

01:19:21.560 --> 01:19:27.150
It's t times a raised cosine
over this part of the

01:19:27.150 --> 01:19:29.550
frequency band here.

01:19:29.550 --> 01:19:33.420
t times cosine squared, and the
cosine squared just raises

01:19:33.420 --> 01:19:37.350
things up to be average
out at 1/2, and it's

01:19:37.350 --> 01:19:39.810
0 everywhere else.

01:19:39.810 --> 01:19:45.370
So what you're doing here in
that formula is simply making

01:19:45.370 --> 01:19:47.340
things t up to here.

01:19:50.080 --> 01:19:55.560
Making it a raised
cosine here.

01:19:55.560 --> 01:19:58.200
And making it 0 everywhere
else.

01:20:00.890 --> 01:20:06.410
And, depending on what do choose
as alpha, that makes

01:20:06.410 --> 01:20:08.830
this sharper or less sharp.

01:20:08.830 --> 01:20:11.520
And people usually choose it
to be about, alpha to be

01:20:11.520 --> 01:20:14.080
about 15% or so.

01:20:14.080 --> 01:20:18.000
Which means these filters chop
off very, very rapidly.

01:20:18.000 --> 01:20:19.960
Is that hard to build?

01:20:19.960 --> 01:20:22.420
Doesn't make any difference.

01:20:22.420 --> 01:20:25.370
I mean, these days, anything
which you can figure out how

01:20:25.370 --> 01:20:28.550
to compute, you can put it on
a little chip and it costs

01:20:28.550 --> 01:20:30.720
nothing if you make
enough of them.

01:20:30.720 --> 01:20:34.540
So you want to raise the cosine
filter which cuts off

01:20:34.540 --> 01:20:35.830
at 15%, fine.

01:20:35.830 --> 01:20:37.950
Somebody spends a year
designing it.

01:20:37.950 --> 01:20:42.400
And then you cookie-cut
it forever after.

01:20:42.400 --> 01:20:45.240
So it doesn't cost anything.

01:20:45.240 --> 01:20:51.340
And, well, g of t also has an
inverse transform which we

01:20:51.340 --> 01:20:52.590
won't worry about.

