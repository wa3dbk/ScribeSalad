WEBVTT
Kind: captions
Language: en

00:00:01.550 --> 00:00:03.920
The following content is
provided under a Creative

00:00:03.920 --> 00:00:05.310
Commons license.

00:00:05.310 --> 00:00:07.520
Your support will help
MIT OpenCourseWare

00:00:07.520 --> 00:00:11.610
continue to offer high quality
educational resources for free.

00:00:11.610 --> 00:00:14.180
To make a donation or to
view additional materials

00:00:14.180 --> 00:00:18.140
from hundreds of MIT courses,
visit MIT OpenCourseWare

00:00:18.140 --> 00:00:19.026
at ocw.mit.edu.

00:00:23.130 --> 00:00:26.790
GILBERT STRANG: So I've
got a list of things

00:00:26.790 --> 00:00:28.740
I'm hoping to do today.

00:00:28.740 --> 00:00:32.445
I'll begin with a few final
words about saddle points.

00:00:32.445 --> 00:00:35.100
The reason I'm interested
in saddle points

00:00:35.100 --> 00:00:42.780
is when we get to this
deep learning direction,

00:00:42.780 --> 00:00:45.900
you know that the
big step there is

00:00:45.900 --> 00:00:53.020
finding a minimum of the total
cost function and gradient

00:00:53.020 --> 00:00:55.350
descent, which we'll
certainly discuss

00:00:55.350 --> 00:01:00.520
as the usual method or
stochastic gradient descent.

00:01:00.520 --> 00:01:04.360
And all kinds of
issues arise, what

00:01:04.360 --> 00:01:11.860
happens if you have a saddle
point or a degenerate minimum?

00:01:11.860 --> 00:01:17.560
All these possibilities and the
understanding of deep learning

00:01:17.560 --> 00:01:20.890
is focusing more
and more on what

00:01:20.890 --> 00:01:24.700
does that the gradient
descent algorithm produce.

00:01:24.700 --> 00:01:31.180
So I just thought minima
and maxima we know about.

00:01:31.180 --> 00:01:34.135
Saddle points are kind
of a little hazier.

00:01:38.110 --> 00:01:41.300
So this is a perfect example.

00:01:41.300 --> 00:01:43.450
And I'll just say a few
more words about it.

00:01:43.450 --> 00:01:48.310
Then, I want to
talk about the Lab 3

00:01:48.310 --> 00:01:54.900
that I boldly posted
on the Stellar

00:01:54.900 --> 00:02:00.970
and also about projects, just
to get us thinking about those.

00:02:00.970 --> 00:02:09.900
And then my real math topic
for today and this week

00:02:09.900 --> 00:02:15.420
is basic ideas of
statistics, particularly

00:02:15.420 --> 00:02:17.190
the covariance matrix.

00:02:17.190 --> 00:02:19.470
I'm sure you've met
mean and variance.

00:02:19.470 --> 00:02:21.480
Those are the most used words.

00:02:21.480 --> 00:02:23.130
And we'll use them again.

00:02:23.130 --> 00:02:26.190
But then I want to
go on to covariance.

00:02:26.190 --> 00:02:28.050
So that's what's coming today--

00:02:28.050 --> 00:02:30.990
a few words on
saddle points, a lot

00:02:30.990 --> 00:02:36.210
of words about the
lab and anything

00:02:36.210 --> 00:02:43.080
you want to ask about projects,
and then some basic statistics.

00:02:43.080 --> 00:02:47.520
OK, saddle point so
the example I'm taking

00:02:47.520 --> 00:02:49.785
is this Rayleigh quotient.

00:02:52.310 --> 00:02:55.310
And I'm taking a
simple matrix S.

00:02:55.310 --> 00:02:59.180
I might as well take
a diagonal matrix.

00:02:59.180 --> 00:03:00.770
It's symmetric, of course.

00:03:00.770 --> 00:03:05.030
And any symmetric matrix I
could just change variables

00:03:05.030 --> 00:03:08.180
by a cube matrix,
an orthogonal matrix

00:03:08.180 --> 00:03:10.520
to get to something like that.

00:03:10.520 --> 00:03:13.850
And then the x, we're in 3D.

00:03:13.850 --> 00:03:17.900
So we got a sort of
manageable size here.

00:03:17.900 --> 00:03:22.930
And the x vector is uvw.

00:03:22.930 --> 00:03:27.110
So this is the quotient.

00:03:27.110 --> 00:03:30.440
x transpose x, you
see is just exactly

00:03:30.440 --> 00:03:35.780
5 u squareds, 3 v
squareds, and 1 w squared.

00:03:35.780 --> 00:03:39.800
And I divide by the length
to normalize things.

00:03:39.800 --> 00:03:42.050
So what are the
main facts that we

00:03:42.050 --> 00:03:44.660
know that I'm not
going to prove,

00:03:44.660 --> 00:03:46.880
but what are the main facts?

00:03:46.880 --> 00:03:50.630
What's the maximum value of R?

00:03:50.630 --> 00:03:56.030
What's the minimum value
of R, of that function?

00:03:56.030 --> 00:03:59.570
And is there a saddle point?

00:03:59.570 --> 00:04:05.950
So saddle of R.

00:04:05.950 --> 00:04:08.290
OK, what's the maximum value?

00:04:08.290 --> 00:04:11.380
How large could you make
that ratio, capital R?

00:04:11.380 --> 00:04:14.260
I just think, you
know, this isn't

00:04:14.260 --> 00:04:18.579
a standard topic in 18.06.

00:04:18.579 --> 00:04:22.880
But with an example like this,
you'll see the whole point.

00:04:22.880 --> 00:04:27.070
OK, so how large could I make R?

00:04:27.070 --> 00:04:28.880
Yeah, go ahead and say it.

00:04:28.880 --> 00:04:29.760
AUDIENCE: Sigma.

00:04:29.760 --> 00:04:30.760
GILBERT STRANG: Sigma 1.

00:04:30.760 --> 00:04:31.780
And what is it here?

00:04:31.780 --> 00:04:34.660
Let's just do with
these numbers.

00:04:34.660 --> 00:04:37.380
How big can I make that ratio R?

00:04:37.380 --> 00:04:41.730
And what choice of
uvw makes it big?

00:04:41.730 --> 00:04:43.970
So how big I can get it is?

00:04:43.970 --> 00:04:44.690
AUDIENCE: 5.

00:04:44.690 --> 00:04:45.850
GILBERT STRANG: 5.

00:04:45.850 --> 00:04:49.160
That ratio can't be more than 5.

00:04:49.160 --> 00:04:51.140
You see it would be 5.

00:04:51.140 --> 00:04:52.550
Well, how do I get to 5?

00:04:52.550 --> 00:04:55.190
Maximum of R is 5.

00:04:55.190 --> 00:04:58.970
And what is the uvw that--

00:04:58.970 --> 00:05:01.010
so I'll say at--

00:05:01.010 --> 00:05:05.180
what choice of uvw
would give us 5 here?

00:05:05.180 --> 00:05:10.220
You see it immediately, 1, 0, 0.

00:05:10.220 --> 00:05:12.410
And what about the minimum of R?

00:05:12.410 --> 00:05:16.550
The minimum of this ratio, how
do I make that ratio small?

00:05:16.550 --> 00:05:20.600
Well, I put I load stuff on
the w instead of loading it up

00:05:20.600 --> 00:05:22.170
on to u.

00:05:22.170 --> 00:05:24.260
It's just clear.

00:05:24.260 --> 00:05:26.420
So what is the
minimum value of R?

00:05:26.420 --> 00:05:27.880
AUDIENCE: 1.

00:05:27.880 --> 00:05:32.220
GILBERT STRANG: 1, because
I'll load everything into w.

00:05:32.220 --> 00:05:34.260
So the minimum value will be 1.

00:05:34.260 --> 00:05:37.530
And that will be at
the vector 0, 0, 1.

00:05:37.530 --> 00:05:39.810
I've loaded everything there.

00:05:39.810 --> 00:05:43.500
And then the point of
this short discussion

00:05:43.500 --> 00:05:48.450
is, is there another place
where the derivatives,

00:05:48.450 --> 00:05:51.690
first derivatives,
of R are all zero?

00:05:51.690 --> 00:05:55.360
That's where, of course,
the first derivatives

00:05:55.360 --> 00:06:00.510
are 0 at the max, at the min.

00:06:00.510 --> 00:06:04.260
But we also have
three variables here.

00:06:04.260 --> 00:06:07.380
And we're going to
find a third point.

00:06:07.380 --> 00:06:08.790
And what is that point?

00:06:08.790 --> 00:06:11.100
You probably guess.

00:06:11.100 --> 00:06:13.780
And what will be
the saddle value?

00:06:13.780 --> 00:06:18.490
So you have to see some
kind of a surface that--

00:06:18.490 --> 00:06:19.810
I guess, what are we in?

00:06:19.810 --> 00:06:21.250
4D.

00:06:21.250 --> 00:06:28.060
So we have base
coordinates, uvw.

00:06:28.060 --> 00:06:31.360
And R goes vertically.

00:06:31.360 --> 00:06:33.880
And we plot that surface.

00:06:33.880 --> 00:06:39.340
And we don't really understand
it unless we think a lot about

00:06:39.340 --> 00:06:40.570
it, which we haven't.

00:06:40.570 --> 00:06:43.810
But we can pretty well
guess what's what.

00:06:43.810 --> 00:06:46.720
And so what do you think
is the saddle value?

00:06:46.720 --> 00:06:50.800
And where is it
going to be reached?

00:06:50.800 --> 00:06:52.960
Everybody is going
to tell me correctly.

00:06:52.960 --> 00:06:54.970
Saddle value would be?

00:06:54.970 --> 00:06:57.400
Three at this middle point.

00:07:03.460 --> 00:07:07.920
And what are these three
with respect to the matrix?

00:07:07.920 --> 00:07:10.120
They're its eigenvectors.

00:07:10.120 --> 00:07:13.710
What are these three numbers
with respect to the matrix?

00:07:13.710 --> 00:07:16.090
There its eigenvalues.

00:07:16.090 --> 00:07:18.820
That's why that
Rayleigh quotient

00:07:18.820 --> 00:07:20.830
is such an important function.

00:07:20.830 --> 00:07:23.020
It's kind of a messy function.

00:07:23.020 --> 00:07:24.940
If you take its
derivative, you've

00:07:24.940 --> 00:07:31.710
got to use the quotient rule
or use Lagrange multiplier.

00:07:31.710 --> 00:07:34.600
That's the way to make
it more manageable.

00:07:34.600 --> 00:07:35.770
But it's kind of messy.

00:07:35.770 --> 00:07:38.320
But the results
could not be better.

00:07:38.320 --> 00:07:43.300
The values there
are the eigenvalues.

00:07:43.300 --> 00:07:47.110
And the places where you reach
them are the eigenvectors.

00:07:47.110 --> 00:07:50.530
And so the max is
the most important.

00:07:50.530 --> 00:07:54.160
So that's sigma 1.

00:07:54.160 --> 00:07:58.630
Here's sigma 3-- or
lambda and sigma,

00:07:58.630 --> 00:08:01.780
because the matrix is
symmetric positive definite.

00:08:01.780 --> 00:08:04.910
And here in the
middle is sigma 2.

00:08:04.910 --> 00:08:10.750
And if we want to
compute eigenvectors,

00:08:10.750 --> 00:08:13.060
which I'm not
planning to do today,

00:08:13.060 --> 00:08:15.040
just to make this remark--

00:08:15.040 --> 00:08:20.350
computing eigenvectors, getting
the largest one or the smallest

00:08:20.350 --> 00:08:25.810
one is a lot quicker in
general than getting these ones

00:08:25.810 --> 00:08:27.940
in the middle.

00:08:27.940 --> 00:08:36.460
You have to use good
codes and pay attention

00:08:36.460 --> 00:08:39.490
to computing those
saddle point values.

00:08:39.490 --> 00:08:44.620
So is there anything nice I
can do with saddle points?

00:08:44.620 --> 00:08:46.840
How does one think
about saddle points?

00:08:46.840 --> 00:08:49.390
So again, saddle
point is defined

00:08:49.390 --> 00:08:56.820
by first derivatives equals 0.

00:08:56.820 --> 00:09:00.230
That's-- and the
second derivatives--

00:09:03.890 --> 00:09:06.480
OK, so that's a matrix.

00:09:06.480 --> 00:09:09.420
Here's a vector,
the gradient vector.

00:09:09.420 --> 00:09:11.060
The derivative
with respect to u,

00:09:11.060 --> 00:09:15.620
the derivative with respect
to v, NDR DW, just a vector.

00:09:15.620 --> 00:09:19.160
And all those
components are zero.

00:09:19.160 --> 00:09:21.380
The gradient vector is zero.

00:09:21.380 --> 00:09:24.170
But what about
second derivatives?

00:09:24.170 --> 00:09:25.640
Well, that's getting more.

00:09:25.640 --> 00:09:28.460
There are nine of
those now, because I've

00:09:28.460 --> 00:09:36.440
got Ruu, second derivative
with respect to u.

00:09:36.440 --> 00:09:37.850
But I've got mixed derivative.

00:09:37.850 --> 00:09:42.780
Second derivative of R
with respect to Du Dv.

00:09:42.780 --> 00:09:45.770
So I have a 3 by 3 matrix.

00:09:45.770 --> 00:09:49.410
Fortunately, that
matrix is symmetric,

00:09:49.410 --> 00:09:53.100
because we're blessed
by that wonderful fact

00:09:53.100 --> 00:09:56.550
that the derivative
with respect to u

00:09:56.550 --> 00:09:59.770
and then v is the
same as v and then u.

00:09:59.770 --> 00:10:01.290
So we get a symmetric matrix.

00:10:07.950 --> 00:10:18.360
Well, I won't write
it down, but it's

00:10:18.360 --> 00:10:24.180
got the maximum, minimum, and
saddle information built in.

00:10:24.180 --> 00:10:27.210
Here's this one
additional thought

00:10:27.210 --> 00:10:32.860
that I want to communicate
about saddle points,

00:10:32.860 --> 00:10:35.680
because it's really
nice to somehow get back

00:10:35.680 --> 00:10:38.540
to maxima and minima.

00:10:38.540 --> 00:10:43.390
So this idea for
a saddle point is

00:10:43.390 --> 00:10:46.975
to be able to write it as
the maximum of a minimum.

00:10:51.570 --> 00:10:55.410
So let me do that,
and then I'm all done.

00:10:55.410 --> 00:10:59.370
So I'm going to say that
lambda 2, that value,

00:10:59.370 --> 00:11:05.340
is the maximum of over something
of the minimum over something

00:11:05.340 --> 00:11:09.300
of x trans of our function.

00:11:13.470 --> 00:11:17.610
Now, of course, I have to tell
you what you're maximizing over

00:11:17.610 --> 00:11:19.200
and what you're minimizing over.

00:11:19.200 --> 00:11:21.990
But that's the idea
is that one way

00:11:21.990 --> 00:11:25.680
to get into the middle
place there where

00:11:25.680 --> 00:11:31.140
the saddles are sitting is to
have a maximum of a minimum.

00:11:31.140 --> 00:11:35.700
And that leads-- that's what
I'm about to complete here--

00:11:35.700 --> 00:11:38.490
would lead you, for
example, very quickly

00:11:38.490 --> 00:11:45.660
to the interlacing theorem that
I spoke about for eigenvalues

00:11:45.660 --> 00:11:51.690
and for singular values
of when you perturb S

00:11:51.690 --> 00:11:55.320
or when you throw away
a row and column of S.

00:11:55.320 --> 00:11:58.170
The eigenvalues go in between.

00:11:58.170 --> 00:12:03.240
That is the kind of
conclusion that this max min

00:12:03.240 --> 00:12:05.460
stuff is set up to produce.

00:12:05.460 --> 00:12:08.610
So here, let me just tell
you what it would be.

00:12:08.610 --> 00:12:10.650
I'm aiming to get lambda 2.

00:12:13.250 --> 00:12:20.353
So I'm going to take a maximum
over two dimensional spaces--

00:12:27.200 --> 00:12:28.790
subspaces of R3.

00:12:28.790 --> 00:12:31.690
We're in 3D.

00:12:31.690 --> 00:12:37.360
So you can see that's sort
of like 2-dimensional spaces.

00:12:37.360 --> 00:12:44.980
Let me give that subspace
a name like V. That'll do.

00:12:44.980 --> 00:12:48.800
Cap V, everybody can
see that that's a cap V.

00:12:48.800 --> 00:12:53.680
And then this will be
the minimum over V.

00:12:53.680 --> 00:12:58.760
So it's is kind of tricky.

00:12:58.760 --> 00:13:03.700
So I take any subspace
that's two dimensional.

00:13:03.700 --> 00:13:06.660
And I'll take one in a moment.

00:13:06.660 --> 00:13:08.430
And I'll figure out the minimum.

00:13:08.430 --> 00:13:14.820
Well, suppose I take
this subspace V,

00:13:14.820 --> 00:13:18.420
which is spanned
by the first two--

00:13:18.420 --> 00:13:20.760
it's supposed to
be a 2D subspace--

00:13:20.760 --> 00:13:23.280
spanned by the first.

00:13:23.280 --> 00:13:24.945
Suppose I try example.

00:13:28.980 --> 00:13:39.370
The span of 1, 0,
0, and 0, 1, 0.

00:13:39.370 --> 00:13:47.800
In other words, all
vectors u, v, 0.

00:13:47.800 --> 00:13:50.860
That's a 2D space.

00:13:50.860 --> 00:13:57.460
What is the minimum of
that Rayleigh quotient

00:13:57.460 --> 00:14:00.310
over that two-dimensional space?

00:14:00.310 --> 00:14:01.730
So now I'm taking a minimum.

00:14:01.730 --> 00:14:03.490
I don't have to think
about saddle points.

00:14:06.640 --> 00:14:09.910
So I'm looking at the
thing, but w is zero now.

00:14:09.910 --> 00:14:14.260
Everybody sees that I've
squeezed it down to 2D.

00:14:14.260 --> 00:14:15.760
So w is zero.

00:14:15.760 --> 00:14:18.540
So what is the minimum now?

00:14:18.540 --> 00:14:22.090
So this thing would become--

00:14:22.090 --> 00:14:26.640
for this space-- would
become the 5u squared

00:14:26.640 --> 00:14:31.440
and the 3v squared over the
u squared plus the v squared,

00:14:31.440 --> 00:14:33.750
but the W is 0.

00:14:33.750 --> 00:14:35.550
So what's the minimum of that?

00:14:35.550 --> 00:14:36.050
AUDIENCE: 3.

00:14:36.050 --> 00:14:36.920
GILBERT STRANG: 3.

00:14:36.920 --> 00:14:38.190
3.

00:14:38.190 --> 00:14:42.770
OK, the minimum is 3 for
this particular space.

00:14:42.770 --> 00:14:47.040
Let me call it V special.

00:14:47.040 --> 00:14:52.080
For that particular space,
the minimum is 3, correct?

00:14:52.080 --> 00:14:53.570
Everybody sees that.

00:14:53.570 --> 00:14:57.170
Because I just have u and v
to play with, the 5 and the 3.

00:14:57.170 --> 00:15:00.690
So if I put everything
into v, I get to 3.

00:15:00.690 --> 00:15:02.605
And now, I take the maximum.

00:15:06.870 --> 00:15:12.300
So the maximum is at least 3,
because this particular choice

00:15:12.300 --> 00:15:15.750
of V gave me the answer 3.

00:15:15.750 --> 00:15:20.920
And now, I'm taking the maximum
over all possible 2D spaces.

00:15:20.920 --> 00:15:26.460
And I got 3 for one of
the possible spaces of V.

00:15:26.460 --> 00:15:29.880
And I might get higher
than 3 for some other one.

00:15:29.880 --> 00:15:32.160
But actually I don't.

00:15:32.160 --> 00:15:35.660
The truth is that
this turns out to be

00:15:35.660 --> 00:15:39.520
3 which is, of course,
exactly what we wanted.

00:15:39.520 --> 00:15:43.350
So I'm saying that this
particular two-dimensional

00:15:43.350 --> 00:15:49.260
space, the minimum of
over that, minimum,

00:15:49.260 --> 00:15:52.200
the minimum there is 3.

00:15:52.200 --> 00:15:55.080
And now, I maximize
over all others.

00:15:55.080 --> 00:16:00.820
And so the idea is
that for any other one,

00:16:00.820 --> 00:16:04.320
the minimum value
will be below 3.

00:16:04.320 --> 00:16:09.695
And, therefore, when I go for
the max of the mins, I get 3.

00:16:09.695 --> 00:16:12.330
So I just repeat that
and then be quiet

00:16:12.330 --> 00:16:14.910
about this whole subject.

00:16:14.910 --> 00:16:20.340
So it's a maximum over subspaces
of a minimum of the Rayleigh

00:16:20.340 --> 00:16:21.540
quotient.

00:16:21.540 --> 00:16:26.130
If that subspace is exactly
the perfect choice, this one,

00:16:26.130 --> 00:16:28.410
I get the value 3.

00:16:28.410 --> 00:16:31.640
And I'm claiming that's the
biggest value I can get,

00:16:31.640 --> 00:16:33.840
because if I pick
any other subspace--

00:16:33.840 --> 00:16:37.200
what if I picked
a subspace that--

00:16:37.200 --> 00:16:46.200
suppose another v would
be all vectors 0, v, w.

00:16:49.940 --> 00:16:56.130
What would I get for the
minimum of this thing?

00:16:56.130 --> 00:17:00.740
But now w is in the picture
and u is not in the picture.

00:17:00.740 --> 00:17:02.660
What I get for
the minimum there?

00:17:02.660 --> 00:17:03.160
AUDIENCE: 1.

00:17:03.160 --> 00:17:03.910
GILBERT STRANG: 1.

00:17:03.910 --> 00:17:05.089
I'd get 1.

00:17:05.089 --> 00:17:08.010
The minimum would be when
I put everything into w

00:17:08.010 --> 00:17:09.220
and I got one.

00:17:09.220 --> 00:17:12.130
And then when I take the
max, it's not a winner.

00:17:12.130 --> 00:17:13.750
It's thrown out.

00:17:13.750 --> 00:17:19.040
The winner will be the
that space and the 3.

00:17:19.040 --> 00:17:22.390
So I guess I'm hoping
that you sort of see

00:17:22.390 --> 00:17:28.540
in this small example that you
can express this middle saddle

00:17:28.540 --> 00:17:31.990
value as--

00:17:31.990 --> 00:17:35.200
it's reasonable to think of it
as a maximum in some directions

00:17:35.200 --> 00:17:36.880
and a minimum in another.

00:17:36.880 --> 00:17:42.380
Think of the-- well, try to
think of some surface, which

00:17:42.380 --> 00:17:45.370
is going up in some direction.

00:17:45.370 --> 00:17:47.960
So it's a minimum
in those directions.

00:17:47.960 --> 00:17:50.360
And it's going down
in other directions.

00:17:50.360 --> 00:17:52.940
So it's a max in
those directions.

00:17:52.940 --> 00:17:59.960
And saddle points is perched
in there right at that place,

00:17:59.960 --> 00:18:02.150
at the saddle point.

00:18:02.150 --> 00:18:04.220
You know, if you're
like hiking from here

00:18:04.220 --> 00:18:07.210
to California or
something, you're

00:18:07.210 --> 00:18:09.290
going to pass a saddle point.

00:18:09.290 --> 00:18:11.290
Actually, you see
it on the Mass--

00:18:11.290 --> 00:18:16.970
the Mass Pike has an
amazing little sign.

00:18:16.970 --> 00:18:19.160
I don't know if
you've noticed it.

00:18:19.160 --> 00:18:23.600
If you drive west on the
Mass Pike, pretty far west

00:18:23.600 --> 00:18:27.710
of Boston, there's a little
sign telling you the altitude

00:18:27.710 --> 00:18:29.490
or elevation, whatever.

00:18:29.490 --> 00:18:33.830
And it says there this is
the highest point until you

00:18:33.830 --> 00:18:36.380
reach the Rockies basically.

00:18:36.380 --> 00:18:40.930
I'd say like, OK, Midwest
is pretty flat, right?

00:18:44.980 --> 00:18:46.280
Because that's a long way away.

00:18:46.280 --> 00:18:49.220
You don't think of
Massachusetts as like really

00:18:49.220 --> 00:18:52.130
in the big league
with high spots.

00:18:52.130 --> 00:18:53.570
But there it is.

00:18:53.570 --> 00:18:55.650
It's the highest
one until you get--

00:18:55.650 --> 00:18:58.280
and I think it tells you
where the next one will

00:18:58.280 --> 00:18:59.570
be in Colorado.

00:18:59.570 --> 00:19:06.130
Anyway, those highest
points tend to be saddles.

00:19:06.130 --> 00:19:08.570
The very, very highest point--

00:19:08.570 --> 00:19:11.770
where's that in
Alaska or somewhere--

00:19:11.770 --> 00:19:15.020
that's a max, of
course, by definition.

00:19:15.020 --> 00:19:21.370
But there are a lot of saddle
points in other places.

00:19:21.370 --> 00:19:25.700
And those would be maxima of
minima or minima of maxima.

00:19:25.700 --> 00:19:26.200
Good.

00:19:26.200 --> 00:19:29.250
I'm stopping there.

00:19:29.250 --> 00:19:34.860
We might see this again when
we start gradient descent.

00:19:34.860 --> 00:19:38.250
But at least,
because saddle points

00:19:38.250 --> 00:19:41.190
don't come up much
in teaching calculus,

00:19:41.190 --> 00:19:42.940
I thought that was good.

00:19:42.940 --> 00:19:52.170
OK, second point
is models, Lab 3,

00:19:52.170 --> 00:19:56.910
and projects, anything you'd
like to ask about projects.

00:19:56.910 --> 00:20:00.360
So, please, this is
your chance to ask.

00:20:00.360 --> 00:20:04.020
You could also ask by email.

00:20:04.020 --> 00:20:08.110
If you have suggested
or idea for our project,

00:20:08.110 --> 00:20:15.880
let me encourage you or a team
to work on it or just yourself.

00:20:15.880 --> 00:20:18.120
And if you want to
think, OK, shall I

00:20:18.120 --> 00:20:21.900
get some feedback of
does this sound sensible?

00:20:21.900 --> 00:20:23.910
Any suggestions?

00:20:23.910 --> 00:20:26.000
Send me an email.

00:20:26.000 --> 00:20:27.890
I'd be happy to--

00:20:27.890 --> 00:20:32.560
of course, I'm a total
beginner here, too.

00:20:32.560 --> 00:20:40.300
When I created this Lab
3, I was like desperate,

00:20:40.300 --> 00:20:42.730
not for model 1.

00:20:42.730 --> 00:20:47.690
For model 1, have
you looked at--

00:20:47.690 --> 00:20:49.876
it's reached Stellar.

00:20:49.876 --> 00:20:52.420
And it's only one printed page.

00:20:52.420 --> 00:20:55.430
Have people had a look at this?

00:20:55.430 --> 00:20:57.910
So I'll just repeat quickly.

00:20:57.910 --> 00:21:05.590
Model 1 is an example
where of overfitting.

00:21:05.590 --> 00:21:08.440
And what's going
on with model 1?

00:21:08.440 --> 00:21:10.030
So model 1 says take--

00:21:14.490 --> 00:21:18.420
5 would be enough, but I
probably said 10 or something.

00:21:18.420 --> 00:21:21.450
So I'll make it six points.

00:21:21.450 --> 00:21:29.330
And put a curve through them.

00:21:29.330 --> 00:21:31.780
So if you put a curve--

00:21:31.780 --> 00:21:34.940
and the curve is going
to be a polynomial.

00:21:34.940 --> 00:21:41.750
So we're going to set
a fit by polynomial.

00:21:41.750 --> 00:21:49.055
Everybody knows polynomial is
C0 plus C1x plus whatever CK

00:21:49.055 --> 00:21:53.780
x to the K, let's say.

00:21:56.480 --> 00:22:00.470
For K equals 0--

00:22:00.470 --> 00:22:02.780
well, I don't know
if I even ask 0.

00:22:02.780 --> 00:22:04.640
That would be the
best straight line.

00:22:04.640 --> 00:22:07.190
That would run
along the average.

00:22:07.190 --> 00:22:11.300
K equal to 1, that would
be a straight line fit.

00:22:11.300 --> 00:22:14.900
And you would compute
that by least squares,

00:22:14.900 --> 00:22:17.980
because of course no
straight line is going

00:22:17.980 --> 00:22:19.820
to go through all the points.

00:22:19.820 --> 00:22:23.120
You're going to have some
error by least squares.

00:22:23.120 --> 00:22:27.990
2 would be fitting
by a parabola.

00:22:27.990 --> 00:22:31.710
Again, you'll have some error,
but smaller since parabolas

00:22:31.710 --> 00:22:33.520
include straight lines.

00:22:33.520 --> 00:22:38.820
So you can only reduce the
total sum of squares error

00:22:38.820 --> 00:22:40.800
by going to a degree two.

00:22:40.800 --> 00:22:43.760
Degree 3 and up to--

00:22:43.760 --> 00:22:45.480
how many points shall we take?

00:22:45.480 --> 00:22:49.140
1, 2-- let me just use the
same letter I've used here.

00:22:54.240 --> 00:22:57.270
Well, m is the number
of points, but m

00:22:57.270 --> 00:23:07.230
varies between 1 point I
guess and probably n point.

00:23:07.230 --> 00:23:09.450
Maybe K here.

00:23:09.450 --> 00:23:16.680
Up to n-- up to 6, let's say.

00:23:19.400 --> 00:23:24.680
And I want to make
a comment about 6.

00:23:24.680 --> 00:23:27.140
No, 5 would do it.

00:23:27.140 --> 00:23:33.260
Degree 5 will fit the 6 points
that we've got 6 points here.

00:23:33.260 --> 00:23:36.860
But if I stop at degree
5, I was better there,

00:23:36.860 --> 00:23:40.640
because degree 5 polynomial
also has a constant term.

00:23:40.640 --> 00:23:43.070
So it really has
six coefficients.

00:23:43.070 --> 00:23:47.390
So there is a 1
degree 5 polynomial

00:23:47.390 --> 00:23:50.360
with six numbers, six
coefficients, that

00:23:50.360 --> 00:23:53.040
goes through those six points.

00:23:53.040 --> 00:23:55.760
And so it's a perfect fit.

00:23:55.760 --> 00:23:58.650
That would be an
exact fit of the data.

00:23:58.650 --> 00:24:00.500
So here's the data.

00:24:00.500 --> 00:24:05.150
Create a polynomial of degree 5
that goes through those points

00:24:05.150 --> 00:24:06.140
exactly.

00:24:06.140 --> 00:24:09.350
And look at the result.
And what would you

00:24:09.350 --> 00:24:12.210
see if you look at the result?

00:24:12.210 --> 00:24:13.710
Would it be smooth?

00:24:13.710 --> 00:24:16.380
Of course, it's a polynomial.

00:24:16.380 --> 00:24:18.940
Would it be nice?

00:24:18.940 --> 00:24:21.290
No, it will be horrible.

00:24:21.290 --> 00:24:24.460
To get through those points--

00:24:24.460 --> 00:24:26.090
did I get six points?

00:24:26.090 --> 00:24:26.830
Yeah.

00:24:26.830 --> 00:24:30.250
To get through those
points, I'm guessing

00:24:30.250 --> 00:24:33.760
that that fifth degree
polynomial, the perfect fit,

00:24:33.760 --> 00:24:36.400
is an example that
occurs to practically

00:24:36.400 --> 00:24:40.900
everybody of
overfitting, because

00:24:40.900 --> 00:24:45.100
making that decision,
perfect fit, learn the data,

00:24:45.100 --> 00:24:49.960
training data exactly
will send a polynomial--

00:24:49.960 --> 00:24:52.510
I don't know what it looks like.

00:24:52.510 --> 00:24:54.430
I don't want to-- well,
I do want to know,

00:24:54.430 --> 00:24:55.720
but not right now.

00:24:55.720 --> 00:24:59.870
Anyway, craziness.

00:24:59.870 --> 00:25:03.500
And, of course,
I'm going to ask--

00:25:03.500 --> 00:25:06.290
it doesn't look
like that probably--

00:25:06.290 --> 00:25:09.380
I'm going to ask you
to plot the results.

00:25:09.380 --> 00:25:12.080
Well, what's the least
squares error when

00:25:12.080 --> 00:25:14.060
you fit by a straight line?

00:25:14.060 --> 00:25:17.990
When you fit by horizontal
line, a constant,

00:25:17.990 --> 00:25:20.590
fit by a straight line,
move up to parabolas,

00:25:20.590 --> 00:25:22.010
move up to a cubics?

00:25:22.010 --> 00:25:27.200
But when you hit this, you're
not making any error at all.

00:25:27.200 --> 00:25:30.470
You're not really needing
to use least squares.

00:25:30.470 --> 00:25:36.600
You can solve Ax
equals B. Ac equal b.

00:25:36.600 --> 00:25:40.040
So this is the b thing.

00:25:40.040 --> 00:25:46.070
And c is the vector
of coefficients.

00:25:46.070 --> 00:25:53.850
And the matrix A is bad
news when it's a 6 by 6,

00:25:53.850 --> 00:25:55.970
when you get up
to a complete fit.

00:25:55.970 --> 00:26:00.080
And I guess what I
wanted just to see is--

00:26:00.080 --> 00:26:05.120
a lot of things I don't know,
like suppose I change six to 20

00:26:05.120 --> 00:26:06.740
or something.

00:26:06.740 --> 00:26:11.660
Then I'm pretty sure that
out there at 18, 19, 20,

00:26:11.660 --> 00:26:14.000
this thing is
really off the map.

00:26:14.000 --> 00:26:20.990
And you could compute
its max and you'd

00:26:20.990 --> 00:26:24.020
see a very big number.

00:26:24.020 --> 00:26:25.670
But, of course, for
a straight line,

00:26:25.670 --> 00:26:28.690
that would be pretty safe.

00:26:28.690 --> 00:26:32.820
The slope would be
pretty moderate.

00:26:32.820 --> 00:26:36.100
And I don't know where you--

00:26:36.100 --> 00:26:38.970
so that's probably
underfitting to try

00:26:38.970 --> 00:26:41.310
to fit this by a straight line.

00:26:41.310 --> 00:26:44.250
It's not as close
as you would want.

00:26:44.250 --> 00:26:49.080
But fitting by a full perfect
filling, a high degree

00:26:49.080 --> 00:26:51.530
polynomial, is
certainly overfitting.

00:26:51.530 --> 00:26:52.990
Where is the boundary?

00:26:52.990 --> 00:26:55.920
I'm sure people know
about this, but I

00:26:55.920 --> 00:27:00.720
think it is something
we could learn from.

00:27:00.720 --> 00:27:04.560
So that's what that
model 1 is about.

00:27:04.560 --> 00:27:06.960
And just to make
one final comment,

00:27:06.960 --> 00:27:10.440
that matrix A has
a name in the case

00:27:10.440 --> 00:27:16.980
where it's a square matrix,
where you're fitting exactly--

00:27:16.980 --> 00:27:19.200
interpolating would be the word.

00:27:19.200 --> 00:27:28.010
So that exact fit,
that corresponds

00:27:28.010 --> 00:27:36.600
to square matrix A. And the
word for it is interpolation.

00:27:41.960 --> 00:27:47.050
And I guess it's Lagrange again.

00:27:47.050 --> 00:27:50.280
Seeing that guy too often here.

00:27:50.280 --> 00:27:52.220
So it would be Lagrange
interpretation.

00:27:52.220 --> 00:27:55.940
But the matrix has
a different name.

00:27:55.940 --> 00:27:58.775
And whose name is
associated with that matrix?

00:27:58.775 --> 00:27:59.692
AUDIENCE: Vandermonde.

00:27:59.692 --> 00:28:01.300
GILBERT STRANG: Vandermonde.

00:28:01.300 --> 00:28:02.670
Vandermonde.

00:28:02.670 --> 00:28:06.720
So this is the square
matrix, which was--

00:28:06.720 --> 00:28:08.090
so let me write it.

00:28:08.090 --> 00:28:09.900
It's called a
Vandermonde matrix.

00:28:18.600 --> 00:28:23.130
And it's a matrix that
has a crazy large inverse,

00:28:23.130 --> 00:28:27.780
because just as I'm saying,
the C that comes out

00:28:27.780 --> 00:28:30.660
from the perfect fit,
from the interpolation,

00:28:30.660 --> 00:28:35.280
from the square matrix,
the C is going to be giant.

00:28:35.280 --> 00:28:44.910
And so you will construct a
matrix, of course, to do this,

00:28:44.910 --> 00:28:48.900
and it will be
identical to the--

00:28:48.900 --> 00:28:50.780
so we've heard this
word Vandermonde

00:28:50.780 --> 00:28:54.380
matrix in this class
within the last week.

00:28:54.380 --> 00:29:00.590
Anybody remember where the word
Vandermonde came up in class?

00:29:00.590 --> 00:29:03.840
It was in professor
Townsend's lecture.

00:29:03.840 --> 00:29:05.570
So you could go
back to that video

00:29:05.570 --> 00:29:10.940
if you wanted as an
example of a matrix which

00:29:10.940 --> 00:29:15.350
had a horrible inverse,
a giant matrix.

00:29:15.350 --> 00:29:17.720
The Hilbert matrix
was another example.

00:29:17.720 --> 00:29:21.480
I think he did two examples,
Vandermonde and Hilbert.

00:29:21.480 --> 00:29:23.570
So this Vandermonde matrix--

00:29:23.570 --> 00:29:28.170
I could write it down, but
I'll leave that to you--

00:29:28.170 --> 00:29:30.410
has a big inverse.

00:29:30.410 --> 00:29:34.910
And its eigenvalues--
well, no singular values,

00:29:34.910 --> 00:29:37.330
because it's not symmetric--

00:29:37.330 --> 00:29:41.450
it's singular values
are way scattered.

00:29:41.450 --> 00:29:44.060
It has tiny little
singular values

00:29:44.060 --> 00:29:47.460
plus an ordinary
sized singular values.

00:29:47.460 --> 00:29:52.220
So that's the example that I
just think you could go with.

00:29:52.220 --> 00:29:58.180
And as far as I can see, sending
it to auto grader as a Julia

00:29:58.180 --> 00:30:02.870
file, it would be
even worse than usual,

00:30:02.870 --> 00:30:05.750
sending it to auto grader.

00:30:05.750 --> 00:30:08.870
I think it wouldn't know what
to do as far as I can see.

00:30:08.870 --> 00:30:13.520
So I'm thinking of submissions
coming to Gradescope.

00:30:13.520 --> 00:30:18.200
And I'm thinking of
some plots to show

00:30:18.200 --> 00:30:26.330
what happens as K increases
and some tables of data maybe

00:30:26.330 --> 00:30:33.800
and then maybe a paragraph of
conclusion, like what degree

00:30:33.800 --> 00:30:36.530
is safe and when
does it become risky

00:30:36.530 --> 00:30:39.160
and when does it
become disaster.

00:30:39.160 --> 00:30:42.230
So stuff like that.

00:30:42.230 --> 00:30:47.350
Really, these are sort
of open-ended labs

00:30:47.350 --> 00:30:49.970
and you use any language.

00:30:49.970 --> 00:30:54.560
Questions about that example?

00:30:54.560 --> 00:31:00.070
Which is really that's what
I'm expecting to be ready

00:31:00.070 --> 00:31:09.260
and quite a good example for
Wednesday after the break.

00:31:12.500 --> 00:31:13.970
Question?

00:31:13.970 --> 00:31:17.030
Anyway, you can email me.

00:31:17.030 --> 00:31:21.830
You probably see that see
what the model looks like.

00:31:21.830 --> 00:31:30.470
Then the second one I've taken
that first jump into, networks.

00:31:30.470 --> 00:31:35.210
Made a very simple network
without any hidden layers

00:31:35.210 --> 00:31:36.770
at all actually.

00:31:36.770 --> 00:31:41.240
And just wrote down
what I think might work.

00:31:41.240 --> 00:31:46.240
But you may find that you
want to modify model 2.

00:31:46.240 --> 00:31:48.310
Go for it.

00:31:48.310 --> 00:31:56.500
I don't have any patent or
personal stake in the way model

00:31:56.500 --> 00:31:57.410
2 is written.

00:31:57.410 --> 00:32:01.850
But the idea is fit data--

00:32:01.850 --> 00:32:08.030
well, start with data, but
don't make it too perfect,

00:32:08.030 --> 00:32:14.830
because we want some
learning to happen here.

00:32:14.830 --> 00:32:17.080
So it's the
classification problem.

00:32:17.080 --> 00:32:24.400
So it won't be least squares
with variables like u and v

00:32:24.400 --> 00:32:25.150
and w.

00:32:25.150 --> 00:32:29.560
It's just plus 1 or
minus 1, or 1, 0,

00:32:29.560 --> 00:32:32.860
or cat and dog, whatever
that classification is.

00:32:32.860 --> 00:32:38.800
So that's the basic problem to
start with in deep learning.

00:32:38.800 --> 00:32:42.220
For quite a long time,
that's the natural problem.

00:32:42.220 --> 00:32:45.860
So it's a
classification problem.

00:32:45.860 --> 00:32:50.320
And the description
here suggests one way

00:32:50.320 --> 00:33:01.900
to set up that training data
and execute a neural net

00:33:01.900 --> 00:33:08.140
like experiment, but without
getting very far away

00:33:08.140 --> 00:33:11.140
from ordinary linear algebra.

00:33:11.140 --> 00:33:18.130
So as I say, if you want
to change this, develop it

00:33:18.130 --> 00:33:23.380
further, get some
ideas about it,

00:33:23.380 --> 00:33:26.520
that's what the
whole point is here.

00:33:29.500 --> 00:33:35.130
Actually, the faculty meeting
this week, maybe today--

00:33:35.130 --> 00:33:36.007
what's today?

00:33:36.007 --> 00:33:36.840
AUDIENCE: Wednesday.

00:33:36.840 --> 00:33:38.580
GILBERT STRANG: Wednesday?

00:33:38.580 --> 00:33:40.780
Yeah, so it's this afternoon.

00:33:40.780 --> 00:33:43.150
And the faculty
doesn't come to much.

00:33:43.150 --> 00:33:46.090
Of course, it's late
in the afternoon.

00:33:46.090 --> 00:33:49.360
But faculty meeting
this afternoon

00:33:49.360 --> 00:33:55.770
is about MIT's plans for
requirements or courses

00:33:55.770 --> 00:33:58.630
in computational thinking.

00:33:58.630 --> 00:34:02.470
And in a way, this course
within the math department

00:34:02.470 --> 00:34:07.270
is among the ones that
are in that direction.

00:34:07.270 --> 00:34:11.940
Of course, in other departments,
those are further along.

00:34:11.940 --> 00:34:20.889
Anyway, when Raj Rao taught
the course last spring,

00:34:20.889 --> 00:34:26.739
he had the Julia system
better developed.

00:34:26.739 --> 00:34:31.630
And it was a chance to bring
computers and bring laptops

00:34:31.630 --> 00:34:35.889
and do things in class.

00:34:35.889 --> 00:34:37.780
And you'll have that
chance again when

00:34:37.780 --> 00:34:41.480
he visits in a month.

00:34:41.480 --> 00:34:44.110
OK, enough.

00:34:44.110 --> 00:34:48.820
And I'm open to questions
about the project.

00:34:48.820 --> 00:34:54.639
Should I maybe ask you to email
me a rough idea of a project?

00:34:54.639 --> 00:34:57.460
And tell me if
you are in a group

00:34:57.460 --> 00:35:02.380
or if you would like to find
a group, maybe two or three

00:35:02.380 --> 00:35:02.920
people.

00:35:02.920 --> 00:35:05.362
I'm not thinking
of groups of 50.

00:35:05.362 --> 00:35:10.820
Two or three would be sensible.

00:35:10.820 --> 00:35:13.430
Questions about projects?

00:35:13.430 --> 00:35:17.890
I mean, I just introduced
this idea of our project,

00:35:17.890 --> 00:35:24.400
and I apologize for not
bringing it up the first week.

00:35:24.400 --> 00:35:26.140
But I just couldn't see--

00:35:26.140 --> 00:35:28.840
I don't want to do
exams on linear algebra.

00:35:28.840 --> 00:35:30.520
We passed that point.

00:35:30.520 --> 00:35:34.210
So this seemed the
right way to go.

00:35:34.210 --> 00:35:40.830
But I'm not looking
for a PhD thesis here.

00:35:40.830 --> 00:35:41.520
Questions?

00:35:41.520 --> 00:35:44.170
Thoughts?

00:35:44.170 --> 00:35:46.690
I guess I hope you
know you can ask.

00:35:46.690 --> 00:35:47.854
Yeah, oh good.

00:35:47.854 --> 00:35:50.274
AUDIENCE: So could
you maybe describe

00:35:50.274 --> 00:35:54.503
the scope of the project?

00:35:54.503 --> 00:35:55.420
GILBERT STRANG: Right.

00:35:55.420 --> 00:36:04.740
How will I-- yeah, so the
scope is connected to the time

00:36:04.740 --> 00:36:06.950
that you would devote to it.

00:36:06.950 --> 00:36:10.730
And what should I
say about scope?

00:36:10.730 --> 00:36:14.210
Maybe the equivalent of
three homeworks or something.

00:36:14.210 --> 00:36:21.602
Because I'll tamp down homeworks
as project date gets closer.

00:36:21.602 --> 00:36:22.560
Does that give an idea?

00:36:25.560 --> 00:36:33.030
So it's not infinite, but it's
not something tiny and trivial.

00:36:33.030 --> 00:36:34.140
Yeah, good.

00:36:34.140 --> 00:36:37.320
AUDIENCE: Do you have an
example projects of what

00:36:37.320 --> 00:36:38.640
were done in past years?

00:36:38.640 --> 00:36:40.265
GILBERT STRANG: Well,
that's the thing.

00:36:40.265 --> 00:36:42.600
There aren't really past years.

00:36:42.600 --> 00:36:46.840
We are the ones.

00:36:46.840 --> 00:36:56.585
So I will have next year if
contribute some good ideas.

00:36:59.230 --> 00:37:03.690
Maybe I should I
ask Professor Rao

00:37:03.690 --> 00:37:08.350
to maybe send us the
projects he uses in Michigan?

00:37:08.350 --> 00:37:10.330
That would be some ideas.

00:37:10.330 --> 00:37:15.160
But remember that he
hasn't, up to now anyway,

00:37:15.160 --> 00:37:18.220
moved the course
toward deep learning.

00:37:18.220 --> 00:37:22.910
He did other topics, all
of which would be fine.

00:37:22.910 --> 00:37:26.980
But quite a few people
have had some 6.036

00:37:26.980 --> 00:37:31.290
or know something about
conventional neural nets.

00:37:31.290 --> 00:37:36.240
And I'm certainly excited
to get to that topic.

00:37:36.240 --> 00:37:38.880
So the project could get
there or it could not.

00:37:38.880 --> 00:37:40.900
Both totally fine.

00:37:40.900 --> 00:37:42.500
OK, that's a good idea.

00:37:42.500 --> 00:37:48.400
I'll ask Raj for
just the projects--

00:37:48.400 --> 00:37:51.490
and you'll recognize a couple,
because you've done a couple.

00:37:51.490 --> 00:37:55.480
But there are a bunch more.

00:37:55.480 --> 00:37:58.780
Then there was another
question or thought?

00:37:58.780 --> 00:38:04.000
And I'm remembering that I
think maybe everybody got

00:38:04.000 --> 00:38:07.360
an email or a
Stellar announcement

00:38:07.360 --> 00:38:12.280
that some members of the class
took an initiative, which

00:38:12.280 --> 00:38:17.230
was wonderful, to open
the possibility of people

00:38:17.230 --> 00:38:21.400
just showing up one evening a
week in the Media Lab was it?

00:38:21.400 --> 00:38:24.240
Or was there a location?

00:38:24.240 --> 00:38:29.327
And has it happened or
is it a future event?

00:38:29.327 --> 00:38:30.790
AUDIENCE: It happened.

00:38:30.790 --> 00:38:33.400
GILBERT STRANG: It happened.

00:38:33.400 --> 00:38:36.800
But I hadn't
mentioned it in class,

00:38:36.800 --> 00:38:38.690
so probably you didn't have--

00:38:38.690 --> 00:38:41.500
and we're not really
into projects yet,

00:38:41.500 --> 00:38:43.890
so it was probably
a quiet evening?

00:38:43.890 --> 00:38:44.473
AUDIENCE: Yep.

00:38:44.473 --> 00:38:45.348
GILBERT STRANG: Yeah.

00:38:45.348 --> 00:38:46.240
Yeah, and that's--

00:38:46.240 --> 00:38:47.080
AUDIENCE: Productive but quiet.

00:38:47.080 --> 00:38:48.370
GILBERT STRANG:
Productive but quiet.

00:38:48.370 --> 00:38:49.090
OK.

00:38:49.090 --> 00:38:51.350
So will it happen again?

00:38:51.350 --> 00:38:53.186
AUDIENCE: Sure, I
think maybe now we'll

00:38:53.186 --> 00:38:54.682
be looking after spring break.

00:38:54.682 --> 00:38:56.140
GILBERT STRANG:
After spring break.

00:38:56.140 --> 00:39:02.590
OK, so post again on Stellar
the plan for the next meeting

00:39:02.590 --> 00:39:06.460
that people could come to.

00:39:06.460 --> 00:39:12.060
So this is David Anderton,
so you'll recognize his name.

00:39:12.060 --> 00:39:14.980
And did you have the
meeting in the Media Lab?

00:39:14.980 --> 00:39:18.430
AUDIENCE: Yeah, we had it
on the Thursday and Friday.

00:39:18.430 --> 00:39:19.330
GILBERT STRANG: OK.

00:39:19.330 --> 00:39:22.930
So with the break
coming and spring

00:39:22.930 --> 00:39:31.000
hopefully coming after
today's potential storm,

00:39:31.000 --> 00:39:33.600
when we come back, good.

00:39:33.600 --> 00:39:34.510
OK.

00:39:34.510 --> 00:39:36.640
Is that good?

00:39:36.640 --> 00:39:38.080
I hope some of that is helpful.

00:39:38.080 --> 00:39:39.850
You'll get an idea.

00:39:39.850 --> 00:39:42.370
You're seeing about
as much as I know,

00:39:42.370 --> 00:39:48.660
which is model 1 is definitely
doable and very significant.

00:39:48.660 --> 00:39:53.460
And Vandermonde matrices and
so on are truly important.

00:39:53.460 --> 00:39:55.890
And their instability
is a big issue.

00:39:58.530 --> 00:40:06.370
But then moving toward weights
and training data and test data

00:40:06.370 --> 00:40:08.270
is where we want to go.

00:40:08.270 --> 00:40:08.850
Good.

00:40:08.850 --> 00:40:10.160
OK.

00:40:10.160 --> 00:40:11.520
So do I have some time?

00:40:11.520 --> 00:40:18.660
I do just to speak about
mean and variance, the two

00:40:18.660 --> 00:40:21.480
golden words of statistics--

00:40:21.480 --> 00:40:25.050
and covariance, the
matrix, the intersection

00:40:25.050 --> 00:40:29.370
of linear algebra with
statistics, and then

00:40:29.370 --> 00:40:31.530
some famous inequality.

00:40:31.530 --> 00:40:35.100
So I'll continue
with this on Friday

00:40:35.100 --> 00:40:37.630
and post some other material.

00:40:37.630 --> 00:40:40.620
So that it's coming from
a later section of notes.

00:40:40.620 --> 00:40:51.740
OK, can I just so I either
have probabilities P1 up to Pn

00:40:51.740 --> 00:40:56.780
adding to 1, or I have our
continuous distribution

00:40:56.780 --> 00:41:01.850
of probabilities, maybe from
all x's from minus infinity

00:41:01.850 --> 00:41:04.970
to infinity, again, giving 1.

00:41:04.970 --> 00:41:07.010
Let me work with the
discrete example.

00:41:07.010 --> 00:41:10.010
That's where people
naturally start.

00:41:10.010 --> 00:41:14.520
So what is the mean?

00:41:14.520 --> 00:41:25.410
So I've n possible outcomes
with those probabilities.

00:41:25.410 --> 00:41:35.840
And I can ask you
about the sample mean

00:41:35.840 --> 00:41:40.958
or I can ask you about
the expected mean.

00:41:45.230 --> 00:41:47.780
So the sample means,
we've done an experiment.

00:41:47.780 --> 00:41:50.000
We've got some output.

00:41:50.000 --> 00:41:53.030
The expected mean means
we know probabilities,

00:41:53.030 --> 00:41:54.350
but we haven't used them yet.

00:41:57.410 --> 00:42:01.160
So this uses actual output.

00:42:01.160 --> 00:42:04.490
And the sample mean is simply--

00:42:04.490 --> 00:42:13.120
shall I just say,
m for mean-- well,

00:42:13.120 --> 00:42:15.970
these two are
importantly different.

00:42:15.970 --> 00:42:20.140
One is something where
you've done the experiment.

00:42:20.140 --> 00:42:23.290
And this is before
you do the experiment.

00:42:23.290 --> 00:42:27.190
And the letters get-- maybe
mu, I'll change it to mu.

00:42:29.760 --> 00:42:34.470
I don't want to use S because
S gets used with variance.

00:42:34.470 --> 00:42:39.495
So it's just the average,
average output from the sample.

00:42:44.750 --> 00:42:48.080
Like I've flipped a
coin a million times.

00:42:48.080 --> 00:42:51.590
And the output was 0 or 1.

00:42:51.590 --> 00:42:55.210
So I got a million 1s and 0s.

00:42:55.210 --> 00:42:58.960
And I take the average, and I'm
expecting a number like half

00:42:58.960 --> 00:43:00.132
a million.

00:43:00.132 --> 00:43:03.620
Of course, I'm thinking
of a fair coin.

00:43:03.620 --> 00:43:05.930
And the law of
large numbers would

00:43:05.930 --> 00:43:15.410
say that this sample mean does
approach 1/2 with probability 1

00:43:15.410 --> 00:43:18.050
as the number of
samples gets larger.

00:43:18.050 --> 00:43:21.240
So sample mean is
straightforward.

00:43:21.240 --> 00:43:23.060
The expected mean means--

00:43:23.060 --> 00:43:27.185
these are actual sample outputs.

00:43:32.530 --> 00:43:38.060
They happened, whereas the
expected mean is just the--

00:43:38.060 --> 00:43:40.100
and I'll use m for that--

00:43:40.100 --> 00:43:46.430
it's the probability of the
first output times that output

00:43:46.430 --> 00:43:49.760
plus the probability of the
second output times that output

00:43:49.760 --> 00:43:52.370
plus, plus Pn xn.

00:43:56.690 --> 00:44:05.140
So that will approach that with
probability 1 as this number

00:44:05.140 --> 00:44:06.760
capital N-- notice
the difference.

00:44:06.760 --> 00:44:12.320
Capital N here is the number of
samples, the number of trials.

00:44:12.320 --> 00:44:14.650
And it gets big.

00:44:14.650 --> 00:44:17.710
We keep doing things
more and more.

00:44:17.710 --> 00:44:21.730
This little n is the number
of possible different outputs

00:44:21.730 --> 00:44:23.170
with their probabilities.

00:44:23.170 --> 00:44:24.930
And there you see it.

00:44:24.930 --> 00:44:27.370
And, of course, in
the continuous case,

00:44:27.370 --> 00:44:30.392
we would take the
interval of x p of x dx.

00:44:34.480 --> 00:44:38.680
So let me just, by
analogy, that you

00:44:38.680 --> 00:44:41.140
should know what the
continuous version is

00:44:41.140 --> 00:44:43.420
and what the
discrete version is.

00:44:43.420 --> 00:44:44.900
OK, that's the mean.

00:44:47.770 --> 00:44:49.870
Now, for variance.

00:44:49.870 --> 00:45:00.120
Sample variance-- and, shall
I say, expected variance?

00:45:00.120 --> 00:45:00.960
I don't know.

00:45:00.960 --> 00:45:05.442
Just variance is what
people would usually say.

00:45:05.442 --> 00:45:07.650
I don't know if I've remembered
the right word there,

00:45:07.650 --> 00:45:10.690
sample variance.

00:45:10.690 --> 00:45:19.770
Is that-- I included this
topic in linear algebra book.

00:45:19.770 --> 00:45:21.300
Anyway, yeah.

00:45:21.300 --> 00:45:21.990
OK.

00:45:21.990 --> 00:45:23.565
So what's the sample variance?

00:45:28.010 --> 00:45:32.948
So I guess I'm--

00:45:32.948 --> 00:45:37.800
hm-- yeah, so what is
the sample variance?

00:45:37.800 --> 00:45:39.390
What's the variance
about anyway?

00:45:39.390 --> 00:45:42.990
What's the key
point of variance?

00:45:42.990 --> 00:45:46.210
It's the distance from the mean.

00:45:46.210 --> 00:45:49.050
So this will be a distance
from the sample mean,

00:45:49.050 --> 00:45:51.480
and this will be a distance
from the expected mean.

00:45:54.600 --> 00:45:59.260
So not distance from zero,
but distance from mu and m,

00:45:59.260 --> 00:46:02.270
from the center of the thing.

00:46:02.270 --> 00:46:04.360
So the sample variance--

00:46:04.360 --> 00:46:06.790
so, again, we have n samples.

00:46:06.790 --> 00:46:10.720
But for some wonderful
reason in statistics

00:46:10.720 --> 00:46:14.820
you divide by n
minus 1 this time.

00:46:14.820 --> 00:46:19.500
And the reason has to do with
the fact that you used one--

00:46:19.500 --> 00:46:21.510
this will involve the mean.

00:46:21.510 --> 00:46:26.850
So this would be
the first output

00:46:26.850 --> 00:46:37.750
minus mu squared up to the
n-th output minus mu squared.

00:46:37.750 --> 00:46:41.500
So it's the average
distance from mu--

00:46:41.500 --> 00:46:44.140
average squared
distance from mu--

00:46:44.140 --> 00:46:48.490
but with this little twist that,
of course, when n is large,

00:46:48.490 --> 00:46:52.100
it's not a very significant
difference between n and n

00:46:52.100 --> 00:46:52.600
minus 1.

00:46:52.600 --> 00:46:56.190
I Think that's about right.

00:46:56.190 --> 00:47:00.940
All this is that I'm just doing
one experiment over and over.

00:47:00.940 --> 00:47:07.460
Covariance, which
is the deeper idea,

00:47:07.460 --> 00:47:09.200
is where linear
algebra comes in.

00:47:09.200 --> 00:47:11.660
I have a matrix because why?

00:47:11.660 --> 00:47:17.450
Because I'm doing multiple
experiments at the same time.

00:47:17.450 --> 00:47:20.160
I'm flipping two coins.

00:47:20.160 --> 00:47:22.270
I'm flipping 15 coins.

00:47:22.270 --> 00:47:25.960
I'm doing other things.

00:47:25.960 --> 00:47:30.010
So that will be covariances when
I'm doing several experiments

00:47:30.010 --> 00:47:31.960
at once.

00:47:31.960 --> 00:47:35.680
That will involve
matrices of that size.

00:47:35.680 --> 00:47:38.960
So what's the variance?

00:47:38.960 --> 00:47:42.340
I should have given
you the usual notation.

00:47:42.340 --> 00:47:47.710
The expected value of x,
that's what's the mean.

00:47:47.710 --> 00:47:53.300
And here, I'm looking at
the expected value of what?

00:47:53.300 --> 00:47:58.880
So when I'm computing a
variance, using probabilities--

00:47:58.880 --> 00:48:04.510
so I'm using expectations,
not trial runs--

00:48:04.510 --> 00:48:07.880
expectations means
use the probabilities.

00:48:07.880 --> 00:48:11.950
And it's the expectation
of the distance

00:48:11.950 --> 00:48:17.300
from x to the mean squared.

00:48:19.990 --> 00:48:27.550
And that is when I'm doing is an
expectation for a discrete set,

00:48:27.550 --> 00:48:30.700
I think of the probability,
the first probability,

00:48:30.700 --> 00:48:37.950
that goes with an output x1
and a second probability that

00:48:37.950 --> 00:48:41.010
goes with an output x2.

00:48:41.010 --> 00:48:48.210
And each time I subtract
from the mean and square.

00:48:48.210 --> 00:48:51.090
So that's the variance that
everybody calls sigma squared.

00:48:54.610 --> 00:49:02.020
Now, two minutes left is
enough to say a few more

00:49:02.020 --> 00:49:04.650
words about covariance.

00:49:04.650 --> 00:49:06.770
Oh, to get to
covariance, I really

00:49:06.770 --> 00:49:09.440
have to speak about
joint probabilities.

00:49:09.440 --> 00:49:11.510
That's a key idea--

00:49:11.510 --> 00:49:13.140
joint probabilities.

00:49:13.140 --> 00:49:15.320
So I'm doing two
experiments at once.

00:49:18.450 --> 00:49:22.720
So each one has its
own probabilities.

00:49:22.720 --> 00:49:27.760
But together, I have to ask--

00:49:27.760 --> 00:49:30.770
so here are two easy cases.

00:49:30.770 --> 00:49:34.030
Suppose I'm flipping two coins.

00:49:34.030 --> 00:49:35.770
So that I might get heads heads.

00:49:35.770 --> 00:49:38.140
I may get heads
tails, tails heads,

00:49:38.140 --> 00:49:40.450
or tails tails,
four possibility,

00:49:40.450 --> 00:49:44.890
four possible outputs
there, four possible pairs.

00:49:44.890 --> 00:49:49.510
And if you're flipping one coin
and I'm flipping another one,

00:49:49.510 --> 00:49:52.250
those are independent results.

00:49:52.250 --> 00:49:54.500
Those are independent results.

00:49:54.500 --> 00:50:01.190
There won't be a
covariance where

00:50:01.190 --> 00:50:04.040
by knowing what my flip
was I would know more

00:50:04.040 --> 00:50:04.910
about your flip.

00:50:07.440 --> 00:50:10.260
But now, the other
possibility would

00:50:10.260 --> 00:50:12.000
be to glue the coins together.

00:50:14.820 --> 00:50:20.620
Now, if I do a flip, they
always come up heads and heads

00:50:20.620 --> 00:50:22.720
or tails and tails.

00:50:22.720 --> 00:50:26.230
So the heads tails
combination is not possible.

00:50:26.230 --> 00:50:30.970
In fact, one output is totally
dependent on the other outputs.

00:50:30.970 --> 00:50:32.770
So that's the other extreme.

00:50:32.770 --> 00:50:36.970
We have independent
outputs with covariance 0,

00:50:36.970 --> 00:50:40.900
and we have totally
dependent outputs

00:50:40.900 --> 00:50:44.620
when the things are just glued
together, when one result tells

00:50:44.620 --> 00:50:46.960
us what the other
result is, then

00:50:46.960 --> 00:50:51.340
that's a situation where
the covariance is a maximum.

00:50:51.340 --> 00:50:53.290
It couldn't be bigger than that.

00:50:53.290 --> 00:50:58.360
And say in polling, if
you were polling a family,

00:50:58.360 --> 00:51:04.720
say political
polling, well, there

00:51:04.720 --> 00:51:10.110
would be some covariance
expected there.

00:51:10.110 --> 00:51:13.110
The two or three
or five people that

00:51:13.110 --> 00:51:15.280
are living in the
same house wouldn't

00:51:15.280 --> 00:51:18.310
be independent,
entirely independent,

00:51:18.310 --> 00:51:24.370
but nor would all five
give the same answer.

00:51:24.370 --> 00:51:31.690
So their covariance matrix
would have some off diagonal,

00:51:31.690 --> 00:51:36.970
but it would still
be invertible.

00:51:36.970 --> 00:51:40.000
And actually, what I wanted
to tell you about next time

00:51:40.000 --> 00:51:42.880
at the start is that
covariance matrix, which

00:51:42.880 --> 00:51:47.760
I have to define for you, will
be symmetric positive definite,

00:51:47.760 --> 00:51:49.710
or semidefinite.

00:51:49.710 --> 00:51:52.200
What's the semidefinite case?

00:51:52.200 --> 00:51:57.610
Of course, that's the case where
the coins are glued together.

00:51:57.610 --> 00:51:59.820
OK, thanks.

00:51:59.820 --> 00:52:01.480
So you know what's
coming Friday.

00:52:01.480 --> 00:52:04.470
I know that holiday
is also coming Friday.

00:52:04.470 --> 00:52:08.070
And just you'll
make a good plan,

00:52:08.070 --> 00:52:11.830
and I'll move on
after the break.

00:52:11.830 --> 00:52:13.380
Good.

