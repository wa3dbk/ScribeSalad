WEBVTT
Kind: captions
Language: en

00:00:01.550 --> 00:00:03.920
The following content is
provided under a Creative

00:00:03.920 --> 00:00:05.310
Commons license.

00:00:05.310 --> 00:00:07.520
Your support will help
MIT OpenCourseWare

00:00:07.520 --> 00:00:11.610
continue to offer high quality
educational resources for free.

00:00:11.610 --> 00:00:14.180
To make a donation or to
view additional materials

00:00:14.180 --> 00:00:17.795
from hundreds of MIT courses,
visit MIT OpenCourseWare

00:00:17.795 --> 00:00:19.026
at ocw.mit.edu.

00:00:22.800 --> 00:00:26.180
GILBERT STRANG: Well, OK,
I am happy to be back,

00:00:26.180 --> 00:00:29.910
and I am really happy
about the project

00:00:29.910 --> 00:00:32.229
proposals that are coming in.

00:00:32.229 --> 00:00:36.840
This is like, OK, this is really
a good part of the course.

00:00:36.840 --> 00:00:42.360
And so keep them coming, and
I'm happy to give whatever

00:00:42.360 --> 00:00:45.000
feedback I can on
those proposals,

00:00:45.000 --> 00:00:49.120
and do make a start there.

00:00:49.120 --> 00:00:53.100
They're really good, and
if some are completed

00:00:53.100 --> 00:00:55.230
before the end of
the semester and we

00:00:55.230 --> 00:01:00.860
can to offer you a
chance to report on them,

00:01:00.860 --> 00:01:04.019
that that's good too.

00:01:04.019 --> 00:01:07.930
So well done with
those proposals.

00:01:07.930 --> 00:01:12.830
So today, I'm
jumping to part six.

00:01:12.830 --> 00:01:15.870
So part six and part
seven are optimization

00:01:15.870 --> 00:01:17.910
which is the fundamental
algorithm that

00:01:17.910 --> 00:01:19.980
goes into deep learning.

00:01:19.980 --> 00:01:22.140
So we've got to start
with optimization.

00:01:22.140 --> 00:01:26.010
Everybody has to
get that picture,

00:01:26.010 --> 00:01:29.580
and then part seven
will be the structure

00:01:29.580 --> 00:01:33.660
of CNNs, Convolution
Neural Nets,

00:01:33.660 --> 00:01:37.560
and all kinds of applications.

00:01:37.560 --> 00:01:41.430
And so can we start
with optimization?

00:01:41.430 --> 00:01:44.370
So first, can I like
get the basic facts

00:01:44.370 --> 00:01:49.840
about three terms
of a Taylor series?

00:01:49.840 --> 00:01:51.030
So that's the typical.

00:01:51.030 --> 00:01:55.590
It's seldom that we would
go up to third derivatives

00:01:55.590 --> 00:01:56.970
in optimization.

00:01:56.970 --> 00:02:01.920
So that's the most useful
approximation to a function.

00:02:01.920 --> 00:02:03.720
Everybody recognizes it.

00:02:03.720 --> 00:02:08.729
Here, I'm thinking of
F as just one function,

00:02:08.729 --> 00:02:12.990
and x as just one
variable, but now

00:02:12.990 --> 00:02:18.180
I really want to go
to more variables.

00:02:18.180 --> 00:02:21.360
So what do I have
to change if F is

00:02:21.360 --> 00:02:26.790
a function of more variables?

00:02:26.790 --> 00:02:31.050
So now, I'm thinking of x as--

00:02:31.050 --> 00:02:32.730
well, now let me see.

00:02:37.670 --> 00:02:42.470
Yeah, I want n variables here.

00:02:42.470 --> 00:02:45.170
x is x1 up to xn.

00:02:48.830 --> 00:02:52.840
So just to get
the words straight

00:02:52.840 --> 00:02:56.800
so we can begin on
optimization, so what

00:02:56.800 --> 00:03:02.740
will be the similar step
so the function F at x--

00:03:02.740 --> 00:03:06.590
remember, x is n variables.

00:03:06.590 --> 00:03:07.330
OK?

00:03:07.330 --> 00:03:08.350
Now, what do I have?

00:03:08.350 --> 00:03:13.170
Delta x, so what's the
point about delta x now?

00:03:13.170 --> 00:03:17.500
It's a vector, delta
x1 to delta xn,

00:03:17.500 --> 00:03:20.190
and what about the
derivative of F?

00:03:20.190 --> 00:03:22.810
It's a vector too,
the derivative

00:03:22.810 --> 00:03:26.175
of F with respect to
x1, the derivative of F

00:03:26.175 --> 00:03:29.660
with respect to x2, and so on.

00:03:29.660 --> 00:03:32.420
What do I have to
change about that?

00:03:32.420 --> 00:03:36.310
I know those guys are vectors,
so it's their dot product.

00:03:36.310 --> 00:03:44.360
So it's delta x transpose
at vector times this dF/dx.

00:03:48.340 --> 00:03:52.620
So now I'm replacing this
by all the derivatives,

00:03:52.620 --> 00:03:58.040
and it's the gradient.

00:03:58.040 --> 00:04:03.820
So the gradient of F at
x is the derivatives--

00:04:07.257 --> 00:04:09.920
let's see.

00:04:09.920 --> 00:04:14.320
It's essential to get the
notation straight here.

00:04:14.320 --> 00:04:17.140
Yeah, so it'll be the
partial derivatives

00:04:17.140 --> 00:04:26.290
of the function F. So grad F
is the partial derivatives of F

00:04:26.290 --> 00:04:30.846
with respect to x1 down to
partial derivative with respect

00:04:30.846 --> 00:04:32.650
to xn.

00:04:32.650 --> 00:04:34.180
OK, good.

00:04:34.180 --> 00:04:37.490
That's the linear term, and
now what's the quadratic term?

00:04:37.490 --> 00:04:42.010
1/2, now delta x isn't
a scalar anymore.

00:04:42.010 --> 00:04:43.160
It's a vector.

00:04:43.160 --> 00:04:47.500
So I'm going to have
delta x transpose

00:04:47.500 --> 00:04:51.970
and a delta x, and
what goes in between

00:04:51.970 --> 00:04:55.240
is the second
derivatives, but I've

00:04:55.240 --> 00:04:57.460
got a function of n variables.

00:05:00.190 --> 00:05:04.240
So now, I have a matrix
of second derivatives,

00:05:04.240 --> 00:05:10.510
and I'll call it H. This is the
matrix of second derivatives,

00:05:10.510 --> 00:05:19.840
Hjk is the second derivative
of F with respect to xj and xk,

00:05:19.840 --> 00:05:23.360
and what's the
name for this guy?

00:05:23.360 --> 00:05:26.485
The Hessian, Hessian matrix.

00:05:30.350 --> 00:05:33.260
How the Hessians got into
this picture I don't know.

00:05:33.260 --> 00:05:34.960
The only Hessians
I know are the ones

00:05:34.960 --> 00:05:39.530
who fought in the
Revolutionary War for somebody.

00:05:39.530 --> 00:05:40.030
Who?

00:05:40.030 --> 00:05:42.220
Which side were they on?

00:05:42.220 --> 00:05:43.750
I think maybe the wrong side.

00:05:43.750 --> 00:05:47.470
The French were
on our side and--

00:05:47.470 --> 00:05:50.530
Anyway, Hessian
matrix, and what are

00:05:50.530 --> 00:05:52.345
the facts about that matrix?

00:05:52.345 --> 00:05:54.220
Well, the first fact is
that it's [INAUDIBLE]

00:05:54.220 --> 00:05:56.883
and the key fact
is it's symmetric.

00:06:00.200 --> 00:06:02.140
Yeah.

00:06:02.140 --> 00:06:07.420
OK, and again, it's
an approximation.

00:06:07.420 --> 00:06:12.880
And everybody recognizes
that if n is very large,

00:06:12.880 --> 00:06:16.360
and we have a function
of many variables.

00:06:16.360 --> 00:06:19.540
Then, we had n derivatives
to compute here,

00:06:19.540 --> 00:06:22.650
and about 1/2 n
squared derivatives.

00:06:22.650 --> 00:06:25.960
The 1/2 comes from the
symmetry, but the key point

00:06:25.960 --> 00:06:30.670
is the n squared derivatives
to compute there.

00:06:30.670 --> 00:06:36.240
So computing the
gradient is feasible

00:06:36.240 --> 00:06:40.050
if n is small or
moderately large.

00:06:40.050 --> 00:06:43.900
Actually, by using
automatic differentiation,

00:06:43.900 --> 00:06:46.050
the key idea of
back propagation,

00:06:46.050 --> 00:06:54.750
back prop, you can
speed up the computation

00:06:54.750 --> 00:06:56.820
of derivatives quite amazingly.

00:06:56.820 --> 00:07:02.400
But still for the size
of deep learning problems

00:07:02.400 --> 00:07:04.300
that's out of reach.

00:07:04.300 --> 00:07:05.340
OK.

00:07:05.340 --> 00:07:10.140
So that's the
picture, and then I

00:07:10.140 --> 00:07:16.170
will want to use this
to solve equations.

00:07:16.170 --> 00:07:25.140
There is a parallel
picture for a vector f.

00:07:25.140 --> 00:07:26.940
So now, this is a
vector function.

00:07:26.940 --> 00:07:39.210
This is f1 of x up to fn
of x, an x is x1 to xn.

00:07:42.030 --> 00:07:45.950
So I have n functions
of n variables,

00:07:45.950 --> 00:07:47.700
n functions of n variables.

00:07:47.700 --> 00:07:51.380
Well, that's exactly what
I have in the gradient.

00:07:54.200 --> 00:07:57.230
Think of these two as
parallel, the parallel

00:07:57.230 --> 00:08:02.230
being f corresponds
to the gradient of F,

00:08:02.230 --> 00:08:06.980
n functions of n variables.

00:08:10.800 --> 00:08:11.300
OK.

00:08:15.010 --> 00:08:18.730
Now maybe, what I'm after
here is to solve f equals 0.

00:08:18.730 --> 00:08:25.570
So I'm going to think about
the f at x plus delta x,

00:08:25.570 --> 00:08:28.890
so it starts with f of x.

00:08:28.890 --> 00:08:34.539
And then we have the
correction times the matrix

00:08:34.539 --> 00:08:42.309
of first derivatives, and
what's the name for that matrix

00:08:42.309 --> 00:08:45.850
of first derivatives?

00:08:45.850 --> 00:08:51.670
Well, if I'm just
given n functions--

00:08:54.280 --> 00:08:56.830
yeah, what am I after here?

00:08:56.830 --> 00:09:05.120
I'm looking for the Jacobian.

00:09:05.120 --> 00:09:10.580
So here we'll go the Jacobian,
J. This is the Jacobian named

00:09:10.580 --> 00:09:16.320
after Jacoby, Jacobian matrix.

00:09:16.320 --> 00:09:18.920
And what are its entries?

00:09:18.920 --> 00:09:23.060
J, the jk entry
is the derivative

00:09:23.060 --> 00:09:30.780
of the J function with
respect to the kth variable,

00:09:30.780 --> 00:09:34.520
and I'm stopping at
first order there.

00:09:34.520 --> 00:09:38.360
OK, so these are sort of
like facts of calculus,

00:09:38.360 --> 00:09:41.000
facts of 18.02 you could say.

00:09:41.000 --> 00:09:44.550
Multivariable calculus,
that's the point.

00:09:44.550 --> 00:09:47.870
Notice that we're doing just
like the first half of 18.02,

00:09:47.870 --> 00:09:52.880
just do differential calculus,
derivatives, Taylor series.

00:09:52.880 --> 00:09:54.750
We're not doing
multiple integrals.

00:09:54.750 --> 00:09:57.050
That's not part
of our world here.

00:09:57.050 --> 00:10:00.330
OK, so that's the background.

00:10:00.330 --> 00:10:05.840
Now, I want to look
at optimization.

00:10:05.840 --> 00:10:12.610
So over here, I
want to optimize--

00:10:12.610 --> 00:10:24.290
well, over here, let me
try to minimize F of x,

00:10:24.290 --> 00:10:27.340
and I'll be in the
vector case here.

00:10:27.340 --> 00:10:38.610
And over here, I want
to solve f equals 0,

00:10:38.610 --> 00:10:45.330
and of course, that
means f of 1 equals 0

00:10:45.330 --> 00:10:48.720
all the way along
to fn equals 0.

00:10:48.720 --> 00:10:54.080
Here, I have n equations,
and n unknowns.

00:10:59.720 --> 00:11:02.660
Let me start with
that one, and I'll

00:11:02.660 --> 00:11:05.450
start with Newton's
method, Newton's method

00:11:05.450 --> 00:11:09.890
to solve these n
equations and n unknowns.

00:11:09.890 --> 00:11:22.440
OK, so Newton, Newton's
method which is often

00:11:22.440 --> 00:11:24.210
not presented in 18.02.

00:11:24.210 --> 00:11:29.130
That's a crime, because
that's the big application

00:11:29.130 --> 00:11:33.180
of gradients in Jacobians.

00:11:33.180 --> 00:11:37.150
OK, so I'm trying to solve
n equations and n unknowns,

00:11:37.150 --> 00:11:42.930
and so I want f at x
plus delta x to be 0.

00:11:42.930 --> 00:11:43.500
Right?

00:11:43.500 --> 00:11:46.740
So I want f of x
plus delta x to be 0.

00:11:46.740 --> 00:11:49.890
So f at x plus delta x is--

00:11:49.890 --> 00:11:51.150
I'm putting in a 0.

00:11:51.150 --> 00:11:53.340
I'm just copying that equation--

00:11:53.340 --> 00:11:58.440
is f at where I am.

00:11:58.440 --> 00:12:02.370
Let me use K for
the case iteration.

00:12:02.370 --> 00:12:04.490
So I'm at a point xK.

00:12:04.490 --> 00:12:07.230
I want to get to
a point xK plus 1.

00:12:07.230 --> 00:12:14.890
And so I have 0 is f of
x plus J, at that point,

00:12:14.890 --> 00:12:21.700
times delta x which
is xK plus 1 minus xK.

00:12:21.700 --> 00:12:24.030
Good.

00:12:24.030 --> 00:12:25.020
That's Newton's method.

00:12:25.020 --> 00:12:29.010
Of course, 0 isn't quite true.

00:12:29.010 --> 00:12:35.250
Well, 0 will be true if I'm
constructing xK plus 1 here.

00:12:35.250 --> 00:12:37.950
I'm constructing xK plus 1.

00:12:37.950 --> 00:12:39.030
OK.

00:12:39.030 --> 00:12:43.410
So let me just rewrite that,
and we've got Newton's method.

00:12:43.410 --> 00:12:52.750
So we're looking for this
change, xK plus 1 minus xK.

00:12:52.750 --> 00:13:00.010
I'll put it on this side
as plus xK, so that's this.

00:13:00.010 --> 00:13:01.960
Now, I have to invert
that and put it

00:13:01.960 --> 00:13:04.010
on the other side
of the equation.

00:13:04.010 --> 00:13:06.280
So that will go with a minus.

00:13:06.280 --> 00:13:12.880
This guy will be
inverted and f at xK.

00:13:15.472 --> 00:13:17.170
So that's Newton's methods.

00:13:17.170 --> 00:13:19.980
It's natural.

00:13:19.980 --> 00:13:21.310
So let me just repeat that.

00:13:21.310 --> 00:13:26.140
You see where the xK plus
1 minus xK is sitting?

00:13:26.140 --> 00:13:27.670
Right?

00:13:27.670 --> 00:13:32.930
And I moved f of xK to the
other side with a minus sign,

00:13:32.930 --> 00:13:37.150
and then I multiplied through
by J inverse, so I got that.

00:13:37.150 --> 00:13:45.020
So that's Newton's method
for a system of equations,

00:13:45.020 --> 00:13:47.380
and over there, I'm going to
write down Newton's method

00:13:47.380 --> 00:13:49.030
for minimizing a function.

00:13:49.030 --> 00:13:53.890
This is such basic stuff
that we have to begin here.

00:13:53.890 --> 00:13:58.540
Let me even begin with an
extremely straightforward

00:13:58.540 --> 00:14:02.440
example of Newton's method here.

00:14:02.440 --> 00:14:05.950
Suppose my function--
suppose I've only

00:14:05.950 --> 00:14:10.900
got one function actually.

00:14:10.900 --> 00:14:14.450
Suppose I only had one function.

00:14:14.450 --> 00:14:21.240
So suppose my function
is x squared minus 9,

00:14:21.240 --> 00:14:26.020
and I want to solve
f of x equals 0.

00:14:26.020 --> 00:14:28.990
I want to find the
square root of 9.

00:14:28.990 --> 00:14:33.790
OK, so what is
Newton's method for it?

00:14:33.790 --> 00:14:37.360
My point is just to see how
Newton's method is written

00:14:37.360 --> 00:14:43.200
and then rewrite it a little bit
so that we see the convergence.

00:14:43.200 --> 00:14:48.100
OK, so of course,
the Jacobian is 2x.

00:14:48.100 --> 00:14:51.580
So Newton's method
says that xK plus 1--

00:14:51.580 --> 00:14:56.230
I'm just going to copy
that Newton's method--

00:14:56.230 --> 00:14:59.170
minus 1 over 2xK.

00:14:59.170 --> 00:15:01.060
Right?

00:15:01.060 --> 00:15:08.680
That's the derivative times f at
xK which is xK squared minus 9.

00:15:12.190 --> 00:15:13.280
OK.

00:15:13.280 --> 00:15:17.720
We followed the formula,
this determines xK plus 1,

00:15:17.720 --> 00:15:20.350
and let's simplify it.

00:15:20.350 --> 00:15:26.500
So here I have xK minus
that looks like 1/2 of xK,

00:15:26.500 --> 00:15:30.730
so I think I have
1/2xK, and then

00:15:30.730 --> 00:15:40.137
this times this is
9/2 of 1 over xK.

00:15:40.137 --> 00:15:40.720
Is that right?

00:15:43.450 --> 00:15:50.100
1/2 of xk from this stuff
and plus 9/2 of 1 over xK.

00:15:50.100 --> 00:15:50.600
OK.

00:15:53.940 --> 00:15:59.850
Can I just like check that
I know the answer is 3?

00:15:59.850 --> 00:16:04.260
Can I be sure that I
get the right answer, 3?

00:16:04.260 --> 00:16:10.170
That if xK was exactly 3, then
of course, I expect xK plus 1

00:16:10.170 --> 00:16:11.730
to stay at 3.

00:16:11.730 --> 00:16:13.740
So does that happen?

00:16:13.740 --> 00:16:24.330
So 1/2 of 3 and 9/2 of 1/3,
what's that, 1/2 of 3 and 9/2

00:16:24.330 --> 00:16:25.440
of 1/3?

00:16:25.440 --> 00:16:28.720
OK, that's 3/2 and 3/2.

00:16:28.720 --> 00:16:31.580
That's 6/2, and that's 3.

00:16:31.580 --> 00:16:32.080
OK.

00:16:35.690 --> 00:16:38.870
So we've checked that the method
is consistent which just means

00:16:38.870 --> 00:16:41.690
we kept the algebra straight.

00:16:41.690 --> 00:16:46.430
But then the really important
point about Newton's method

00:16:46.430 --> 00:16:50.700
is to discover how
fast it converges.

00:16:50.700 --> 00:16:55.350
So now let me do
xK plus 1 minus 3.

00:16:55.350 --> 00:17:00.650
So now, I'm looking at the
error which is, I hope,

00:17:00.650 --> 00:17:02.420
approaching 0.

00:17:02.420 --> 00:17:03.770
Is it approaching 0?

00:17:03.770 --> 00:17:05.540
How quickly is it approaching 0?

00:17:05.540 --> 00:17:09.470
These are the fundamental
questions of optimization.

00:17:09.470 --> 00:17:14.940
So I'm going to subtract
3 from both sides somehow.

00:17:14.940 --> 00:17:20.970
OK, from here, I guess,
I'm going to subtract 3.

00:17:20.970 --> 00:17:24.569
So I was just checking
that it was correct.

00:17:24.569 --> 00:17:25.950
OK.

00:17:25.950 --> 00:17:30.310
Now, so xK plus 1
minus 3, I'm going

00:17:30.310 --> 00:17:32.310
to subtract 3 from both sides.

00:17:32.310 --> 00:17:38.190
I'm going to subtract 3
there, and then I hope that--

00:17:38.190 --> 00:17:40.110
that box is what goes down here.

00:17:40.110 --> 00:17:41.070
Right?

00:17:41.070 --> 00:17:48.960
Subtracted 3 from both sides, so
I'm hoping now things go to 0.

00:17:48.960 --> 00:17:52.200
OK, so what do I have there?

00:17:52.200 --> 00:17:55.330
Let me factor out the 1 over xK.

00:17:58.860 --> 00:18:01.850
So what do I have then left?

00:18:01.850 --> 00:18:08.750
1 over xK, so there's a
9/2 from there, 1 over xK.

00:18:08.750 --> 00:18:12.710
So I really have
1/2 of xK squared,

00:18:12.710 --> 00:18:15.080
because I've divided by an xK.

00:18:15.080 --> 00:18:21.120
And this minus 3, I
better put minus 3xK,

00:18:21.120 --> 00:18:23.230
because I'm dividing by xK.

00:18:23.230 --> 00:18:25.570
I claim that that's--

00:18:25.570 --> 00:18:26.360
now I've got it.

00:18:30.740 --> 00:18:34.970
And let's see, let
me take out the 2--

00:18:34.970 --> 00:18:41.480
2, forget these 2s,
and make that a 6.

00:18:41.480 --> 00:18:47.210
So I have 1 over 2xK times
9 plus xK squared minus 6.

00:18:47.210 --> 00:18:48.410
Anything good about that?

00:18:50.980 --> 00:18:51.880
We hope so.

00:18:56.450 --> 00:19:00.450
We hope that that is
something attractive.

00:19:00.450 --> 00:19:05.730
So this is, again, the
error at set K plus 1,

00:19:05.730 --> 00:19:11.940
and it's 1 over 2xK times
this thing in brackets--

00:19:11.940 --> 00:19:15.000
9 plus xK squared minus 6xK.

00:19:15.000 --> 00:19:23.235
And we recognize that
as xK minus 3 squared.

00:19:25.980 --> 00:19:34.710
xK squared minus 6 of them plus
9, that's xK minus 3 squared.

00:19:34.710 --> 00:19:40.370
OK, that was the
goal, of course.

00:19:40.370 --> 00:19:44.390
That's the goal that shows why
Newton's method is fantastic.

00:19:44.390 --> 00:19:48.080
If you can execute it, if
you can start near enough,

00:19:48.080 --> 00:19:50.850
notice that--

00:19:50.850 --> 00:19:54.750
so how do I describe
this great equation?

00:19:54.750 --> 00:19:58.850
It says that the error is
squared at every step, squared

00:19:58.850 --> 00:20:00.770
at every step.

00:20:00.770 --> 00:20:08.132
So if I'm converging to a
limit, it will satisfy the--

00:20:08.132 --> 00:20:15.050
it'll be 3, or I guess
minus 3, is that possible?

00:20:15.050 --> 00:20:17.940
Yeah, minus 3 is
another solution here.

00:20:17.940 --> 00:20:20.810
So we've got two solutions.

00:20:20.810 --> 00:20:22.955
Newton's method
could converge to 3.

00:20:25.585 --> 00:20:29.190
Am I right, it could
converge to minus 3?

00:20:29.190 --> 00:20:34.920
So I'd have a similar equation
sort of centered at minus 3,

00:20:34.920 --> 00:20:37.920
or does it always
do one of those?

00:20:40.710 --> 00:20:41.700
It could blow up.

00:20:44.730 --> 00:20:48.430
So there are sort of
regions of attraction.

00:20:48.430 --> 00:20:51.690
They're all the starting
points that approach 3,

00:20:51.690 --> 00:20:53.190
and the whole point
of that equation

00:20:53.190 --> 00:20:57.870
is with quadratic
convergence the error

00:20:57.870 --> 00:20:59.730
being squared at every step.

00:20:59.730 --> 00:21:01.760
It zooms in on 3.

00:21:01.760 --> 00:21:04.060
Then, there is all
the starting points

00:21:04.060 --> 00:21:07.420
that would go to minus
3, and then there

00:21:07.420 --> 00:21:10.920
are the starting points
that would blow up.

00:21:10.920 --> 00:21:15.170
And those, maybe for
this very simple problem,

00:21:15.170 --> 00:21:17.760
the picture is not too
difficult to sort out

00:21:17.760 --> 00:21:18.675
those three regions.

00:21:21.600 --> 00:21:29.010
And this is allowing for a
vector, two equations or n

00:21:29.010 --> 00:21:31.950
equations, then
we're in n variables,

00:21:31.950 --> 00:21:35.310
and really you get
beautiful pictures.

00:21:35.310 --> 00:21:39.900
You get some of the type
of pictures that gave rise

00:21:39.900 --> 00:21:45.030
to these books on fractals,
picture books on fractals

00:21:45.030 --> 00:21:46.920
for these basins of attraction.

00:21:46.920 --> 00:21:52.410
Does the starting point lead
you to one of the solutions,

00:21:52.410 --> 00:21:54.780
or does it lead you to infinity?

00:21:54.780 --> 00:21:59.790
Here, that would be interesting
to just draw it for this,

00:21:59.790 --> 00:22:03.900
but the essential point is
the quadratic convergence,

00:22:03.900 --> 00:22:05.670
if it's close enough.

00:22:05.670 --> 00:22:09.210
You see that it has to be close.

00:22:09.210 --> 00:22:15.760
If x0 is pretty near 3, then
this is about 1/6 of that,

00:22:15.760 --> 00:22:21.430
and there would be a good region
of attraction in this case.

00:22:21.430 --> 00:22:22.140
OK.

00:22:22.140 --> 00:22:29.640
So that's Newton's
method for equations.

00:22:29.640 --> 00:22:31.860
And now I want to
do Newton's method.

00:22:31.860 --> 00:22:34.230
I just want to convert
all those words over

00:22:34.230 --> 00:22:38.450
to Newton's method
for optimization.

00:22:38.450 --> 00:22:45.810
So remember, these boards
were solving f equals 0.

00:22:45.810 --> 00:22:48.570
This board is
minimizing capital F,

00:22:48.570 --> 00:22:50.860
and what's the
connection between them?

00:22:50.860 --> 00:23:02.200
Well of course, this corresponds
to solving the gradient

00:23:02.200 --> 00:23:04.330
equals 0.

00:23:04.330 --> 00:23:07.420
At a minimum, if
I'm minimizing, I'm

00:23:07.420 --> 00:23:11.750
finding a point where all
the first derivatives are 0.

00:23:11.750 --> 00:23:16.060
So that will be the
match between these.

00:23:16.060 --> 00:23:22.510
This grad F in this picture is
the small f in that picture.

00:23:22.510 --> 00:23:23.010
OK.

00:23:25.740 --> 00:23:28.710
Now, I guess here I have--

00:23:28.710 --> 00:23:33.930
and this is sort of the
heart of our applications

00:23:33.930 --> 00:23:38.070
to deep learning-- we have
very complicated loss functions

00:23:38.070 --> 00:23:40.260
to minimize,
functions of thousands

00:23:40.260 --> 00:23:42.900
or hundreds of
thousands of variables.

00:23:42.900 --> 00:23:44.840
OK.

00:23:44.840 --> 00:23:47.840
So that means that we would
like to use Newton's method,

00:23:47.840 --> 00:23:49.970
but often we can't.

00:23:49.970 --> 00:23:54.380
So I need him to put
down here two methods--

00:23:54.380 --> 00:23:59.060
one that doesn't involve
those high second derivatives

00:23:59.060 --> 00:24:02.750
and Newton's that does.

00:24:02.750 --> 00:24:10.430
So first, I'll write down a
method that does not involve,

00:24:10.430 --> 00:24:16.390
so method one, and this
will be steepest descent.

00:24:25.040 --> 00:24:26.910
And what is that?

00:24:26.910 --> 00:24:35.430
That says that xK plus 1--
the new x is the old x minus--

00:24:35.430 --> 00:24:40.110
steepest descent means that I
move in the steepest direction

00:24:40.110 --> 00:24:45.240
which is the direction
of the gradient of F.

00:24:45.240 --> 00:24:47.520
I move some distance,
and I better

00:24:47.520 --> 00:24:51.360
have freedom to decide what
that distance should be.

00:24:51.360 --> 00:25:01.670
So this is a step size, s, or in
the language of deep learning,

00:25:01.670 --> 00:25:03.830
it's often called
the learning rate,

00:25:03.830 --> 00:25:06.450
so if you see learning rate.

00:25:11.910 --> 00:25:12.410
OK.

00:25:19.000 --> 00:25:23.440
So and it's natural
to choose sK.

00:25:23.440 --> 00:25:27.340
We're going along, do you see
what this right-hand side looks

00:25:27.340 --> 00:25:27.970
like?

00:25:27.970 --> 00:25:31.480
I'm at a point in n dimensions.

00:25:31.480 --> 00:25:34.640
We're in n dimensions here.

00:25:34.640 --> 00:25:37.770
We have functions
of n variables.

00:25:37.770 --> 00:25:39.020
There is a vector.

00:25:39.020 --> 00:25:43.780
There is a direction to
move down the steepest slope

00:25:43.780 --> 00:25:46.360
of the graph.

00:25:46.360 --> 00:25:51.910
And here is a distance to
move, and we will stop.

00:25:51.910 --> 00:25:59.350
We'll have to get off
this step, normally.

00:25:59.350 --> 00:26:02.650
If we stay on it,
it will swing back,

00:26:02.650 --> 00:26:06.910
it'll take us off to infinity.

00:26:06.910 --> 00:26:12.450
You would like to choose
sK so that you minimize

00:26:12.450 --> 00:26:20.080
capital F. You take the point on
this line, so this a line in R

00:26:20.080 --> 00:26:24.350
n, a direction in R n.

00:26:28.820 --> 00:26:33.790
And for all the points on
that line, in that direction,

00:26:33.790 --> 00:26:39.090
F has some value, and what
you expect is that initially,

00:26:39.090 --> 00:26:45.460
because you chose it sensibly,
the value of F will drop.

00:26:45.460 --> 00:26:49.830
But then at a certain point,
it will turn back on you

00:26:49.830 --> 00:26:51.130
and increase.

00:26:51.130 --> 00:26:54.430
So that would be the
natural stopping point.

00:26:54.430 --> 00:26:57.460
I would call that an
exact line search.

00:26:57.460 --> 00:27:08.930
So I exact line search would be,
exact line search is the best

00:27:08.930 --> 00:27:09.430
s.

00:27:11.950 --> 00:27:16.390
Of course, that would
take time to compute,

00:27:16.390 --> 00:27:19.360
and you probably,
in deep learning,

00:27:19.360 --> 00:27:25.690
that's time you can't afford,
so you fix the learning rate s.

00:27:25.690 --> 00:27:29.240
Maybe you choose 0.01
to be pretty safe.

00:27:29.240 --> 00:27:32.800
OK, so that's method
one, steepest descent.

00:27:32.800 --> 00:27:34.800
Now, method two will
be Newton's method.

00:27:43.380 --> 00:27:53.070
So now, we have xK plus 1
equal to xK minus something

00:27:53.070 --> 00:28:01.810
times delta F, and now I'm
going to do the right thing.

00:28:01.810 --> 00:28:05.760
I'm going to live right
here, and the right thing

00:28:05.760 --> 00:28:08.850
is the Hessian, the
second derivative.

00:28:08.850 --> 00:28:10.770
This was cheap.

00:28:10.770 --> 00:28:14.280
We just took the direction
and went along it.

00:28:14.280 --> 00:28:17.850
Now, we're getting really
the right direction

00:28:17.850 --> 00:28:23.850
by using the second derivative,
so that's H inverse.

00:28:23.850 --> 00:28:32.430
OK, and what I've
done is to set that 0.

00:28:37.900 --> 00:28:40.090
Do you see that's
Newton's method?

00:28:40.090 --> 00:28:44.110
It's totally
parallel to this guy.

00:28:44.110 --> 00:28:48.850
Actually, I'm really happy to
have these two on the board

00:28:48.850 --> 00:28:52.540
parallel to each other, because
you have to keep straight,

00:28:52.540 --> 00:28:56.800
are you solving equations, or
are you minimizing functions?

00:28:56.800 --> 00:29:00.430
And you're using different
letters in the two problems,

00:29:00.430 --> 00:29:02.850
but now you see how they match.

00:29:02.850 --> 00:29:08.680
The Jacobian of-- so
again the matches,

00:29:08.680 --> 00:29:13.600
think of f as the
gradient of F. That's

00:29:13.600 --> 00:29:16.510
the way you should think of it.

00:29:16.510 --> 00:29:24.940
So the Jacobian of the
gradient is the Hessian.

00:29:24.940 --> 00:29:27.880
The Jacobian of the
gradient is the Hessian,

00:29:27.880 --> 00:29:30.790
and that makes sense,
because the first derivative

00:29:30.790 --> 00:29:33.460
of the first derivative
is the second derivative.

00:29:33.460 --> 00:29:39.640
Only we're doing matrix y, so
the Jacobian of the gradient--

00:29:39.640 --> 00:29:41.620
we're doing a vector
matrix sentence

00:29:41.620 --> 00:29:46.090
instead of a scalar sentence--
the Jacobian of the gradient

00:29:46.090 --> 00:29:48.110
is a Hessian.

00:29:48.110 --> 00:29:50.600
Yeah, right.

00:29:50.600 --> 00:29:53.770
OK, so that's what I
wanted to start with,

00:29:53.770 --> 00:29:56.110
just to get those
basic facts down.

00:29:56.110 --> 00:30:03.280
And so the basic facts were
the three-term Taylor series.

00:30:03.280 --> 00:30:09.400
And then the basic algorithms
followed naturally from it

00:30:09.400 --> 00:30:13.420
by setting f F at
the new point to 0,

00:30:13.420 --> 00:30:16.010
if that's what you were
solving or by assuming

00:30:16.010 --> 00:30:17.020
you had the minimum.

00:30:17.020 --> 00:30:19.420
Right, good, good, good, good.

00:30:19.420 --> 00:30:21.100
OK.

00:30:21.100 --> 00:30:22.900
Now, what?

00:30:22.900 --> 00:30:26.380
Now, we have to think about
solving these problems,

00:30:26.380 --> 00:30:27.730
studying.

00:30:27.730 --> 00:30:29.170
Do they converge?

00:30:29.170 --> 00:30:30.970
What rate do they converge?

00:30:30.970 --> 00:30:35.770
Well, the rate of
convergence is like why

00:30:35.770 --> 00:30:39.410
I separated off this example.

00:30:39.410 --> 00:30:41.800
So the convergence rate
for Newton's method

00:30:41.800 --> 00:30:43.840
will be quadratic.

00:30:43.840 --> 00:30:47.980
The error gets squared,
and of course, that

00:30:47.980 --> 00:30:53.160
means super-fast convergence,
if you start and close enough.

00:30:53.160 --> 00:30:55.860
The rate of convergence
for a steepest descent

00:30:55.860 --> 00:30:57.600
is, of course, not.

00:30:57.600 --> 00:31:00.480
You're not squaring errors
here, because you're just

00:31:00.480 --> 00:31:03.630
taking some number
instead of the inverse

00:31:03.630 --> 00:31:12.610
of the correct matrix, so
you can't expect super speed.

00:31:12.610 --> 00:31:15.810
So a linear rate of
convergence would be right.

00:31:18.430 --> 00:31:21.240
You would like to
know that the error is

00:31:21.240 --> 00:31:26.850
multiplied at every step
by some constant below 1.

00:31:26.850 --> 00:31:29.400
That would be a
linear rate compared

00:31:29.400 --> 00:31:35.420
to being squared at every step.

00:31:35.420 --> 00:31:42.310
OK, and so this will
be our basic formula

00:31:42.310 --> 00:31:50.830
that we build on for really
large scale problems.

00:31:50.830 --> 00:31:53.590
And there are
methods, of course,

00:31:53.590 --> 00:31:57.790
people are going to come up
with methods that they're sort

00:31:57.790 --> 00:32:01.260
of a cheap Newton's method.

00:32:01.260 --> 00:32:05.020
Levenberg-Marquardt,
and it's in the notes

00:32:05.020 --> 00:32:09.850
at the end of this
section, at the end of 6.4

00:32:09.850 --> 00:32:11.105
that we'll get to.

00:32:11.105 --> 00:32:16.130
So Levenberg-Marquardt is a sort
of cheap man's Newton's method.

00:32:16.130 --> 00:32:22.180
It does not compute to
Hessian, but it says, OK,

00:32:22.180 --> 00:32:27.160
from the gradient, I can
see one term in the Hessian.

00:32:27.160 --> 00:32:34.570
So it grabs that term, but
it's not fully second order.

00:32:34.570 --> 00:32:36.270
OK.

00:32:36.270 --> 00:32:40.860
So now, we have to
think about problems,

00:32:40.860 --> 00:32:45.360
and I guess the message here
is, at our starting point,

00:32:45.360 --> 00:32:49.100
has to be convexity.

00:32:49.100 --> 00:32:55.090
Convexity is the key
word for these problems,

00:32:55.090 --> 00:32:57.740
for the function that
we want to minimize.

00:32:57.740 --> 00:33:01.800
If that's a convex
function, well first of all,

00:33:01.800 --> 00:33:07.570
the convex function is
likely to have one minimum.

00:33:07.570 --> 00:33:13.850
And the picture that's in
our mind of steepest descent,

00:33:13.850 --> 00:33:17.410
this picture of a bowl,
a bowl is the graph

00:33:17.410 --> 00:33:19.480
of the convex function.

00:33:19.480 --> 00:33:21.970
So I'm turning to convexity now.

00:33:21.970 --> 00:33:28.260
I'll leave that board there,
because that's pretty crucial,

00:33:28.260 --> 00:33:32.010
and speak about the
idea of convexity.

00:33:32.010 --> 00:33:39.960
Convex function, convex
set, so let's call

00:33:39.960 --> 00:33:45.540
the function f of x,
and a typical convex set

00:33:45.540 --> 00:33:50.590
would be I'll call it K. OK.

00:33:50.590 --> 00:33:54.760
So we just want to remember what
does that word can convex mean,

00:33:54.760 --> 00:33:57.910
and how do you know if
you have a convex function

00:33:57.910 --> 00:33:59.470
or a convex set?

00:33:59.470 --> 00:34:01.930
OK, let me start
with convex set.

00:34:01.930 --> 00:34:05.290
So because here is
my general problem,

00:34:05.290 --> 00:34:16.960
my convex minimization,
which you hope to have,

00:34:16.960 --> 00:34:19.480
and in many applications,
you do have.

00:34:19.480 --> 00:34:26.199
So you minimize
a convex function

00:34:26.199 --> 00:34:31.330
for points in a convex set.

00:34:31.330 --> 00:34:34.570
So that's like the
ideal situation.

00:34:34.570 --> 00:34:38.230
That's the ideal
situation, to get something

00:34:38.230 --> 00:34:41.590
on your side, something
powerful, convexity.

00:34:41.590 --> 00:34:44.409
The function is convex,
and you say, well,

00:34:44.409 --> 00:34:47.710
let me draw a convex
function, the graph.

00:34:47.710 --> 00:34:54.980
OK, so I'll draw a convex
function, say a bowl.

00:34:54.980 --> 00:35:00.640
So that's a graph of f of x,
and then here are the x's.

00:35:00.640 --> 00:35:08.800
Let me maybe put x1 and x2 in
the base and the graph of f

00:35:08.800 --> 00:35:11.680
of 1x x2 up here.

00:35:16.110 --> 00:35:16.610
OK.

00:35:16.610 --> 00:35:17.820
Actually, I'm over there.

00:35:17.820 --> 00:35:21.340
I should be calling this
function F, I think.

00:35:24.268 --> 00:35:25.732
Is that right?

00:35:29.160 --> 00:35:33.220
Yeah, a little f would be
the gradient of this guy.

00:35:33.220 --> 00:35:34.000
Yeah, I think so.

00:35:38.543 --> 00:35:39.043
OK.

00:35:43.200 --> 00:35:49.120
Now, I'm minimizing it over
certain x's, not all x's.

00:35:49.120 --> 00:35:55.750
I might be minimizing,
for example,

00:35:55.750 --> 00:36:05.230
K might be the set where
Ax equals B. K might be,

00:36:05.230 --> 00:36:09.460
in that case, a subspace
or a shifted subspace.

00:36:09.460 --> 00:36:14.350
I said subspace, but then 18.06
is reminding me in my mind

00:36:14.350 --> 00:36:16.710
that I only have a
subspace when B is 0.

00:36:19.330 --> 00:36:23.530
You know the word for a subspace
that's sort of moved over?

00:36:23.530 --> 00:36:27.110
Affine, so I'll just
put that word down here.

00:36:30.190 --> 00:36:35.200
Bunch of words to
learn for this topic,

00:36:35.200 --> 00:36:36.760
but they're worth learning.

00:36:36.760 --> 00:36:37.630
OK.

00:36:37.630 --> 00:36:42.700
So it's like a plane but not
necessarily through the origin.

00:36:42.700 --> 00:36:44.640
If B is 0, it doesn't
go through it.

00:36:44.640 --> 00:36:46.980
If B it's not 0, it doesn't
go through the origin.

00:36:46.980 --> 00:36:47.480
OK.

00:36:47.480 --> 00:36:49.940
Anyway, or I have
some other convex set.

00:36:49.940 --> 00:36:57.519
Let me just put this convex
set K in the base for you,

00:36:57.519 --> 00:36:59.470
and did I make it convex?

00:36:59.470 --> 00:37:04.060
I think pretty luckily I did.

00:37:04.060 --> 00:37:05.200
So now what's the?

00:37:07.840 --> 00:37:11.540
Well, the convex
sets the constraint,

00:37:11.540 --> 00:37:13.840
so this is the constraint set.

00:37:13.840 --> 00:37:23.290
Constraint is that x
must be in the set K. OK,

00:37:23.290 --> 00:37:28.120
and I drew it as a convex blob.

00:37:28.120 --> 00:37:32.770
Here was an example
where it's flat, not

00:37:32.770 --> 00:37:36.110
a blob but a flat plane.

00:37:36.110 --> 00:37:41.020
But let me come back to
what does convex mean.

00:37:41.020 --> 00:37:42.450
What's a convex set?

00:37:42.450 --> 00:37:47.430
Yeah, we have to do that,
should have done that before.

00:37:55.140 --> 00:38:00.040
In the notes, I had the
fun of figuring out,

00:38:00.040 --> 00:38:04.870
if I took a triangle,
is that a convex set?

00:38:04.870 --> 00:38:05.830
Let's just be sure.

00:38:09.640 --> 00:38:11.530
So what's a convex set?

00:38:11.530 --> 00:38:14.020
That is a convex
set, because if I

00:38:14.020 --> 00:38:19.120
take any two points in the set
and draw the line between them,

00:38:19.120 --> 00:38:21.710
it stays in the set.

00:38:21.710 --> 00:38:31.370
So that's convexity,
any edge, line, from x1

00:38:31.370 --> 00:38:37.970
to x2 stays in the set.

00:38:37.970 --> 00:38:39.920
OK, good.

00:38:39.920 --> 00:38:43.380
So here's my little
exercise to myself.

00:38:43.380 --> 00:38:46.720
What if I took the
union of two triangles?

00:38:46.720 --> 00:38:52.760
All I want to get you to do is
just visualize convex and not

00:38:52.760 --> 00:38:54.620
convex possibilities.

00:38:54.620 --> 00:39:02.120
Suppose I have one triangle,
even if it was obtuse,

00:39:02.120 --> 00:39:04.730
that's still convex, right?

00:39:04.730 --> 00:39:06.230
No problem.

00:39:06.230 --> 00:39:08.630
But now what if I put
those two triangles

00:39:08.630 --> 00:39:11.310
together, take their union?

00:39:11.310 --> 00:39:15.740
Well, if I take them sitting
with a big gap between,

00:39:15.740 --> 00:39:17.330
like I've lost.

00:39:17.330 --> 00:39:20.390
I mean, I never had
a chance that way,

00:39:20.390 --> 00:39:24.290
because if it was the
union of these two-- well,

00:39:24.290 --> 00:39:25.880
you know what I'm going to say.

00:39:25.880 --> 00:39:28.100
If I'm doing that point
and that point, of course,

00:39:28.100 --> 00:39:30.270
it goes outside and stupid.

00:39:30.270 --> 00:39:30.770
All right.

00:39:36.140 --> 00:39:41.150
What if what if that
triangle, that lower triangle,

00:39:41.150 --> 00:39:42.980
overlaps the upper triangle?

00:39:42.980 --> 00:39:44.240
Is that a convex set?

00:39:47.000 --> 00:39:49.130
Everybody's right saying no.

00:39:49.130 --> 00:39:53.240
Why how do I see that the
union of those two triangles

00:39:53.240 --> 00:39:56.200
is not a convex set?

00:39:56.200 --> 00:39:59.530
Guys, you tell me where
to pick two points,

00:39:59.530 --> 00:40:00.940
where the line goes out.

00:40:00.940 --> 00:40:03.700
Well, I take one from
that corner and one

00:40:03.700 --> 00:40:08.260
from that corner, and the line
between them went outside.

00:40:08.260 --> 00:40:17.305
So union is usually not convex.

00:40:21.990 --> 00:40:24.240
Well, if I think of
the union of two sets,

00:40:24.240 --> 00:40:29.560
my mind automatically goes
to the other corresponding

00:40:29.560 --> 00:40:36.540
possibility which is the
intersection of the two sets.

00:40:36.540 --> 00:40:39.384
So if I take the
intersection of two sets.

00:40:43.680 --> 00:40:46.890
Now, what's the deal with that?

00:40:46.890 --> 00:40:50.670
When I had two triangles,
two separated triangles,

00:40:50.670 --> 00:40:54.780
what can we say about the
intersection of those two

00:40:54.780 --> 00:40:55.440
triangles?

00:40:55.440 --> 00:40:56.790
AUDIENCE: [INAUDIBLE]

00:40:56.790 --> 00:40:58.510
GILBERT STRANG: It's empty.

00:40:58.510 --> 00:41:02.380
So should we regard the
empty set as a convex set?

00:41:02.380 --> 00:41:03.950
Yes.

00:41:03.950 --> 00:41:04.797
Isn't it?

00:41:04.797 --> 00:41:06.290
AUDIENCE: Yeah, it's vacuous.

00:41:06.290 --> 00:41:09.770
GILBERT STRANG: Vacuous, so
it hasn't got any problems.

00:41:09.770 --> 00:41:11.430
Right?

00:41:11.430 --> 00:41:18.770
OK, but now the intersection
is always convex.

00:41:18.770 --> 00:41:22.640
I'm assuming the two sets
that we start with are.

00:41:22.640 --> 00:41:26.640
Now, that's an important fact,
that the intersection of convex

00:41:26.640 --> 00:41:27.140
sets.

00:41:27.140 --> 00:41:30.305
Let's just draw a picture
that shows an example.

00:41:34.160 --> 00:41:35.840
So what's the intersection?

00:41:35.840 --> 00:41:39.560
Just this part and it's convex.

00:41:39.560 --> 00:41:42.480
OK, can you give
me a little proof

00:41:42.480 --> 00:41:48.670
that the intersection is convex?

00:41:48.670 --> 00:41:51.060
So I take two points
in the intersection--

00:41:51.060 --> 00:41:54.070
let me start the proof.

00:41:54.070 --> 00:41:57.550
To test if something's
convex, how do you test it?

00:41:57.550 --> 00:42:01.570
You take two points in the
set in the intersection,

00:42:01.570 --> 00:42:03.580
and you want to show that
the line between them

00:42:03.580 --> 00:42:06.850
is in the intersection.

00:42:06.850 --> 00:42:08.420
OK, why is that?

00:42:08.420 --> 00:42:14.350
So take two points, take
x1 in the intersection.

00:42:14.350 --> 00:42:17.530
We've got two sets
here, and that's

00:42:17.530 --> 00:42:19.870
the symbol for
intersection, and we've

00:42:19.870 --> 00:42:22.017
got another point
in the intersection.

00:42:24.640 --> 00:42:27.950
And now, we want to look
at the line between them,

00:42:27.950 --> 00:42:33.100
the line from x1 to 2x.

00:42:33.100 --> 00:42:35.236
What's the deal with that one?

00:42:38.070 --> 00:42:41.038
Is that fully in K1?

00:42:41.038 --> 00:42:42.830
AUDIENCE: Yes.

00:42:42.830 --> 00:42:45.510
GILBERT STRANG: Why
is it fully in K1?

00:42:45.510 --> 00:42:48.830
I took two points
in the intersection,

00:42:48.830 --> 00:42:52.850
I'm looking at the line
between them, and I'm asking,

00:42:52.850 --> 00:42:54.680
is it in the first set K1?

00:42:54.680 --> 00:42:57.950
And the answer is yes,
because those points

00:42:57.950 --> 00:43:02.010
were in K1, and K1's convex.

00:43:02.010 --> 00:43:05.520
And is that line
between them in K2?

00:43:05.520 --> 00:43:11.550
Yes, same reason, the
two endpoints were in K2,

00:43:11.550 --> 00:43:13.890
so the line between
them is in K2.

00:43:13.890 --> 00:43:17.490
So the intersection of
convex sets is always convex.

00:43:17.490 --> 00:43:22.990
The intersection of
convex sets is convex.

00:43:22.990 --> 00:43:23.490
Good.

00:43:26.420 --> 00:43:29.570
So you'll see in the
note these possibilities

00:43:29.570 --> 00:43:31.560
with two triangles.

00:43:31.560 --> 00:43:35.770
Sometimes, you can take the
union but not very often.

00:43:35.770 --> 00:43:36.590
OK.

00:43:36.590 --> 00:43:41.130
Now, what's the next
thing I have to do?

00:43:41.130 --> 00:43:45.230
Convex functions,
we got convex sets,

00:43:45.230 --> 00:43:48.590
what are convex
functions, and we're good.

00:43:48.590 --> 00:43:53.140
Because this is our
prototype of a problem,

00:43:53.140 --> 00:43:57.930
and I now want to know what
it means for that F to be--

00:43:57.930 --> 00:43:58.640
oh, I'm sorry.

00:43:58.640 --> 00:44:03.920
I now know what it means for
the set K to be convex set,

00:44:03.920 --> 00:44:07.525
but now I have to look at the
other often more important part

00:44:07.525 --> 00:44:08.150
of the problem.

00:44:08.150 --> 00:44:10.550
What's the function
I'm minimizing,

00:44:10.550 --> 00:44:15.110
and I'm looking for functions
with this kind of a picture.

00:44:15.110 --> 00:44:16.610
OK.

00:44:16.610 --> 00:44:22.580
The coolest way is to connect
the definition of a convex

00:44:22.580 --> 00:44:28.340
function to the definition
of a convex set.

00:44:28.340 --> 00:44:31.820
This is really the nicest way.

00:44:31.820 --> 00:44:32.780
It's a little quick.

00:44:32.780 --> 00:44:34.610
It just swishes by you.

00:44:34.610 --> 00:44:38.240
But tell me, do you see a
convex set in that picture?

00:44:38.240 --> 00:44:40.930
[INAUDIBLE]

00:44:40.930 --> 00:44:43.550
You see a convex
set in that picture.

00:44:43.550 --> 00:44:46.300
That's the picture of a
graph of a convex function.

00:44:46.300 --> 00:44:48.850
It's a picture of a bowl.

00:44:48.850 --> 00:44:53.400
Are the points on that
surface, is that a convex set?

00:44:53.400 --> 00:44:55.650
No, certainly not.

00:44:55.650 --> 00:45:03.310
No, but where is a convex set to
be found here, in that picture?

00:45:03.310 --> 00:45:04.290
Yes.

00:45:04.290 --> 00:45:07.230
AUDIENCE: The set of y, if y
is greater than [INAUDIBLE]

00:45:07.230 --> 00:45:11.210
GILBERT STRANG: Yes, the
points on and above the bowl,

00:45:11.210 --> 00:45:16.470
inside the bowl, we
could say, these points.

00:45:16.470 --> 00:45:21.600
So convex function,
yes, a function's

00:45:21.600 --> 00:45:37.320
convex when the points on and
above the graph are convex set.

00:45:48.650 --> 00:45:51.110
You could say,
OK, mathematicians

00:45:51.110 --> 00:45:52.100
are just being lazy.

00:45:52.100 --> 00:45:55.190
Having got one definition
straight for a convex set,

00:45:55.190 --> 00:45:59.045
now they're just using that
to give an easy definition

00:45:59.045 --> 00:46:00.930
of a convex function.

00:46:00.930 --> 00:46:04.190
Actually, it's quite
useful for functions

00:46:04.190 --> 00:46:09.080
that could maybe equal infinity,
sort of generalized functions.

00:46:09.080 --> 00:46:15.260
But it's not the quickest way to
tell if the function is convex.

00:46:15.260 --> 00:46:19.070
It's not our usual test
for convex functions.

00:46:19.070 --> 00:46:21.520
So now I want to
give such a test.

00:46:21.520 --> 00:46:22.020
OK.

00:46:24.740 --> 00:46:36.030
So now, the definition of convex
function, of a smooth convex,

00:46:36.030 --> 00:46:36.530
yeah.

00:46:39.640 --> 00:46:45.190
This fact, I shouldn't
rush off away from it,

00:46:45.190 --> 00:46:51.560
from the definition
of a convex function

00:46:51.560 --> 00:46:55.020
as having a convex
set above its graph.

00:46:55.020 --> 00:46:59.450
The really official French name
for the set above the graph

00:46:59.450 --> 00:47:03.780
is the epigraph, but I won't
even write that word down.

00:47:03.780 --> 00:47:05.690
OK.

00:47:05.690 --> 00:47:08.300
Why do I come back
to that for a minute?

00:47:08.300 --> 00:47:16.720
Because I would like to think
about two functions, F1 and F2.

00:47:16.720 --> 00:47:18.640
Out of two functions,
I can always

00:47:18.640 --> 00:47:23.140
create the minimum
or the maximum.

00:47:23.140 --> 00:47:28.795
So suppose I have to convex
functions, convex function

00:47:28.795 --> 00:47:30.535
F1 and F2.

00:47:34.440 --> 00:47:35.050
OK.

00:47:35.050 --> 00:47:38.840
Then, I could choose a minimum.

00:47:38.840 --> 00:47:41.450
I could choose my new function.

00:47:41.450 --> 00:47:44.090
Shall I call it
little m for minimum?

00:47:44.090 --> 00:47:51.340
m of x is the
minimum of F1 and F2.

00:47:51.340 --> 00:47:54.380
And I could choose
a maximum function

00:47:54.380 --> 00:47:59.330
which would be the
maximum of F1 of x and F2

00:47:59.330 --> 00:48:03.870
of x at the same point x.

00:48:03.870 --> 00:48:07.280
It's just a natural to think,
OK, I have two functions.

00:48:07.280 --> 00:48:11.212
I've got a bowl and
I've got another bowl,

00:48:11.212 --> 00:48:12.545
and suppose they're both convex.

00:48:18.030 --> 00:48:20.472
So I'm just stretching
you to think here.

00:48:20.472 --> 00:48:25.060
If I've got the graphs
of two convex functions,

00:48:25.060 --> 00:48:29.560
and I would like to consider the
minimum of those two functions

00:48:29.560 --> 00:48:32.150
and also the maximum
of those two functions.

00:48:32.150 --> 00:48:34.730
I believe life is good.

00:48:34.730 --> 00:48:38.830
One of these will be
convex, and the other won't.

00:48:38.830 --> 00:48:41.980
And can you identify
which one is convex

00:48:41.980 --> 00:48:43.300
and which one is not convex?

00:48:46.310 --> 00:48:49.100
What about the minimum?

00:48:49.100 --> 00:48:50.690
Is that a convex function?

00:48:50.690 --> 00:48:52.470
So just look at the graph.

00:48:52.470 --> 00:48:54.110
What does the minimum look like?

00:48:54.110 --> 00:48:58.250
The minimum is this guy
until they meet somehow

00:48:58.250 --> 00:49:01.190
on some surface
and then this guy.

00:49:01.190 --> 00:49:02.240
Is that convex?

00:49:02.240 --> 00:49:05.900
We have like one minute
to answer that question.

00:49:05.900 --> 00:49:07.160
Absolutely no.

00:49:07.160 --> 00:49:10.370
It's got this bad kink in it.

00:49:10.370 --> 00:49:14.220
What about the maximum
of the two functions?

00:49:14.220 --> 00:49:16.490
So the maximum is the
one that is above,

00:49:16.490 --> 00:49:22.150
all the points or things
that are above or on.

00:49:26.010 --> 00:49:27.770
There is the maximum function.

00:49:27.770 --> 00:49:29.990
That was the minimum function.

00:49:29.990 --> 00:49:31.460
It had a kink.

00:49:31.460 --> 00:49:33.380
The maximum function
is like that,

00:49:33.380 --> 00:49:41.530
and it is convex, so
maximum yes, minimum no.

00:49:44.310 --> 00:49:50.740
OK, and we could have a
maximum of 1,500 functions.

00:49:50.740 --> 00:49:53.800
If the 1,500 functions
are all convex,

00:49:53.800 --> 00:49:55.600
the maximum will
be, because it's

00:49:55.600 --> 00:50:00.520
the part way above
everybody's graph,

00:50:00.520 --> 00:50:03.580
and that would be the
graph of the maximum.

00:50:03.580 --> 00:50:05.410
OK, good.

00:50:05.410 --> 00:50:11.260
And now finally, let me
just say, how do you know

00:50:11.260 --> 00:50:13.480
whether a function is convex?

00:50:13.480 --> 00:50:16.570
How to test, how of test.

00:50:20.330 --> 00:50:24.890
OK, so let me take just a
function of one variable.

00:50:24.890 --> 00:50:28.130
What's the test you learned
in calculus, freshman

00:50:28.130 --> 00:50:36.360
calculus actually, just show
that this is a convex function?

00:50:36.360 --> 00:50:38.542
What's the test for that?

00:50:38.542 --> 00:50:40.067
AUDIENCE: Use second derivative.

00:50:40.067 --> 00:50:41.900
GILBERT STRANG: Second
derivative should be?

00:50:41.900 --> 00:50:42.692
AUDIENCE: Positive.

00:50:42.692 --> 00:50:45.200
GILBERT STRANG:
Positive or possibly 0,

00:50:45.200 --> 00:50:53.360
so second derivative greater
or equals 0 everywhere.

00:50:53.360 --> 00:50:54.360
That's convex.

00:50:57.480 --> 00:51:02.920
OK, final question,
suppose F is a vector.

00:51:02.920 --> 00:51:09.370
So this is a vector, and so I
have n functions of n variable.

00:51:09.370 --> 00:51:10.020
No, I don't.

00:51:10.020 --> 00:51:14.030
I have one, sorry,
I've got one function,

00:51:14.030 --> 00:51:15.490
but I'm in n variables.

00:51:19.510 --> 00:51:21.680
So this was just one.

00:51:21.680 --> 00:51:23.360
What's the test for convexity?

00:51:27.350 --> 00:51:33.080
So it would be passed, for
example, by x1 squared plus x2

00:51:33.080 --> 00:51:35.150
squared.

00:51:35.150 --> 00:51:37.683
Would it be passed by--

00:51:37.683 --> 00:51:39.350
so here would be the
question-- would it

00:51:39.350 --> 00:51:45.830
be passed by x transpose
some symmetric matrix S?

00:51:45.830 --> 00:51:51.000
That would be a quadratic,
a pure quadratic.

00:51:51.000 --> 00:51:53.760
Would it be convex?

00:51:53.760 --> 00:51:54.780
What would be the test?

00:51:57.500 --> 00:52:00.350
I'm looking for an n
dimensional equivalent

00:52:00.350 --> 00:52:04.460
of positive second derivative.

00:52:04.460 --> 00:52:07.370
The n dimensional equivalent
of positive second derivative

00:52:07.370 --> 00:52:12.560
is convexity, and we have to
recognize what's the test.

00:52:12.560 --> 00:52:14.990
So I could apply it
to this function,

00:52:14.990 --> 00:52:20.090
or I could apply it to any
function of n variables.

00:52:24.170 --> 00:52:25.280
It should be OK.

00:52:27.960 --> 00:52:29.220
What's the test here?

00:52:29.220 --> 00:52:33.340
Here, I have a matrix
instead of a number.

00:52:33.340 --> 00:52:38.090
So what's the
requirement going to be?

00:52:38.090 --> 00:52:39.527
Times out, yeah?

00:52:39.527 --> 00:52:43.380
[INAUDIBLE] Positive
definite or semidefinite,

00:52:43.380 --> 00:52:45.440
or semidefinite just as here.

00:52:45.440 --> 00:52:45.940
Yeah.

00:52:45.940 --> 00:52:51.680
So the test is positive,
semidefinite, Hessian.

00:52:54.520 --> 00:52:56.830
And here, the Hessian
is actually that S,

00:52:56.830 --> 00:53:00.700
because the second
derivatives will produce--

00:53:00.700 --> 00:53:02.220
I'll put a 1/2 in there--

00:53:02.220 --> 00:53:06.940
the second derivatives will
produce S equal the Hessian H.

00:53:06.940 --> 00:53:08.590
So here, the S--

00:53:08.590 --> 00:53:11.800
so positive
semidefinite, Hessian

00:53:11.800 --> 00:53:17.920
in general, second derivative
matrix for a quadratic.

00:53:17.920 --> 00:53:19.150
OK.

00:53:19.150 --> 00:53:23.950
So its convex
problems that we're

00:53:23.950 --> 00:53:28.150
going to get farther with.

00:53:28.150 --> 00:53:31.040
We run into no saddle points.

00:53:31.040 --> 00:53:34.220
We run into no local minimum.

00:53:34.220 --> 00:53:36.950
Once we found the minimum,
it's the global minimum.

00:53:36.950 --> 00:53:38.520
These are the good problems.

00:53:38.520 --> 00:53:41.180
OK, again, happy
to see you today,

00:53:41.180 --> 00:53:43.822
and I look forward to Wednesday.

