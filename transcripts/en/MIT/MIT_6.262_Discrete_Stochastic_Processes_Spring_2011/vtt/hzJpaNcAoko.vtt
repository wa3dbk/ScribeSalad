WEBVTT
Kind: captions
Language: en

00:00:00.530 --> 00:00:02.960
The following content is
provided under a Creative

00:00:02.960 --> 00:00:04.370
Commons license.

00:00:04.370 --> 00:00:07.410
Your support will help MIT
OpenCourseWare continue to

00:00:07.410 --> 00:00:11.060
offer high quality educational
resources for free.

00:00:11.060 --> 00:00:13.960
To make a donation or view
additional materials from

00:00:13.960 --> 00:00:17.890
hundreds of MIT courses, visit
MIT OpenCourseWare at

00:00:17.890 --> 00:00:19.140
ocw.mit.edu.

00:00:22.580 --> 00:00:24.800
PROFESSOR: I want to review a
little bit the things we did

00:00:24.800 --> 00:00:28.030
last time, and carry them
just a little bit

00:00:28.030 --> 00:00:30.440
further, in some cases.

00:00:30.440 --> 00:00:34.380
Remember, we're talking about
the Markov processes now.

00:00:34.380 --> 00:00:39.360
We defined a countable state
Markov process in terms of an

00:00:39.360 --> 00:00:43.090
embedded Markov chain
and in each state an

00:00:43.090 --> 00:00:45.930
embedded Markov chain.

00:00:45.930 --> 00:00:49.610
There's a parameter nu sub i,
which is the rate of an

00:00:49.610 --> 00:00:54.070
exponential process for
leaving that state.

00:00:54.070 --> 00:00:58.100
And that describes what the
Markov process does because

00:00:58.100 --> 00:01:00.650
knowing what the Markov chain
does and knowing what these

00:01:00.650 --> 00:01:04.900
exponential random variables do
that gives you everything

00:01:04.900 --> 00:01:10.470
you need, if you will, to
simulate this process.

00:01:10.470 --> 00:01:13.200
It gives you, in principle,
everything you need to

00:01:13.200 --> 00:01:16.880
calculate everything you want to
know about what's going on

00:01:16.880 --> 00:01:18.130
in the process.

00:01:18.130 --> 00:01:21.760
So if you remember, we
found out that the--

00:01:21.760 --> 00:01:26.030
we essentially defined steady
state probabilities for the

00:01:26.030 --> 00:01:31.100
process in terms of the steady
state probabilities for the

00:01:31.100 --> 00:01:32.410
Markov chain.

00:01:32.410 --> 00:01:36.420
And what we were doing there
is to first, restrict

00:01:36.420 --> 00:01:42.270
ourselves to the case where the
Markov chain is positive--

00:01:42.270 --> 00:01:44.670
well, at least, is recurrent.

00:01:44.670 --> 00:01:47.150
But let's imagine this positive
recurrence so we

00:01:47.150 --> 00:01:49.170
don't have to worry about those
distinctions for the

00:01:49.170 --> 00:01:51.040
time being.

00:01:51.040 --> 00:01:55.140
If it's positive recurrent
there's a set of pi sub j,

00:01:55.140 --> 00:02:00.260
which gives the steady state
probability that a transition,

00:02:00.260 --> 00:02:05.760
over the long term, is going
to go into state j.

00:02:05.760 --> 00:02:08.740
If you look at the process
overall time, if you look at a

00:02:08.740 --> 00:02:13.470
sample path of it, with
probability one pi sub j, is a

00:02:13.470 --> 00:02:17.010
fraction of transitions
that go into state j.

00:02:17.010 --> 00:02:20.450
Since when you're in state j,
the expected amount of time

00:02:20.450 --> 00:02:23.830
that you stay in state
j is 1 over nu sub j.

00:02:23.830 --> 00:02:28.320
Nu sub j is the rate of an
exponential random variable so

00:02:28.320 --> 00:02:32.560
the expected value of that
random variable is 1 over nu

00:02:32.560 --> 00:02:40.740
sub j, so the fraction of time
that you should be in state j

00:02:40.740 --> 00:02:45.730
should be proportional to
pi j over nu sub j.

00:02:45.730 --> 00:02:49.440
But since these fractions have
to add up to 1, we normalize

00:02:49.440 --> 00:02:55.040
it by dividing by the
sum of pi k over--

00:02:55.040 --> 00:02:57.700
pi sub k over nu sub k.

00:02:57.700 --> 00:02:59.830
That's the formula
that we into it.

00:02:59.830 --> 00:03:02.560
We showed last time that this
formula made sense.

00:03:02.560 --> 00:03:06.563
What we showed is that
if you look at the--

00:03:06.563 --> 00:03:10.220
What's that peculiar
thing doing there?

00:03:16.310 --> 00:03:20.040
If you look at the rate at
which transitions are

00:03:20.040 --> 00:03:23.040
occurring in this Markov chain,
which is determined by

00:03:23.040 --> 00:03:27.450
everything going on, but this
rate at which transitions are

00:03:27.450 --> 00:03:32.010
occurring, assuming we start in
state i is equal to 1 over

00:03:32.010 --> 00:03:35.540
the sum over k of pi sub
k over nu sub k.

00:03:35.540 --> 00:03:39.570
Namely, it's the same
denominator over here, and you

00:03:39.570 --> 00:03:43.970
can interpret this in the same
way as you interpret this.

00:03:43.970 --> 00:03:47.180
This is independent of the
state that you start in.

00:03:49.940 --> 00:03:52.700
These probabilities here, these
fractions of time in

00:03:52.700 --> 00:03:55.740
each state, are independent
of where you start.

00:03:55.740 --> 00:03:59.450
So Mi of t is a sample path,
average rate at which

00:03:59.450 --> 00:04:03.410
transitions occur with
probability 1.

00:04:03.410 --> 00:04:10.710
Namely, no matter where you
start, and no matter what

00:04:10.710 --> 00:04:14.440
sample path you're looking at
with probability one, what you

00:04:14.440 --> 00:04:19.010
wind up with is a number of
transitions up to time t

00:04:19.010 --> 00:04:24.130
divided by t, which goes to a
limit of probability one, and

00:04:24.130 --> 00:04:26.380
it's independent of the
starting state.

00:04:26.380 --> 00:04:37.700
OK, so if this sum here is
infinite then something

00:04:37.700 --> 00:04:42.190
peculiar is happening because
when this sum is infinite,

00:04:42.190 --> 00:04:45.610
even though the pi sub j's are
positive, even though the

00:04:45.610 --> 00:04:50.720
embedded Markov chain is
positive recurrent, these

00:04:50.720 --> 00:04:53.230
fractions of time in
each state don't

00:04:53.230 --> 00:04:55.530
make any sense anymore.

00:04:55.530 --> 00:05:01.690
So that in fact, what you have
is a Markov process which is

00:05:01.690 --> 00:05:03.900
getting slower and slower
and slower.

00:05:07.380 --> 00:05:09.620
That's what this equals
infinity means.

00:05:09.620 --> 00:05:13.180
It means that these pi sub k's,
the probability of being

00:05:13.180 --> 00:05:15.690
in certain states
with very high--

00:05:19.440 --> 00:05:24.200
If the probability is being high
of states where you spend

00:05:24.200 --> 00:05:27.350
a long time in those states
and that sum adds up to

00:05:27.350 --> 00:05:31.660
infinity, this transition
rate is equal to zero.

00:05:31.660 --> 00:05:34.190
It means that the transitions
rates are getting slower and

00:05:34.190 --> 00:05:39.470
slower and slower, we showed an
example of that of an mm1q.

00:05:39.470 --> 00:05:43.940
Well, it wasn't quite mm1, but
it was like an mm1q, but it

00:05:43.940 --> 00:05:50.040
had the property that as you
moved up to higher and higher

00:05:50.040 --> 00:05:54.880
states, both arrivals got slower
and slower, and also

00:05:54.880 --> 00:05:56.780
the service time got
slower and slower.

00:05:56.780 --> 00:06:01.160
We called this a rattled
server, and

00:06:01.160 --> 00:06:02.680
also discouraged arrival.

00:06:02.680 --> 00:06:05.820
So with the two of those things,
as you move up to

00:06:05.820 --> 00:06:10.050
higher and higher states, what
happens is transitions get

00:06:10.050 --> 00:06:12.990
slower and slower and
eventually, you settle down in

00:06:12.990 --> 00:06:16.890
a situation where nothing
is getting done.

00:06:16.890 --> 00:06:18.440
And that's because
of the sum here.

00:06:18.440 --> 00:06:20.840
It's not because of anything
peculiar in

00:06:20.840 --> 00:06:22.120
any one given state.

00:06:27.580 --> 00:06:28.780
Let's see, where we're we?

00:06:28.780 --> 00:06:31.960
OK, if this is equal to
infinity, the transition rate

00:06:31.960 --> 00:06:35.160
goes to 0 and the process has
no meaningful steady state.

00:06:35.160 --> 00:06:37.430
Otherwise, the steady
state uniquely

00:06:37.430 --> 00:06:39.340
satisfies this equation.

00:06:39.340 --> 00:06:42.060
That's what we showed
less time.

00:06:42.060 --> 00:06:45.150
This says that the rate in
equals the rate out for each

00:06:45.150 --> 00:06:48.760
state, that's what this
equation says.

00:06:48.760 --> 00:06:52.620
This quantity over here p sub
j is the probability that

00:06:52.620 --> 00:06:56.480
you're in state j, at any
given time, nu sub j is

00:06:56.480 --> 00:06:59.390
expected amount of time
you stay in there.

00:06:59.390 --> 00:07:08.710
So this is the, in a sense, this
product here corresponds

00:07:08.710 --> 00:07:11.840
to the rate in state j.

00:07:11.840 --> 00:07:14.870
The sum over here corresponds
to the rate at which you're

00:07:14.870 --> 00:07:16.880
going into state j.

00:07:16.880 --> 00:07:20.740
All of these P's are greater
than 0, and the sum of the P's

00:07:20.740 --> 00:07:22.600
are equal to 1.

00:07:22.600 --> 00:07:26.200
This is the case though we
normally call a positive

00:07:26.200 --> 00:07:27.290
recurrent process.

00:07:27.290 --> 00:07:30.250
I notice I didn't define
that in the notes.

00:07:30.250 --> 00:07:32.700
I now notice, thinking about
it even more, that I'm not

00:07:32.700 --> 00:07:36.100
sure I want to define it that
way, but anyway that's what

00:07:36.100 --> 00:07:37.060
it's usually called.

00:07:37.060 --> 00:07:39.540
It's called a positive
recurrent process.

00:07:39.540 --> 00:07:46.290
It requires both that this is
less than infinity and the

00:07:46.290 --> 00:07:51.580
steady state probabilities
pi sub j have a solution.

00:07:51.580 --> 00:07:54.510
If you have a birth/death
process, you can deal with the

00:07:54.510 --> 00:07:58.860
birth/death process in exactly
the same way as we dealt with

00:07:58.860 --> 00:08:00.810
birth/death Markov chains.

00:08:00.810 --> 00:08:05.120
Namely, what goes up must
eventually come down.

00:08:05.120 --> 00:08:10.080
And what that means is a P sub
j, this is the fraction of

00:08:10.080 --> 00:08:12.010
time you're in state j.

00:08:12.010 --> 00:08:15.690
This is the rate at which you
move up, so this combination

00:08:15.690 --> 00:08:19.690
here is the overall rate at
which you're moving up, not

00:08:19.690 --> 00:08:21.720
conditional on being
in state j.

00:08:21.720 --> 00:08:25.460
This is the overall probability
of moving down.

00:08:25.460 --> 00:08:29.960
So the transition rate up, in
any given state, equals the

00:08:29.960 --> 00:08:32.539
transition rate down.

00:08:32.539 --> 00:08:35.309
OK, so that's a useful
equation.

00:08:35.309 --> 00:08:36.845
That's the way you
usually solve

00:08:36.845 --> 00:08:39.159
for birth/death processes.

00:08:39.159 --> 00:08:41.770
The same as you use for solving
for change, you can

00:08:41.770 --> 00:08:46.440
get it from the chain equation
very easily.

00:08:46.440 --> 00:08:49.400
If you have an irreducible
process, an irreducible

00:08:49.400 --> 00:08:52.540
process is one where every state
communicates with every

00:08:52.540 --> 00:08:54.650
other state, remember?

00:08:54.650 --> 00:08:57.190
Same as the definition
for change.

00:08:57.190 --> 00:09:02.000
If there is a solution to these
equations here, mainly,

00:09:02.000 --> 00:09:05.860
these are the equations where
you directly solved for the

00:09:05.860 --> 00:09:07.625
average time in each state.

00:09:12.280 --> 00:09:16.760
The time spent in blah,
blah, blah--

00:09:24.410 --> 00:09:28.850
What this is saying again, is
the rate at which you leave

00:09:28.850 --> 00:09:33.030
state j is what this is, is
equal to the rate at which you

00:09:33.030 --> 00:09:35.770
enter state j, which
is what this is.

00:09:35.770 --> 00:09:40.760
So this again, is saying what
comes in must go out.

00:09:40.760 --> 00:09:43.640
And if this equation is
satisfied, then the embedded

00:09:43.640 --> 00:09:49.700
chain is positive recurrent and
instead of solving for the

00:09:49.700 --> 00:09:52.830
P's from the pi's, you're
solving for the pi's from the

00:09:52.830 --> 00:09:55.400
P. It's the same kind
of equation.

00:09:55.400 --> 00:09:59.995
Notice before P sub j was equal
to pi sub j over nu sub

00:09:59.995 --> 00:10:02.030
j normalized.

00:10:02.030 --> 00:10:04.995
Now, pi sub j is equal
to a P sub j, nu sub

00:10:04.995 --> 00:10:06.930
j normalized again.

00:10:06.930 --> 00:10:09.910
And you normalize this because
the rate at which transitions

00:10:09.910 --> 00:10:12.950
are taking place is
normally not 1.

00:10:17.660 --> 00:10:23.480
We also found that this rate
at which transitions are

00:10:23.480 --> 00:10:27.570
taking place, 1 over this
quantity, is also equal to

00:10:27.570 --> 00:10:28.420
this sum here.

00:10:28.420 --> 00:10:32.490
So the sum of Pj nu sub j is
the overall rate at which

00:10:32.490 --> 00:10:35.040
transitions are taking place.

00:10:35.040 --> 00:10:40.370
Now, if the sum of nu i times
p sub i is infinite then,

00:10:40.370 --> 00:10:44.740
according to this formula, each
pi sub j is equal to 0,

00:10:44.740 --> 00:10:50.170
which says that the embedded
Markov chain has to be either

00:10:50.170 --> 00:10:56.430
transient or is null-recurrent,
and something

00:10:56.430 --> 00:10:58.410
very, very strange
is going on.

00:10:58.410 --> 00:11:03.800
Because here we've solved these
equations, we found

00:11:03.800 --> 00:11:07.860
positive P sub j's, positive
probabilities of being in each

00:11:07.860 --> 00:11:11.020
state or, at least, that's the
way we try to interpret it.

00:11:11.020 --> 00:11:15.330
And then we find that as far
as the transition rates are

00:11:15.330 --> 00:11:18.660
concerned all of the
transition rates

00:11:18.660 --> 00:11:19.640
are equal to 0.

00:11:19.640 --> 00:11:23.090
So what's going on?

00:11:23.090 --> 00:11:26.260
I must admit frankly, I don't
know what's going on.

00:11:26.260 --> 00:11:30.030
Because, I mean, I know
mathematically

00:11:30.030 --> 00:11:30.860
what's going on.

00:11:30.860 --> 00:11:32.850
This is what the
equations say.

00:11:32.850 --> 00:11:35.560
There's a nice example of it,
which I'll look at in the next

00:11:35.560 --> 00:11:39.930
slide, but to interpret this in
any very satisfactory way

00:11:39.930 --> 00:11:41.730
seems to be very hard.

00:11:41.730 --> 00:11:44.970
OK, here's the example, and
we talked about this a

00:11:44.970 --> 00:11:46.940
little bit last time.

00:11:46.940 --> 00:11:50.070
If we look at the embedded
chain, we can call this a

00:11:50.070 --> 00:11:52.600
hyperactive birth/death chain.

00:11:52.600 --> 00:11:56.570
It's hyperactive in the sense
that the higher the state gets

00:11:56.570 --> 00:11:58.300
the faster the thing runs.

00:12:00.890 --> 00:12:04.380
So as you get more customers
into this queue, you could

00:12:04.380 --> 00:12:08.400
imagine then the server
runs faster, the

00:12:08.400 --> 00:12:10.160
customers arrive faster.

00:12:10.160 --> 00:12:14.010
And as it builds up, this keeps
on going faster and

00:12:14.010 --> 00:12:15.520
faster and faster.

00:12:15.520 --> 00:12:19.750
For this chain, it looks like
it's stable, doesn't it?

00:12:19.750 --> 00:12:23.290
Because the probability
of going up--

00:12:23.290 --> 00:12:25.980
No, it looks like
it's not stable.

00:12:25.980 --> 00:12:28.190
The probability of
going up is 0.6.

00:12:28.190 --> 00:12:30.850
The probability of going
down this 0.4.

00:12:30.850 --> 00:12:33.290
We have that in each state.

00:12:33.290 --> 00:12:37.970
This is what we've seen often
for mm1q's, where things come

00:12:37.970 --> 00:12:42.290
in at a faster rate than they
go out, and the only thing

00:12:42.290 --> 00:12:45.380
that could happen is the
queue builds up.

00:12:45.380 --> 00:12:49.270
Namely, you start out down here,
and you keep moving up

00:12:49.270 --> 00:12:53.210
and you move up forever, so
something bazaar is happening.

00:12:53.210 --> 00:12:59.730
But then, we solve these
equations for the steady state

00:12:59.730 --> 00:13:02.730
fraction of time we spend
in each state,

00:13:02.730 --> 00:13:04.510
and what do we get?

00:13:04.510 --> 00:13:07.010
Because this is the probability
of going up, and

00:13:07.010 --> 00:13:10.840
this is the probability of
going down from state 1.

00:13:10.840 --> 00:13:14.620
And the rate at which things
happen from state 1 is 2.

00:13:14.620 --> 00:13:18.150
And the rate at which things
happen in state 2 is 4,

00:13:18.150 --> 00:13:19.680
what's going on?

00:13:19.680 --> 00:13:26.600
Since the rate at which things
happen here is 4, this rate of

00:13:26.600 --> 00:13:35.630
going down is 2 times 1.6.

00:13:35.630 --> 00:13:36.830
Here we are in--

00:13:36.830 --> 00:13:40.040
If we're in state 1 and moving
up, the rate at which

00:13:40.040 --> 00:13:43.950
transitions occur
in state 1 is 2.

00:13:43.950 --> 00:13:47.880
The probability that the
transition is up is 0.6, so

00:13:47.880 --> 00:13:50.520
the rate of an upper
transition is

00:13:50.520 --> 00:13:53.500
0.6 times 2, 1.2.

00:13:53.500 --> 00:13:57.720
If we're in this state, higher
rate of transitions there

00:13:57.720 --> 00:14:04.940
twice as high, you're moving
down with probability 0.4, but

00:14:04.940 --> 00:14:10.150
since the rate is twice as
high, the overall rate at

00:14:10.150 --> 00:14:12.720
which things are going
down is 1.6.

00:14:12.720 --> 00:14:15.930
This looks like a stable mm1q.

00:14:15.930 --> 00:14:18.280
OK, bizarre.

00:14:18.280 --> 00:14:19.130
We can solve this.

00:14:19.130 --> 00:14:23.310
We can solve this with the
formula for solving for the

00:14:23.310 --> 00:14:25.440
average time in each state.

00:14:25.440 --> 00:14:31.860
And what do we get using p sub
j times q sub j, j plus 1. p

00:14:31.860 --> 00:14:35.460
sub j is the fraction
of time in state j.

00:14:35.460 --> 00:14:39.320
q sub j, j plus 1, is
the rate at which we

00:14:39.320 --> 00:14:41.120
move out of that state.

00:14:41.120 --> 00:14:49.100
That's equal to the fraction of
time or in state j plus 1

00:14:49.100 --> 00:14:51.650
times the rate at which we're
moving down from j plus 1 to

00:14:51.650 --> 00:14:54.200
j, if these equations
make any sense.

00:14:54.200 --> 00:14:58.600
But at any rate, we can solve
these equations p sub j plus 1

00:14:58.600 --> 00:15:02.260
has to equal 3/4 p sub j.

00:15:02.260 --> 00:15:05.860
That ratio there is 3/4.

00:15:05.860 --> 00:15:10.530
And the average time in each
state, according to these

00:15:10.530 --> 00:15:13.810
equations, which we can see
there's something funny about

00:15:13.810 --> 00:15:18.550
it, but it's 1/4 times 3/4
to the j because of this

00:15:18.550 --> 00:15:21.960
relationship here.

00:15:21.960 --> 00:15:26.660
But the sum of p sub j times nu
sub j is equal to infinity,

00:15:26.660 --> 00:15:30.060
which is why we can't get any
steady state embedded chain

00:15:30.060 --> 00:15:31.940
probabilities.

00:15:31.940 --> 00:15:35.180
OK, well, in an effort to
understand this, what you can

00:15:35.180 --> 00:15:39.010
do is you can truncate
this as mm1 Markov

00:15:39.010 --> 00:15:41.355
process to just k states.

00:15:44.160 --> 00:15:47.960
And it's very easy to truncate
the Markov process because all

00:15:47.960 --> 00:15:53.000
you have to do is just cut off
all the transition rates that

00:15:53.000 --> 00:15:55.710
go into these higher
rate states.

00:15:55.710 --> 00:15:59.930
When you cut off everything
going beyond here, what you

00:15:59.930 --> 00:16:04.970
get is just this Markov
process shown here.

00:16:04.970 --> 00:16:10.940
Well, Markov chain shown here,
Markov process shown here,

00:16:10.940 --> 00:16:14.070
and, at this point, we have
a finite number of states.

00:16:14.070 --> 00:16:16.880
Nothing funny can happen.

00:16:16.880 --> 00:16:19.770
And what we get if we actually
go through all the

00:16:19.770 --> 00:16:24.690
calculations is that the
fraction of time in state j is

00:16:24.690 --> 00:16:28.400
1/4 times 1 minus
3/4 to the k.

00:16:28.400 --> 00:16:32.030
This term goes to 0 with k,
so this whole term here is

00:16:32.030 --> 00:16:33.210
unimportant.

00:16:33.210 --> 00:16:35.620
Times 3/4 to the j.

00:16:35.620 --> 00:16:40.100
This 1/4 times 3/4 to the j
was the result we had here

00:16:40.100 --> 00:16:42.870
when we looked at all
the states there.

00:16:42.870 --> 00:16:47.670
Pi sub j, looking at this
chain here, which is an

00:16:47.670 --> 00:16:49.710
unstable chain, you
go up with higher

00:16:49.710 --> 00:16:52.860
probability than you go down.

00:16:52.860 --> 00:16:58.470
What you get is pi sub j is 1/3
times 1 minus 2/3 to the

00:16:58.470 --> 00:17:03.160
k, this term is going to 0
as k increases, times 2/3

00:17:03.160 --> 00:17:05.470
to the k minus j.

00:17:05.470 --> 00:17:11.700
In other words, when you
truncate this change here,

00:17:11.700 --> 00:17:15.859
which is unstable, what's going
to happen is that you're

00:17:15.859 --> 00:17:19.520
going to tend to stay in
the higher states.

00:17:19.520 --> 00:17:22.250
And you're going to dribble down
to the lower states with

00:17:22.250 --> 00:17:24.910
very little probability
in the lower states.

00:17:24.910 --> 00:17:27.910
As you increase k and increase
the number of states that

00:17:27.910 --> 00:17:31.460
you're dealing with what's
happening is that you're

00:17:31.460 --> 00:17:36.210
increasingly spending most of
your time and these higher

00:17:36.210 --> 00:17:36.940
ordered states.

00:17:36.940 --> 00:17:41.900
So as you increase k, you just
move up one on which states

00:17:41.900 --> 00:17:44.060
are most likely.

00:17:44.060 --> 00:17:52.130
So you get this kind of result
here 2/3 to the k minus j,

00:17:52.130 --> 00:17:55.500
which is decreasing
rapidly with k.

00:17:55.500 --> 00:17:59.970
Which says, as k goes to
infinity, this goes to 0 for

00:17:59.970 --> 00:18:04.840
all j, and this goes to a
sensible quantity for each j.

00:18:04.840 --> 00:18:09.120
When you sum up p sub j times
nu sub j, this is giving you

00:18:09.120 --> 00:18:13.010
the rate at which transitions
occur, what you get is this

00:18:13.010 --> 00:18:17.750
term, which doesn't amount to
anything times 1/2 times 3/2

00:18:17.750 --> 00:18:19.730
to the k minus 1.

00:18:19.730 --> 00:18:25.090
This term here is approaching
infinity exponentially, which

00:18:25.090 --> 00:18:28.440
says that something doesn't
make any sense here.

00:18:28.440 --> 00:18:37.380
What's happening in this queue
is that the rate at which

00:18:37.380 --> 00:18:40.330
transitions occur is
going to infinity.

00:18:40.330 --> 00:18:43.790
If you start out at state
0, very rapidly the

00:18:43.790 --> 00:18:45.730
state builds up.

00:18:45.730 --> 00:18:47.830
The higher the state
goes up, the faster

00:18:47.830 --> 00:18:50.430
the transitions become.

00:18:50.430 --> 00:18:55.120
So the transition rate is
approaching infinity, and this

00:18:55.120 --> 00:18:59.170
solution here, which looks like
it makes perfect sense,

00:18:59.170 --> 00:19:02.260
doesn't make any sense at all.

00:19:02.260 --> 00:19:06.520
Because in fact, what's
happening is the transition

00:19:06.520 --> 00:19:10.660
rate is increasing as time goes
on, whether the number of

00:19:10.660 --> 00:19:14.560
transitions in a finite time
is infinite are not with

00:19:14.560 --> 00:19:16.670
probability 1.

00:19:16.670 --> 00:19:18.410
I don't know.

00:19:18.410 --> 00:19:21.160
I can't figure out how to
solve that problem.

00:19:21.160 --> 00:19:24.240
If I figure out how to do
it, I will let you know.

00:19:24.240 --> 00:19:30.490
But anyway, the point is, for
this embedded chain in this

00:19:30.490 --> 00:19:34.790
same process, the idea of
steady state is totally

00:19:34.790 --> 00:19:36.040
meaningless.

00:19:37.760 --> 00:19:41.990
So the caution there is as you
deal with Markov processes

00:19:41.990 --> 00:19:45.450
more and more, and any time you
deal with killing a great

00:19:45.450 --> 00:19:49.540
deal, you deal with this kind
of process all the time.

00:19:49.540 --> 00:19:53.170
What you normally do then is
you start solving for these

00:19:53.170 --> 00:19:54.760
probabilities.

00:19:54.760 --> 00:19:57.680
You simulate something, you
figure out what these

00:19:57.680 --> 00:20:01.160
probabilities are from the
simulation, everything looks

00:20:01.160 --> 00:20:03.690
fine until you look at
the embedded chain.

00:20:03.690 --> 00:20:06.130
And then when you look at the
embedded chain, you realize

00:20:06.130 --> 00:20:07.380
that this is all nonsense.

00:20:10.186 --> 00:20:13.860
I wish I could say more about
this but I can't.

00:20:13.860 --> 00:20:16.800
But anyway, it's a note of
caution when you're dealing

00:20:16.800 --> 00:20:18.500
with Markov processes.

00:20:18.500 --> 00:20:21.840
Check what the embedded chain is
doing because it might not

00:20:21.840 --> 00:20:23.765
be doing something very nice.

00:20:26.270 --> 00:20:29.870
Let's go onto reversibility
for Markov processes.

00:20:29.870 --> 00:20:35.050
We talked about reversibility
for Markov chains, sort of

00:20:35.050 --> 00:20:37.880
half understood what
was going on there.

00:20:37.880 --> 00:20:40.870
Fortunately, for Markov
processes, I think it's a

00:20:40.870 --> 00:20:43.590
little easier to see what's
going on than it was for

00:20:43.590 --> 00:20:44.990
Markov chains.

00:20:44.990 --> 00:20:49.450
So if you almost understand
reversibility for Markov

00:20:49.450 --> 00:20:53.170
chains then I'll be easy
to get the extra things

00:20:53.170 --> 00:20:54.720
that you need here.

00:20:54.720 --> 00:20:59.860
For any Markov chain in steady
state, the backward transition

00:20:59.860 --> 00:21:05.750
probabilities were defined as
pi sub i times Pi(j) star is

00:21:05.750 --> 00:21:08.550
equal to pi j times
P(j)i star.

00:21:08.550 --> 00:21:14.210
In other words, the transition
from i to j, the probability

00:21:14.210 --> 00:21:19.350
of being in state i and going
to state j, which is this

00:21:19.350 --> 00:21:20.980
expression right here.

00:21:20.980 --> 00:21:24.120
You can write it in two
different ways.

00:21:24.120 --> 00:21:27.530
And there's nothing magical
or sophisticated here.

00:21:27.530 --> 00:21:31.790
It's the probability that Xn
plus 1 is equal to i times the

00:21:31.790 --> 00:21:35.430
probability that Xn is equal
to j, given that the next

00:21:35.430 --> 00:21:37.280
state is equal to i.

00:21:37.280 --> 00:21:38.200
We can do that.

00:21:38.200 --> 00:21:40.980
There's nothing wrong with
talking about the probability

00:21:40.980 --> 00:21:44.990
that the state now as j, given
that the state one time from

00:21:44.990 --> 00:21:46.820
now, the state i.

00:21:46.820 --> 00:21:49.750
And that's also equal to the
probability that Xn is equal

00:21:49.750 --> 00:21:51.700
to j times the probability.

00:21:51.700 --> 00:21:56.680
You go from j to i.

00:21:56.680 --> 00:22:02.050
This is just base law written in
a particularly simple form.

00:22:02.050 --> 00:22:06.360
This also holds for the embed
chain of a Markov process.

00:22:06.360 --> 00:22:10.510
So to draw a picture for this,
you're sitting here in state

00:22:10.510 --> 00:22:16.010
i, eventually, at some time
t1, there's a transition.

00:22:16.010 --> 00:22:17.790
You go to state j.

00:22:17.790 --> 00:22:21.560
In state j, there's a transition
rate nu sub j, so

00:22:21.560 --> 00:22:25.560
after some time, whose expected
value is 1 over nu

00:22:25.560 --> 00:22:30.330
sub j at time t2, you
go to another state.

00:22:30.330 --> 00:22:34.520
That new state say is state k,
so you start out in state i,

00:22:34.520 --> 00:22:38.400
you go to state j, you stick
there for a time, nu sub j,

00:22:38.400 --> 00:22:40.510
and then you go on to state k.

00:22:45.110 --> 00:22:49.850
OK, so if we look at this
picture again, and we look at

00:22:49.850 --> 00:22:54.210
it in terms of the sample time
Markov chain, what's going on?

00:22:56.990 --> 00:22:59.050
If you're moving right,
in other words,

00:22:59.050 --> 00:23:01.350
moving up in time.

00:23:01.350 --> 00:23:05.600
And I again, urge you if you
have trouble thinking of time

00:23:05.600 --> 00:23:09.640
running backwards, think of left
and right, because you

00:23:09.640 --> 00:23:12.890
will have no trouble thinking
of things going to the left

00:23:12.890 --> 00:23:16.550
and things going to the right.

00:23:16.550 --> 00:23:19.830
So moving to the right, which
is the normal way to move

00:23:19.830 --> 00:23:24.950
after entering state j, the
exit rate is nu sub j.

00:23:24.950 --> 00:23:29.030
In other words, we exit in each
delta with a probability

00:23:29.030 --> 00:23:31.370
nu sub j times delta.

00:23:31.370 --> 00:23:33.020
The same holds moving left.

00:23:33.020 --> 00:23:38.970
In other words, if you're at
time t2, if moving this way

00:23:38.970 --> 00:23:42.900
you move into state
j, what happens?

00:23:42.900 --> 00:23:48.930
You leave state j going leftward
in each delta unit

00:23:48.930 --> 00:23:50.460
time with--

00:23:50.460 --> 00:23:55.030
There's this same rate here,
which is the rate when you

00:23:55.030 --> 00:23:58.475
look at it either way, is this
rate here at which you leave

00:23:58.475 --> 00:24:02.810
this state j, which is delta nu
sub j, and all we're using

00:24:02.810 --> 00:24:07.180
here is the memorylessness of
the exponential distribution.

00:24:07.180 --> 00:24:08.890
That's the only thing
going on here.

00:24:18.030 --> 00:24:22.240
So Poisson process is clearly
reversible from the

00:24:22.240 --> 00:24:26.040
incremental definition, and
that's what we're using here.

00:24:26.040 --> 00:24:29.050
And that what this means is
that the steady state

00:24:29.050 --> 00:24:33.960
probabilities, the pi sub i's,
and also, the nu sub i's,

00:24:33.960 --> 00:24:36.960
which are the rates at which
transitions occur, are the

00:24:36.960 --> 00:24:40.210
same going left as
going right.

00:24:40.210 --> 00:24:43.340
So this is the same kind of
thing we had before, but I

00:24:43.340 --> 00:24:45.920
think now, you can see it more
easily because there's a

00:24:45.920 --> 00:24:50.010
finite amount of time that
you're sticking in state j,

00:24:50.010 --> 00:24:51.970
and it's a random
amount of time.

00:24:51.970 --> 00:24:56.290
And moving in one direction,
you're moving out of state j

00:24:56.290 --> 00:25:01.640
with this constant rate delta,
delta times nu j.

00:25:01.640 --> 00:25:04.720
When you're going the other way,
it's the same probability

00:25:04.720 --> 00:25:11.490
that you're going out, going
backwards in time, over a

00:25:11.490 --> 00:25:14.230
period delta of delta
times nu sub j.

00:25:14.230 --> 00:25:15.980
It's the same both ways.

00:25:15.980 --> 00:25:18.760
And I think this is easier
to see than the

00:25:18.760 --> 00:25:22.010
thing we did before.

00:25:22.010 --> 00:25:25.940
OK, so the probability of having
a right transition from

00:25:25.940 --> 00:25:31.210
j to k in a little period of
time as p sub j, fraction of

00:25:31.210 --> 00:25:35.160
time in state j, times
q sub jk transition

00:25:35.160 --> 00:25:37.410
rating k times delta.

00:25:37.410 --> 00:25:43.560
Similarly, if q star sub kj
is the left going process

00:25:43.560 --> 00:25:46.760
transition rate, the probability
of having the same

00:25:46.760 --> 00:25:52.120
transition as pk times
q star sub kj, thus

00:25:52.120 --> 00:25:53.370
we have this equation.

00:26:04.100 --> 00:26:08.290
And this equation,
turns into this.

00:26:08.290 --> 00:26:13.320
The rate going backwards is
going from k to j is equal to

00:26:13.320 --> 00:26:19.050
nu sub k times the rate for
the embedding Markov chain

00:26:19.050 --> 00:26:20.690
going backwards.

00:26:20.690 --> 00:26:25.290
So we define a Markov process as
being reversible if q sub i

00:26:25.290 --> 00:26:31.220
j star, if the backward rate is
equal to the forward rate

00:26:31.220 --> 00:26:33.420
for all i and j.

00:26:33.420 --> 00:26:36.830
And if we assume positive
recurrence and we assume that

00:26:36.830 --> 00:26:42.850
pi sub i over nu sub i is less
than infinity, which is the

00:26:42.850 --> 00:26:47.170
stability equation, which says
that the rate of transitions

00:26:47.170 --> 00:26:52.560
has to be finite, then the
Markov process is reversible,

00:26:52.560 --> 00:26:55.060
if and only if, the
embedded chain is.

00:26:55.060 --> 00:26:57.940
OK, so this gives you a nice
easy condition to talk about

00:26:57.940 --> 00:26:59.540
reversibility.

00:26:59.540 --> 00:27:04.020
You can either show that the
chain is reversible or the

00:27:04.020 --> 00:27:05.510
process is reversible.

00:27:05.510 --> 00:27:08.100
They both work the same way.

00:27:08.100 --> 00:27:11.000
And if you understand chains
then you can use that to

00:27:11.000 --> 00:27:12.560
understand processes.

00:27:12.560 --> 00:27:14.300
If you understand processes,
you can use that

00:27:14.300 --> 00:27:16.650
to understand chains.

00:27:16.650 --> 00:27:19.490
OK, so from that we get
what I like to call

00:27:19.490 --> 00:27:21.440
the guessing theorem.

00:27:21.440 --> 00:27:25.290
Suppose a Markov process is
irreducible, this means every

00:27:25.290 --> 00:27:28.500
state communicates with every
other state, it's easy to

00:27:28.500 --> 00:27:31.000
verify irreducibility.

00:27:31.000 --> 00:27:36.740
It's sometimes hard to verify
whether a chain is positive

00:27:36.740 --> 00:27:39.440
recurrent or not and whether
the process is positive

00:27:39.440 --> 00:27:40.730
recurrent or not.

00:27:40.730 --> 00:27:45.010
So this guessing theorem says
if a Markov process is

00:27:45.010 --> 00:27:49.170
irreducible and if p sub i is
a set of probabilities that

00:27:49.170 --> 00:27:51.730
satisfy this equation here--

00:27:51.730 --> 00:27:54.360
these are the reversibility
equations--

00:27:54.360 --> 00:27:58.840
p sub i times qij, rate at which
you're going up is equal

00:27:58.840 --> 00:28:02.900
to the rate at which you're
going down for all i and j.

00:28:02.900 --> 00:28:09.820
And when you find that p sub i
that satisfies these equations

00:28:09.820 --> 00:28:16.640
here, if it also satisfies this
equation, then, in fact,

00:28:16.640 --> 00:28:18.770
you're in business.

00:28:18.770 --> 00:28:26.230
And what the theorem says is
first, all of these average

00:28:26.230 --> 00:28:32.570
probabilities are greater than
zero for all i, 2, these

00:28:32.570 --> 00:28:37.600
average probabilities, this p
sub i, is the sample-path

00:28:37.600 --> 00:28:42.320
fraction of time in state 1
with probability 1, 3, the

00:28:42.320 --> 00:28:45.530
process is reversible,
and 4, the embedded

00:28:45.530 --> 00:28:47.290
chain is positive recurrent.

00:28:47.290 --> 00:28:48.855
You get, all at once--

00:28:48.855 --> 00:28:50.480
all you've got to
do is find the

00:28:50.480 --> 00:28:52.390
solution to those equations.

00:28:52.390 --> 00:28:55.250
If you can find the solution
to those equations and it

00:28:55.250 --> 00:28:58.030
satisfies this, then
you're done.

00:28:58.030 --> 00:29:01.440
You don't need a birth/death
chain or anything else.

00:29:01.440 --> 00:29:05.730
All you need to do is guess a
solution to those equations.

00:29:05.730 --> 00:29:08.600
If you get one, then
that establishes

00:29:08.600 --> 00:29:10.770
everything you need.

00:29:10.770 --> 00:29:12.730
OK, useful application.

00:29:12.730 --> 00:29:17.670
All birth/death processes which
satisfy this finite

00:29:17.670 --> 00:29:21.890
number of transition conditions
are reversible.

00:29:21.890 --> 00:29:23.620
Remember, we talked about trees

00:29:23.620 --> 00:29:25.950
with the Markov condition.

00:29:25.950 --> 00:29:29.230
If you have a Markov graph
which is a tree, and it

00:29:29.230 --> 00:29:33.210
satisfies this condition, then
that process has to be

00:29:33.210 --> 00:29:34.300
reversible.

00:29:34.300 --> 00:29:38.120
You get it from this theorem
also because, again, you have

00:29:38.120 --> 00:29:43.600
this condition that what goes
over a branch in a tree, you

00:29:43.600 --> 00:29:47.020
can only go up on that
branch one more time,

00:29:47.020 --> 00:29:48.050
then you can go down.

00:29:48.050 --> 00:29:52.490
So you've got the same equality
and the argument is

00:29:52.490 --> 00:29:54.950
exactly the same.

00:29:54.950 --> 00:29:55.250
OK.

00:29:55.250 --> 00:29:57.710
So what do we get out of this?

00:29:57.710 --> 00:30:00.800
What we get is Burke's
theorem.

00:30:00.800 --> 00:30:02.440
And what this Burke's
theorem says--

00:30:05.520 --> 00:30:09.100
Burke's theorem here, for
processes, is what you usually

00:30:09.100 --> 00:30:10.540
think of as Burke's theorem.

00:30:10.540 --> 00:30:14.000
It was the original statement
of Burke's Theorem before

00:30:14.000 --> 00:30:17.840
people started to look at
sample time chains.

00:30:17.840 --> 00:30:23.380
And what it says is, given an
M/M/1 queue in steady-state

00:30:23.380 --> 00:30:26.760
with arrival rate lambda
less than mu--

00:30:26.760 --> 00:30:29.950
if you look at that condition,
that's exactly what you need

00:30:29.950 --> 00:30:33.550
for the stability condition
we've been talking about--

00:30:33.550 --> 00:30:38.720
the departure process is Poisson
with rate lambda.

00:30:38.720 --> 00:30:41.210
Now you would think
the process--

00:30:41.210 --> 00:30:47.210
and when we're thinking about
Markov chains, you all thought

00:30:47.210 --> 00:30:51.350
when you saw this that the
departure process had rate mu

00:30:51.350 --> 00:30:54.320
because the server was operating
at rate and mu, who

00:30:54.320 --> 00:30:56.240
said no, that's not right.

00:30:56.240 --> 00:31:02.250
Because the service rate is mu
anytime the server is active.

00:31:02.250 --> 00:31:04.580
But is lambda is less
than mu the server

00:31:04.580 --> 00:31:05.650
is not always active.

00:31:05.650 --> 00:31:12.240
The server is sometimes idle and
therefore, what comes in

00:31:12.240 --> 00:31:15.960
is the same as what goes out.

00:31:15.960 --> 00:31:19.230
The only possibility is that
this queue is gradually

00:31:19.230 --> 00:31:22.740
building up over time, and then
what goes out is less

00:31:22.740 --> 00:31:23.680
than what comes in.

00:31:23.680 --> 00:31:27.210
But otherwise, what comes
in is what goes out.

00:31:27.210 --> 00:31:31.150
And the rate at which the server
is working is not mu,

00:31:31.150 --> 00:31:34.460
that the server is working
a rate lambda.

00:31:34.460 --> 00:31:38.280
Mainly, the server is working at
rate mu when it has things

00:31:38.280 --> 00:31:42.530
to do but every time it doesn't
have things to do it

00:31:42.530 --> 00:31:43.630
takes a coffee break.

00:31:43.630 --> 00:31:45.770
It's not doing anything.

00:31:45.770 --> 00:31:50.460
And at that point, when you
amortized over the time the

00:31:50.460 --> 00:31:53.330
server is taking coffee breaks
and the time the server is

00:31:53.330 --> 00:31:58.310
working the rate is lambda
because that's the rate at

00:31:58.310 --> 00:32:00.350
which things are coming in.

00:32:00.350 --> 00:32:03.540
You can only get one thing
out for each thing in.

00:32:03.540 --> 00:32:07.040
So either the queue builds up
forever, in which case, what's

00:32:07.040 --> 00:32:10.520
going out is less than what's
coming in or the queue is not

00:32:10.520 --> 00:32:14.430
building up and what goes out
is exactly what's coming in.

00:32:14.430 --> 00:32:18.790
I hope looking at it that way
convinces you that what's

00:32:18.790 --> 00:32:24.070
coming out here is the same
rate as what's going in.

00:32:24.070 --> 00:32:31.160
OK, the next thing is that the
state x of t is independent of

00:32:31.160 --> 00:32:33.540
the departures before t.

00:32:33.540 --> 00:32:37.730
This was the same as the
condition we had when we were

00:32:37.730 --> 00:32:40.110
looking at Markov chains.

00:32:40.110 --> 00:32:42.750
The state x of t is
independent of the

00:32:42.750 --> 00:32:43.960
departure before t.

00:32:43.960 --> 00:32:48.770
Whether you look at this thing
forward or backward, the state

00:32:48.770 --> 00:32:51.210
is just the height of
that graph there.

00:32:51.210 --> 00:32:55.690
So when you're looking at it
backwards, the state that you

00:32:55.690 --> 00:33:00.080
go through over the process is
the same as the set of states

00:33:00.080 --> 00:33:02.475
when you're looking at
it the other way.

00:33:02.475 --> 00:33:05.660
It's just that you have
to flip it around.

00:33:05.660 --> 00:33:08.120
And it says that the state
is independent of

00:33:08.120 --> 00:33:09.760
departures before t.

00:33:09.760 --> 00:33:13.230
The reason for that is that when
you look at the backward

00:33:13.230 --> 00:33:18.610
process, every departure here
is an arrival here.

00:33:18.610 --> 00:33:20.280
These arrivals here--

00:33:20.280 --> 00:33:22.480
This is an arrival, this
is an arrival,

00:33:22.480 --> 00:33:24.195
this is a third arrival.

00:33:24.195 --> 00:33:28.300
This is the fourth arrival
and those depart later.

00:33:28.300 --> 00:33:32.250
When you're looking at it that
way, what's going on?

00:33:32.250 --> 00:33:35.740
These things that are called
departures up in the top graph

00:33:35.740 --> 00:33:38.500
are called arrivals when you're
looking at it going

00:33:38.500 --> 00:33:39.360
this way like.

00:33:39.360 --> 00:33:45.390
What this is saying is-- when
you're looking at this as an

00:33:45.390 --> 00:33:51.200
M/M/1 process, what it's saying
is for the backward

00:33:51.200 --> 00:33:56.130
M/M/1 process, the arrivals that
come in later than me do

00:33:56.130 --> 00:33:58.810
not affect my service time.

00:33:58.810 --> 00:34:02.750
This is because we have first
come first serve service, and

00:34:02.750 --> 00:34:04.670
if somebody with an
enormous service

00:34:04.670 --> 00:34:09.920
requirement comes after me--

00:34:09.920 --> 00:34:12.719
If I'm at the supermarket and
somebody comes in with three

00:34:12.719 --> 00:34:17.469
big barrels of food, I breath
a sigh of relief because I'm

00:34:17.469 --> 00:34:18.420
there first.

00:34:18.420 --> 00:34:20.050
And I get out quickly,
and I don't have to

00:34:20.050 --> 00:34:21.480
wait for this person.

00:34:21.480 --> 00:34:24.520
And that's essentially,
what this is saying.

00:34:24.520 --> 00:34:27.850
OK, the third one for first come
first serve service, a

00:34:27.850 --> 00:34:33.520
customer's arrival time, given
that it departs at time t, is

00:34:33.520 --> 00:34:36.429
independent of the departures
before time t.

00:34:36.429 --> 00:34:39.070
That's almost the same
as two, but it's not

00:34:39.070 --> 00:34:40.179
quite the same as two.

00:34:40.179 --> 00:34:42.639
It only applies if
you have first

00:34:42.639 --> 00:34:45.460
come first serve service.

00:34:45.460 --> 00:34:48.929
This is not quite true
for Markov chains.

00:34:48.929 --> 00:34:50.690
You remember when we
stated Burke's

00:34:50.690 --> 00:34:52.429
theorem for Markov chains.

00:34:52.429 --> 00:34:55.790
We only had these first two
statements, and now we have

00:34:55.790 --> 00:34:57.150
this third statement.

00:34:57.150 --> 00:35:01.060
Let me try to explain in the
next slide why that's true.

00:35:07.130 --> 00:35:10.590
If I can explain this to you and
make you believe it on the

00:35:10.590 --> 00:35:13.370
first time I feel very
proud of myself.

00:35:13.370 --> 00:35:16.940
Because this is the kind of
thing you have to have

00:35:16.940 --> 00:35:18.710
explained to you three times.

00:35:18.710 --> 00:35:20.780
You have to sit down
and think about it.

00:35:20.780 --> 00:35:23.400
The first three times you
think about it, you say,

00:35:23.400 --> 00:35:24.880
that's all baloney.

00:35:24.880 --> 00:35:26.720
The fourth time you
think about it,

00:35:26.720 --> 00:35:29.340
you say, that's false.

00:35:29.340 --> 00:35:31.860
And maybe the fifth time you
say, it's false again.

00:35:31.860 --> 00:35:35.410
And the sixth time you look at
it just the right way, and it

00:35:35.410 --> 00:35:37.700
turns out to be true.

00:35:37.700 --> 00:35:41.510
And then you go back a week
later, and it's false again.

00:35:41.510 --> 00:35:44.810
But anyway, let me try.

00:35:44.810 --> 00:35:47.220
We're going to first look
at the right moving

00:35:47.220 --> 00:35:49.410
sample path, OK?

00:35:49.410 --> 00:35:58.390
So departure at t up here is
an arrival [? in ?] the

00:35:58.390 --> 00:36:02.890
departure in the right moving
sample path is an arrival in

00:36:02.890 --> 00:36:06.320
the M/M/1 left moving
sample path.

00:36:06.320 --> 00:36:09.190
For first come first
service, looking at

00:36:09.190 --> 00:36:11.740
the left moving process--

00:36:11.740 --> 00:36:13.850
that's the left moving
process--

00:36:13.850 --> 00:36:18.340
the departure time of an
arrival at time t--

00:36:18.340 --> 00:36:20.730
here's an arrival at time t--

00:36:20.730 --> 00:36:24.940
the departure of that arrival,
in this case, is that

00:36:24.940 --> 00:36:28.900
departure right there.

00:36:28.900 --> 00:36:33.930
That departure time depends on
the arrivals and the service

00:36:33.930 --> 00:36:38.640
requirements back over here
because when we're looking at

00:36:38.640 --> 00:36:41.980
things going this way, this
is what happens first.

00:36:41.980 --> 00:36:43.580
This is what happens later.

00:36:43.580 --> 00:36:46.360
So we have these arrivals
coming in this way, this

00:36:46.360 --> 00:36:49.870
arrival here has
nothing to do--

00:36:49.870 --> 00:36:52.830
Well, this arrival here, in
general, does have something

00:36:52.830 --> 00:36:55.810
to do with this arrival, because
if this arrival were

00:36:55.810 --> 00:37:01.390
not finished by this time, we
would have to wait for it.

00:37:01.390 --> 00:37:05.070
The waiting time of this arrival
here depends on what

00:37:05.070 --> 00:37:07.370
happened before, which
is over here.

00:37:07.370 --> 00:37:10.920
It does not depend on the
arrivals after it because it's

00:37:10.920 --> 00:37:13.280
first come first
serve service.

00:37:13.280 --> 00:37:15.770
And therefore, anything
that comes after is

00:37:15.770 --> 00:37:17.150
sitting behind us.

00:37:17.150 --> 00:37:20.120
It doesn't bother us at all.

00:37:20.120 --> 00:37:24.350
OK, for the corresponding right
moving process then

00:37:24.350 --> 00:37:28.860
coming back the other way, the
arrival time of that departure

00:37:28.860 --> 00:37:32.370
is independent as the departures
before t, and

00:37:32.370 --> 00:37:35.660
that's exactly what this
theorem is saying.

00:37:35.660 --> 00:37:38.210
In order to make sense of this
you have to do something that

00:37:38.210 --> 00:37:40.800
we've been doing all along.

00:37:40.800 --> 00:37:45.490
We don't always talk about is
very much, but I hope you're

00:37:45.490 --> 00:37:47.220
getting used to doing it.

00:37:47.220 --> 00:37:49.380
You talk about sample paths.

00:37:49.380 --> 00:37:52.030
When you're talking about sample
paths, you're talking

00:37:52.030 --> 00:37:56.360
about one particular
instantiation of the process.

00:37:56.360 --> 00:38:00.800
You prove a result for that
sample path, then you say,

00:38:00.800 --> 00:38:03.160
that's true for all
sample paths.

00:38:03.160 --> 00:38:05.680
And then, you say, a ha, then it
must be true for the random

00:38:05.680 --> 00:38:11.890
variables of which these sample
paths are sample cases.

00:38:11.890 --> 00:38:15.030
That's an argument we go
through so many times.

00:38:15.030 --> 00:38:17.710
In fact, when you take
probability the first time,

00:38:17.710 --> 00:38:20.650
pretty soon it becomes second
nature to you, and

00:38:20.650 --> 00:38:22.280
every once in while--

00:38:22.280 --> 00:38:25.250
In fact it becomes so much
second nature, that most

00:38:25.250 --> 00:38:31.930
people don't even distinguish
between random variables and

00:38:31.930 --> 00:38:34.750
actual sample path numbers.

00:38:34.750 --> 00:38:37.440
They use the same symbol
for both of them.

00:38:37.440 --> 00:38:40.150
Every once in a while I get
confused by this, but

00:38:40.150 --> 00:38:43.360
eventually, they get it
straightened out, and that's

00:38:43.360 --> 00:38:47.120
part of what this argument
is here.

00:38:47.120 --> 00:38:52.140
OK, let's test our understanding
now.

00:38:52.140 --> 00:38:56.650
Let's look at tandem
M/M/1 queues.

00:38:56.650 --> 00:39:00.630
First, we don't quite know what
that means, but let's

00:39:00.630 --> 00:39:06.050
consider two queues, one sitting
right after the other.

00:39:06.050 --> 00:39:10.050
Departures from the first queue
move directly into the

00:39:10.050 --> 00:39:11.600
second queue.

00:39:11.600 --> 00:39:14.700
The partition here moved
directly into here.

00:39:14.700 --> 00:39:19.940
I've shown these departures at
rate lambda because what

00:39:19.940 --> 00:39:24.230
Burke's theorem says, is that
these departures are, in fact,

00:39:24.230 --> 00:39:27.060
a Poisson process,
we rate lambda.

00:39:27.060 --> 00:39:29.410
You think you're through at this
point, but you're not.

00:39:29.410 --> 00:39:33.570
You still have some difficult
thinking to go through.

00:39:33.570 --> 00:39:38.990
So you're assuming originally,
that the input is Poisson.

00:39:38.990 --> 00:39:42.800
You're assuming that the first
queue has exponential

00:39:42.800 --> 00:39:47.870
services, the service times are
independent of each other,

00:39:47.870 --> 00:39:51.080
they're all iid, and they're
independent of the

00:39:51.080 --> 00:39:52.460
inter-arrival times.

00:39:52.460 --> 00:39:55.510
So this first thing here is
just an M/M/1 queue like

00:39:55.510 --> 00:39:56.830
you're used to.

00:39:56.830 --> 00:40:00.270
We now know that what's coming
out of that first queue is a

00:40:00.270 --> 00:40:02.640
Poisson process.

00:40:02.640 --> 00:40:04.740
It's a little hard to
look at as a Poisson

00:40:04.740 --> 00:40:05.970
process but it is.

00:40:05.970 --> 00:40:11.520
It is not a Poisson process if
you're conditioning on what's

00:40:11.520 --> 00:40:13.300
going on in the first queue.

00:40:13.300 --> 00:40:17.220
It's not a Poisson process
conditional on these arrival

00:40:17.220 --> 00:40:20.070
times here or on these
service times here.

00:40:20.070 --> 00:40:28.680
It's only a Poisson process if
we're looking at the departure

00:40:28.680 --> 00:40:33.920
times unconditional
on anything else.

00:40:33.920 --> 00:40:38.420
Conditional only on the other
departure times or iid, That's

00:40:38.420 --> 00:40:40.640
what we've proven.

00:40:40.640 --> 00:40:43.210
But conditional on what's going
on in the first queues

00:40:43.210 --> 00:40:46.860
of those departures, are
sure as hell not even

00:40:46.860 --> 00:40:49.770
close to doing iid.

00:40:49.770 --> 00:40:52.700
I mean, if you've got a lot of
arrivals and you've got a lot

00:40:52.700 --> 00:40:56.290
of long service times, the queue
is going to get busy.

00:40:56.290 --> 00:40:59.640
And what's coming out of that
queue is going to be at rate

00:40:59.640 --> 00:41:02.460
mu, which is what all of you
believe until you see this

00:41:02.460 --> 00:41:05.340
theorem, and think it through.

00:41:05.340 --> 00:41:11.700
And that's why you believe it,
because you look normally, at

00:41:11.700 --> 00:41:13.385
time moving from
left to right.

00:41:16.680 --> 00:41:19.830
We're going to assume the
service time at rates mu 1 and

00:41:19.830 --> 00:41:24.100
mu 2 are independent from
queue to queue.

00:41:24.100 --> 00:41:26.360
And they're independent--

00:41:26.360 --> 00:41:29.440
Ah, I didn't change that.

00:41:29.440 --> 00:41:31.730
This doesn't make any sense.

00:41:31.730 --> 00:41:37.460
The service rates at mu1 and mu2
are independent from this

00:41:37.460 --> 00:41:40.140
queue to this queue, and they're
independent of the

00:41:40.140 --> 00:41:44.230
inter-arrival times
back there.

00:41:44.230 --> 00:41:45.920
Now, why do I have
to change that?

00:41:45.920 --> 00:41:51.490
Why can't I say that the service
times at mu1 and mu2

00:41:51.490 --> 00:41:56.130
are independent of both the
arrivals to the first queue

00:41:56.130 --> 00:41:58.200
and the arrivals to
the second queue?

00:41:58.200 --> 00:42:04.680
Because first, I can't assume
things that are not given by

00:42:04.680 --> 00:42:07.230
the model of the problem.

00:42:07.230 --> 00:42:11.690
The things going from queue 1
into queue 2, I know they're

00:42:11.690 --> 00:42:15.070
Poisson now, but I don't know
that they're Poisson

00:42:15.070 --> 00:42:18.920
independent of what's going
into the first queue, and

00:42:18.920 --> 00:42:21.210
independent of the service times
in the first queue, and

00:42:21.210 --> 00:42:22.790
they certainly aren't.

00:42:22.790 --> 00:42:26.100
So what I have to say here is
they're independent from queue

00:42:26.100 --> 00:42:29.940
to queue and independent
of the arrivals

00:42:29.940 --> 00:42:31.190
at the first queue.

00:42:34.690 --> 00:42:37.390
OK, we know that the arrivals
at queue 2 are

00:42:37.390 --> 00:42:39.660
Poisson at rate lambda.

00:42:39.660 --> 00:42:42.730
That's true by Burke's
theorem.

00:42:42.730 --> 00:42:47.140
And they're independent of
the service times at 2.

00:42:47.140 --> 00:42:51.320
That alone makes the
second queue M/M/1.

00:42:51.320 --> 00:42:54.040
Because the second queue
has arrivals.

00:42:54.040 --> 00:42:55.240
It has service times.

00:42:55.240 --> 00:42:57.940
The arrivals are independent
of the service times.

00:42:57.940 --> 00:43:01.780
It is, by definition, what
an M/M/1 queue is.

00:43:01.780 --> 00:43:05.290
It might depend on what's going
on in the first queue to

00:43:05.290 --> 00:43:06.200
a certain extent.

00:43:06.200 --> 00:43:09.330
Burke's theorem tells us, in one
sense, it doesn't depend

00:43:09.330 --> 00:43:13.330
on it, and that's the next thing
we have to deal with.

00:43:13.330 --> 00:43:17.320
The states of the two systems
are independent and the time

00:43:17.320 --> 00:43:20.380
of a customer in system
one is independent of

00:43:20.380 --> 00:43:22.090
that in state two.

00:43:22.090 --> 00:43:24.950
I'm not going to try to convince
you of that here.

00:43:24.950 --> 00:43:29.050
The text does, I think, do
a pretty good job of

00:43:29.050 --> 00:43:31.050
convincing you of it.

00:43:31.050 --> 00:43:34.170
It's a more complicated
argument than this.

00:43:34.170 --> 00:43:38.270
And s something which you just
have to either say, I believe

00:43:38.270 --> 00:43:42.050
it because I'm too tired
to do anything else.

00:43:42.050 --> 00:43:47.240
Or you have to go through it
several times, sometimes

00:43:47.240 --> 00:43:50.200
disbelieving and sometimes
believing until finally,

00:43:50.200 --> 00:43:52.320
saying, I guess it
must be true.

00:43:52.320 --> 00:43:56.940
OK, so enough of that.

00:43:56.940 --> 00:44:00.640
Let's go on to random walks,
which is where we should have

00:44:00.640 --> 00:44:04.170
been 45 minutes ago.

00:44:04.170 --> 00:44:07.980
You see our problem here is
that we would like to talk

00:44:07.980 --> 00:44:11.150
about martingales a little bit
in this course because

00:44:11.150 --> 00:44:15.270
martingales are extremely
important things.

00:44:15.270 --> 00:44:18.700
You can prove all sorts
of things from them.

00:44:18.700 --> 00:44:22.260
An enormous amount of modern
research in the field uses

00:44:22.260 --> 00:44:27.300
martingales, and you can do a
great deal with martingales

00:44:27.300 --> 00:44:30.580
without knowing anything
about measure theory.

00:44:30.580 --> 00:44:34.320
And almost all the books that
talk about martingales use

00:44:34.320 --> 00:44:37.520
measure theory, and therefore,
require you to take a couple

00:44:37.520 --> 00:44:40.940
of extra terms of math courses
before you can understand

00:44:40.940 --> 00:44:42.050
what's going on.

00:44:42.050 --> 00:44:45.140
So it's important to be able to
say something about them,

00:44:45.140 --> 00:44:47.850
in this course, so
you have enough--

00:44:47.850 --> 00:44:49.950
so you know when you
have to learn more

00:44:49.950 --> 00:44:51.640
about them at least.

00:44:51.640 --> 00:44:57.790
So I want to get to that point,
which is why I've been

00:44:57.790 --> 00:45:00.370
speeding up a little bit from
what I would like to do.

00:45:00.370 --> 00:45:03.815
If you noticed, if you've been
reading the notes, I have

00:45:03.815 --> 00:45:05.290
skipped the [INAUDIBLE]

00:45:05.290 --> 00:45:07.070
equations.

00:45:07.070 --> 00:45:09.830
I have skipped semi-Markov
processes.

00:45:09.830 --> 00:45:12.360
We're not going to
deal with them.

00:45:12.360 --> 00:45:13.040
The [INAUDIBLE]

00:45:13.040 --> 00:45:14.580
equations are quite important.

00:45:14.580 --> 00:45:16.910
You use them quite a bit.

00:45:16.910 --> 00:45:21.260
They play the same role for
Markov processes that all the

00:45:21.260 --> 00:45:23.790
business with eigenvalues
we went through

00:45:23.790 --> 00:45:26.620
play with Markov chains.

00:45:26.620 --> 00:45:29.470
The trouble is we don't
have time to do it.

00:45:29.470 --> 00:45:32.760
And the other trouble is all the
time that we would spend

00:45:32.760 --> 00:45:37.460
understanding that would be
time understanding linear

00:45:37.460 --> 00:45:41.550
systems of differential
equations.

00:45:41.550 --> 00:45:45.375
And if you study that in math
and you have a good feel for

00:45:45.375 --> 00:45:48.750
it, fine, you can read the
section and notes.

00:45:48.750 --> 00:45:51.780
If you're not familiar with
that, then you have to learn

00:45:51.780 --> 00:45:52.980
something about it.

00:45:52.980 --> 00:45:56.690
It's not something we should
waste our time on because it's

00:45:56.690 --> 00:46:01.370
not something we will use at any
other time in this course.

00:46:01.370 --> 00:46:04.800
OK, so we want to get
onto random walks.

00:46:04.800 --> 00:46:08.360
A random walk is, in fact, a
very simple kind of animal.

00:46:08.360 --> 00:46:13.780
Let x sub i, i greater than or
equal to 1, be a sequence of

00:46:13.780 --> 00:46:17.960
independent identically
distributed random variables.

00:46:17.960 --> 00:46:23.780
And let s sub n be x1 plus x2
plus x sub n for n greater

00:46:23.780 --> 00:46:26.250
than or equal to 1.

00:46:26.250 --> 00:46:29.370
Is this something that we
spent a 1/4 of the term

00:46:29.370 --> 00:46:32.950
talking about, or 1/2 the term
talking about, or 3/4 of the

00:46:32.950 --> 00:46:35.200
term talking about?

00:46:35.200 --> 00:46:38.000
I don't know, but we've
certainly spent an awful lot

00:46:38.000 --> 00:46:42.330
of time talking about the sums
of iid random variables.

00:46:42.330 --> 00:46:46.200
What is different here is that
instead of being interested in

00:46:46.200 --> 00:46:50.360
these random variables s sub n,
what we're interested in is

00:46:50.360 --> 00:46:52.840
this process of the s sub n's.

00:46:52.840 --> 00:46:55.340
I mean, we're interested in
looking at the whole sequence

00:46:55.340 --> 00:46:58.970
of them, and saying
things about that.

00:46:58.970 --> 00:47:04.870
What we would like to
be able to do is say

00:47:04.870 --> 00:47:07.260
questions of this sort.

00:47:07.260 --> 00:47:11.120
You pick some alpha bigger
than 0, doesn't

00:47:11.120 --> 00:47:12.940
matter what it is, 1.

00:47:12.940 --> 00:47:16.590
What's the probability that s
sub n is greater than or equal

00:47:16.590 --> 00:47:21.450
to alpha for at least one in
greater than or equal to 0?

00:47:21.450 --> 00:47:25.060
So we're not asking, what's the
probability that s sub 10

00:47:25.060 --> 00:47:26.900
is bigger than alpha.

00:47:26.900 --> 00:47:32.410
We're asking, if we look at this
sequence, s1, s2, s3, s4,

00:47:32.410 --> 00:47:34.330
what's the probability
that any one of

00:47:34.330 --> 00:47:36.740
those terms cross alpha?

00:47:36.740 --> 00:47:39.680
In other words, when we look
at the entire sample path,

00:47:39.680 --> 00:47:44.410
does the entire sample path lie
below alpha or does the

00:47:44.410 --> 00:47:48.080
sample path, at some point,
cross alpha, then perhaps,

00:47:48.080 --> 00:47:50.910
come back down again or continue
to go up or do

00:47:50.910 --> 00:47:52.000
whatever it wants to do.

00:47:52.000 --> 00:47:54.550
What the question is, does it
cross it at least once?

00:47:58.140 --> 00:48:03.430
The questions connected with
that are, if it does cross the

00:48:03.430 --> 00:48:06.920
threshold, when does it
cross the threshold?

00:48:06.920 --> 00:48:08.510
That's important.

00:48:08.510 --> 00:48:12.160
If it crosses the threshold,
what's the overshoot with

00:48:12.160 --> 00:48:14.280
which it crosses
the threshold?

00:48:14.280 --> 00:48:16.590
Does it go shooting way
above the threshold

00:48:16.590 --> 00:48:17.840
when it crosses it?

00:48:23.810 --> 00:48:26.780
If you have one of these random
variables that has very

00:48:26.780 --> 00:48:30.240
long tails, then it's very
likely that when you cross the

00:48:30.240 --> 00:48:33.860
threshold, you've crossed a
threshold because of some

00:48:33.860 --> 00:48:37.110
sample value which
is humongous.

00:48:37.110 --> 00:48:40.710
And therefore, you typically
are going way above the

00:48:40.710 --> 00:48:43.140
threshold when you cross it.

00:48:43.140 --> 00:48:46.500
That makes the study of random
walks very much more

00:48:46.500 --> 00:48:50.820
complicated than it would
be otherwise.

00:48:50.820 --> 00:48:56.230
These overshoot problems are
extremely difficult, extremely

00:48:56.230 --> 00:49:00.680
tedious, and unfortunately, very
often, very important.

00:49:00.680 --> 00:49:05.320
So that's a bad combination, to
be tedious and important.

00:49:05.320 --> 00:49:07.920
You would rather avoid
things like that.

00:49:07.920 --> 00:49:11.290
Next one is given two thresholds
for a given alpha

00:49:11.290 --> 00:49:15.730
bigger than 0, and a given
beta less than 0--

00:49:15.730 --> 00:49:17.480
here we are, starting
out at 0.

00:49:17.480 --> 00:49:21.650
We have a threshold up here
and a threshold down here.

00:49:21.650 --> 00:49:24.710
And we want to know what's the
probability that we ever cross

00:49:24.710 --> 00:49:27.110
one of the two thresholds.

00:49:27.110 --> 00:49:29.060
That, I hope, you can
answer right away.

00:49:29.060 --> 00:49:31.810
Can you?

00:49:31.810 --> 00:49:35.510
s sub n, after a long
period of time, is

00:49:35.510 --> 00:49:36.760
going to look Gaussian.

00:49:39.630 --> 00:49:43.070
It's going to look Gaussian
with a standard deviation,

00:49:43.070 --> 00:49:44.850
which is growing gradually.

00:49:44.850 --> 00:49:48.590
And as a standard deviation
grows gradually, and we have

00:49:48.590 --> 00:49:51.260
these two finite limits,
eventually, we're going to

00:49:51.260 --> 00:49:54.090
cross one of those thresholds.

00:49:54.090 --> 00:49:57.460
So the question is not do you
cross one of the thresholds?

00:49:57.460 --> 00:50:00.920
The question is, which threshold
do you cross?

00:50:00.920 --> 00:50:04.890
What's the probability of
crossing each of them?

00:50:04.890 --> 00:50:09.050
At what end do you cross
the one that you cross?

00:50:09.050 --> 00:50:12.020
And what's the overshoot
when you cross it?

00:50:12.020 --> 00:50:13.700
So those are the questions
there.

00:50:17.120 --> 00:50:19.300
These threshold crossing
problems are important.

00:50:21.870 --> 00:50:24.350
They're important in almost
everything that you use

00:50:24.350 --> 00:50:27.500
stochastic processes for.

00:50:27.500 --> 00:50:30.710
They're familiar to me for
studying errors in digital

00:50:30.710 --> 00:50:34.770
communications systems because
that's sort of the basis of

00:50:34.770 --> 00:50:36.980
all of that study.

00:50:36.980 --> 00:50:39.940
They're important in studying
the overflowing queues.

00:50:39.940 --> 00:50:43.160
Anytime you build a queue, I
mean, what are you building?

00:50:43.160 --> 00:50:47.880
You're building a waiting room
for customers, and you're

00:50:47.880 --> 00:50:50.160
building a service facility.

00:50:50.160 --> 00:50:52.920
And one of the first questions
is, how big do I have to make

00:50:52.920 --> 00:50:54.400
the waiting room?

00:50:54.400 --> 00:50:57.080
How much storage do I need,
is the question.

00:50:57.080 --> 00:50:59.890
So you really want to be able to
answer the question, what's

00:50:59.890 --> 00:51:03.790
the probability that the queue
will ever overflow the queue?

00:51:03.790 --> 00:51:05.190
That's a threshold problem.

00:51:07.780 --> 00:51:11.900
In hypothesis testing, when you
want to find out which of

00:51:11.900 --> 00:51:16.630
two hypotheses is true,
this is important.

00:51:16.630 --> 00:51:20.420
When you want to look at, what's
often a more important

00:51:20.420 --> 00:51:26.740
question, is you test for one
or two hypotheses, and if

00:51:26.740 --> 00:51:30.080
you're smart, and if you can
do it, you keep the test

00:51:30.080 --> 00:51:32.330
running until you're pretty
sure that you

00:51:32.330 --> 00:51:34.710
have the right answer.

00:51:34.710 --> 00:51:37.540
If you've got the right answer
right away, then you can stop,

00:51:37.540 --> 00:51:39.890
and save time from there on.

00:51:39.890 --> 00:51:42.760
If you have to go for a long
time before you get the

00:51:42.760 --> 00:51:44.700
answer, then you go
for a long time

00:51:44.700 --> 00:51:45.950
before you get the answer.

00:51:45.950 --> 00:51:48.280
That's called sequential
hypothesis

00:51:48.280 --> 00:51:50.620
testing, and that's important.

00:51:50.620 --> 00:51:55.450
There are these questions or
ruin and other catastrophes,

00:51:55.450 --> 00:51:59.040
which, as you've observed in
this paper over the last few

00:51:59.040 --> 00:52:02.860
years, most people don't know
how to deal with them.

00:52:02.860 --> 00:52:05.630
If they do know how to deal
with them, they're too

00:52:05.630 --> 00:52:08.890
affected by short term profits
and things like that that do

00:52:08.890 --> 00:52:11.030
what they should do.

00:52:11.030 --> 00:52:13.780
But part of the trouble is
people don't know how to deal

00:52:13.780 --> 00:52:15.220
with those problems.

00:52:15.220 --> 00:52:17.250
And therefore, if you're
trying to maximize your

00:52:17.250 --> 00:52:21.590
profits and you can say, I've
talked to 10 experts, they've

00:52:21.590 --> 00:52:23.300
told me five different things.

00:52:23.300 --> 00:52:25.940
It gives you a perfectly good
excuse for maximizing your

00:52:25.940 --> 00:52:29.100
profits rather than doing
something reasonably sensible

00:52:29.100 --> 00:52:30.410
or reasonably safe.

00:52:30.410 --> 00:52:31.910
So these are important
problems that

00:52:31.910 --> 00:52:32.970
we're dealing with.

00:52:32.970 --> 00:52:35.690
I'm going to start with a brief
discussion of three

00:52:35.690 --> 00:52:37.400
simple cases.

00:52:37.400 --> 00:52:39.600
First one is called simple
random walks.

00:52:39.600 --> 00:52:42.240
The next one is called integer
random walks, and the third

00:52:42.240 --> 00:52:43.770
one is called renewal
processes.

00:52:47.830 --> 00:52:50.150
These random walks we're talking
about sound like very

00:52:50.150 --> 00:52:52.990
simple minded things, but if
you think about it for a

00:52:52.990 --> 00:52:57.730
minute, all of these renewal
processes we've spent all this

00:52:57.730 --> 00:53:00.800
time studying and struggling
with, are just a special case

00:53:00.800 --> 00:53:02.500
of random walks.

00:53:02.500 --> 00:53:08.520
I mean, a renewal process has
non-negative random variables.

00:53:08.520 --> 00:53:10.960
We're adding up those
non-negative random variables

00:53:10.960 --> 00:53:12.340
to see what's going on.

00:53:12.340 --> 00:53:17.560
So a renewal process is just
dealing with sums of iid

00:53:17.560 --> 00:53:21.630
non-negative random variables,
where this is dealing with a

00:53:21.630 --> 00:53:24.000
case where the random variables
can be either

00:53:24.000 --> 00:53:27.070
positive or negative.

00:53:27.070 --> 00:53:30.850
OK, so we're going to start with
a little bit about them.

00:53:35.400 --> 00:53:41.110
A random walk is called simple
if the underlying random

00:53:41.110 --> 00:53:47.740
variable xn n is the simplest
random random variable of all.

00:53:47.740 --> 00:53:50.350
Mainly, it's binary.

00:53:50.350 --> 00:53:55.400
The probability that it's 1,
that it's equal to 1, and the

00:53:55.400 --> 00:53:58.770
probability is minus
1 is equal to q.

00:53:58.770 --> 00:54:04.190
You could make it just 0 or 1,
namely, make it Bernoulli, but

00:54:04.190 --> 00:54:05.440
that makes it too special.

00:54:08.000 --> 00:54:11.420
I mean, since we've been
studying renewal processes,

00:54:11.420 --> 00:54:14.670
where all these random variables
are non-negative,

00:54:14.670 --> 00:54:18.390
let's take the plunge and study
the simplest case of

00:54:18.390 --> 00:54:21.840
random variables, which can be
either positive or negative.

00:54:21.840 --> 00:54:25.440
The simplest case is this binary
case, where the random

00:54:25.440 --> 00:54:28.430
variables can be 1 or minus 1.

00:54:28.430 --> 00:54:31.240
That's what a simple
random walk is.

00:54:31.240 --> 00:54:33.960
OK, it's just a scaling
variation on our Bernoulli

00:54:33.960 --> 00:54:36.090
process though.

00:54:36.090 --> 00:54:41.160
The probability that xi
is equal to 1 for m

00:54:41.160 --> 00:54:42.900
out of the n trials.

00:54:42.900 --> 00:54:48.090
Mainly, you're going to flip
this coin x sub i, heads is 1,

00:54:48.090 --> 00:54:50.495
tails is minus 1.

00:54:50.495 --> 00:54:54.840
The probability that you get
heads for m out of the n

00:54:54.840 --> 00:54:59.130
trials is n factorial
over m factorial

00:54:59.130 --> 00:55:00.960
times n minus m factorial.

00:55:00.960 --> 00:55:04.570
Number of combinations of n
things taken m at a time times

00:55:04.570 --> 00:55:08.890
p to the m times 1 minus
p to the n minus m.

00:55:11.760 --> 00:55:13.760
We've known that forever.

00:55:13.760 --> 00:55:15.580
You probably knew
in high school.

00:55:15.580 --> 00:55:19.850
You relearned it when you were
taking elementary probability.

00:55:19.850 --> 00:55:23.050
We've talked about it
in this course.

00:55:23.050 --> 00:55:26.520
We can also view this
as a Markov chain.

00:55:26.520 --> 00:55:30.420
You start off in state
0 with probability p.

00:55:30.420 --> 00:55:34.380
You go to state 1 with
probability 1 minus p.

00:55:34.380 --> 00:55:37.300
You go to state minus 1.

00:55:37.300 --> 00:55:41.620
On the second trial, you either
go to state 2 or to 1,

00:55:41.620 --> 00:55:46.950
2 or to 0, or to minus
2, and so forth.

00:55:46.950 --> 00:55:51.050
That's just another way of
analyzing the same simple

00:55:51.050 --> 00:55:52.300
random walk.

00:55:55.260 --> 00:55:58.840
Just like in the Stop When
You're Ahead game that we

00:55:58.840 --> 00:56:05.470
talked about in class, the
probability that we ever cross

00:56:05.470 --> 00:56:11.820
a threshold at k equal to p
over 1 minus p to the k.

00:56:11.820 --> 00:56:16.600
Let me remind you of why this
is true, remind you of

00:56:16.600 --> 00:56:19.530
something else.

00:56:19.530 --> 00:56:23.860
I want to find the probability
that any s sub n is greater

00:56:23.860 --> 00:56:29.050
than or equal to k, so I
need this union here.

00:56:29.050 --> 00:56:34.330
Many people write that as
a maximum over n as the

00:56:34.330 --> 00:56:38.430
probability that the maximum
over the s sub n's is greater

00:56:38.430 --> 00:56:40.180
than or equal to k.

00:56:40.180 --> 00:56:44.210
Other people write it as the
probability that the supremum

00:56:44.210 --> 00:56:47.370
of this sum is greater
than or equal to k.

00:56:47.370 --> 00:56:50.930
Why do I like to use the union,
other than the fact

00:56:50.930 --> 00:56:54.490
that I like set theory
more than algebra?

00:56:54.490 --> 00:56:58.770
Anybody know what some simple
reasons for that are?

00:57:03.038 --> 00:57:04.010
Yeah?

00:57:04.010 --> 00:57:05.954
AUDIENCE: It might not be
the max [INAUDIBLE].

00:57:08.870 --> 00:57:09.370
PROFESSOR: What?

00:57:09.370 --> 00:57:11.346
AUDIENCE: You were just trying
to decide whether it crosses--

00:57:11.346 --> 00:57:14.150
the sum-- that k right now,
whether it-- the maximum

00:57:14.150 --> 00:57:15.840
implied that was as high
as it [INAUDIBLE].

00:57:21.610 --> 00:57:24.810
PROFESSOR: Not necessarily,
no.

00:57:24.810 --> 00:57:26.715
I mean, I might not be able
to find the maximum.

00:57:26.715 --> 00:57:31.260
If I'm not dealing with a simple
random walk, and I look

00:57:31.260 --> 00:57:35.290
at the sequence S1, S2, S3,
S4, and I ask what's the

00:57:35.290 --> 00:57:40.480
maximum of that sequence, the
maximum might not exist.

00:57:40.480 --> 00:57:44.260
The supremum always exists if
you include infinity as a

00:57:44.260 --> 00:57:46.540
possible value for it.

00:57:46.540 --> 00:57:49.300
But the supremum is not
very nice either.

00:57:49.300 --> 00:57:55.330
Because suppose I had a random
walk where it was possible for

00:57:55.330 --> 00:57:59.300
the random variables to take on
arbitrarily small values.

00:57:59.300 --> 00:58:02.680
And suppose I had a sequence
which went crawling up towards

00:58:02.680 --> 00:58:07.830
1, as n increases, it keeps
climbing up towards 1, but it

00:58:07.830 --> 00:58:10.950
never quite gets there.

00:58:10.950 --> 00:58:13.940
What's the supremum of
that set of numbers?

00:58:13.940 --> 00:58:14.340
1.

00:58:14.340 --> 00:58:18.170
So the supremum is greater
than or equal to 1.

00:58:18.170 --> 00:58:19.870
But that's not what
I'm interested in.

00:58:19.870 --> 00:58:22.060
I'm interested in the
question, what's the

00:58:22.060 --> 00:58:25.980
probability that any one of
these random variables is

00:58:25.980 --> 00:58:28.440
greater than or equal to 1?

00:58:28.440 --> 00:58:32.400
So that's the straightforward
way to write it.

00:58:32.400 --> 00:58:35.640
Now does anybody care about
these minor differences of a

00:58:35.640 --> 00:58:39.160
maximum or a supremum
or what have you?

00:58:39.160 --> 00:58:39.880
No.

00:58:39.880 --> 00:58:42.510
The only reason you care about
it is when you write a

00:58:42.510 --> 00:58:46.610
maximum, after you become a
little bit careful with your

00:58:46.610 --> 00:58:50.060
mathematics, you say,
is that a maximum?

00:58:50.060 --> 00:58:52.530
Or is it a supremum?

00:58:52.530 --> 00:58:55.320
So you write it as a supremum
and then you go through the

00:58:55.320 --> 00:58:56.860
argument I just went through.

00:58:56.860 --> 00:58:59.980
And the point is, you're not
interested in that at all.

00:58:59.980 --> 00:59:02.360
All you're interested in
is do you cross the

00:59:02.360 --> 00:59:03.970
threshold or don't you?

00:59:03.970 --> 00:59:06.560
And this is the, natural
way to write it.

00:59:06.560 --> 00:59:06.950
OK.

00:59:06.950 --> 00:59:11.210
Now the next thing is, how do
we get this formula here?

00:59:11.210 --> 00:59:16.150
Well, if we're going to cross a
threshold at k, and k is an

00:59:16.150 --> 00:59:19.710
integer, there's not going
to be any overshoot.

00:59:19.710 --> 00:59:23.920
We're either going to get there
and hit it, and then we

00:59:23.920 --> 00:59:25.750
might go on beyond there.

00:59:25.750 --> 00:59:27.940
We'll either hit it
on [INAUDIBLE]

00:59:27.940 --> 00:59:29.460
or we won't hit it.

00:59:29.460 --> 00:59:32.210
Now if we're going to hit it
at [INAUDIBLE], how are we

00:59:32.210 --> 00:59:33.810
going to do that?

00:59:33.810 --> 00:59:38.290
Well we have to hit
1 at some point.

00:59:38.290 --> 00:59:41.690
Because you can only move up
or down by one at a time.

00:59:41.690 --> 00:59:44.540
So you're going to have to
hit 1 at some point.

00:59:44.540 --> 00:59:47.880
Given that you've hit 1, you
have to go on and hit 2 at

00:59:47.880 --> 00:59:49.490
some point.

00:59:49.490 --> 00:59:52.470
Given that you've hit 2, you
have to move up and hit 3 at

00:59:52.470 --> 00:59:53.320
some point.

00:59:53.320 --> 00:59:56.170
Those events are independent
of each other.

00:59:56.170 --> 01:00:00.030
So the probability that you
ever get to k is the

01:00:00.030 --> 01:00:02.800
probability that you can
stop when you're ahead.

01:00:02.800 --> 01:00:07.540
Namely, it's the probability
that you go from 0 to 1 up to

01:00:07.540 --> 01:00:09.080
the k-th power.

01:00:09.080 --> 01:00:13.550
So I'm saying that p over 1
minus p is the probability you

01:00:13.550 --> 01:00:18.780
ever move from state
0 up to state 1.

01:00:18.780 --> 01:00:22.060
And when we were looking at Stop
When You're Ahead, it was

01:00:22.060 --> 01:00:24.220
exactly this game here.

01:00:24.220 --> 01:00:27.990
We moved up with probability
p, down with probability q.

01:00:27.990 --> 01:00:31.220
We were looking at the
probability we'd ever be ahead

01:00:31.220 --> 01:00:35.690
in this gambling game where we
kept betting money, and we

01:00:35.690 --> 01:00:37.140
always had an infinite
capital.

01:00:37.140 --> 01:00:39.890
So we could keep on
betting forever.

01:00:39.890 --> 01:00:41.990
And we stopped as soon
as we hit 1.

01:00:41.990 --> 01:00:44.800
So it's exactly this
question here.

01:00:44.800 --> 01:00:48.230
The way we solved it at that
point was to write an

01:00:48.230 --> 01:00:50.700
equation for it.

01:00:50.700 --> 01:00:56.860
On the first step, you either
move up, and you're there.

01:00:56.860 --> 01:00:59.650
Or you move down.

01:00:59.650 --> 01:01:03.540
And if you move down, you
have to move up twice.

01:01:03.540 --> 01:01:07.830
So the probability you ever
get from here to here is p

01:01:07.830 --> 01:01:13.190
plus q times the probability
you get from here squared.

01:01:13.190 --> 01:01:15.450
You solve that quadratic
equation.

01:01:15.450 --> 01:01:17.910
You might remember solving
that before.

01:01:17.910 --> 01:01:21.060
And that's where you get
to p over 1 minus p.

01:01:21.060 --> 01:01:23.920
OK, so that's not terribly
important.

01:01:23.920 --> 01:01:27.770
What is important is that this
is the only problem in random

01:01:27.770 --> 01:01:31.670
walks I know which has
a trivial solution.

01:01:31.670 --> 01:01:34.200
Most problems have a harder
solution than that.

01:01:34.200 --> 01:01:39.700
But there is at least one
problem which is easy.

01:01:39.700 --> 01:01:43.280
Now for those of you worried
about a final exam, final

01:01:43.280 --> 01:01:46.340
exams have to find problems
which are easy.

01:01:46.340 --> 01:01:51.450
So that tells you something.

01:01:51.450 --> 01:01:53.840
I mean, we have to take the easy
results in this course,

01:01:53.840 --> 01:01:56.020
and we have to use
them in someway.

01:01:56.020 --> 01:01:57.640
And since I'm telling
you that, I probably

01:01:57.640 --> 01:02:00.145
will include it, so--

01:02:00.145 --> 01:02:01.510
OK.

01:02:01.510 --> 01:02:02.920
Integer random walks.

01:02:02.920 --> 01:02:04.835
That's the next kind of random
walk to talk about.

01:02:07.880 --> 01:02:10.400
x is an integer random
variable.

01:02:10.400 --> 01:02:15.300
So similarly, s sub n is an
integer random variable.

01:02:15.300 --> 01:02:18.400
So when we're moving up, we
move up in integer values,

01:02:18.400 --> 01:02:20.680
which makes things a
little bit easier.

01:02:20.680 --> 01:02:25.200
This means, again, that we can
model this as a Markov chain.

01:02:25.200 --> 01:02:29.740
In the Markov chain, we start
at 0, we move up whatever

01:02:29.740 --> 01:02:33.450
number of values x sub n can
have, or we move down whatever

01:02:33.450 --> 01:02:35.160
number of values it can have.

01:02:35.160 --> 01:02:37.770
And then from there, we
keep moving again.

01:02:37.770 --> 01:02:42.900
So it's this Markov chain with
a very regular structure,

01:02:42.900 --> 01:02:46.520
where for each state, the set
of various states of up

01:02:46.520 --> 01:02:50.990
transition and set of down
transitions are all the same

01:02:50.990 --> 01:02:52.030
for all the states.

01:02:52.030 --> 01:02:56.730
So that's a simple Markov
chain to deal with.

01:02:56.730 --> 01:03:00.020
OK, and then we said that
renewal processes are special

01:03:00.020 --> 01:03:06.020
cases of random walks where x is
a positive random variable.

01:03:06.020 --> 01:03:08.940
When you're sketching sample
paths, the axes are usually

01:03:08.940 --> 01:03:13.600
reversed from random processes
to random walks.

01:03:13.600 --> 01:03:16.890
I'm just pointing this out
because every time you try to

01:03:16.890 --> 01:03:25.670
go from a random walk to a
two-way renewal process, you

01:03:25.670 --> 01:03:30.620
will be happily writing
equations and drawing pictures

01:03:30.620 --> 01:03:32.670
of what you're doing.

01:03:32.670 --> 01:03:37.300
And you will suddenly come up
against this problem, which is

01:03:37.300 --> 01:03:42.810
that whenever you draw a figure
for a renewal process,

01:03:42.810 --> 01:03:44.792
you draw it this way.

01:03:44.792 --> 01:03:45.290
OK?

01:03:45.290 --> 01:03:48.050
In other words, what you're
looking at is the time at

01:03:48.050 --> 01:03:49.780
which the n-th arrival occurs.

01:03:49.780 --> 01:03:51.570
Time is going this way.

01:03:51.570 --> 01:03:54.840
The time at which the first
arrival occurs is S1.

01:03:54.840 --> 01:03:57.000
The time at which the second
arrival occurs is

01:03:57.000 --> 01:03:59.310
S2, S3, and so forth.

01:03:59.310 --> 01:04:00.880
These intervals here--

01:04:00.880 --> 01:04:06.950
x1, x2, x3, x4, x5,
and so forth--

01:04:06.950 --> 01:04:09.070
whenever you're dealing with
a random walk, what you're

01:04:09.070 --> 01:04:15.140
interested in directly
these s's and x's.

01:04:15.140 --> 01:04:16.940
And you always draw it
the opposite way

01:04:16.940 --> 01:04:19.580
with the axes reversed.

01:04:19.580 --> 01:04:23.650
And what's confusing to you at
first is that when you see

01:04:23.650 --> 01:04:27.550
this picture, it doesn't look
like this picture at all.

01:04:27.550 --> 01:04:29.530
These two pictures
are identical

01:04:29.530 --> 01:04:32.030
with the axes reversed.

01:04:32.030 --> 01:04:35.930
One is the way you draw
renewal processes.

01:04:35.930 --> 01:04:39.880
The other is the way you
draw random walks--

01:04:39.880 --> 01:04:42.590
which also suggests that
sometimes when you're dealing

01:04:42.590 --> 01:04:46.250
with random walks, you want
to draw a picture like the

01:04:46.250 --> 01:04:50.150
picture you draw for renewal
processes and vice versa.

01:04:50.150 --> 01:04:53.670
So the two are very
closely related.

01:04:53.670 --> 01:05:00.235
OK, so I want to do a little bit
about the queuing delay in

01:05:00.235 --> 01:05:02.010
a G/G/1 queue.

01:05:02.010 --> 01:05:05.190
We've talked about G/G/1 queues
a little bit in terms

01:05:05.190 --> 01:05:09.640
of Little's Theorem and
various other things.

01:05:09.640 --> 01:05:13.470
And there's one very simple
thing about G/G/1 queues that

01:05:13.470 --> 01:05:17.290
follows from looking
at random walks.

01:05:17.290 --> 01:05:20.220
And there's a nice part of the
problem you can solve dealing

01:05:20.220 --> 01:05:22.940
with random walks.

01:05:22.940 --> 01:05:31.980
Let's let x sub i be the IID
random variables, which are

01:05:31.980 --> 01:05:36.320
inter-arrival intervals
for the G/G/1 queue.

01:05:36.320 --> 01:05:38.640
Here's the first inter-arrival
interval.

01:05:38.640 --> 01:05:39.880
Here's the second.

01:05:39.880 --> 01:05:41.075
Here's the third.

01:05:41.075 --> 01:05:45.760
Here's the fourth up
here, and so forth.

01:05:45.760 --> 01:05:50.460
The departure occurs--

01:05:50.460 --> 01:05:55.670
there's an inherent arrival that
we visualize at time 0.

01:05:55.670 --> 01:06:02.720
y 0 is a service time that
that arrival requires.

01:06:02.720 --> 01:06:08.730
y1 is the service time that this
first arrival requires.

01:06:08.730 --> 01:06:11.540
y2 is the service time
of the third arrival.

01:06:11.540 --> 01:06:14.570
These service times are all
independent of each other.

01:06:14.570 --> 01:06:16.850
We've looked at this
picture before.

01:06:16.850 --> 01:06:19.520
What we haven't done is
to draw the obvious

01:06:19.520 --> 01:06:20.770
conclusion from it.

01:06:23.670 --> 01:06:30.420
If you try to talk about what
is the waiting time in queue

01:06:30.420 --> 01:06:34.670
of the n-th customer, how can
we draw that in a figure?

01:06:34.670 --> 01:06:37.150
Well here it is in
this figure.

01:06:40.490 --> 01:06:43.470
The 0-th customer has no waiting
time at all, because

01:06:43.470 --> 01:06:45.290
there's nothing in the queue.

01:06:45.290 --> 01:06:47.440
It goes directly into
the server.

01:06:47.440 --> 01:06:48.700
I'm just talking about queueing

01:06:48.700 --> 01:06:50.690
time, not system time.

01:06:50.690 --> 01:06:55.050
So 0 goes directly into the
server, takes some long

01:06:55.050 --> 01:06:58.080
service time, ends here.

01:06:58.080 --> 01:07:03.440
The next arrival occurs at
this time here after this

01:07:03.440 --> 01:07:05.650
inter-arrival time.

01:07:05.650 --> 01:07:07.090
And what's it have to do?

01:07:07.090 --> 01:07:11.590
It has to wait until this
customer finishes.

01:07:11.590 --> 01:07:14.650
And then it has to wait for its
own service time before it

01:07:14.650 --> 01:07:16.720
gets out of the system.

01:07:16.720 --> 01:07:23.990
The next customer comes in x2
units of time after the first

01:07:23.990 --> 01:07:25.690
customer comes in.

01:07:25.690 --> 01:07:27.250
So it comes in here.

01:07:27.250 --> 01:07:31.020
It has to wait in queue until
after the first customer gets

01:07:31.020 --> 01:07:32.580
finished, and so forth.

01:07:32.580 --> 01:07:40.410
Now look at the time from the
arrival of the second customer

01:07:40.410 --> 01:07:45.090
until the second customer
goes into service.

01:07:45.090 --> 01:07:46.190
What is that?

01:07:46.190 --> 01:07:48.040
It can be written in
two different ways.

01:07:48.040 --> 01:07:52.090
And this is the observation
that makes it possible to

01:07:52.090 --> 01:07:55.010
analyze G/G/1 queues.

01:07:55.010 --> 01:07:59.500
The inter-arrival time of the
second customer plus the

01:07:59.500 --> 01:08:03.080
queueing time of the second
customer is equal to the

01:08:03.080 --> 01:08:06.470
queuing time of the first
customer plus the service time

01:08:06.470 --> 01:08:07.865
of the first customer.

01:08:07.865 --> 01:08:09.115
OK?

01:08:15.210 --> 01:08:17.700
This time is equal
to that time.

01:08:17.700 --> 01:08:20.350
Put little ovals around them
so you can spot it.

01:08:20.350 --> 01:08:28.020
So in general, if the arrival n
is queued, then xn plus wn,

01:08:28.020 --> 01:08:31.220
the inter-arrival time for the
n-th customer plus its waiting

01:08:31.220 --> 01:08:35.609
time in queue is equal to the
service time of the previous

01:08:35.609 --> 01:08:38.560
customer plus the waiting
time of the

01:08:38.560 --> 01:08:40.380
previous customer in queue.

01:08:40.380 --> 01:08:45.514
If arrival n sees an empty
queue, then wn equals 0.

01:08:45.514 --> 01:08:46.939
OK?

01:08:46.939 --> 01:08:49.960
This is easy.

01:08:49.960 --> 01:08:51.269
If you didn't draw
this picture, it

01:08:51.269 --> 01:08:52.630
would be very hard.

01:08:52.630 --> 01:08:55.870
If you tried to do this
by algebra, you

01:08:55.870 --> 01:08:57.000
would never get done.

01:08:57.000 --> 01:09:00.910
But when you draw a picture,
there's nothing to it.

01:09:00.910 --> 01:09:08.220
OK, so that says w sub n is
equal to y sub n minus 1 minus

01:09:08.220 --> 01:09:15.939
xn plus w sub n minus 1 if w sub
n minus 1 plus yn minus 1

01:09:15.939 --> 01:09:18.529
is greater than or equal
to xn, namely, if xn

01:09:18.529 --> 01:09:20.279
gets queued at all.

01:09:20.279 --> 01:09:22.740
And otherwise, w sub
n is equal to 0.

01:09:22.740 --> 01:09:25.670
In other words, if the n-th
customer enters a queue which

01:09:25.670 --> 01:09:30.529
is empty, it doesn't wait
in queue at all.

01:09:30.529 --> 01:09:34.050
It goes right into service if
that condition is satisfied.

01:09:34.050 --> 01:09:38.729
So we can write this whole thing
as w sub n is equal to

01:09:38.729 --> 01:09:48.950
the maximum of w sub n minus 1
plus yn minus 1 minus xn or 0.

01:09:48.950 --> 01:09:52.229
Namely, what's happening is that
the amount of time that

01:09:52.229 --> 01:09:55.780
each customer has to wait in the
queue, not in the system--

01:09:55.780 --> 01:10:00.570
waiting time in the queue is
either this quantity here

01:10:00.570 --> 01:10:05.100
that's related to the waiting
time of the previous customer

01:10:05.100 --> 01:10:07.960
added to the service time of
the previous customer minus

01:10:07.960 --> 01:10:12.240
the n arrival time of
the customer before.

01:10:12.240 --> 01:10:21.360
If you define u sub n as y sub
n minus 1 minus plus xn--

01:10:21.360 --> 01:10:23.610
y sub n minus 1 minus xn--

01:10:23.610 --> 01:10:27.180
namely, the service time of
the n minus first customer

01:10:27.180 --> 01:10:32.220
minus the inter-arrival time for
the n-th customer, this is

01:10:32.220 --> 01:10:34.930
independent over n.

01:10:34.930 --> 01:10:38.180
So this is a IID random
variable.

01:10:38.180 --> 01:10:41.750
Each of the arrivals
x sub n are IID.

01:10:41.750 --> 01:10:43.130
Each of the departures
and each of the

01:10:43.130 --> 01:10:45.560
service times are IID.

01:10:45.560 --> 01:10:49.220
Arrivals and services are
independent of each other.

01:10:49.220 --> 01:10:53.760
So this u sub n random variable
is a sequence of IID

01:10:53.760 --> 01:10:55.840
random variables.

01:10:55.840 --> 01:10:59.750
And what this is saying then,
is that w sub n is equal to

01:10:59.750 --> 01:11:05.570
the maximum of w sub n
minus 1 plus un or 0.

01:11:05.570 --> 01:11:08.950
If for the time being,
ignore that maximum.

01:11:08.950 --> 01:11:11.170
I mean, suppose the queue
is very, very long.

01:11:11.170 --> 01:11:12.850
The queue is very, very long.

01:11:12.850 --> 01:11:14.590
You don't have 0.

01:11:14.590 --> 01:11:19.080
Everybody has some
service time.

01:11:19.080 --> 01:11:22.870
This would be a random walk
then. w sub n, the waiting

01:11:22.870 --> 01:11:27.480
time for the n-th customer, is
equal to the waiting time of

01:11:27.480 --> 01:11:30.160
the n minus first customer
plus u sub n.

01:11:30.160 --> 01:11:35.710
u sub n is playing the role of
the inter-arrival in the

01:11:35.710 --> 01:11:37.180
random walk.

01:11:37.180 --> 01:11:39.800
That's the x's that we had
in the random walk.

01:11:39.800 --> 01:11:42.320
w sub n minus 1 is the sum.

01:11:42.320 --> 01:11:46.870
w sub n is equal to the previous
sum plus the new

01:11:46.870 --> 01:11:48.710
random variable coming in.

01:11:48.710 --> 01:11:51.670
So without the maximum, this
is just a random walk.

01:11:51.670 --> 01:11:59.960
The w sub n's are the random
walk based on these peculiar

01:11:59.960 --> 01:12:03.750
random variables u sub i, which
are the peculiar random

01:12:03.750 --> 01:12:09.500
variables of service times minus
inter-arrival times.

01:12:09.500 --> 01:12:10.190
OK.

01:12:10.190 --> 01:12:16.110
With the max, this says w sub
n is like a random walk.

01:12:16.110 --> 01:12:17.860
What does it do?

01:12:17.860 --> 01:12:21.060
It keeps going up for a while
you have a queue.

01:12:21.060 --> 01:12:24.220
Then it might start dropping.

01:12:24.220 --> 01:12:26.090
But it can't go negative.

01:12:26.090 --> 01:12:31.390
Whenever it gets down to 0, it
gets down below 0, the next

01:12:31.390 --> 01:12:33.330
time you start at 0 again.

01:12:33.330 --> 01:12:34.960
And you start going again.

01:12:34.960 --> 01:12:36.980
The next time it goes
negative, you get

01:12:36.980 --> 01:12:39.590
bumped up to 0 again.

01:12:39.590 --> 01:12:45.630
It's like a naughty kid who
goes through all sorts of

01:12:45.630 --> 01:12:48.900
problems, but as soon as he goes
negative, somebody picks

01:12:48.900 --> 01:12:51.910
him up and starts him
back up again.

01:12:51.910 --> 01:12:54.740
So it's a--

01:12:54.740 --> 01:12:56.730
well, it's a different
kind of process.

01:12:56.730 --> 01:13:00.650
But anyway, it's like a random
walk, except it has this

01:13:00.650 --> 01:13:04.650
peculiar characteristic that
any time it crosses the 0

01:13:04.650 --> 01:13:07.910
threshold, it gets bumped back
up, and you start over again.

01:13:07.910 --> 01:13:10.630
So you have these segments
which make it a renewal

01:13:10.630 --> 01:13:11.610
process also.

01:13:11.610 --> 01:13:14.460
You have renewals whenever
you get to 0 and

01:13:14.460 --> 01:13:17.020
you start over again.

01:13:17.020 --> 01:13:20.460
And we'll talk about how those
processes work later.

01:13:20.460 --> 01:13:23.830
The text has another way of
looking at it, where you look

01:13:23.830 --> 01:13:27.040
at this random walk going
backwards in time rather than

01:13:27.040 --> 01:13:28.490
forward in time.

01:13:28.490 --> 01:13:31.380
But we don't want to spend
a lot of time on that.

01:13:31.380 --> 01:13:34.920
We just want to see this does
have something to do with

01:13:34.920 --> 01:13:36.920
random walks.

01:13:36.920 --> 01:13:40.840
I want to spend some real time
studying detection and

01:13:40.840 --> 01:13:45.460
decisions and hypothesis
testing, because each one of

01:13:45.460 --> 01:13:47.990
these things are very important
in a number of

01:13:47.990 --> 01:13:51.160
different fields.

01:13:51.160 --> 01:13:58.410
If you study radar or any kind
of military problem, detection

01:13:58.410 --> 01:13:59.300
is very important.

01:13:59.300 --> 01:14:02.620
You want to see whether
something has happened or not.

01:14:02.620 --> 01:14:05.500
And often you have to see
whether it's happened in the

01:14:05.500 --> 01:14:08.190
presence of a great deal
of noise or something.

01:14:08.190 --> 01:14:10.710
So you're not sure of whether
it's happened or not.

01:14:10.710 --> 01:14:14.750
So you make many observations to
see whether it's happened.

01:14:14.750 --> 01:14:18.390
And then you try to make a
decision on the basis of all

01:14:18.390 --> 01:14:19.900
of those things.

01:14:19.900 --> 01:14:21.690
Decisions--

01:14:21.690 --> 01:14:24.150
that's what control people
think about all the time.

01:14:24.150 --> 01:14:27.830
Control freaks always want
to make decisions.

01:14:27.830 --> 01:14:29.330
They don't want you
to make decisions.

01:14:29.330 --> 01:14:32.420
They want to make the decisions
themselves.

01:14:32.420 --> 01:14:35.320
But that's part of what
studying random

01:14:35.320 --> 01:14:37.400
processes is all about.

01:14:37.400 --> 01:14:40.650
How do you make sensible
decisions?

01:14:40.650 --> 01:14:41.490
You generally--

01:14:41.490 --> 01:14:44.880
when you make a decision, you
make it with some uncertainty

01:14:44.880 --> 01:14:46.570
connected to it.

01:14:46.570 --> 01:14:49.640
I mean, people respond to this
by pretending there isn't any

01:14:49.640 --> 01:14:50.870
uncertainty.

01:14:50.870 --> 01:14:53.190
They pretend when they
made a decision that

01:14:53.190 --> 01:14:54.530
they must be right.

01:14:54.530 --> 01:14:57.670
But actually, they
could be wrong.

01:14:57.670 --> 01:15:01.400
And sensible people face the
fact that they might be wrong.

01:15:01.400 --> 01:15:04.830
And therefore, they try to
analyze what's the probability

01:15:04.830 --> 01:15:07.520
of being wrong, what's the
probability of being right,

01:15:07.520 --> 01:15:08.780
and all of these things.

01:15:08.780 --> 01:15:10.740
Hypothesis testing--

01:15:10.740 --> 01:15:12.650
all scientists deal with this.

01:15:12.650 --> 01:15:16.930
There are competing theories
in some field, and in these

01:15:16.930 --> 01:15:20.660
competing theories, you want to
find out which is the right

01:15:20.660 --> 01:15:21.990
hypothesis.

01:15:21.990 --> 01:15:24.190
So you do a lot of tests.

01:15:24.190 --> 01:15:28.660
And after all those tests, you
try to decide which hypothesis

01:15:28.660 --> 01:15:30.510
you believe in.

01:15:30.510 --> 01:15:34.140
Why is it important
to make a choice?

01:15:34.140 --> 01:15:37.630
Well it's important to make a
choice in all sorts of reasons

01:15:37.630 --> 01:15:41.040
and all sorts of areas.

01:15:41.040 --> 01:15:44.290
Even in a scientific
area, I mean-- the

01:15:44.290 --> 01:15:46.740
field has to move somehow.

01:15:46.740 --> 01:15:50.480
If the field has all sorts
of open questions and if

01:15:50.480 --> 01:15:53.850
everybody says, well, there
might be quantum theory, or

01:15:53.850 --> 01:15:56.000
there might not be
quantum theory.

01:15:56.000 --> 01:15:57.410
It might all be wrong.

01:15:57.410 --> 01:15:58.660
It might all be right.

01:15:58.660 --> 01:16:01.070
I don't know.

01:16:01.070 --> 01:16:02.830
I mean there's noise
in the system,

01:16:02.830 --> 01:16:04.390
we can't tell anything.

01:16:04.390 --> 01:16:07.360
So all they're dealing with is
a set of very complicated

01:16:07.360 --> 01:16:08.440
probabilities.

01:16:08.440 --> 01:16:11.530
Instead of that, people
do make decisions.

01:16:11.530 --> 01:16:15.820
They say, let's proceed on the
basis of what they think is

01:16:15.820 --> 01:16:17.860
the best course of action.

01:16:17.860 --> 01:16:22.230
You're voting for a candidate
for public office.

01:16:22.230 --> 01:16:25.390
Well you can say,
I don't care.

01:16:25.390 --> 01:16:28.520
But you can't vote for both.

01:16:28.520 --> 01:16:30.530
You have to choose
one or the other.

01:16:30.530 --> 01:16:31.800
And you have to make
a decision,

01:16:31.800 --> 01:16:33.500
what's the best choice?

01:16:33.500 --> 01:16:38.300
So these problems arise
everywhere.

01:16:38.300 --> 01:16:41.090
And if you leave these problems
out of the study of

01:16:41.090 --> 01:16:44.460
random processes, what
are you left with?

01:16:44.460 --> 01:16:47.736
A purely academic exercise.

01:16:47.736 --> 01:16:48.390
OK?

01:16:48.390 --> 01:16:50.790
All you're doing is you're
establishing probabilities,

01:16:50.790 --> 01:16:52.650
but you're not using them.

01:16:52.650 --> 01:16:56.010
So this is sort of where the
rubber hits the road, when you

01:16:56.010 --> 01:16:59.740
study decision making or
hypothesis testing or

01:16:59.740 --> 01:17:02.190
detection, whichever one
you want to call it.

01:17:02.190 --> 01:17:06.790
But you absolutely need to make
these hypothesis tests at

01:17:06.790 --> 01:17:07.550
some point.

01:17:07.550 --> 01:17:11.020
We're going to call them
hypothesis tests because the

01:17:11.020 --> 01:17:17.420
language used there seems to
be easier to live with than

01:17:17.420 --> 01:17:20.070
the language in other places.

01:17:20.070 --> 01:17:23.750
Not that I like the way the
statisticians do these things.

01:17:23.750 --> 01:17:27.170
Statisticians talk about errors
of the first kind and

01:17:27.170 --> 01:17:28.990
errors of the second kind.

01:17:28.990 --> 01:17:34.980
And I and everyone else I know
never knows what's the first

01:17:34.980 --> 01:17:38.180
kind and what's the
second kind?

01:17:38.180 --> 01:17:41.940
But they never talk about giving
names to these things.

01:17:41.940 --> 01:17:45.760
It's always first kind and
seconds kind and so forth.

01:17:45.760 --> 01:17:49.850
But we won't bother ourselves
about that.

01:17:49.850 --> 01:17:53.990
What we're going to do is we'll
consider only problems

01:17:53.990 --> 01:17:56.650
where you have two possible
decisions.

01:17:56.650 --> 01:18:00.400
So it's a binary decision
problem, binary hypothesis

01:18:00.400 --> 01:18:02.380
testing problem.

01:18:02.380 --> 01:18:05.910
We're going to consider a
sample space which has a

01:18:05.910 --> 01:18:08.860
random variable called
h in it.

01:18:08.860 --> 01:18:14.460
The random variable h can have
two possible values, 0 or 1.

01:18:14.460 --> 01:18:18.800
And you're going to have the
multiple observations that

01:18:18.800 --> 01:18:24.190
you're going to make, Y1, Y2,
Y3, Y4, up to y sub n, say.

01:18:24.190 --> 01:18:27.500
To make the problem easy and
to make it correspond to

01:18:27.500 --> 01:18:33.070
random walks, we're going to
assume that the observations

01:18:33.070 --> 01:18:37.900
are IID conditional on h equals
0, and they're also ID

01:18:37.900 --> 01:18:40.710
conditional on h equals 1.

01:18:40.710 --> 01:18:43.840
If you're used to studying noise
problems, a nice example

01:18:43.840 --> 01:18:48.730
of this is you send one of two
binary values over some

01:18:48.730 --> 01:18:50.620
communication channel.

01:18:50.620 --> 01:18:52.910
There's additive noise
added to it.

01:18:52.910 --> 01:18:57.050
At the receiver, what you see
is what you sent plus some

01:18:57.050 --> 01:18:58.280
additive noise.

01:18:58.280 --> 01:19:03.440
You try to figure out from that
sum of signal plus noise

01:19:03.440 --> 01:19:05.760
was a 0 sent, or was a 1 sent?

01:19:05.760 --> 01:19:08.040
And you don't know which
was sent, and you

01:19:08.040 --> 01:19:10.300
have to take a guess.

01:19:10.300 --> 01:19:12.920
And usually if the noise is not
too big, you can make a

01:19:12.920 --> 01:19:16.410
very good guess, so you don't
make many errors.

01:19:16.410 --> 01:19:18.480
This is exactly the
problem that we're

01:19:18.480 --> 01:19:19.440
concerned with here.

01:19:19.440 --> 01:19:22.090
If you have multiple
observations, you can think of

01:19:22.090 --> 01:19:26.440
sending that same binary
digit n times.

01:19:26.440 --> 01:19:29.650
And then on the basis of all
of those observations, see

01:19:29.650 --> 01:19:31.330
what you want to guess.

01:19:31.330 --> 01:19:33.680
I mean scientific
experiments--

01:19:33.680 --> 01:19:36.700
does a scientific theory ever
get resolved by one

01:19:36.700 --> 01:19:37.950
experiment?

01:19:39.910 --> 01:19:42.830
I mean sometimes you read in
textbooks that it does.

01:19:42.830 --> 01:19:44.020
But it never does.

01:19:44.020 --> 01:19:46.660
Somebody does an experiment.

01:19:46.660 --> 01:19:48.790
They come up with
some conclusion.

01:19:48.790 --> 01:19:51.920
And immediately 10 other groups
around the world are

01:19:51.920 --> 01:19:54.835
all doing the same experiment
to validate it or

01:19:54.835 --> 01:19:56.150
not validate it.

01:19:56.150 --> 01:19:58.950
Maybe they don't all publish
papers about it.

01:19:58.950 --> 01:20:01.520
But pretty soon, that experiment
is done so many

01:20:01.520 --> 01:20:05.240
times with so many variations
on it that you can think of

01:20:05.240 --> 01:20:10.360
having multiple observations
of the same thing.

01:20:10.360 --> 01:20:10.980
OK.

01:20:10.980 --> 01:20:14.030
So when we got all done with it,
we're going to assume that

01:20:14.030 --> 01:20:19.440
the observations will have
probability density functions.

01:20:19.440 --> 01:20:23.880
They're analog and the
hypothesis is binary.

01:20:23.880 --> 01:20:26.250
Doesn't make any difference, you
can have observations that

01:20:26.250 --> 01:20:27.900
are discrete also.

01:20:27.900 --> 01:20:34.150
It's just whether you want to
use probability mass functions

01:20:34.150 --> 01:20:36.120
or probability density
functions.

01:20:36.120 --> 01:20:40.290
It looks a little bit easier
to do it this way.

01:20:40.290 --> 01:20:45.900
So the probability density of
having n observations given

01:20:45.900 --> 01:20:50.810
that the hypothesis l is
the correct one is

01:20:50.810 --> 01:20:52.430
this product here.

01:20:52.430 --> 01:20:57.060
And that's true for both l
equals 1 and l equals 0.

01:20:57.060 --> 01:20:59.830
Let me go just a little
bit further.

01:20:59.830 --> 01:21:02.990
Baye's law then says that the
probability that h is equal to

01:21:02.990 --> 01:21:11.230
the hypothesis l is equal to the
a priori probability that

01:21:11.230 --> 01:21:17.960
the alt hypothesis is correct
times this density divided by

01:21:17.960 --> 01:21:19.990
the sum of the two densities.

01:21:19.990 --> 01:21:22.300
Nothing fancy here at all.

01:21:22.300 --> 01:21:25.420
If you compare these
two probabilities--

01:21:25.420 --> 01:21:29.460
probability that 0 is a correct
hypothesis with the

01:21:29.460 --> 01:21:32.160
probability that 1 is the
correct hypothesis--

01:21:32.160 --> 01:21:34.100
you take the ratio of these.

01:21:34.100 --> 01:21:39.180
What you get this p0 over p1
times this density divided by

01:21:39.180 --> 01:21:41.320
that density.

01:21:41.320 --> 01:21:47.440
Well this was thought of well
over 100 years ago, probably

01:21:47.440 --> 01:21:49.860
150 years ago.

01:21:49.860 --> 01:21:54.440
People fought about it terribly,
terrible fights

01:21:54.440 --> 01:21:54.890
about this.

01:21:54.890 --> 01:21:59.120
One of the worst fights in
science that's ever happened.

01:21:59.120 --> 01:22:02.440
Then we had Bayesian
statisticians and non-Bayesian

01:22:02.440 --> 01:22:04.010
statisticians.

01:22:04.010 --> 01:22:06.520
And you know, these people were
all perfectly willing to

01:22:06.520 --> 01:22:09.880
talk about conditional
probabilities so long as they

01:22:09.880 --> 01:22:12.690
were looking forward in time.

01:22:12.690 --> 01:22:15.650
When they started looking
backward in time, namely

01:22:15.650 --> 01:22:19.720
looking at these conditional
probabilities going backwards,

01:22:19.720 --> 01:22:23.610
and saying, we have hypotheses
in our model.

01:22:23.610 --> 01:22:25.390
These have probabilities.

01:22:25.390 --> 01:22:27.380
We will talk about these
probabilities

01:22:27.380 --> 01:22:28.680
of everything involved.

01:22:28.680 --> 01:22:31.230
We have a complete probabilistic
model.

01:22:31.230 --> 01:22:35.080
We will talk about-- in this
probabilistic model, when you

01:22:35.080 --> 01:22:37.890
get a certain observation,
what's the probability the one

01:22:37.890 --> 01:22:39.020
hypothesis is correct?

01:22:39.020 --> 01:22:42.810
What's the probability the other
hypothesis is correct?

01:22:42.810 --> 01:22:46.240
People suddenly lost everything
they'd learned

01:22:46.240 --> 01:22:50.690
about probability, and said
this can't be right.

01:22:50.690 --> 01:22:52.790
So there were enormous
fights about it.

01:22:52.790 --> 01:22:54.840
Anyway, I wanted to
get to that point.

01:22:54.840 --> 01:22:56.290
Think about that.

01:22:56.290 --> 01:22:59.260
Next time we're going to
start out and do a

01:22:59.260 --> 01:23:00.860
little bit with this.

01:23:00.860 --> 01:23:03.360
And suddenly a random walk
is going to emerge.

01:23:03.360 --> 01:23:05.210
So we will do that next time.

