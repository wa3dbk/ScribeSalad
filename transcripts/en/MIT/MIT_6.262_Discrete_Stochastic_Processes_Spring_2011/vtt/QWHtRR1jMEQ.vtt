WEBVTT
Kind: captions
Language: en

00:00:00.530 --> 00:00:02.960
The following content is
provided under a Creative

00:00:02.960 --> 00:00:04.370
Commons license.

00:00:04.370 --> 00:00:07.410
Your support will help MIT
OpenCourseWare continue to

00:00:07.410 --> 00:00:11.060
offer high quality educational
resources for free.

00:00:11.060 --> 00:00:13.960
To make a donation or view
additional materials from

00:00:13.960 --> 00:00:17.890
hundreds of MIT courses, visit
MIT OpenCourseWare at

00:00:17.890 --> 00:00:19.140
ocw.mit.edu.

00:00:21.936 --> 00:00:22.840
ROBERT GALLAGER: OK.

00:00:22.840 --> 00:00:25.910
Today, I want to review
a little bit of

00:00:25.910 --> 00:00:27.920
what we did last time.

00:00:27.920 --> 00:00:32.860
I think with all the details,
some of you sort of lost the

00:00:32.860 --> 00:00:34.900
main pattern of what
was going on.

00:00:34.900 --> 00:00:37.920
So let me try to talk about
the main pattern.

00:00:37.920 --> 00:00:40.490
I don't want to talk about
the details anymore.

00:00:44.920 --> 00:00:48.920
I think, for the most part in
this course, the best way to

00:00:48.920 --> 00:00:52.200
understand the details of proofs
is to read the notes

00:00:52.200 --> 00:00:56.380
where you can read them at your
own rate, unless there's

00:00:56.380 --> 00:00:58.560
something wrong in the notes.

00:00:58.560 --> 00:01:02.240
I will typically avoid
that from now on.

00:01:02.240 --> 00:01:10.600
The main story that we want to
go through, first is the idea

00:01:10.600 --> 00:01:14.920
of what convergence with
probability 1 means.

00:01:14.920 --> 00:01:17.700
This is a very peculiar
concept.

00:01:17.700 --> 00:01:20.490
And I have to keep going through
it, have to keep

00:01:20.490 --> 00:01:23.800
talking about it with different
notation so that you

00:01:23.800 --> 00:01:30.970
can see it coming any
way you look at it.

00:01:30.970 --> 00:01:34.460
So what the theorem is-- and
it's a very useful theorem--

00:01:34.460 --> 00:01:39.580
it says let Y n be a set of
random variables, which

00:01:39.580 --> 00:01:44.970
satisfy the condition that the
sum of the expectations of the

00:01:44.970 --> 00:01:49.550
absolute value of each
random variable

00:01:49.550 --> 00:01:50.800
is less than infinity.

00:01:53.330 --> 00:01:55.570
First thing I want to point
out is what that means,

00:01:55.570 --> 00:01:59.015
because it's not entirely clear
what is the means to

00:01:59.015 --> 00:01:59.720
start with.

00:01:59.720 --> 00:02:06.990
When you talk about a limit
from n to infinity, from n

00:02:06.990 --> 00:02:17.750
equals 1 to infinity, what it
remains is this sum here

00:02:17.750 --> 00:02:20.010
really means the limit
as m goes to

00:02:20.010 --> 00:02:22.590
infinity of a finite sum.

00:02:22.590 --> 00:02:26.370
Anytime you talk about a limit
of a set of numbers, that's

00:02:26.370 --> 00:02:28.640
exactly what you mean by it.

00:02:28.640 --> 00:02:31.570
So if we're saying that this
quantity is less than

00:02:31.570 --> 00:02:35.210
infinity, it says two things.

00:02:35.210 --> 00:02:38.790
It says that these finite
sums are lesson

00:02:38.790 --> 00:02:41.390
infinity for all a m.

00:02:41.390 --> 00:02:44.970
And it also says that these
finite sums go to a limit.

00:02:44.970 --> 00:02:48.830
And the fact that these finite
sums are going to a limit as m

00:02:48.830 --> 00:02:55.180
gets big says what's a more
intuitive thing, which is that

00:02:55.180 --> 00:02:57.680
the limit as m goes
to infinity.

00:02:57.680 --> 00:03:01.470
And here, instead of going from
1 thing m, I'm going from

00:03:01.470 --> 00:03:05.020
m plus 1 to infinity, and it
doesn't make any difference

00:03:05.020 --> 00:03:09.050
whether I go from m plus 1 to
infinity or m to infinity.

00:03:09.050 --> 00:03:13.500
And what has to happen for this
limit to exist is the

00:03:13.500 --> 00:03:16.790
difference between this and
this, which is this,

00:03:16.790 --> 00:03:19.870
has to go to 0.

00:03:19.870 --> 00:03:23.230
So that what we're really saying
here is that the tail

00:03:23.230 --> 00:03:28.030
sum has to go to 0 here.

00:03:28.030 --> 00:03:32.370
Now, this is a much stronger
requirement than just saying

00:03:32.370 --> 00:03:36.080
that the expected value of the
magnitudes of these random

00:03:36.080 --> 00:03:39.370
variables has to go to 0.

00:03:39.370 --> 00:03:46.010
If, for example, the expected
value of y sub n is 1/n, then

00:03:46.010 --> 00:03:50.580
1/n, the limit of that as n
goes to infinity, is 0.

00:03:50.580 --> 00:03:52.350
So this is satisfied.

00:03:52.350 --> 00:03:58.760
But when you sum one/n here,
you don't get 0.

00:03:58.760 --> 00:04:00.010
And in fact, you get infinity.

00:04:05.000 --> 00:04:07.380
Yes, you get infinity.

00:04:07.380 --> 00:04:10.690
So this requires
more than this.

00:04:10.690 --> 00:04:14.280
This limit here, the requirement
that that equals

00:04:14.280 --> 00:04:22.540
0, implies that the sequence y
sub n actually converges to 0

00:04:22.540 --> 00:04:26.580
in probability, rather than
with probability 1.

00:04:26.580 --> 00:04:30.660
And the first problem in the
homework for this coming week

00:04:30.660 --> 00:04:32.690
is to actually show that.

00:04:32.690 --> 00:04:36.530
And when you show it, I hope
you find out that it is

00:04:36.530 --> 00:04:38.620
absolutely trivial to show it.

00:04:38.620 --> 00:04:41.340
It takes two lines to show
it, and that's really

00:04:41.340 --> 00:04:43.210
all you need here.

00:04:43.210 --> 00:04:47.370
The stronger requirement here,
let's us say something about

00:04:47.370 --> 00:04:48.880
the entire sample path.

00:04:48.880 --> 00:04:52.820
You see, this requirement really
is focusing on each

00:04:52.820 --> 00:04:55.110
individual value of
n, but it's not

00:04:55.110 --> 00:04:58.140
focusing on the sequences.

00:04:58.140 --> 00:05:02.250
This stronger quantity here is
really focusing on these

00:05:02.250 --> 00:05:05.270
entire sample paths.

00:05:05.270 --> 00:05:05.600
OK.

00:05:05.600 --> 00:05:08.550
So let's go on and review what
the strong law of large

00:05:08.550 --> 00:05:11.100
numbers says.

00:05:11.100 --> 00:05:16.460
And another note about the
theorem on convergence, how

00:05:16.460 --> 00:05:21.840
useful that theorem is depends
on how you choose these random

00:05:21.840 --> 00:05:24.745
variable, Y1, Y2,
and so forth.

00:05:27.430 --> 00:05:30.420
When we were proving the strong
law of large numbers in

00:05:30.420 --> 00:05:34.670
class last time, and in the
notes, we started off by

00:05:34.670 --> 00:05:38.150
assuming that the mean
of x is equal to 0.

00:05:38.150 --> 00:05:42.145
In fact, we'll see that that's
just for convenience.

00:05:44.690 --> 00:05:48.290
It's not something that has
anything to do with anything.

00:05:48.290 --> 00:05:51.310
It's just to get rid of a
lot of extra numbers.

00:05:51.310 --> 00:05:52.660
So we assume this.

00:05:52.660 --> 00:05:55.530
We also assume that the expected
value of x to the

00:05:55.530 --> 00:05:57.720
fourth was less than infinity.

00:05:57.720 --> 00:06:01.510
In other words, we assume that
this random variable that we

00:06:01.510 --> 00:06:05.220
were adding, these IID
random variables,

00:06:05.220 --> 00:06:07.720
had a fourth moment.

00:06:07.720 --> 00:06:09.540
Now, an awful lot of
random variables

00:06:09.540 --> 00:06:11.230
have a fourth moment.

00:06:11.230 --> 00:06:16.110
The real strong law of large
numbers, all that assumes is

00:06:16.110 --> 00:06:19.490
that the expected value
of the magnitude of x

00:06:19.490 --> 00:06:22.900
is less than infinity.

00:06:22.900 --> 00:06:26.740
So it has a much weaker
set of conditions.

00:06:26.740 --> 00:06:29.530
Most of the problems you run
into, doesn't make any

00:06:29.530 --> 00:06:33.830
difference whether you assume
this or you make the stronger

00:06:33.830 --> 00:06:37.070
assumption that the mean
is equal to 0.

00:06:37.070 --> 00:06:40.180
When you start applying this
theorem, it doesn't make any

00:06:40.180 --> 00:06:44.510
difference at all, because
there's no way you can tell,

00:06:44.510 --> 00:06:48.570
in a physical situation, whether
it is reasonable to

00:06:48.570 --> 00:06:57.970
assume that the fourth moment
is finite and the

00:06:57.970 --> 00:06:59.110
first moment isn't.

00:06:59.110 --> 00:07:02.930
Because that question has to
do only with the very far

00:07:02.930 --> 00:07:05.030
tails of the distribution.

00:07:05.030 --> 00:07:10.630
I can take any distribution x
and I can truncate it at 10 to

00:07:10.630 --> 00:07:11.735
the google.

00:07:11.735 --> 00:07:15.750
And if I truncate it at 10 to
the google, it has a finite

00:07:15.750 --> 00:07:16.840
fourth moment.

00:07:16.840 --> 00:07:19.010
If I don't truncate
it, it might not

00:07:19.010 --> 00:07:20.900
have a fourth moment.

00:07:20.900 --> 00:07:24.240
There's no way you can tell
from looking at physical

00:07:24.240 --> 00:07:26.090
situations.

00:07:26.090 --> 00:07:30.190
So this question here is
primarily a question of

00:07:30.190 --> 00:07:34.020
modeling and what you're going
to do with the models.

00:07:34.020 --> 00:07:37.890
It's not something
which is crucial.

00:07:37.890 --> 00:07:42.710
But anyway, after we did that,
what we said was when we

00:07:42.710 --> 00:07:46.480
assume that this is less than
infinity, we can look at s sub

00:07:46.480 --> 00:07:54.000
n, which is x1, up to x n all
IID We then took this sum

00:07:54.000 --> 00:07:57.210
here, took to the fourth moment
of it, and we looked at

00:07:57.210 --> 00:07:59.630
all the cross terms and
what could happen.

00:07:59.630 --> 00:08:03.660
And we found that the only cross
terms that worked that

00:08:03.660 --> 00:08:09.250
were non-0 was where either
these quantities were paired

00:08:09.250 --> 00:08:15.080
together, x1 x1, x2, x2, or
where they were all the same.

00:08:15.080 --> 00:08:18.530
And then when we looked at that,
we very quickly realized

00:08:18.530 --> 00:08:22.880
that the expected value of s n
to the fourth was proportional

00:08:22.880 --> 00:08:25.020
to n squared.

00:08:25.020 --> 00:08:28.370
It was upper banded the by three
times n squared times

00:08:28.370 --> 00:08:30.880
the fourth moment of x.

00:08:30.880 --> 00:08:35.000
So when we look at s n to the
fourth divided by n to the

00:08:35.000 --> 00:08:41.429
fourth, that quantity, summed
over n, goes as 1 over n

00:08:41.429 --> 00:08:44.810
squared, which has a
finite sum over n.

00:08:44.810 --> 00:08:48.420
And therefore, the probability
of the limit as n approaches

00:08:48.420 --> 00:08:53.150
infinity of s n to the fourth
over n fourth equals 0.

00:08:53.150 --> 00:08:56.560
The probability of the
sample path, the

00:08:56.560 --> 00:08:58.390
sample paths converge.

00:08:58.390 --> 00:09:03.200
The probability of that is equal
to 1, which says all

00:09:03.200 --> 00:09:07.580
sample paths converge
with probability 1.

00:09:07.580 --> 00:09:12.030
So this is enough, not quite
enough to prove the strong law

00:09:12.030 --> 00:09:13.210
of large numbers.

00:09:13.210 --> 00:09:16.690
Because what we're interested
in is not s n to the fourth

00:09:16.690 --> 00:09:17.730
over n fourth.

00:09:17.730 --> 00:09:21.810
We're interested in
s sub n over n.

00:09:21.810 --> 00:09:24.740
So we have to go one
step further.

00:09:24.740 --> 00:09:28.430
This is why it's tricky to
figure out what random

00:09:28.430 --> 00:09:33.950
variables you want to use when
you're trying to go from this

00:09:33.950 --> 00:09:37.390
theorem about convergence with
probability 1 to the strong

00:09:37.390 --> 00:09:42.480
law, or the strong law for
renewals, or any other kind of

00:09:42.480 --> 00:09:45.510
strong law doing
anything else.

00:09:45.510 --> 00:09:49.170
It's a fairly tricky matter to
choose what random variables

00:09:49.170 --> 00:09:50.940
you want to talk about there.

00:09:50.940 --> 00:09:57.180
But here, it doesn't make any
difference, as we've said.

00:09:57.180 --> 00:10:03.970
If you let s n of omega over
n be a sub n to the 1/4.

00:10:03.970 --> 00:10:07.520
In other words, what I'm doing
now is I'm focusing on just

00:10:07.520 --> 00:10:09.510
one sample path.

00:10:09.510 --> 00:10:14.080
If I focus on one sample path,
omega, and then each one of

00:10:14.080 --> 00:10:23.690
these terms has some value, a
sub n, and then s sub n of

00:10:23.690 --> 00:10:29.220
omega over n is going to be
equal to a sub n to the 1/4

00:10:29.220 --> 00:10:33.910
power, the 1/4 power of this
quantity over here.

00:10:33.910 --> 00:10:38.900
Now, the question is if the
limit of these numbers--

00:10:38.900 --> 00:10:41.600
And now remember, now we're
talking about a single sample

00:10:41.600 --> 00:10:45.710
path, but all of these sample
paths behave the same way.

00:10:45.710 --> 00:10:51.090
So if this limit here for one
sample path is equal to 0,

00:10:51.090 --> 00:10:58.090
then the limit of a sub n to
the 1/4 is also equal to 0.

00:10:58.090 --> 00:10:59.960
Why is that true?

00:10:59.960 --> 00:11:04.290
That's just a result about
the real number system.

00:11:04.290 --> 00:11:07.500
It's a result about convergence
of real numbers.

00:11:07.500 --> 00:11:10.400
If you take a bunch of real
numbers, which are getting

00:11:10.400 --> 00:11:13.920
very, very small, and you take
the fourth root of those

00:11:13.920 --> 00:11:16.880
numbers, which are getting very
small, the fourth root is

00:11:16.880 --> 00:11:19.740
a lot bigger than the
number itself.

00:11:19.740 --> 00:11:23.980
But nonetheless, the fourth
root is being driven to 0

00:11:23.980 --> 00:11:27.530
also, or at least the absolute
value of the fourth root is

00:11:27.530 --> 00:11:30.720
being driven to 0 also.

00:11:30.720 --> 00:11:32.790
You can see this intuitively
without

00:11:32.790 --> 00:11:38.640
even proving any theorems.

00:11:38.640 --> 00:11:43.390
Except, it is a standard result,
just talking about the

00:11:43.390 --> 00:11:45.960
real number system.

00:11:45.960 --> 00:11:46.440
OK.

00:11:46.440 --> 00:11:50.770
So what that says is the
probability that the limit of

00:11:50.770 --> 00:11:55.680
s sub n over n equals 0.

00:11:55.680 --> 00:11:58.230
This is now is talking
about sample paths

00:11:58.230 --> 00:11:59.930
for each sample path.

00:11:59.930 --> 00:12:04.360
This limit s n over n either
exists or it doesn't exist.

00:12:04.360 --> 00:12:07.170
If it does exist, it's either
equal to 0 or equal to

00:12:07.170 --> 00:12:08.380
something else.

00:12:08.380 --> 00:12:11.550
It says that the probability
that it exists and that it's

00:12:11.550 --> 00:12:15.120
equal to 0 is equal to 1.

00:12:15.120 --> 00:12:15.360
OK.

00:12:15.360 --> 00:12:19.510
Now, remember last time we
talked about something a

00:12:19.510 --> 00:12:20.260
little bit funny.

00:12:20.260 --> 00:12:23.150
We talked about the
Bernoulli process.

00:12:23.150 --> 00:12:27.150
And we talked about the
Bernoulli process using one

00:12:27.150 --> 00:12:32.170
value of p for the probability
that x is equal to 1.

00:12:32.170 --> 00:12:38.670
And what we found is that the
probability of the sample path

00:12:38.670 --> 00:12:43.260
where s sub n over n approach to
p, the probability of that

00:12:43.260 --> 00:12:46.010
set was equal to 1.

00:12:46.010 --> 00:12:50.020
If we change the probability
that x is equal to 1 to some

00:12:50.020 --> 00:12:54.160
other value, that is still a
perfectly well-defined event.

00:12:58.020 --> 00:13:04.460
The event that a sample path,
that sub n over n for a sample

00:13:04.460 --> 00:13:05.880
path approaches p.

00:13:05.880 --> 00:13:11.820
But that sample path, that
event, becomes 0 as soon as

00:13:11.820 --> 00:13:14.190
you change p to some
other value.

00:13:14.190 --> 00:13:17.490
So what we're talking
about here is really

00:13:17.490 --> 00:13:19.510
a probability measure.

00:13:19.510 --> 00:13:23.450
We're not using any kind of
measure theory here, but you

00:13:23.450 --> 00:13:25.970
really have to be careful about
the fact that you're not

00:13:25.970 --> 00:13:35.390
talking about the number of
sequences for which this limit

00:13:35.390 --> 00:13:36.910
is equal to 0.

00:13:36.910 --> 00:13:39.790
You're really talking about
the probability of it.

00:13:39.790 --> 00:13:43.460
And you can't think of it in
terms of number of sequences.

00:13:43.460 --> 00:13:47.800
What's the most probable
sequence for a Bernoulli

00:13:47.800 --> 00:13:55.980
process where p is,
say, 0.317?

00:13:55.980 --> 00:13:57.360
Who knows what the
most probable

00:13:57.360 --> 00:14:00.965
sequence of length n is?

00:14:00.965 --> 00:14:02.378
What?

00:14:02.378 --> 00:14:03.520
There is none?

00:14:03.520 --> 00:14:06.170
Yes, there is.

00:14:06.170 --> 00:14:08.510
It's all 0's, yes.

00:14:08.510 --> 00:14:13.010
The all 0's sequence is more
likely than anything else.

00:14:13.010 --> 00:14:18.720
So why don't these sequences
converge to 0?

00:14:18.720 --> 00:14:23.940
I'm In this case, these
sequences actually converge to

00:14:23.940 --> 00:14:29.550
0.317, if that's
the value of p.

00:14:29.550 --> 00:14:31.010
So what's going on?

00:14:31.010 --> 00:14:34.910
What's going on is a trade-off
between the number of

00:14:34.910 --> 00:14:40.000
sequences and the probability
of those sequences.

00:14:40.000 --> 00:14:43.620
You look at a particular value
of n, there's only one

00:14:43.620 --> 00:14:46.890
sequence which is all 0's.

00:14:46.890 --> 00:14:50.270
It's much more likely than any
of the other sequences.

00:14:50.270 --> 00:14:55.070
There's an enormous number of
sequences where the relative

00:14:55.070 --> 00:14:59.560
frequency of them is
close to 0.317.

00:14:59.560 --> 00:15:02.500
They're very improbable, but
because of the very large

00:15:02.500 --> 00:15:06.040
number of them, those are the
ones that turn out to have all

00:15:06.040 --> 00:15:07.630
the probability here.

00:15:07.630 --> 00:15:10.880
So that's what's going
on in this strong

00:15:10.880 --> 00:15:11.880
law of large numbers.

00:15:11.880 --> 00:15:15.860
You have all of these effects
playing off against each

00:15:15.860 --> 00:15:19.080
other, and it's kind of
phenomenal that you wind up

00:15:19.080 --> 00:15:21.780
with an extraordinarily strong
theorem like this.

00:15:21.780 --> 00:15:25.320
When you call this the strong
law of large numbers, it, in

00:15:25.320 --> 00:15:30.710
fact, is an incredibly strong
theorem, which is not at all

00:15:30.710 --> 00:15:31.860
intuitively obvious.

00:15:31.860 --> 00:15:34.910
It's very, very far from
intuitively obvious.

00:15:34.910 --> 00:15:38.210
If you think it's intuitively
obvious, and you haven't

00:15:38.210 --> 00:15:42.040
studied it for a very long time,
go back and think about

00:15:42.040 --> 00:15:45.100
it again, because there's
something wrong in the way

00:15:45.100 --> 00:15:46.200
you're thinking about it.

00:15:46.200 --> 00:15:51.890
Because this is an absolutely
incredible theorem.

00:15:51.890 --> 00:15:52.500
OK.

00:15:52.500 --> 00:15:59.820
So I want to be a little more
general now and talk about

00:15:59.820 --> 00:16:03.490
sequences converging to
a constant alpha with

00:16:03.490 --> 00:16:05.200
probability 1.

00:16:05.200 --> 00:16:10.060
If the probability of the set of
omega such as the limit as

00:16:10.060 --> 00:16:12.160
n goes to infinity
is z n of omega.

00:16:12.160 --> 00:16:14.260
In other words, we will look
look at a sample path for a

00:16:14.260 --> 00:16:15.600
given omega.

00:16:15.600 --> 00:16:19.000
Let's look at the probability
that that's equal to alpha

00:16:19.000 --> 00:16:21.420
rather than equal to 0.

00:16:21.420 --> 00:16:25.400
That's the case of the
Bernoulli process.

00:16:25.400 --> 00:16:29.930
Bernoulli process with
probability p, if you're

00:16:29.930 --> 00:16:36.210
looking at s n of omega as s n
of omega over n, then this

00:16:36.210 --> 00:16:37.530
converges to alpha.

00:16:37.530 --> 00:16:41.030
You're looking at s n to the
fourth over n to the fourth,

00:16:41.030 --> 00:16:46.390
it converges to p to
the fourth power.

00:16:46.390 --> 00:16:48.420
And all those are equal to 1.

00:16:48.420 --> 00:16:54.300
Now, note that z n converges to
alpha if, and only if, z n

00:16:54.300 --> 00:16:58.290
minus alpha converges to 0.

00:16:58.290 --> 00:17:01.070
In other words, we're talking
about something that's

00:17:01.070 --> 00:17:03.030
relatively trivial here.

00:17:03.030 --> 00:17:05.450
It's not very important.

00:17:05.450 --> 00:17:08.609
Any time I have a sequence
of random variables that

00:17:08.609 --> 00:17:13.220
converges to some non-0
quantity, p, or alpha, or

00:17:13.220 --> 00:17:21.180
whatever, I can also talk
about z n minus alpha.

00:17:21.180 --> 00:17:24.180
And that's another sequence
of random variables.

00:17:24.180 --> 00:17:29.200
And if this converges to alpha,
this converges to 0.

00:17:29.200 --> 00:17:36.540
So all I was doing when I was
talking about this convergence

00:17:36.540 --> 00:17:39.940
theorem of everything converging
to 0, what I was

00:17:39.940 --> 00:17:44.800
doing was really taking these
random variables and looking

00:17:44.800 --> 00:17:47.830
at their variation around the
mean, at their fluctuation

00:17:47.830 --> 00:17:50.160
around the mean, rather
than the actual

00:17:50.160 --> 00:17:51.650
random variable itself.

00:17:51.650 --> 00:17:53.250
You can always do that.

00:17:53.250 --> 00:17:56.560
And by doing it, you need to
introduce a little more

00:17:56.560 --> 00:18:00.480
terminology, and you get rid of
a lot of mess, because then

00:18:00.480 --> 00:18:02.210
the mean doesn't
appear anymore.

00:18:02.210 --> 00:18:06.740
So when we start talking about
renewal processes, which we're

00:18:06.740 --> 00:18:09.350
going to do here, the
inter-renewal

00:18:09.350 --> 00:18:11.840
intervals are positive.

00:18:11.840 --> 00:18:16.820
It's important, the the fact
that they're positive, and

00:18:16.820 --> 00:18:18.690
that they never go negative.

00:18:18.690 --> 00:18:21.370
And because of that, we don't
really want to subtract the

00:18:21.370 --> 00:18:24.950
mean off them, because then we
would have a sequence of

00:18:24.950 --> 00:18:28.180
random variables that weren't
positive anymore.

00:18:28.180 --> 00:18:33.800
So instead of taking away the
mean to avoid a couple of

00:18:33.800 --> 00:18:39.710
extra symbols, we're going to
leave the mean in from now on,

00:18:39.710 --> 00:18:42.760
so these random variables are
going to converge to some

00:18:42.760 --> 00:18:47.300
constant, generally, rather
than converge to 0.

00:18:47.300 --> 00:18:49.850
And it doesn't make
any difference.

00:18:49.850 --> 00:18:50.340
OK.

00:18:50.340 --> 00:18:53.450
So now, the next thing I want to
do is talk about the strong

00:18:53.450 --> 00:18:56.230
law for renewal processes.

00:18:56.230 --> 00:18:59.640
In other words, I want to talk
about what happens when you

00:18:59.640 --> 00:19:05.130
have a renewal counting process
where n of t is the

00:19:05.130 --> 00:19:08.850
number of arrivals
up until time t.

00:19:08.850 --> 00:19:11.540
And we'd like to see if there's
any kind of law of

00:19:11.540 --> 00:19:16.530
large numbers about what happens
to n of t over t as t

00:19:16.530 --> 00:19:18.650
gets very large.

00:19:18.650 --> 00:19:21.610
And there is such a law, and
that's the kind of thing that

00:19:21.610 --> 00:19:23.650
we want to focus on here.

00:19:23.650 --> 00:19:27.630
And that's what we're now
starting to talk about.

00:19:31.070 --> 00:19:35.790
What was it that made us want
to talk about the strong law

00:19:35.790 --> 00:19:39.390
of large numbers instead of the
weak law of large numbers?

00:19:39.390 --> 00:19:45.850
It was really the fact that
these sample paths converge.

00:19:45.850 --> 00:19:48.580
All these other kinds of
convergence, it's the

00:19:48.580 --> 00:19:52.030
distribution function that
converges, it's some

00:19:52.030 --> 00:19:54.020
probability of something
that converges.

00:19:54.020 --> 00:19:58.080
It's always something gross that
you can look at at every

00:19:58.080 --> 00:20:01.780
value of n, and then you can
find the limit of the

00:20:01.780 --> 00:20:04.940
distribution function, or find
the limit of the mean, or find

00:20:04.940 --> 00:20:07.820
the limit of the relative
frequency, or find the limit

00:20:07.820 --> 00:20:10.150
of something else.

00:20:10.150 --> 00:20:14.060
When we talk about the strong
law of large numbers, we are

00:20:14.060 --> 00:20:17.650
really talking about
these sample paths.

00:20:17.650 --> 00:20:22.440
And the fact that we could go
from a convergence theorem

00:20:22.440 --> 00:20:25.770
saying that s n to the fourth
over n to the fourth

00:20:25.770 --> 00:20:29.660
approached the limit, this was
the convergence theorem.

00:20:29.660 --> 00:20:34.300
And from that, we could show
the s sub n over n also

00:20:34.300 --> 00:20:37.900
approached the limit, that
really is the key to why the

00:20:37.900 --> 00:20:42.200
strong law of large numbers
gets used so much, and

00:20:42.200 --> 00:20:44.050
particularly gets used
when we were talking

00:20:44.050 --> 00:20:46.430
about renewal processes.

00:20:46.430 --> 00:20:48.650
What you will find when we
study we study renewal

00:20:48.650 --> 00:20:53.480
processes is that there's
a small part of renewal

00:20:53.480 --> 00:20:54.730
processes--

00:20:59.280 --> 00:21:02.930
there's a small part of the
theory which really says 80%

00:21:02.930 --> 00:21:04.970
of what's important.

00:21:04.970 --> 00:21:08.820
And it's almost trivially
simple, and it's built on the

00:21:08.820 --> 00:21:12.070
strong law for renewal
processes.

00:21:12.070 --> 00:21:15.230
Then there's a bunch of other
things which are not built on

00:21:15.230 --> 00:21:17.970
the strong law, they're built
on the weak law or something

00:21:17.970 --> 00:21:21.600
else, which are quite
tedious, and quite

00:21:21.600 --> 00:21:23.710
difficult, and quite messy.

00:21:23.710 --> 00:21:26.550
We go through them because
they get used in a lot of

00:21:26.550 --> 00:21:29.600
other places, and they let us
learn about a lot of things

00:21:29.600 --> 00:21:31.140
that are very important.

00:21:31.140 --> 00:21:35.380
But they still are
more difficult.

00:21:35.380 --> 00:21:37.860
And they're more difficult
because we're not talking

00:21:37.860 --> 00:21:39.730
about sample paths anymore.

00:21:39.730 --> 00:21:41.010
And you're going to see
that at the end

00:21:41.010 --> 00:21:42.630
of the lecture today.

00:21:42.630 --> 00:21:48.660
You'll see, I think, what is
probably the best illustration

00:21:48.660 --> 00:21:51.880
of why the strong law
of large numbers

00:21:51.880 --> 00:21:54.450
makes your life simple.

00:21:54.450 --> 00:21:55.560
OK.

00:21:55.560 --> 00:21:58.140
So we had this fact here.

00:21:58.140 --> 00:22:03.160
This theorem is what's going
to generalize this.

00:22:03.160 --> 00:22:07.450
Assume that z sub n and greater
than or equal to 1,

00:22:07.450 --> 00:22:10.440
this is a sequence of
random variables.

00:22:10.440 --> 00:22:13.710
And assume that this sequence of
random variables converges

00:22:13.710 --> 00:22:17.900
to some number, alpha,
with probability 1.

00:22:17.900 --> 00:22:22.570
In other words, you take sample
paths of this sequence

00:22:22.570 --> 00:22:24.320
of a random variables.

00:22:24.320 --> 00:22:29.480
And those sample paths, there
two sets of sample paths.

00:22:29.480 --> 00:22:33.810
One set of sample paths
converged to alpha, and that

00:22:33.810 --> 00:22:35.540
has probability 1.

00:22:35.540 --> 00:22:38.430
There's another set of sample
paths, some of them converge

00:22:38.430 --> 00:22:42.510
to something else, some of them
converge to nothing, some

00:22:42.510 --> 00:22:45.460
of them don't converge at all.

00:22:45.460 --> 00:22:46.870
Well, they converge
to nothing.

00:22:46.870 --> 00:22:49.320
They don't converge at all.

00:22:49.320 --> 00:22:53.230
But that set has probability 0,
so we don't worry about it.

00:22:53.230 --> 00:22:56.450
All we're worrying about is this
good set, which is the

00:22:56.450 --> 00:22:58.210
set which converges.

00:22:58.210 --> 00:23:02.570
And then what the theorem says
is if we have a function f of

00:23:02.570 --> 00:23:07.410
x, if it's a real-valued
function of a real variable,

00:23:07.410 --> 00:23:10.690
what does that mean?

00:23:10.690 --> 00:23:14.460
As an engineer, it means
it's a function.

00:23:14.460 --> 00:23:18.400
When you're an engineer and you
talk about functions, you

00:23:18.400 --> 00:23:24.080
don't talk about things that
aren't continuous.

00:23:24.080 --> 00:23:27.160
You talk about things
that are continuous.

00:23:27.160 --> 00:23:32.150
So all that's saying is it gives
us a nice, respectable

00:23:32.150 --> 00:23:35.710
function of a variable.

00:23:35.710 --> 00:23:39.860
It belongs to the national
academy of real variables that

00:23:39.860 --> 00:23:43.120
people like to use.

00:23:43.120 --> 00:23:47.300
Then what the theorem says is
that the sequence of random

00:23:47.300 --> 00:23:50.160
variables f of z sub n--

00:23:50.160 --> 00:23:53.850
OK, we have a real-valued
function of a real variable.

00:23:53.850 --> 00:23:59.660
It maps, then, sample values
of z sub n into f of those

00:23:59.660 --> 00:24:01.010
sample values.

00:24:01.010 --> 00:24:04.380
And because of that, just as
we've done a dozen times

00:24:04.380 --> 00:24:07.550
already when you take a
real-valued function of a

00:24:07.550 --> 00:24:11.340
random variable, you have
a two-step mapping.

00:24:11.340 --> 00:24:17.480
You map from omega into
z n of omega.

00:24:17.480 --> 00:24:23.070
And then you map from z n of
omega into f of z n of omega.

00:24:23.070 --> 00:24:28.290
That's a simple-minded idea.

00:24:28.290 --> 00:24:28.890
OK.

00:24:28.890 --> 00:24:35.410
Example one of this, suppose
that f of x is x plus beta.

00:24:35.410 --> 00:24:38.000
All this is just a translation,
simple-minded

00:24:38.000 --> 00:24:42.970
function, president
of the academy.

00:24:42.970 --> 00:24:45.690
And supposes that the sequence
of random variables

00:24:45.690 --> 00:24:47.480
converges to alpha.

00:24:47.480 --> 00:24:51.690
Then this new set of random
variable u sub n equals z sub

00:24:51.690 --> 00:24:52.640
n plus beta.

00:24:52.640 --> 00:24:57.425
The translated version converges
to alpha plus beta.

00:24:57.425 --> 00:24:59.740
Well, you don't even need
a theorem to see that.

00:24:59.740 --> 00:25:03.270
I mean, you can just look at
it and say, of course.

00:25:03.270 --> 00:25:06.680
Example two is the one
we've already used.

00:25:06.680 --> 00:25:09.660
This one you do have to sweat
over a little bit, but we've

00:25:09.660 --> 00:25:12.110
already sweated over it, and
then we're not going to worry

00:25:12.110 --> 00:25:14.440
about it anymore.

00:25:14.440 --> 00:25:18.930
If f of x is equal to x to the
1/4 for x greater than or

00:25:18.930 --> 00:25:27.290
equal to 0, and z n, random
variable, the sequence of

00:25:27.290 --> 00:25:31.840
random variables, converges to 0
with probability 1 and these

00:25:31.840 --> 00:25:35.760
random variables are
non-negative, then f of z n

00:25:35.760 --> 00:25:38.690
converges to f of 0.

00:25:38.690 --> 00:25:41.730
That's the one that's a little
less obvious, because if you

00:25:41.730 --> 00:25:54.210
look at this function, when x is
very close to 0, but when x

00:25:54.210 --> 00:25:59.670
is equal to 0, it's 0,
it goes like this.

00:25:59.670 --> 00:26:03.070
When x is 1, you're
up to 1, I guess.

00:26:03.070 --> 00:26:06.390
But it really goes up with
an infinite slope here.

00:26:06.390 --> 00:26:10.710
It's still continuous at 0 if
you're looking only at the

00:26:10.710 --> 00:26:11.960
non-negative values.

00:26:14.740 --> 00:26:17.770
That's what we use to
prove the strong

00:26:17.770 --> 00:26:19.080
law of large numbers.

00:26:19.080 --> 00:26:22.490
None of you complained about
it last time, so you can't

00:26:22.490 --> 00:26:25.840
complain about it now.

00:26:25.840 --> 00:26:27.850
It's just part of what this
theorem is saying.

00:26:33.750 --> 00:26:35.370
This is what the theorem says.

00:26:38.870 --> 00:26:43.530
I'm just rewriting it
much more briefly.

00:26:43.530 --> 00:26:48.480
Here I'm going to give a "Pf."
For each omega such that limit

00:26:48.480 --> 00:26:52.500
of z n of omega equals alpha,
we use the result for a

00:26:52.500 --> 00:26:56.600
sequence of numbers that says
the limit of f of z n of

00:26:56.600 --> 00:27:01.740
omega, this limit of a set of
sequence of numbers, is equal

00:27:01.740 --> 00:27:05.390
to the function at
the limit value.

00:27:05.390 --> 00:27:08.560
Let me give you a little diagram
which shows you why

00:27:08.560 --> 00:27:09.810
that has to be true.

00:27:12.680 --> 00:27:14.795
Suppose you look at
this function f.

00:27:20.270 --> 00:27:22.820
This is f of x.

00:27:22.820 --> 00:27:30.460
And what we're doing now is
we're looking at a1 here.

00:27:30.460 --> 00:27:38.220
a1 is going to be f
of z1 of omega.

00:27:38.220 --> 00:27:42.620
a2, I'm just drawing random
numbers here.

00:27:42.620 --> 00:27:52.920
a3, a4, a5, a6, a7.

00:27:56.950 --> 00:27:58.360
And then I draw f of a1.

00:28:11.510 --> 00:28:15.940
So what I'm saying here, in
terms of real numbers, is this

00:28:15.940 --> 00:28:17.590
quite trivial thing.

00:28:17.590 --> 00:28:23.580
If this function is continuous
at this point, as n gets

00:28:23.580 --> 00:28:28.590
large, these numbers get
compressed into that limiting

00:28:28.590 --> 00:28:29.870
value there.

00:28:29.870 --> 00:28:32.990
And as these numbers get
compressed into that limiting

00:28:32.990 --> 00:28:38.080
value, these values up here
get compressed into that

00:28:38.080 --> 00:28:41.030
limiting value also.

00:28:41.030 --> 00:28:48.500
This is not a proof, but the way
I construct proofs is not

00:28:48.500 --> 00:28:52.430
to look for them someplace in
a book, but it's to draw a

00:28:52.430 --> 00:28:56.510
picture which shows me what the
idea of the proof is, then

00:28:56.510 --> 00:28:57.740
I prove it.

00:28:57.740 --> 00:29:01.330
This has an advantage in
research, because if you ever

00:29:01.330 --> 00:29:03.550
want to get a result in
research which you can

00:29:03.550 --> 00:29:05.800
publish, it has to be
something that you

00:29:05.800 --> 00:29:08.760
can't find in a book.

00:29:08.760 --> 00:29:12.110
If you do find it in a book
later, then in fact your

00:29:12.110 --> 00:29:14.740
result was not new And you're
not supposed to publish

00:29:14.740 --> 00:29:17.820
results that aren't new.

00:29:17.820 --> 00:29:20.990
So the idea of drawing a picture
and then proving it

00:29:20.990 --> 00:29:24.610
from the picture is really
a very valuable

00:29:24.610 --> 00:29:26.960
aid in doing research.

00:29:26.960 --> 00:29:29.750
And if you draw this picture,
then you can easily construct

00:29:29.750 --> 00:29:32.410
a proof of the picture.

00:29:32.410 --> 00:29:35.590
But I'm not going
to do it here.

00:29:35.590 --> 00:29:39.410
Now, let's go onto renewal
processes.

00:29:39.410 --> 00:29:40.600
Each and inter-renewal

00:29:40.600 --> 00:29:44.610
interval, x sub i, is positive.

00:29:44.610 --> 00:29:47.070
That was what we said
in starting to talk

00:29:47.070 --> 00:29:50.370
about renewal processes.

00:29:50.370 --> 00:29:53.740
Assuming that the expected
value of x exists, the

00:29:53.740 --> 00:29:59.080
expected value of x is then
strictly greater than 0.

00:29:59.080 --> 00:30:02.262
You're going to prove that in
the homework this time, too.

00:30:02.262 --> 00:30:06.460
When I was trying to get this
lecture ready, I didn't want

00:30:06.460 --> 00:30:10.870
to prove anything in detail,
so I had to follow the

00:30:10.870 --> 00:30:15.200
strategy of assigning problems,
and the problem set

00:30:15.200 --> 00:30:17.460
where you would, in fact, prove
these things, which are

00:30:17.460 --> 00:30:20.500
not difficult but
which require a

00:30:20.500 --> 00:30:23.010
little bit of thought.

00:30:23.010 --> 00:30:25.920
And since the expected value of
x is greater than or equal

00:30:25.920 --> 00:30:31.350
to 0, the expected value of s1,
which is expected value of

00:30:31.350 --> 00:30:35.920
x, the expected value of x1 plus
x2, which is s2, and so

00:30:35.920 --> 00:30:40.760
forth, all of these quantities
are greater than 0 also.

00:30:40.760 --> 00:30:44.470
And for each finite n, the
expected value of s sub n over

00:30:44.470 --> 00:30:47.550
n is greater than 0 also. so
we're talking about a whole

00:30:47.550 --> 00:30:51.640
bunch of positive quantities.

00:30:51.640 --> 00:30:54.250
So this strong law of large
numbers is then

00:30:54.250 --> 00:30:56.390
going to apply here.

00:30:56.390 --> 00:31:00.780
The probability of the sample
paths, s n of omega it over n,

00:31:00.780 --> 00:31:05.600
the probability that that sample
path converges to the

00:31:05.600 --> 00:31:09.430
mean of x, that's just 1.

00:31:09.430 --> 00:31:13.896
Then I use the theorem on
the last page about--

00:31:13.896 --> 00:31:16.330
It's this theorem.

00:31:16.330 --> 00:31:21.350
When I use f of x equals 1/x
that's continuous at this

00:31:21.350 --> 00:31:22.280
positive value.

00:31:22.280 --> 00:31:24.960
That's why it's important to
have a positive value for the

00:31:24.960 --> 00:31:26.430
expected value of x.

00:31:26.430 --> 00:31:30.640
The expected value of x is
equal to 0, 1/x is not

00:31:30.640 --> 00:31:35.770
continuous at x equals 0, and
you're in deep trouble.

00:31:35.770 --> 00:31:39.030
That's one of the reasons why
you really want to assume

00:31:39.030 --> 00:31:45.070
renewal theory and not allow
any inter-renewal intervals

00:31:45.070 --> 00:31:46.640
that are equal to 0.

00:31:46.640 --> 00:31:49.810
It just louses up the whole
theory, makes things much more

00:31:49.810 --> 00:31:53.660
difficult, and you gain
nothing by it.

00:31:53.660 --> 00:31:58.760
So we get this statement then,
the probability of sample

00:31:58.760 --> 00:32:04.650
points such that the limit of
n over s n of omega is equal

00:32:04.650 --> 00:32:05.970
to 1 over x bar.

00:32:05.970 --> 00:32:07.750
That limit is equal to 1.

00:32:07.750 --> 00:32:08.800
What does that mean?

00:32:08.800 --> 00:32:11.570
I look at that and it doesn't
mean anything to me.

00:32:11.570 --> 00:32:16.060
I can't see what it means
until I draw a picture.

00:32:16.060 --> 00:32:19.760
I'm really into pictures
today.

00:32:19.760 --> 00:32:22.920
This was the statement that we
said the probability of this

00:32:22.920 --> 00:32:27.300
set of omega such that the limit
of n over s n of omega

00:32:27.300 --> 00:32:30.690
is equal to 1 over x
bar is equal to 1.

00:32:30.690 --> 00:32:35.690
This is valid whenever you have
a renewal process for

00:32:35.690 --> 00:32:42.380
which x bar exists, namely,
the expected value of the

00:32:42.380 --> 00:32:44.690
magnitude of x exists.

00:32:44.690 --> 00:32:48.670
That was one of the assumptions
we had in here.

00:32:48.670 --> 00:32:50.520
And now, this is going
to imply the

00:32:50.520 --> 00:32:53.320
strong law renewal processes.

00:32:53.320 --> 00:32:57.160
Here's the picture, which lets
us interpret what this means

00:32:57.160 --> 00:32:59.860
and let's just go
further with it.

00:32:59.860 --> 00:33:06.420
The picture now is you have this
counting process, which

00:33:06.420 --> 00:33:10.580
also amounts to a picture of
any set of inter-arrival

00:33:10.580 --> 00:33:18.070
instance, x1, x2, x3, x4, and
so forth, and any set of

00:33:18.070 --> 00:33:22.110
arrival epochs, s1,
s2, and so forth.

00:33:22.110 --> 00:33:26.070
We look at a particular
value of t.

00:33:26.070 --> 00:33:33.650
And what I'm interested
in is n of t over t.

00:33:33.650 --> 00:33:37.300
I have a theorem about
n over s n of omega.

00:33:37.300 --> 00:33:38.790
That's not what I'm
interested in.

00:33:38.790 --> 00:33:41.490
I'm interested in
n of t over t.

00:33:41.490 --> 00:33:45.480
And this picture shows me what
the relationship is.

00:33:45.480 --> 00:33:48.630
So I start out with a
given value of t.

00:33:48.630 --> 00:33:52.300
For a given value of t, there's
a well-defined random

00:33:52.300 --> 00:33:56.100
variable, which is the number
of arrivals up to and

00:33:56.100 --> 00:33:58.250
including time t.

00:33:58.250 --> 00:34:02.670
From n of t, I get a
well-defined random variable,

00:34:02.670 --> 00:34:13.750
which is the arrival epoch of
the latest arrival less than

00:34:13.750 --> 00:34:15.790
or equal to time t.

00:34:15.790 --> 00:34:19.570
Now, this is a very funny
kind of random variable.

00:34:19.570 --> 00:34:22.210
I mean, we've talked about
random variables which are

00:34:22.210 --> 00:34:25.840
functions of other
random variables.

00:34:25.840 --> 00:34:28.620
And in a sense, that's
what this.

00:34:28.620 --> 00:34:31.929
But it's a little more
awful than that.

00:34:31.929 --> 00:34:35.280
Because here we have this
well-defined set of arrival

00:34:35.280 --> 00:34:41.760
epochs, and now we're taking a
particular arrival, which is

00:34:41.760 --> 00:34:44.260
determined by this t
we're looking at.

00:34:44.260 --> 00:34:49.770
So t defines n of t, and n of t
defines s sub n of t, if we

00:34:49.770 --> 00:34:53.070
have this entire sample
function.

00:34:53.070 --> 00:34:55.520
So this is well-defined.

00:34:55.520 --> 00:34:59.960
We will find as we proceed with
this that this random

00:34:59.960 --> 00:35:06.030
variable, the time of the
arrival most recently before

00:35:06.030 --> 00:35:09.900
t, it's in fact a very, very
strange random variable.

00:35:09.900 --> 00:35:12.250
There are strange things
associated with it.

00:35:12.250 --> 00:35:17.120
When I look at t minus s sub n
of t, or when I look at the

00:35:17.120 --> 00:35:23.670
arrival after t, s sub n of t
plus 1 minus t, those random

00:35:23.670 --> 00:35:26.700
variables are peculiar.

00:35:26.700 --> 00:35:30.850
And we're going to explain why
they are peculiar and use the

00:35:30.850 --> 00:35:34.650
strong law for renewal processes
to look at them in a

00:35:34.650 --> 00:35:36.150
kind of a simple way.

00:35:36.150 --> 00:35:40.070
But now, the thing we have
here is if we look at the

00:35:40.070 --> 00:35:47.260
slope of b of t, the slope of
this line here at each value

00:35:47.260 --> 00:35:53.220
of t, this slope is n
of t divided by t.

00:35:53.220 --> 00:35:55.050
That's this slope.

00:35:55.050 --> 00:36:02.390
This slope here is n of
t over s sub n of t.

00:36:02.390 --> 00:36:07.970
Namely, this is the slope up
to the point of the arrival

00:36:07.970 --> 00:36:10.960
right before t.

00:36:10.960 --> 00:36:14.600
This slope is then going
to decrease as

00:36:14.600 --> 00:36:16.800
we move across here.

00:36:16.800 --> 00:36:20.020
And at this value here, it's
going to pop up again.

00:36:20.020 --> 00:36:25.800
So we have a family of slopes,
which is going to look like--

00:36:33.220 --> 00:36:35.350
What's it going to do?

00:36:35.350 --> 00:36:38.050
I don't know where it's going to
start out, so I won't even

00:36:38.050 --> 00:36:39.650
worry about that.

00:36:39.650 --> 00:36:42.080
I'll just start someplace
here.

00:36:42.080 --> 00:36:44.702
It's going to be decreasing.

00:36:44.702 --> 00:36:45.952
Then there's going
to be an arrival.

00:36:49.570 --> 00:36:52.430
At that point, it's going to
increase a little bit.

00:36:52.430 --> 00:36:54.330
It's going to be decreasing.

00:36:54.330 --> 00:36:55.890
There's another arrival.

00:36:55.890 --> 00:37:01.550
So this is s sub n, s sub
n plus 1, and so forth.

00:37:04.180 --> 00:37:08.390
So the slope is slowly
decreasing.

00:37:08.390 --> 00:37:11.580
And then it changes
discontinuously every time you

00:37:11.580 --> 00:37:12.570
have an arrival.

00:37:12.570 --> 00:37:15.190
That's the way this behaves.

00:37:15.190 --> 00:37:19.400
You start out here, it decreases
slowly, it jumps up,

00:37:19.400 --> 00:37:21.490
then it decreases slowly
until the next

00:37:21.490 --> 00:37:23.890
arrival, and so forth.

00:37:23.890 --> 00:37:27.070
So that's the kind of thing
we're looking at.

00:37:27.070 --> 00:37:32.830
But one thing we know is that
n of t over t, that's the

00:37:32.830 --> 00:37:37.460
slope in the middle here, is
less than or equal to n of t

00:37:37.460 --> 00:37:38.800
over s sub n of t.

00:37:38.800 --> 00:37:39.770
Why is that?

00:37:39.770 --> 00:37:46.850
Well, n of t is equal to n of
t, but t is greater than or

00:37:46.850 --> 00:37:49.520
equal to the time of the
most recent arrival.

00:37:52.790 --> 00:37:56.510
So we have n of t over t is less
than or equal to n of t

00:37:56.510 --> 00:37:59.250
over s sub n of t.

00:37:59.250 --> 00:38:03.180
The other thing that's important
to observe is that

00:38:03.180 --> 00:38:05.780
now we want to look at
what happens as t

00:38:05.780 --> 00:38:07.930
gets larger and larger.

00:38:07.930 --> 00:38:15.626
And what happens to this
ratio, n of t over t?

00:38:15.626 --> 00:38:19.010
Well, this ratio n of t over t
is this thing we were looking

00:38:19.010 --> 00:38:21.690
at here, which is
kind of a mess.

00:38:21.690 --> 00:38:24.550
It jumps up, it goes down a
little bit, jumps up, goes

00:38:24.550 --> 00:38:26.690
down a little bit, jumps up.

00:38:26.690 --> 00:38:34.550
But the set of values that
it goes through--

00:38:34.550 --> 00:38:38.570
the set of values that this goes
through, namely, the set

00:38:38.570 --> 00:38:41.410
right before each
of these jumps--

00:38:41.410 --> 00:38:46.860
is the same set of values
as n over s sub n.

00:38:46.860 --> 00:38:51.810
As I look through this sequence,
I look at this n of

00:38:51.810 --> 00:38:55.670
t over s sub n of t.

00:38:55.670 --> 00:39:02.050
That's this point here, and then
it's this point there.

00:39:05.350 --> 00:39:15.290
Anyway, n of t over s sub n of t
is going to stay constant as

00:39:15.290 --> 00:39:19.140
t goes from here
over to there.

00:39:19.140 --> 00:39:20.390
That's the way I've
drawn the picture.

00:39:20.390 --> 00:39:24.910
I start out with any t in
this interval here.

00:39:24.910 --> 00:39:29.110
This slope keeps changing as
t goes from there to there.

00:39:29.110 --> 00:39:31.650
This slope does not change.

00:39:31.650 --> 00:39:36.090
This is determined just by which
particular integer value

00:39:36.090 --> 00:39:38.280
of n we're talking about.

00:39:38.280 --> 00:39:45.140
So n of t over s sub n of t
jumps at each value of n.

00:39:45.140 --> 00:39:51.760
So this now becomes just the
sequence of numbers.

00:39:51.760 --> 00:39:56.190
And that sequence of numbers
is the sequence n

00:39:56.190 --> 00:39:59.090
divided by s sub b.

00:39:59.090 --> 00:40:01.650
Why is that important?

00:40:01.650 --> 00:40:04.660
That's the thing we have
some control over.

00:40:04.660 --> 00:40:08.560
That's what appears up here.

00:40:08.560 --> 00:40:10.410
So we know how to
deal with that.

00:40:12.930 --> 00:40:19.040
That's what this result about
convergence added to this

00:40:19.040 --> 00:40:23.890
result about functions of
converging functions tells us.

00:40:27.350 --> 00:40:33.120
So redrawing the same figure,
we've observed that n of t

00:40:33.120 --> 00:40:38.070
over t is less than or equal to
n of t over s sub n of t.

00:40:38.070 --> 00:40:42.330
It goes through the same set of
values as n over s sub n,

00:40:42.330 --> 00:40:46.900
and therefore the limit as t
goes to infinity of n over t

00:40:46.900 --> 00:40:52.190
over s sub n of t is the same
as the limit as n goes to

00:40:52.190 --> 00:40:55.350
infinity of n over s sub n.

00:40:55.350 --> 00:40:57.990
And that limit, with
probability

00:40:57.990 --> 00:40:59.940
1, is 1 over x bar.

00:40:59.940 --> 00:41:03.650
That's the thing that this
theorem, we just

00:41:03.650 --> 00:41:06.810
"pfed" said to us.

00:41:06.810 --> 00:41:11.890
There's a bit of a pf in here
too, because you really ought

00:41:11.890 --> 00:41:14.740
to show that as t goes
to infinity, n

00:41:14.740 --> 00:41:16.670
of t goes to infinity.

00:41:16.670 --> 00:41:17.930
And that's not hard to do.

00:41:17.930 --> 00:41:19.580
It's done in the notes.

00:41:19.580 --> 00:41:21.350
You need to do it.

00:41:21.350 --> 00:41:24.990
You can almost see intuitively
that it has to happen.

00:41:24.990 --> 00:41:28.050
And in fact, it does
have to happen.

00:41:28.050 --> 00:41:28.650
OK.

00:41:28.650 --> 00:41:33.170
So this, in fact, is a limit.

00:41:33.170 --> 00:41:35.670
It does exist.

00:41:35.670 --> 00:41:40.440
Now, we go on the other y, and
we look at n of t over t,

00:41:40.440 --> 00:41:45.680
which is now greater than or
equal to n of t over s sub n

00:41:45.680 --> 00:41:47.070
of t plus 1.

00:41:47.070 --> 00:41:51.370
s sub b of t plus 1 is the
arrival epoch which is just

00:41:51.370 --> 00:41:54.620
larger than t.

00:41:54.620 --> 00:41:58.980
Now, n of t over s sub n of t
plus 1 goes through the same

00:41:58.980 --> 00:42:04.170
set of values as n over
s sub n plus 1.

00:42:04.170 --> 00:42:08.620
Namely, each time n increases,
this goes up by 1.

00:42:08.620 --> 00:42:13.410
So the limit as t goes to
infinity of n of t over s sub

00:42:13.410 --> 00:42:17.190
n of t plus 1 is the same as
the limit as n goes to

00:42:17.190 --> 00:42:26.570
infinity of n over the epoch
right after n of t.

00:42:26.570 --> 00:42:34.730
This you can rewrite as n plus
1 over s sub n plus 1 times n

00:42:34.730 --> 00:42:35.930
over n plus 1.

00:42:35.930 --> 00:42:38.320
Why do I want to rewrite
it this way?

00:42:38.320 --> 00:42:41.600
Because this quantity
I have a handle on.

00:42:41.600 --> 00:42:45.770
This is the same as the limit
of n over s sub n.

00:42:45.770 --> 00:42:47.760
I know what that limit is.

00:42:47.760 --> 00:42:52.100
This quantity, I have an even
better handle on, because this

00:42:52.100 --> 00:42:55.780
n over n plus 1 just moves--

00:42:58.380 --> 00:43:00.090
it's something that
starts out low.

00:43:00.090 --> 00:43:05.110
And as n gets bigger, it just
moves up towards 1.

00:43:05.110 --> 00:43:09.230
And therefore, when you look at
this limit, this has to be

00:43:09.230 --> 00:43:12.320
1 over x bar also.

00:43:12.320 --> 00:43:15.760
Since n of t over t is between
these two quantities, they

00:43:15.760 --> 00:43:17.560
both have the same limit.

00:43:17.560 --> 00:43:23.440
The limit of n of t over t is
equal to 1 over the expected

00:43:23.440 --> 00:43:26.088
value of x.

00:43:26.088 --> 00:43:27.080
STUDENT: Professor Gallager?

00:43:27.080 --> 00:43:28.072
ROBERT GALLAGER: Yeah?

00:43:28.072 --> 00:43:31.212
STUDENT: Excuse me if this is
a dumb question, but in the

00:43:31.212 --> 00:43:35.264
previous slide it said the limit
as t goes to infinity of

00:43:35.264 --> 00:43:39.805
the accounting process n of
t, would equal infinity.

00:43:39.805 --> 00:43:43.186
We've also been talking a lot
over the last week about the

00:43:43.186 --> 00:43:46.567
defectiveness and
non-defectiveness of these

00:43:46.567 --> 00:43:48.499
counting processes.

00:43:48.499 --> 00:43:55.300
So we can still find an n that's
sufficiently high, such

00:43:55.300 --> 00:43:58.510
that the probability of n of t
being greater than that n is

00:43:58.510 --> 00:44:00.541
0, so it's not defective.

00:44:00.541 --> 00:44:02.002
But I don't know.

00:44:02.002 --> 00:44:03.950
How do you--

00:44:03.950 --> 00:44:07.710
ROBERT GALLAGER: n of t is
either a random variable or a

00:44:07.710 --> 00:44:11.085
defective random variable
of each value of t.

00:44:14.724 --> 00:44:20.260
And what I'm claiming
here, which is not--

00:44:20.260 --> 00:44:22.110
This is something you
have to prove.

00:44:22.110 --> 00:44:26.400
But what I would like to show is
that for each value of t, n

00:44:26.400 --> 00:44:27.745
of t is not defective.

00:44:27.745 --> 00:44:30.120
In other words, these
arrivals have to

00:44:30.120 --> 00:44:32.320
come sometime or other.

00:44:35.680 --> 00:44:36.540
OK.

00:44:36.540 --> 00:44:41.160
Well, let's backtrack from
that a little bit.

00:44:45.230 --> 00:44:49.280
For n of t to be defective, I
would have to have an infinite

00:44:49.280 --> 00:44:54.010
number of arrivals come in
in some finite time.

00:44:54.010 --> 00:44:56.080
You did a problem in the
homework where that could

00:44:56.080 --> 00:45:02.250
happen, because the x sub i's
you were looking at were not

00:45:02.250 --> 00:45:12.600
identically distributed, so
that as t increased, the

00:45:12.600 --> 00:45:15.480
number of arrivals you had were

00:45:15.480 --> 00:45:18.830
increasing very, very rapidly.

00:45:18.830 --> 00:45:20.890
Here, that can't happen.

00:45:20.890 --> 00:45:25.120
And the reason it can't happen
is because we've started out

00:45:25.120 --> 00:45:29.190
with a renewal process where
by definition the

00:45:29.190 --> 00:45:34.080
inter-arrival intervals all have
the same distribution.

00:45:34.080 --> 00:45:36.710
So the rate of arrivals,
in a sense, is

00:45:36.710 --> 00:45:39.270
staying constant forever.

00:45:39.270 --> 00:45:41.860
Now, that's not a proof.

00:45:41.860 --> 00:45:44.435
If you look at the notes, the
notes have a proof of this.

00:45:47.950 --> 00:45:49.900
After you go through the proof,
you say, that's a

00:45:49.900 --> 00:45:51.900
little bit tedious.

00:45:51.900 --> 00:45:55.840
But you either have to go
through the tedious proof to

00:45:55.840 --> 00:46:01.430
see what is after you go through
it is obvious, or you

00:46:01.430 --> 00:46:04.450
have to say it's obvious,
which is a

00:46:04.450 --> 00:46:05.700
subject to some question.

00:46:09.220 --> 00:46:11.030
So yes, it was not a
stupid question.

00:46:11.030 --> 00:46:13.110
It was a very good question.

00:46:13.110 --> 00:46:15.310
And in fact, you do have
to trace that out.

00:46:15.310 --> 00:46:18.600
And that's involved here in
this in what we've done.

00:46:21.770 --> 00:46:26.020
I want to talk a little bit
about the central limit

00:46:26.020 --> 00:46:29.310
theorem for renewals.

00:46:29.310 --> 00:46:31.310
The notes don't prove
the central

00:46:31.310 --> 00:46:32.890
limit theorem for renewals.

00:46:32.890 --> 00:46:36.280
I'm not going to
prove it here.

00:46:36.280 --> 00:46:41.100
All I'm going to do is give you
an argument why you can

00:46:41.100 --> 00:46:46.280
see that it sort of has to be
true, if you don't look at any

00:46:46.280 --> 00:46:49.410
of the weird special cases you
might want to look at.

00:46:49.410 --> 00:46:53.500
So there is a reference given
in the text for where

00:46:53.500 --> 00:46:54.750
you can find it.

00:46:59.070 --> 00:47:03.950
I mean, I like to give proofs
of very important things.

00:47:03.950 --> 00:47:07.710
I didn't give a proof of this
because the amount of work to

00:47:07.710 --> 00:47:13.680
prove it was far greater than
the importance of the result,

00:47:13.680 --> 00:47:17.600
which means it's a very, very
tricky and very difficult

00:47:17.600 --> 00:47:23.070
thing to prove, even when you're
only talking about

00:47:23.070 --> 00:47:25.910
things like Bernoulli.

00:47:25.910 --> 00:47:26.380
OK.

00:47:26.380 --> 00:47:27.920
But here's the picture.

00:47:27.920 --> 00:47:30.750
And the picture, I think, will
make it sort of clear

00:47:30.750 --> 00:47:32.000
what's going on.

00:47:34.980 --> 00:47:38.740
We're talking now about an
underlying random variable x.

00:47:38.740 --> 00:47:42.590
We assume it has a second
moment, which we need to make

00:47:42.590 --> 00:47:46.000
the central limit
theorem true.

00:47:46.000 --> 00:47:54.480
The probability that s sub n is
less than or equal to t for

00:47:54.480 --> 00:48:00.490
n very large and for the
difference between t--

00:48:07.470 --> 00:48:10.510
Let's look at the
whole statement.

00:48:10.510 --> 00:48:15.270
What it's saying is if you look
at values of t which are

00:48:15.270 --> 00:48:21.070
equal to the mean for s sub n,
which is n x bar, plus some

00:48:21.070 --> 00:48:26.220
quantity alpha times sigma times
the square root of n,

00:48:26.220 --> 00:48:31.350
then as n gets large and t gets
correspondingly large,

00:48:31.350 --> 00:48:34.900
this probability is
approximately equal to the

00:48:34.900 --> 00:48:37.760
normal distribution function.

00:48:37.760 --> 00:48:43.400
In other words, what that's
saying is as I'm looking at

00:48:43.400 --> 00:48:48.820
the random variable, s sub n,
and taking n very, very large.

00:48:48.820 --> 00:48:53.710
The expected value of s sub n is
equal to n times x bar, so

00:48:53.710 --> 00:48:58.175
I'm moving way out where this
number is very, very large.

00:49:02.590 --> 00:49:11.370
As n gets larger and larger, n
increases and x bar increases,

00:49:11.370 --> 00:49:16.370
and they increase on a
slope 1 over x bar.

00:49:16.370 --> 00:49:19.170
So this is n, this
is n over x bar.

00:49:19.170 --> 00:49:24.610
The slope is n over n over x
bar, which is this slope here.

00:49:24.610 --> 00:49:32.510
Now, when you look at this
picture, what it sort of

00:49:32.510 --> 00:49:37.250
involves is you can choose
any n you want.

00:49:37.250 --> 00:49:40.780
We will assume the
x bar is fixed.

00:49:40.780 --> 00:49:43.580
You can choose any t
that you want to.

00:49:43.580 --> 00:49:50.230
Let's first hold b fixed and
look at a third dimension now,

00:49:50.230 --> 00:49:53.680
where for this particular
value of n, I

00:49:53.680 --> 00:49:54.930
want to look at the--

00:49:57.480 --> 00:50:01.370
And instead of looking at the
distribution function, let me

00:50:01.370 --> 00:50:04.050
look at a probability density,
which makes the argument

00:50:04.050 --> 00:50:05.660
easier to see.

00:50:05.660 --> 00:50:22.320
As I look at this for a
particular value of n, what

00:50:22.320 --> 00:50:25.820
I'm going to get is b x bar.

00:50:28.900 --> 00:50:38.840
This will be the probability
density of s sub n of x.

00:50:38.840 --> 00:50:44.820
And that's going to look like,
when n is large enough, it's

00:50:44.820 --> 00:50:50.560
going to look like a Gaussian
probability density.

00:50:50.560 --> 00:50:55.250
And the mean of that Gaussian
probability density will be

00:50:55.250 --> 00:50:59.690
mean n x bar.

00:50:59.690 --> 00:51:04.880
And the variance of this
probability density, now, is

00:51:04.880 --> 00:51:12.500
going to be the square root
of n times sigma.

00:51:18.230 --> 00:51:19.480
What else do I need?

00:51:23.090 --> 00:51:24.690
I guess that's it.

00:51:24.690 --> 00:51:26.050
This is the standard
deviation.

00:51:33.370 --> 00:51:33.740
OK.

00:51:33.740 --> 00:51:36.990
So you can visualize
what happens, now.

00:51:36.990 --> 00:51:42.030
As you start letting n get
bigger and bigger, you have

00:51:42.030 --> 00:51:45.930
this Gaussian density
for each value of n.

00:51:45.930 --> 00:51:50.030
Think of drawing this again for
some larger value of n.

00:51:50.030 --> 00:51:53.880
The mean will shift out
corresponding to a linear

00:51:53.880 --> 00:51:55.340
increase in n.

00:51:55.340 --> 00:51:59.570
The standard deviation will
shift out, but only according

00:51:59.570 --> 00:52:01.200
to the square root of n.

00:52:01.200 --> 00:52:04.490
So what's happening is the same
thing that always happens

00:52:04.490 --> 00:52:09.100
in the central limit theorem, is
that as n gets large, this

00:52:09.100 --> 00:52:14.810
density here is moving out with
n, and it's getting wider

00:52:14.810 --> 00:52:16.080
with the square root of n.

00:52:16.080 --> 00:52:18.840
So it's getting wider much
more slowly than

00:52:18.840 --> 00:52:21.280
it's getting bigger.

00:52:21.280 --> 00:52:24.330
Than It's getting wider
much more slowly

00:52:24.330 --> 00:52:26.580
than it's moving out.

00:52:26.580 --> 00:52:30.080
So if I try to look at what
happens, what's the

00:52:30.080 --> 00:52:34.600
probability that n of t is
greater than or equal to n?

00:52:34.600 --> 00:52:39.960
I now want to look at the same
curve here, but instead of

00:52:39.960 --> 00:52:44.160
looking at it here for a fixed
value of n, I want to look at

00:52:44.160 --> 00:52:50.660
the probability density out here
at some fixed value of t.

00:52:50.660 --> 00:52:52.460
So what's going to happen?

00:52:52.460 --> 00:52:58.450
The probability density here is
going to be the probability

00:52:58.450 --> 00:53:01.960
density that we we're talking
about here, but for

00:53:01.960 --> 00:53:04.490
this value up here.

00:53:04.490 --> 00:53:08.200
The probability density here is
going to correspond to the

00:53:08.200 --> 00:53:12.700
probability density here, and
so forth as we move down.

00:53:12.700 --> 00:53:16.700
So what's happening to this
probability density is that as

00:53:16.700 --> 00:53:20.720
we move up, the standard
deviation is getting

00:53:20.720 --> 00:53:22.070
a little bit wider.

00:53:22.070 --> 00:53:25.120
As we move down, standard
deviation is getting a little

00:53:25.120 --> 00:53:26.320
bit smaller.

00:53:26.320 --> 00:53:29.750
And as n gets bigger and bigger,
this shouldn't make

00:53:29.750 --> 00:53:32.510
any difference.

00:53:32.510 --> 00:53:37.440
So therefore, if you buy for the
moment the fact that this

00:53:37.440 --> 00:53:41.040
doesn't make any difference,
you have a Gaussian density

00:53:41.040 --> 00:53:43.790
going this way, you
have a Gaussian

00:53:43.790 --> 00:53:45.570
density centered here.

00:53:45.570 --> 00:53:51.270
Up here you have a Gaussian
density centered

00:53:51.270 --> 00:53:53.380
here at this point.

00:53:53.380 --> 00:53:56.030
And all those Gaussian densities
are the same, which

00:53:56.030 --> 00:53:59.300
means you have a Gaussian
density going this way, which

00:53:59.300 --> 00:54:02.640
is centered here.

00:54:02.640 --> 00:54:05.150
Here's the upper tail of
that Gaussian density.

00:54:05.150 --> 00:54:07.300
Here's the lower tail of
that Gaussian density.

00:54:11.600 --> 00:54:15.640
Now, to put that analytically,
it's saying that the

00:54:15.640 --> 00:54:21.040
probability that n of t is
greater than or equal to n,

00:54:21.040 --> 00:54:25.580
that's the same as the
probability that s sub n is

00:54:25.580 --> 00:54:27.560
less than or equal to t.

00:54:27.560 --> 00:54:33.460
So that is the distribution
function of s sub n less than

00:54:33.460 --> 00:54:34.350
or equal to t.

00:54:34.350 --> 00:54:39.640
When we go from n to t, what we
find is that n is equal to

00:54:39.640 --> 00:54:41.360
t over x bar--

00:54:41.360 --> 00:54:43.450
that's the mean we have here--

00:54:43.450 --> 00:54:46.860
minus alpha times sigma
times the square

00:54:46.860 --> 00:54:49.160
root of n over x bar.

00:54:49.160 --> 00:54:51.110
In other words, what
is happening is

00:54:51.110 --> 00:54:52.270
the following thing.

00:54:52.270 --> 00:54:59.900
We have a density going this
way, which has variance, which

00:54:59.900 --> 00:55:02.560
has standard deviation
proportional to the

00:55:02.560 --> 00:55:04.130
square root of n.

00:55:04.130 --> 00:55:07.460
When we look at that same
density going this way,

00:55:07.460 --> 00:55:10.550
ignoring the fact that this
distance here that we're

00:55:10.550 --> 00:55:15.030
looking at is very small, this
density here is going to be

00:55:15.030 --> 00:55:18.600
compressed by this slope here.

00:55:18.600 --> 00:55:22.380
In other words, what we have is
the probability that n of t

00:55:22.380 --> 00:55:24.590
greater than or equal to
n is approximately

00:55:24.590 --> 00:55:27.160
equal to phi of alpha.

00:55:27.160 --> 00:55:32.220
n is equal to t over x bar minus
this alpha here times

00:55:32.220 --> 00:55:36.480
sigma times the square
root of n over x bar.

00:55:36.480 --> 00:55:39.720
Nasty equation, because
we have an n on both

00:55:39.720 --> 00:55:41.520
sides of the equation.

00:55:41.520 --> 00:55:44.350
So we will try to solve
this equation.

00:55:44.350 --> 00:55:48.080
And this is approximately equal
to t over x bar minus

00:55:48.080 --> 00:55:52.670
alpha times sigma times the
square root of n over x bar

00:55:52.670 --> 00:55:54.935
times the square root of x.

00:55:54.935 --> 00:55:56.630
Why is that true?

00:55:56.630 --> 00:56:14.500
Because it's approximately equal
to the square root--

00:56:14.500 --> 00:56:22.290
Well, it is equal to the square
root of t over x bar,

00:56:22.290 --> 00:56:25.820
which is this quantity here.

00:56:25.820 --> 00:56:30.630
And since this quantity here
is small relative to this

00:56:30.630 --> 00:56:34.770
quantity here, when you solve
this equation for t, you're

00:56:34.770 --> 00:56:38.760
going to ignore this term
and just get this small

00:56:38.760 --> 00:56:40.310
correction term here.

00:56:40.310 --> 00:56:43.620
That's exactly the same thing
that I said when I was looking

00:56:43.620 --> 00:56:47.620
at this graphically, when I was
saying that if you look at

00:56:47.620 --> 00:56:52.370
the density at larger values
than n, you get a standard

00:56:52.370 --> 00:56:54.440
deviation which is larger.

00:56:54.440 --> 00:56:58.360
When you look at a smaller
value of n, you get is a

00:56:58.360 --> 00:57:01.370
standard deviation
which is smaller.

00:57:01.370 --> 00:57:11.290
Which means that when you look
at it along here, you're going

00:57:11.290 --> 00:57:16.570
to get what looks like a
Gaussian density, except the

00:57:16.570 --> 00:57:20.370
standard deviation is a little
expanded up there and little

00:57:20.370 --> 00:57:22.110
shrunk down here.

00:57:22.110 --> 00:57:24.610
But that doesn't make any
difference as n gets very

00:57:24.610 --> 00:57:28.840
large, because that shrinking
factor is proportional to the

00:57:28.840 --> 00:57:30.985
square root of n
rather than n.

00:57:35.160 --> 00:57:37.270
Now beyond that, you just
have to look at this

00:57:37.270 --> 00:57:39.780
and live with it.

00:57:39.780 --> 00:57:42.650
Or else you have to look up a
proof of it, which I don't

00:57:42.650 --> 00:57:45.580
particularly recommend.

00:57:45.580 --> 00:57:52.950
So this is the central limit
theorem for renewal processes.

00:57:52.950 --> 00:57:58.230
n of t tends to Gaussian with
a mean t over x bar and a

00:57:58.230 --> 00:58:03.970
standard deviation sigma times
square root of t over x bar

00:58:03.970 --> 00:58:07.280
times 1 over square root of x.

00:58:09.860 --> 00:58:14.540
And now you sort of understand
why that is, I hope.

00:58:14.540 --> 00:58:15.230
OK.

00:58:15.230 --> 00:58:17.910
Next thing I want to
go to is the time

00:58:17.910 --> 00:58:20.940
average residual life.

00:58:20.940 --> 00:58:24.990
You were probably somewhat
bothered when you saw with

00:58:24.990 --> 00:58:31.060
Poisson processes that if you
arrived to wait for a bus, the

00:58:31.060 --> 00:58:36.980
expected time between buses
turned out to be twice the

00:58:36.980 --> 00:58:39.060
expected time from one
bus to the next.

00:58:39.060 --> 00:58:43.160
Namely, whenever you arrive to
look for a bus, the time until

00:58:43.160 --> 00:58:48.280
the next bus was an exponential
random variable.

00:58:48.280 --> 00:58:53.020
The time back to the last bus,
if you're far enough in, was

00:58:53.020 --> 00:58:55.160
an exponential random
variable.

00:58:55.160 --> 00:59:00.010
The sum of two, the expected
value from the time before

00:59:00.010 --> 00:59:04.940
until the time later, was
twice what it should be.

00:59:04.940 --> 00:59:08.590
And we went through some kind
of song and dance saying

00:59:08.590 --> 00:59:12.760
that's because you come in at a
given point and you're more

00:59:12.760 --> 00:59:19.540
likely to come in during one of
these longer inter-arrival

00:59:19.540 --> 00:59:22.300
periods than you are to
come in during a short

00:59:22.300 --> 00:59:24.010
inter-arrival.

00:59:24.010 --> 00:59:27.330
And it has to be a song and a
dance, and it didn't really

00:59:27.330 --> 00:59:30.630
explain anything very well,
because we were locked into

00:59:30.630 --> 00:59:32.850
the exponential density.

00:59:32.850 --> 00:59:34.110
Now we have an advantage.

00:59:34.110 --> 00:59:38.310
We can explain things like that,
because we can look at

00:59:38.310 --> 00:59:41.700
any old distribution we want to
look at, and that will let

00:59:41.700 --> 00:59:45.620
us see what this thing which
is called the paradox of

00:59:45.620 --> 00:59:49.430
residual life really
amounts to.

00:59:49.430 --> 00:59:53.070
It's what tells us why we
sometimes have to wait a very

00:59:53.070 --> 00:59:57.190
much longer time than we think
we should if we understand

00:59:57.190 --> 00:59:59.075
some particular kind
of process.

01:00:05.310 --> 01:00:08.430
So here's where we're
going to start.

01:00:08.430 --> 01:00:10.400
What happened?

01:00:10.400 --> 01:00:12.183
I lost a slide.

01:00:12.183 --> 01:00:12.890
Ah.

01:00:12.890 --> 01:00:14.990
There we are.

01:00:14.990 --> 01:00:21.120
Residual life, y of t, of a
renewal process at times t, is

01:00:21.120 --> 01:00:24.420
the remaining time until
the next renewal.

01:00:24.420 --> 01:00:30.290
Namely, we have this counting
process for any

01:00:30.290 --> 01:00:32.990
given renewal process.

01:00:32.990 --> 01:00:38.090
We have this random variable,
which is the time of the first

01:00:38.090 --> 01:00:45.920
arrival after t, which is
s sub n of t plus 1.

01:00:45.920 --> 01:00:50.140
And that difference
is the duration

01:00:50.140 --> 01:00:52.080
until the next arrival.

01:00:52.080 --> 01:00:56.870
Starting at time t, there's a
random variable, which is the

01:00:56.870 --> 01:01:00.790
time from t until the next
arrival after t.

01:01:00.790 --> 01:01:07.670
That is specifically the arrival
epoch of the arrival

01:01:07.670 --> 01:01:12.410
after time t, which is s sub n
of t plus 1 minus the number

01:01:12.410 --> 01:01:15.120
of arrivals that have occurred
up until time t.

01:01:15.120 --> 01:01:22.490
You take any sample path of this
renewal process, and y of

01:01:22.490 --> 01:01:27.100
t will have some value
in that sample path.

01:01:27.100 --> 01:01:29.480
As I say here, this is how long
you have to wait for a

01:01:29.480 --> 01:01:33.740
bus if the bus arrivals were
renewal processes.

01:01:33.740 --> 01:01:37.626
STUDENT: Should it also be
s n t, where there is

01:01:37.626 --> 01:01:40.500
minus sign on that?

01:01:40.500 --> 01:01:43.980
ROBERT GALLAGER: No, because
just by definition, a residual

01:01:43.980 --> 01:01:48.500
life, the residual life starting
at time t is the time

01:01:48.500 --> 01:01:50.330
for the next arrival.

01:01:50.330 --> 01:01:52.850
There's also something called
age that we'll talk about

01:01:52.850 --> 01:02:02.550
later, which is how long is it
back to the last arrival.

01:02:02.550 --> 01:02:07.710
In other words, that age is
the age of the particular

01:02:07.710 --> 01:02:10.125
inter-arrival interval that
you happen to be in.

01:02:10.125 --> 01:02:10.490
Yes?

01:02:10.490 --> 01:02:16.873
STUDENT: It should be s sub n of
t plus 1 minus t instead of

01:02:16.873 --> 01:02:20.801
minus N, because it's
the time from t to--

01:02:30.130 --> 01:02:31.640
ROBERT GALLAGER: Yes,
I agree with you.

01:02:31.640 --> 01:02:33.022
There's something wrong there.

01:02:52.010 --> 01:02:52.600
I'm sorry.

01:02:52.600 --> 01:02:57.090
That I should be s sub n
of t plus 1 minus t.

01:02:57.090 --> 01:02:58.028
Good.

01:02:58.028 --> 01:03:01.490
That's what happens
when you make up

01:03:01.490 --> 01:03:02.990
slides too late at night.

01:03:18.230 --> 01:03:20.950
And as I said, we'll talk about
something called age,

01:03:20.950 --> 01:03:28.210
which is a of t is equal to
t minus s sub n of t.

01:03:31.900 --> 01:03:36.090
So this is a random variable
defined at every value of t.

01:03:36.090 --> 01:03:39.360
What we'd like to look at now is
what does that look like as

01:03:39.360 --> 01:03:43.280
a sample function as
a sample path.

01:03:45.870 --> 01:03:49.350
The residual life is
a function of t--

01:03:55.110 --> 01:03:59.030
Nicest way to view residual
life is that it's a reward

01:03:59.030 --> 01:04:02.150
function on a renewal process.

01:04:02.150 --> 01:04:07.370
A renewal process just
consists of these--

01:04:07.370 --> 01:04:09.790
Well, you can look
at in three ways.

01:04:09.790 --> 01:04:12.740
It's a sequence of inter-arrival
times, all

01:04:12.740 --> 01:04:14.390
identically distributed.

01:04:14.390 --> 01:04:17.470
It's the sequence of
arrival epochs.

01:04:17.470 --> 01:04:24.060
Or it's this unaccountably
infinite number of random

01:04:24.060 --> 01:04:25.360
variables, n of t.

01:04:27.980 --> 01:04:32.280
Given that process, you can
define whatever kind of reward

01:04:32.280 --> 01:04:36.270
you want to, which is the same
kind of reward we were talking

01:04:36.270 --> 01:04:40.210
about with Markov chains, where
you just define some

01:04:40.210 --> 01:04:46.180
kind of reward that you achieve
at each value of t.

01:04:46.180 --> 01:04:48.150
But that reward--

01:04:48.150 --> 01:04:51.740
we'll talk about reward
on renewal processes--

01:04:51.740 --> 01:04:56.190
is restricted to be a reward
which is a function only of

01:04:56.190 --> 01:04:59.090
the particular inter-arrival
interval that you

01:04:59.090 --> 01:05:00.540
happen to be in.

01:05:00.540 --> 01:05:03.760
Now, I don't want to talk about
that too much right now,

01:05:03.760 --> 01:05:08.370
because it is easier to
understand residual life than

01:05:08.370 --> 01:05:12.150
it is to understand the general
idea of these renewal

01:05:12.150 --> 01:05:13.640
reward functions.

01:05:13.640 --> 01:05:17.460
So we'll just talk about
residual life to start with,

01:05:17.460 --> 01:05:20.840
and then get back to the
more general thing.

01:05:20.840 --> 01:05:25.470
We would like, sometimes, to
look at the time-average value

01:05:25.470 --> 01:05:30.560
of residual life, which is you
take the residual life at time

01:05:30.560 --> 01:05:34.080
tau, you integrate it at the
time t, and then you

01:05:34.080 --> 01:05:36.700
divide by time t.

01:05:36.700 --> 01:05:39.450
This is the time average
residual life from

01:05:39.450 --> 01:05:42.360
0 up to time t.

01:05:42.360 --> 01:05:45.880
We will now ask the question,
does this have a limit as t

01:05:45.880 --> 01:05:47.480
goes to infinity?

01:05:47.480 --> 01:05:51.480
And we will see that,
in fact, it does.

01:05:51.480 --> 01:05:52.873
So let's draw a picture.

01:05:55.510 --> 01:06:00.815
Here is a picture of some
arbitrary renewal process.

01:06:00.815 --> 01:06:06.100
I've given the inter-arrival
times, x1, x2, so forth, the

01:06:06.100 --> 01:06:11.500
arrival epochs, s1, s2,
so forth, and n of t.

01:06:11.500 --> 01:06:19.330
Now, let's ask, for this
particular sample function

01:06:19.330 --> 01:06:20.930
what is the residual life?

01:06:20.930 --> 01:06:25.040
Namely, at each value of t,
what's the time until the next

01:06:25.040 --> 01:06:26.760
arrival occurs?

01:06:26.760 --> 01:06:31.700
Well, this is a perfectly
specific function of this

01:06:31.700 --> 01:06:34.670
individual sample
function here.

01:06:34.670 --> 01:06:40.470
This is a sample function, now
in the interval from 0 to s1,

01:06:40.470 --> 01:06:43.330
the time until the
next arrival.

01:06:43.330 --> 01:06:47.440
It starts out as x1,
drops down to 0.

01:06:47.440 --> 01:06:53.480
Now, don't ask the question,
what is my residual life if I

01:06:53.480 --> 01:06:56.330
don't know what the rest of
the sample function is?

01:06:56.330 --> 01:06:59.130
That's not the question
we're asking here.

01:06:59.130 --> 01:07:03.400
The question we're asking is
somebody gives you a picture

01:07:03.400 --> 01:07:07.730
of this entire sample path, and
you want to find out, for

01:07:07.730 --> 01:07:12.070
that particular picture, what is
the residual life at every

01:07:12.070 --> 01:07:13.500
value of t.

01:07:13.500 --> 01:07:19.620
And for a value of t very close
to 0, the residual life

01:07:19.620 --> 01:07:21.320
is the time up to s1.

01:07:21.320 --> 01:07:27.580
So it's decaying linearly
down to 0 at s1.

01:07:27.580 --> 01:07:34.740
At s1, it jumps up immediately
to x2, which is the time from

01:07:34.740 --> 01:07:38.340
any time after s1 to s2.

01:07:38.340 --> 01:07:41.150
I have a little circle
down there.

01:07:41.150 --> 01:07:45.330
And from x2, it decays
down to 0.

01:07:45.330 --> 01:07:50.970
So we have a whole bunch
here of triangles.

01:07:50.970 --> 01:07:55.920
So for any sample function, we
have this sample function of

01:07:55.920 --> 01:08:01.400
residual life, which is, in
fact, just decaying triangles.

01:08:01.400 --> 01:08:04.080
It's nothing more than that.

01:08:04.080 --> 01:08:08.600
For every t in here, the amount
of time until the next

01:08:08.600 --> 01:08:15.580
arrival is simply s2 minus t,
which is that value there.

01:08:15.580 --> 01:08:19.870
This decay is with slope minus
1, so there's nothing to

01:08:19.870 --> 01:08:23.729
finding out what this
is if you know this.

01:08:23.729 --> 01:08:26.990
This is a very simple
function of that.

01:08:26.990 --> 01:08:30.490
So a residual-life sample
function is a sequence of

01:08:30.490 --> 01:08:33.359
isosceles triangles,
one starting at

01:08:33.359 --> 01:08:35.380
each arrival epoch.

01:08:35.380 --> 01:08:41.010
The time average for a given
sample function is, how do I

01:08:41.010 --> 01:08:46.069
find the time average starting
from 0 going up to

01:08:46.069 --> 01:08:48.189
some large value t?

01:08:48.189 --> 01:08:50.876
Well, I simply integrate these
isosceles triangles.

01:08:53.490 --> 01:08:58.930
And I can integrate these, and
you can integrate these, and

01:08:58.930 --> 01:09:02.729
anybody who's had a high school
education can integrate

01:09:02.729 --> 01:09:05.890
these, because it's just the
sum of the areas of all of

01:09:05.890 --> 01:09:07.290
these triangles.

01:09:07.290 --> 01:09:15.020
So this area here is 1 over 2
times x sub i squared, then we

01:09:15.020 --> 01:09:16.109
divide by t.

01:09:16.109 --> 01:09:19.069
So it's 1 over t times
this integral.

01:09:19.069 --> 01:09:23.022
This integral here is the area
of the first triangle plus the

01:09:23.022 --> 01:09:27.830
area of the second triangle plus
1/3 plus 1/4, plus this

01:09:27.830 --> 01:09:33.270
little runt thing at the end,
which is, if I pick t in here,

01:09:33.270 --> 01:09:39.160
this little runt thing is
going to be that little

01:09:39.160 --> 01:09:42.020
trapezoid, which we could figure
out if we wanted to,

01:09:42.020 --> 01:09:43.939
but we don't want to.

01:09:43.939 --> 01:09:50.620
The main thing is we get this
sum of squares here, there

01:09:50.620 --> 01:09:53.109
that's easy enough
to deal with.

01:09:53.109 --> 01:09:55.800
So this is what we found here.

01:09:55.800 --> 01:10:00.410
It is easier to bound this
quantity, instead of having

01:10:00.410 --> 01:10:04.710
that little runt at the end, to
drop the runt to this side

01:10:04.710 --> 01:10:06.860
and to extend the runt
on this side to the

01:10:06.860 --> 01:10:10.060
entire isosceles triangles.

01:10:10.060 --> 01:10:16.220
So this time average residual
life at the time t is between

01:10:16.220 --> 01:10:18.640
this and this.

01:10:18.640 --> 01:10:24.260
The limit of this as t goes
to infinity is what?

01:10:24.260 --> 01:10:26.610
Well, it's just a limit
of a sequence

01:10:26.610 --> 01:10:28.505
of IID random variables.

01:10:39.490 --> 01:10:41.080
No, excuse me.

01:10:41.080 --> 01:10:43.260
We are dealing here with
sample function.

01:10:43.260 --> 01:10:48.300
So what we have is a limit
as t goes to infinity.

01:10:48.300 --> 01:10:52.740
And I want to rewrite this
here as x sub n squared

01:10:52.740 --> 01:10:57.030
divided by n of t times
n has to t over 2t.

01:10:57.030 --> 01:10:59.860
I want to separate it, and
just divide it and

01:10:59.860 --> 01:11:02.360
multiply by n of t.

01:11:02.360 --> 01:11:04.080
I want to look at this term.

01:11:04.080 --> 01:11:09.020
What happens to this term
as t gets large?

01:11:09.020 --> 01:11:13.170
Well, as t gets large,
n of t gets large.

01:11:13.170 --> 01:11:18.230
This quantity here just goes
through the same set of values

01:11:18.230 --> 01:11:23.760
as the sum up to some finite
limit divided by that limit

01:11:23.760 --> 01:11:24.200
goes through.

01:11:24.200 --> 01:11:32.820
So the limit of this quantity
here is just the expected

01:11:32.820 --> 01:11:35.360
value of x squared.

01:11:35.360 --> 01:11:37.830
What is this quantity here?

01:11:37.830 --> 01:11:41.090
Well, this is what the renewal
theorem deals with.

01:11:41.090 --> 01:11:46.550
This limit here is 1 over 2
times the expected value of x.

01:11:46.550 --> 01:11:50.080
That's what we showed before.

01:11:50.080 --> 01:11:53.190
This goes to a limit, this goes
to a limit, the whole

01:11:53.190 --> 01:11:55.060
thing goes to a limit.

01:11:55.060 --> 01:11:57.630
And it goes to limit
with probability 1

01:11:57.630 --> 01:12:00.140
for all sample functions.

01:12:00.140 --> 01:12:04.760
So this time average residual
life has the expected value of

01:12:04.760 --> 01:12:11.070
x squared divided by 2 times
the expected value of x.

01:12:11.070 --> 01:12:14.350
Now if you look at this, you'll
see that what we've

01:12:14.350 --> 01:12:17.580
done is something which is very
simple, because of the

01:12:17.580 --> 01:12:20.310
fact we have renewal theory
at this point.

01:12:20.310 --> 01:12:24.080
If we had to look at the
probabilities of where all of

01:12:24.080 --> 01:12:29.290
these arrival epochs occur,
and then deal with all of

01:12:29.290 --> 01:12:36.210
those random variables, and
go through some enormously

01:12:36.210 --> 01:12:39.660
complex calculation to find
the expected value of this

01:12:39.660 --> 01:12:43.050
residual life at the time
t, it would be an

01:12:43.050 --> 01:12:45.340
incredibly hard problem.

01:12:45.340 --> 01:12:48.590
But looking at it in terms of
sample paths for random

01:12:48.590 --> 01:12:51.340
variables, it's an incredibly
simple problem.

01:12:55.900 --> 01:13:00.000
Want to look at one example
here, because when we look at

01:13:00.000 --> 01:13:05.390
this, well, first thing
is just a couple of

01:13:05.390 --> 01:13:07.040
examples to work out.

01:13:07.040 --> 01:13:10.960
The time average residual life
has expected value of x

01:13:10.960 --> 01:13:14.040
squared over 2 times the
expected value of x.

01:13:14.040 --> 01:13:19.150
If x is almost deterministic,
then the expected value of x

01:13:19.150 --> 01:13:23.900
squared is just a square of
the expected value of x.

01:13:23.900 --> 01:13:28.390
So we wind up with the expected
value of x over 2,

01:13:28.390 --> 01:13:32.250
which is sort of what you would
expect if you look from

01:13:32.250 --> 01:13:35.970
time 0 to time infinity, and
these arrivals come along

01:13:35.970 --> 01:13:40.230
regularly, then the expected
time you have to wait for the

01:13:40.230 --> 01:13:48.890
next arrival varies
from 0 up to x.

01:13:48.890 --> 01:13:51.370
And the average of it is x/2.

01:13:51.370 --> 01:13:54.190
So no problem there.

01:13:54.190 --> 01:13:58.860
If x is exponential, we've
already found out that the

01:13:58.860 --> 01:14:02.900
expected time we have to wait
until the next arrival is the

01:14:02.900 --> 01:14:08.060
expected value of x, because
these arrivals are memoryless.

01:14:08.060 --> 01:14:11.500
So I start looking at this
Poisson process at a given

01:14:11.500 --> 01:14:16.210
value of t, and the time until
the next arrival is

01:14:16.210 --> 01:14:21.910
exponential, and it's the same
as the expected time from one

01:14:21.910 --> 01:14:24.030
arrival to the next arrival.

01:14:24.030 --> 01:14:26.360
So we have that quantity
there, which

01:14:26.360 --> 01:14:27.830
looks a little strange.

01:14:27.830 --> 01:14:32.720
This one, this is a very
peculiar random variable.

01:14:32.720 --> 01:14:37.000
But this really explains what's
going on with this kind

01:14:37.000 --> 01:14:42.080
of paradoxical thing, which we
found with a Poisson process,

01:14:42.080 --> 01:14:47.240
where if you arrive to wait for
a bus, you're waiting time

01:14:47.240 --> 01:14:51.880
is not any less because of the
fact that you've just arrived

01:14:51.880 --> 01:14:54.940
between two arrivals, and it
ought to be the same distance

01:14:54.940 --> 01:14:58.060
back to the last one at a
distance of first one.

01:14:58.060 --> 01:15:00.210
That was always a little
surprising.

01:15:00.210 --> 01:15:04.010
This, I think, explains
what's going on

01:15:04.010 --> 01:15:05.530
better than most things.

01:15:05.530 --> 01:15:11.850
Look at a binary random
variable, x, where the

01:15:11.850 --> 01:15:17.680
probability that x is equal to
epsilon is 1 minus epsilon,

01:15:17.680 --> 01:15:19.830
and the probability that
x is equal to 1

01:15:19.830 --> 01:15:21.960
over epsilon is epsilon.

01:15:21.960 --> 01:15:24.520
And think of epsilon as
being very large.

01:15:24.520 --> 01:15:25.910
So what happens?

01:15:25.910 --> 01:15:29.190
You got a whole bunch of little,
tiny inter-renewal

01:15:29.190 --> 01:15:32.960
intervals, which are
epsilon apart.

01:15:32.960 --> 01:15:35.990
And then with very small
probability, you get an

01:15:35.990 --> 01:15:38.570
enormous one.

01:15:38.570 --> 01:15:41.580
And you wait for 1
over epsilon for

01:15:41.580 --> 01:15:43.100
that one to be finished.

01:15:43.100 --> 01:15:45.530
Then you've got a bunch of
little ones which are all

01:15:45.530 --> 01:15:46.760
epsilon apart.

01:15:46.760 --> 01:15:51.300
Then you get an enormous one,
which is 1 over epsilon long.

01:15:51.300 --> 01:15:56.680
And now you can see perfectly
well that if you arrive to

01:15:56.680 --> 01:16:00.690
wait for a bus and the buses are
distributed this way, this

01:16:00.690 --> 01:16:05.020
is sort of what happens when you
have a bus system which is

01:16:05.020 --> 01:16:07.910
perfectly regular but
subject to failures.

01:16:07.910 --> 01:16:09.580
Whenever you have a failure,
you have an

01:16:09.580 --> 01:16:11.300
incredibly long wait.

01:16:11.300 --> 01:16:13.570
Otherwise, you have
very small waits.

01:16:13.570 --> 01:16:17.320
So what happens here?

01:16:17.320 --> 01:16:21.380
The duration of this whole
interval here of these little,

01:16:21.380 --> 01:16:25.130
tiny inter-arrival times, the
distance between failure in a

01:16:25.130 --> 01:16:30.240
sense, is 1 minus epsilon,
as it turns out.

01:16:30.240 --> 01:16:33.240
It's very close to 1.

01:16:33.240 --> 01:16:37.370
This distance here is
1 over epsilon.

01:16:37.370 --> 01:16:41.633
And this quantity here,
if you work it out--

01:16:46.060 --> 01:16:46.500
Let's see.

01:16:46.500 --> 01:16:48.780
What is it?

01:16:48.780 --> 01:16:52.100
We take this distribution here,
we look for the expected

01:16:52.100 --> 01:16:53.350
value of x squared.

01:16:57.800 --> 01:16:58.450
Let's see.

01:16:58.450 --> 01:17:03.970
1 over epsilon squared times
epsilon, which is 1 over

01:17:03.970 --> 01:17:07.690
epsilon plus 1 minus epsilon
times something.

01:17:07.690 --> 01:17:13.480
So the expected time that you
have to wait if you arrive

01:17:13.480 --> 01:17:17.050
somewhere along here is
1 over 2 epsilon.

01:17:17.050 --> 01:17:20.520
If epsilon is very small, you
have a very, very long waiting

01:17:20.520 --> 01:17:25.900
time because of these very long
distributions in here.

01:17:25.900 --> 01:17:30.180
You normally don't tend to
arrive in any of these periods

01:17:30.180 --> 01:17:31.740
or any of these periods.

01:17:31.740 --> 01:17:35.740
But however you want to
interpret it, this theorem

01:17:35.740 --> 01:17:40.280
about renewals tells you
precisely that the time

01:17:40.280 --> 01:17:45.870
average residual life
is, in fact, this

01:17:45.870 --> 01:17:49.170
quantity 1 over 2 epsilon.

01:17:49.170 --> 01:17:52.220
That's this paradox
of residual life.

01:17:52.220 --> 01:17:55.420
Your residual life is much
larger than it looks like it

01:17:55.420 --> 01:18:00.780
ought to be, because it's not
by any means the same as the

01:18:00.780 --> 01:18:05.940
expected interval between
successive arrivals, which in

01:18:05.940 --> 01:18:07.860
this case is very small.

01:18:07.860 --> 01:18:09.730
OK I think I'll stop there.

01:18:09.730 --> 01:18:13.280
And we will talk more about
this next time.

