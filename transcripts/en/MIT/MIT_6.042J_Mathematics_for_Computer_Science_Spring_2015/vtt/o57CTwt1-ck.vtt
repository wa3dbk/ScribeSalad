WEBVTT
Kind: captions
Language: en

00:00:01.164 --> 00:00:03.580
PROFESSOR: If we're going to
make use of Chebyshev's Bound

00:00:03.580 --> 00:00:05.840
and other results that
depend on the variance,

00:00:05.840 --> 00:00:09.690
we'll need some methods
for calculating variance

00:00:09.690 --> 00:00:10.980
in various circumstances.

00:00:10.980 --> 00:00:13.940
So let's develop that here.

00:00:13.940 --> 00:00:18.382
A basic place to begin is
to ask about the indicator

00:00:18.382 --> 00:00:19.590
variables and their variance.

00:00:19.590 --> 00:00:22.070
Remember, i is an
indicator variable that

00:00:22.070 --> 00:00:24.470
means that it's zero one value.

00:00:24.470 --> 00:00:26.500
It's also called the
Bernoulli variable.

00:00:26.500 --> 00:00:29.340
And if the probability
that it equals 1 is p,

00:00:29.340 --> 00:00:30.890
that's also its expectations.

00:00:30.890 --> 00:00:33.850
So we have an indicator
variable with expectation

00:00:33.850 --> 00:00:36.590
of the indicator is
p, and we're asking

00:00:36.590 --> 00:00:40.130
what's its variance, which by
definition is the expectation

00:00:40.130 --> 00:00:42.310
of i minus p squared.

00:00:42.310 --> 00:00:45.630
Well, this is one of the sort
of almost mechanical proofs that

00:00:45.630 --> 00:00:49.780
follows simply by algebra
and linearity of expectation.

00:00:49.780 --> 00:00:51.810
But let's walk through
it step by step,

00:00:51.810 --> 00:00:54.730
just to reassure you that
that's all that's involved.

00:00:54.730 --> 00:00:57.470
I would recommend against
really trying to memorize this,

00:00:57.470 --> 00:00:59.470
because it's-- I can
remember it anyway,

00:00:59.470 --> 00:01:02.110
I just reprove it
every time I need it.

00:01:02.110 --> 00:01:04.819
And so, let's see how
the proof would go.

00:01:04.819 --> 00:01:10.130
So step one would be
to expand this i minus

00:01:10.130 --> 00:01:11.440
p squared algebraically.

00:01:11.440 --> 00:01:13.880
So we're talking about the
expectation of i squared

00:01:13.880 --> 00:01:16.720
minus 2pi plus p squared.

00:01:16.720 --> 00:01:19.350
Now we can just apply
linearity of expectation,

00:01:19.350 --> 00:01:22.860
and I get the expectation
of i squared minus 2p times

00:01:22.860 --> 00:01:25.106
the expectation of
i plus p squared.

00:01:25.106 --> 00:01:27.480
Of course, the expectation of
a constant is the constant.

00:01:27.480 --> 00:01:30.790
So when I take expectation of
p squared, I get p squared.

00:01:30.790 --> 00:01:32.600
But now look at this.

00:01:32.600 --> 00:01:34.150
i squared is zero one value.

00:01:34.150 --> 00:01:38.760
So in fact, i squared is equal
to i and the expectation of i

00:01:38.760 --> 00:01:41.000
has now appeared here, that's p.

00:01:41.000 --> 00:01:44.210
So this term simplifies
to expectation of i,

00:01:44.210 --> 00:01:47.410
and this term becomes 2p
times p plus p squared.

00:01:47.410 --> 00:01:49.720
Of course that
expectation of i is a p.

00:01:49.720 --> 00:01:53.540
So I've got p minus 2p
squared plus p squared.

00:01:53.540 --> 00:01:56.550
The p squareds cancel, and
I get p minus p squared.

00:01:56.550 --> 00:01:59.820
If you factor out p,
that's p times 1 minus p,

00:01:59.820 --> 00:02:03.110
or pq, which is the
standard way that you

00:02:03.110 --> 00:02:05.840
write the variance of
an indicator variable.

00:02:05.840 --> 00:02:08.560
It's p times 1 minus p.

00:02:08.560 --> 00:02:13.540
OK, that was easy, and
again, completely mechanical.

00:02:13.540 --> 00:02:15.970
There's a couple of other
rules for calculating

00:02:15.970 --> 00:02:20.950
variance of new variables
from old ones that are basic.

00:02:20.950 --> 00:02:22.700
Like [? additivity ?]
of expectation,

00:02:22.700 --> 00:02:25.650
but it doesn't quite work
so simply for variance.

00:02:25.650 --> 00:02:28.190
So the first rule
is that if you ask

00:02:28.190 --> 00:02:31.090
about the variance
of a constant times r

00:02:31.090 --> 00:02:35.510
plus b, that turns out to
be the same as a squared

00:02:35.510 --> 00:02:38.050
times the variance of b-- of r.

00:02:38.050 --> 00:02:40.960
The b doesn't-- the additive
of be doesn't matter,

00:02:40.960 --> 00:02:45.370
and the-- because the variance
is really the expectation

00:02:45.370 --> 00:02:50.610
of something squared, when
you get rid of that constant

00:02:50.610 --> 00:02:53.510
a, you're factoring
out an squared.

00:02:53.510 --> 00:02:56.450
And this is the
rule you get here.

00:02:56.450 --> 00:02:58.260
OK.

00:02:58.260 --> 00:03:01.270
Another basic rule that's
often convenient, instead

00:03:01.270 --> 00:03:05.770
of working with variance in the
form of the expectation of r

00:03:05.770 --> 00:03:08.240
minus mu squared,
is to say that it's

00:03:08.240 --> 00:03:13.170
the expectation of r
squared minus the square

00:03:13.170 --> 00:03:15.910
of the expectation of r.

00:03:15.910 --> 00:03:19.850
Now, this expression-- the
square of the expectation of r

00:03:19.850 --> 00:03:22.247
comes up so often that
there's a shorthand for it.

00:03:22.247 --> 00:03:23.830
Where instead of
writing [? parends ?]

00:03:23.830 --> 00:03:27.230
you write e squared of
r, just means the same

00:03:27.230 --> 00:03:29.600
as expectation of r squared.

00:03:29.600 --> 00:03:32.800
And so much for the second rule,
which we'll use all the time,

00:03:32.800 --> 00:03:34.650
because it's a
convenient rule to have.

00:03:34.650 --> 00:03:36.580
I'm going to prove the
second one, just again,

00:03:36.580 --> 00:03:38.330
just to show you have
nothing to worry about.

00:03:38.330 --> 00:03:40.913
You don't even have to remember
how the proof goes, of course,

00:03:40.913 --> 00:03:42.730
you can reconstruct
it every time.

00:03:42.730 --> 00:03:47.220
So it's again simple proofs
just by linearity of expectation

00:03:47.220 --> 00:03:49.370
and doing the algebra.

00:03:49.370 --> 00:03:53.690
So the variance of r is by
definition the expectation

00:03:53.690 --> 00:03:55.840
of r minus mu squared.

00:03:55.840 --> 00:03:58.100
Let's expand our
minus mu squared.

00:03:58.100 --> 00:03:59.810
It's the expectation
of r squared

00:03:59.810 --> 00:04:02.450
minus 2mu r plus mu squared.

00:04:02.450 --> 00:04:04.540
Now we apply linearity to that.

00:04:04.540 --> 00:04:06.800
I get the expectation
of r squared

00:04:06.800 --> 00:04:12.470
minus 2mu expectation of r, plus
the expectation of mu squared,

00:04:12.470 --> 00:04:14.390
if I'm really being
completely mechanical

00:04:14.390 --> 00:04:16.459
about linearity of expectation.

00:04:16.459 --> 00:04:20.070
Now expectation of a constant
mu squared is simply mu squared.

00:04:20.070 --> 00:04:21.940
And here, I've get
the expectation

00:04:21.940 --> 00:04:23.630
of r, that's mu again.

00:04:23.630 --> 00:04:26.100
So I wind up with the
expectation of r squared

00:04:26.100 --> 00:04:29.910
minus 2mu mu plus r squared.

00:04:29.910 --> 00:04:33.370
This is 2mu squared-- minus
2mu squared plus mu squared.

00:04:33.370 --> 00:04:35.810
It winds up with
minus mu squared.

00:04:35.810 --> 00:04:40.200
And of course, mu squared is
the expectation squared of r,

00:04:40.200 --> 00:04:41.990
I've proved the formula.

00:04:41.990 --> 00:04:45.560
Again, as claimed, there's
nothing interesting here,

00:04:45.560 --> 00:04:49.740
just algebra and
linearity of expectation.

00:04:49.740 --> 00:04:52.440
And the first result
about factoring out an a

00:04:52.440 --> 00:04:55.340
and squaring it follows
from a similar proof, which

00:04:55.340 --> 00:04:58.620
I'm not going to include here.

00:04:58.620 --> 00:05:00.840
So let's look at the
space station Mir

00:05:00.840 --> 00:05:04.630
again, which we used as
an example of calculating

00:05:04.630 --> 00:05:06.450
mean time to failure.

00:05:06.450 --> 00:05:08.380
So the hypothesis
that we're making

00:05:08.380 --> 00:05:12.450
is that with probability
p, the Mir space station

00:05:12.450 --> 00:05:18.270
will run into some huge space
garbage that will clobber it.

00:05:18.270 --> 00:05:21.150
And the probability of that
happening in any given hour

00:05:21.150 --> 00:05:23.260
is probability p.

00:05:23.260 --> 00:05:27.990
So we know that that means
the expected number of hours

00:05:27.990 --> 00:05:30.310
for the Mir to fail
is 1 over p, that's

00:05:30.310 --> 00:05:31.990
the mean time to failure.

00:05:31.990 --> 00:05:36.200
And what we're asking is what's
the variance of f, if f is

00:05:36.200 --> 00:05:38.310
the number of hours to failure?

00:05:38.310 --> 00:05:41.140
What's the variance of f?

00:05:41.140 --> 00:05:43.640
Well, one way we can
do is just plug-in

00:05:43.640 --> 00:05:48.100
the definition of expectation
and this will work.

00:05:48.100 --> 00:05:52.120
The probability that it
takes k hours to fail

00:05:52.120 --> 00:05:55.050
is-- we know the
geometric distribution.

00:05:55.050 --> 00:05:58.560
The probability of not
failing for k minus 1 hours,

00:05:58.560 --> 00:06:02.180
and failing after that, q
to the k minus 1 times p.

00:06:02.180 --> 00:06:05.800
So the variance of f,
using our previous formula

00:06:05.800 --> 00:06:07.270
about the expectation
of f squared

00:06:07.270 --> 00:06:10.810
minus the expectation squared
of f, this becomes a minus 1

00:06:10.810 --> 00:06:12.220
over p squared.

00:06:12.220 --> 00:06:14.010
We can forget
about that, we want

00:06:14.010 --> 00:06:19.090
to focus on calculating the
expectation of f squared.

00:06:19.090 --> 00:06:21.680
So f is 1, 2, 3, and so on.

00:06:21.680 --> 00:06:24.200
That means f squared
is 1, 4, 9, k squared.

00:06:24.200 --> 00:06:27.320
The point being that the
only values that f squared

00:06:27.320 --> 00:06:28.930
can take our
squares, so we don't

00:06:28.930 --> 00:06:32.220
have to worry about
counting them in to sum

00:06:32.220 --> 00:06:33.560
that defines the expectation.

00:06:33.560 --> 00:06:36.320
So let's go look at that.

00:06:36.320 --> 00:06:38.620
So the expectation of
f squared is the sum

00:06:38.620 --> 00:06:41.750
over the possible values
that f squared can take,

00:06:41.750 --> 00:06:45.320
namely the sum from k equals
1 to infinity of k squared,

00:06:45.320 --> 00:06:48.307
times the probability that f
squared is equal to k squared.

00:06:48.307 --> 00:06:49.890
Well of course, the
probability that f

00:06:49.890 --> 00:06:52.110
squared is equal to k squared
is the same as the probability

00:06:52.110 --> 00:06:53.010
that f equals k.

00:06:57.119 --> 00:06:58.660
And we know what
the probability of f

00:06:58.660 --> 00:07:01.190
equals k is, it's a
geometric distribution, so

00:07:01.190 --> 00:07:07.380
the probability that f equals k
is q to the k minus 1 times p.

00:07:07.380 --> 00:07:11.550
If I factor out a p over q, this
simplifies to the sum from k

00:07:11.550 --> 00:07:15.530
equals 0 to infinity of
k squared, q to the k.

00:07:15.530 --> 00:07:18.090
And this is a kind
of sum that we've

00:07:18.090 --> 00:07:19.940
seen before that
has a closed form

00:07:19.940 --> 00:07:21.800
and we could perfectly
well calculate then

00:07:21.800 --> 00:07:26.260
the expectation of f squared
by appealing to our generating

00:07:26.260 --> 00:07:28.800
function information to
get a closed form for this,

00:07:28.800 --> 00:07:31.220
and then remember to
subtract 1 minus p squared

00:07:31.220 --> 00:07:34.920
because the variance is
this term minus the square

00:07:34.920 --> 00:07:37.060
of the expectation of f.

00:07:37.060 --> 00:07:40.100
But let's go another way
and use the same technique

00:07:40.100 --> 00:07:43.270
of total expectation
that we used before.

00:07:43.270 --> 00:07:46.755
That is, the expectation of
f squared, of the failure

00:07:46.755 --> 00:07:51.560
of time squared, is equal, by
the law of total probability,

00:07:51.560 --> 00:07:55.980
to the expectation of f
squared, given that f is one.

00:07:55.980 --> 00:07:58.120
That is, we fail f
on the first step,

00:07:58.120 --> 00:08:00.930
times the probability that
we fail on the first step.

00:08:00.930 --> 00:08:03.420
Plus, the expectation
of f squared,

00:08:03.420 --> 00:08:05.890
given that we don't
fail on the first step,

00:08:05.890 --> 00:08:09.290
that f is greater than 1,
times the probability that f

00:08:09.290 --> 00:08:10.930
is greater than 1.

00:08:10.930 --> 00:08:13.120
Now, what's going to
make this manageable

00:08:13.120 --> 00:08:16.380
is that this expression, the
expectation of f squared when

00:08:16.380 --> 00:08:18.790
f is greater than
1, will turn out

00:08:18.790 --> 00:08:20.880
to be something
that we can easily

00:08:20.880 --> 00:08:24.490
convert into a
nonconditional probability,

00:08:24.490 --> 00:08:26.650
and find a value for.

00:08:26.650 --> 00:08:29.150
So the limit that we're
using here is the following.

00:08:29.150 --> 00:08:30.710
What I'm thinking
about in mean time

00:08:30.710 --> 00:08:34.220
to failure-- if I think of
any function whatsoever,

00:08:34.220 --> 00:08:36.730
g of the mean time to failure.

00:08:36.730 --> 00:08:40.130
And I'm interested in the
expectation of g of f,

00:08:40.130 --> 00:08:42.429
And I'm interested in the
expectation of g of f,

00:08:42.429 --> 00:08:44.880
given that f is greater than n.

00:08:44.880 --> 00:08:48.570
That is, it's already taken
n steps to get where I am.

00:08:48.570 --> 00:08:50.790
Then the thing about
the mean time to failure

00:08:50.790 --> 00:08:55.390
is that at any moment
that you haven't failed,

00:08:55.390 --> 00:08:57.800
you're starting off in
essentially the same situation

00:08:57.800 --> 00:09:00.910
you were at the beginning in
waiting for the next failure

00:09:00.910 --> 00:09:01.720
to occur.

00:09:01.720 --> 00:09:04.390
And the probability of
failing in one more step

00:09:04.390 --> 00:09:07.809
is the same probability--
is the same p.

00:09:07.809 --> 00:09:09.350
And the probability
of you're failing

00:09:09.350 --> 00:09:13.830
in two more steps is qp,
and three more steps is qqp.

00:09:13.830 --> 00:09:17.440
The only difference
is that the value of f

00:09:17.440 --> 00:09:20.590
has been shifted by n.

00:09:20.590 --> 00:09:22.230
In the ordinary
case, we start off

00:09:22.230 --> 00:09:24.840
with f equals 0 and
look at the probability

00:09:24.840 --> 00:09:27.010
that we fail in one more
step, two more steps.

00:09:27.010 --> 00:09:30.470
Now we're starting off with
f having the value f plus n,

00:09:30.470 --> 00:09:33.550
and asking about the
probability that it

00:09:33.550 --> 00:09:36.060
fails in the next step or the
next step, or the next step.

00:09:36.060 --> 00:09:39.830
So the punchline is that
the expectation of g of f,

00:09:39.830 --> 00:09:44.080
given that f is greater than
n, is simply the expectation

00:09:44.080 --> 00:09:46.310
of g of f plus n.

00:09:46.310 --> 00:09:48.020
And I'm going to let
you meditate that

00:09:48.020 --> 00:09:49.560
and not say anymore about it.

00:09:49.560 --> 00:09:51.820
But the punchline
is the corollary

00:09:51.820 --> 00:09:54.230
that the expectation
of f squared,

00:09:54.230 --> 00:09:58.050
given that f is greater than
1, is simply the expectation

00:09:58.050 --> 00:10:00.710
of f plus 1 squared.

00:10:00.710 --> 00:10:03.940
And that lets us go back and
simplify this expression.

00:10:03.940 --> 00:10:06.150
That we've had from
total expectation,

00:10:06.150 --> 00:10:09.210
we now have-- here's the
expectation of f squared,

00:10:09.210 --> 00:10:11.040
given that f is greater than 1.

00:10:11.040 --> 00:10:12.540
And let's look at
these other terms.

00:10:12.540 --> 00:10:16.160
This is the expectation of f
squared, given that f equals 1.

00:10:16.160 --> 00:10:19.180
Well, the expectation of f
squared given that f equals 1

00:10:19.180 --> 00:10:22.090
is 1 squared, because
we know what f is

00:10:22.090 --> 00:10:24.040
and that's the end of the story.

00:10:24.040 --> 00:10:26.080
Times the probability
that f equals 1,

00:10:26.080 --> 00:10:29.750
that's p, the probability
of failure on a given step.

00:10:29.750 --> 00:10:34.690
This is the probability that f
is greater than 1, which is q,

00:10:34.690 --> 00:10:36.790
that we didn't fail
on the first step.

00:10:36.790 --> 00:10:39.860
And we just figured out that
this term is the expectation

00:10:39.860 --> 00:10:43.590
of the square of f plus 1.

00:10:43.590 --> 00:10:44.870
So there's the 1 and the p.

00:10:44.870 --> 00:10:47.920
And that becomes a q, and
this is the expectation

00:10:47.920 --> 00:10:50.150
of f plus 1 squared.

00:10:50.150 --> 00:10:52.110
Now again, I apply
limit linearity.

00:10:52.110 --> 00:10:54.340
I'm going to expand
f plus 1 squared

00:10:54.340 --> 00:11:01.110
into f squared plus
2f plus 1, and then

00:11:01.110 --> 00:11:02.485
apply linearity of expectation.

00:11:02.485 --> 00:11:05.840
And I'm going to wind up with
the expectation of f squared

00:11:05.840 --> 00:11:08.680
plus twice the expectation
of f, which remember,

00:11:08.680 --> 00:11:13.690
is twice over-- 2 over
p, plus 1, times the q.

00:11:13.690 --> 00:11:18.120
And now what I've got is a
simple arithmetic equation

00:11:18.120 --> 00:11:20.860
between the expectation
of f squared

00:11:20.860 --> 00:11:24.710
and some other arithmetic and
the expectation of f squared.

00:11:24.710 --> 00:11:27.980
It's easy to solve for the
expectation of f squared.

00:11:27.980 --> 00:11:31.120
And I'll spare you that
elementary simplification.

00:11:31.120 --> 00:11:33.400
But the punchline
is, when-- we also

00:11:33.400 --> 00:11:37.520
remember to subtract
one over p squared,

00:11:37.520 --> 00:11:40.110
because that was the
expectation of the square of f

00:11:40.110 --> 00:11:41.310
of the expectation of f.

00:11:41.310 --> 00:11:43.410
We came up with this
punchline formula.

00:11:43.410 --> 00:11:46.150
The variance of
mean time to failure

00:11:46.150 --> 00:11:50.520
is 1 over the probability
of failure on a given step,

00:11:50.520 --> 00:11:53.780
times 1 minus 1 over--
times the probability-- 1

00:11:53.780 --> 00:11:56.280
over the probability of the
failure in the first step,

00:11:56.280 --> 00:11:58.460
minus 1.

00:11:58.460 --> 00:11:59.970
That's just for
practice and fun,

00:11:59.970 --> 00:12:02.590
let's look at the space
station Mir again.

00:12:02.590 --> 00:12:05.980
Suppose that I tell you that
there is a 1 in [? 10,000ths ?]

00:12:05.980 --> 00:12:10.370
chance that in any given hour,
the Mir is going to crash

00:12:10.370 --> 00:12:12.960
into some debris that's
out there in orbit.

00:12:12.960 --> 00:12:20.100
So the expectation of f is 10 to
the fourth, about 10,000 hours.

00:12:20.100 --> 00:12:28.190
And the sigma is going to be the
variance of f, which is about 1

00:12:28.190 --> 00:12:34.560
over ten thousandths, that is
10,000 times 10,000 minus 1,

00:12:34.560 --> 00:12:38.220
which is pretty close to 10,000
squared for the variance.

00:12:38.220 --> 00:12:40.770
And when I take the square
root, I get back to 10,000.

00:12:40.770 --> 00:12:45.030
So sigma is just a tad less than
10,000, is 10 to the fourth.

00:12:45.030 --> 00:12:48.210
So with those numbers, I can
apply the Chebyshev's Theorem

00:12:48.210 --> 00:12:51.670
and conclude that the
probability that the Mir lasts

00:12:51.670 --> 00:12:54.440
more than 4 times 10
to the fourth hours

00:12:54.440 --> 00:12:57.110
is less than 1 chance in four.

00:12:57.110 --> 00:12:59.810
If we translate that into
years-- if it was really

00:12:59.810 --> 00:13:03.190
the case that there was a 1 in
10,000 chance of the Mir being

00:13:03.190 --> 00:13:05.650
destroyed in any given
hour, then the probability

00:13:05.650 --> 00:13:09.750
that it lasts more than 4.6
years before destructing

00:13:09.750 --> 00:13:11.520
is less than 1/4.

00:13:15.090 --> 00:13:18.500
So another rule for
calculating variance,

00:13:18.500 --> 00:13:21.010
and maybe the most
important general one,

00:13:21.010 --> 00:13:23.780
is that variance is additive.

00:13:23.780 --> 00:13:27.930
That is, the variance of a sum
is the sum of the variances.

00:13:27.930 --> 00:13:31.290
But unlike expectation,
where there's no other side

00:13:31.290 --> 00:13:34.350
condition, and it does not in
any way depend on independence,

00:13:34.350 --> 00:13:36.510
it turns out that
variance is additive

00:13:36.510 --> 00:13:41.310
only if the variables being
added our pairwise independent.

00:13:41.310 --> 00:13:43.700
Now you might wonder where
the pairwise came from,

00:13:43.700 --> 00:13:47.670
and it's because variance is
the square of an expectation.

00:13:47.670 --> 00:13:52.310
So when you wind up multiplying
out and doing the algebra,

00:13:52.310 --> 00:13:54.980
you're just getting
quadratic terms

00:13:54.980 --> 00:14:00.060
for variances of-- for
expectations of ri times rj.

00:14:00.060 --> 00:14:04.530
And so you need to factor those
into expectation of r times

00:14:04.530 --> 00:14:07.410
expect-- ri times
expectation of rj,

00:14:07.410 --> 00:14:09.487
which you only need
pairwise independence for.

00:14:09.487 --> 00:14:11.320
So that's a fast talking
through the algebra

00:14:11.320 --> 00:14:12.640
that I'm going to leave to you.

00:14:12.640 --> 00:14:16.290
It's in the text, and it's
again one of these easy proofs.

