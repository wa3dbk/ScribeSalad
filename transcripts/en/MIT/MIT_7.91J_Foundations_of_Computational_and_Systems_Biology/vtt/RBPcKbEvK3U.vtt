WEBVTT
Kind: captions
Language: en

00:00:00.070 --> 00:00:01.780
The following
content is provided

00:00:01.780 --> 00:00:04.019
under a Creative
Commons license.

00:00:04.019 --> 00:00:06.870
Your support will help MIT
OpenCourseWare continue

00:00:06.870 --> 00:00:10.730
to offer high quality
educational resources for free.

00:00:10.730 --> 00:00:13.340
To make a donation or
view additional materials

00:00:13.340 --> 00:00:17.217
from hundreds of MIT courses,
visit MIT OpenCourseWare

00:00:17.217 --> 00:00:17.842
at ocw.mit.edu.

00:00:26.011 --> 00:00:27.760
PROFESSOR: Going to
finish up a little bit

00:00:27.760 --> 00:00:29.920
from last time on gene
regulatory networks

00:00:29.920 --> 00:00:32.445
and see how the different
methods that we looked at

00:00:32.445 --> 00:00:34.695
compared, and then we'll
dive into protein interaction

00:00:34.695 --> 00:00:35.880
networks.

00:00:35.880 --> 00:00:37.860
Were there any questions
from last time?

00:00:40.371 --> 00:00:40.870
OK.

00:00:40.870 --> 00:00:41.460
Very good.

00:00:41.460 --> 00:00:45.400
So recall that we start off with
this dream challenge in which

00:00:45.400 --> 00:00:51.010
they provided unlabeled data
representing gene expression

00:00:51.010 --> 00:00:56.790
data for either in a completely
synthetic case, in silico data,

00:00:56.790 --> 00:00:59.120
or for three different
actual experiments--

00:00:59.120 --> 00:01:02.967
one in E. coli, one in S.
cerevisiae, and one in aureus.

00:01:02.967 --> 00:01:05.050
For some of those, it was
straight expression data

00:01:05.050 --> 00:01:05.970
under different conditions.

00:01:05.970 --> 00:01:08.450
In other cases, there were
actual knock-down experiments

00:01:08.450 --> 00:01:10.230
or other kinds of perturbations.

00:01:10.230 --> 00:01:12.304
And then they gave that
data out to the community

00:01:12.304 --> 00:01:14.470
and asked people to use
whatever methods they wanted

00:01:14.470 --> 00:01:18.190
to try to rediscover
automatically the gene

00:01:18.190 --> 00:01:19.930
regulatory networks.

00:01:19.930 --> 00:01:22.230
So with some
preliminary analysis,

00:01:22.230 --> 00:01:24.700
we saw that there were a couple
of main clusters of kinds

00:01:24.700 --> 00:01:27.880
of analyses that all had similar
properties across these data

00:01:27.880 --> 00:01:28.455
sets.

00:01:28.455 --> 00:01:29.830
There were the
Bayesian networks,

00:01:29.830 --> 00:01:32.720
that we've discussed now
in two separate contexts.

00:01:32.720 --> 00:01:35.960
And then we looked at
regression-based techniques

00:01:35.960 --> 00:01:37.754
and mutual information
based techniques.

00:01:37.754 --> 00:01:39.920
And there were a bunch of
other kinds of approaches.

00:01:39.920 --> 00:01:42.650
And some of them actually
combine multiple predictors

00:01:42.650 --> 00:01:45.430
from different kinds
of algorithms together.

00:01:45.430 --> 00:01:48.180
And some of them, they
evaluated how well each of these

00:01:48.180 --> 00:01:50.680
did on all the
different data sets.

00:01:50.680 --> 00:01:53.082
So first the results
on the in silico data,

00:01:53.082 --> 00:01:54.540
and they're showing
this as an area

00:01:54.540 --> 00:01:57.030
under the
precision-recall curve.

00:01:57.030 --> 00:01:59.890
Obviously, higher numbers
are going to be better here.

00:01:59.890 --> 00:02:02.050
So in this first
group over here are

00:02:02.050 --> 00:02:05.760
the regression-based
techniques, mutual information,

00:02:05.760 --> 00:02:08.560
correlation, Bayesian networks.

00:02:08.560 --> 00:02:12.000
Things didn't fall into any of
those particular categories.

00:02:12.000 --> 00:02:14.690
Meta were techniques that
use more than one class

00:02:14.690 --> 00:02:18.500
of prediction and then develop
their own prediction based

00:02:18.500 --> 00:02:20.380
on those individual techniques.

00:02:20.380 --> 00:02:23.580
Then they defined something
that they call the community

00:02:23.580 --> 00:02:25.719
definition, which
they combine data

00:02:25.719 --> 00:02:27.260
from many of the
different techniques

00:02:27.260 --> 00:02:30.170
together with their own
algorithms to kind of come up

00:02:30.170 --> 00:02:32.620
with what they call the
"wisdom of the crowds."

00:02:32.620 --> 00:02:36.100
And then R represents
a random collection

00:02:36.100 --> 00:02:39.390
of other predictions.

00:02:39.390 --> 00:02:42.340
And you can see that on
these in silico data,

00:02:42.340 --> 00:02:43.980
the performances
don't dramatically

00:02:43.980 --> 00:02:45.730
differ one from the other.

00:02:45.730 --> 00:02:48.130
Within each class, if you
look at the best performer

00:02:48.130 --> 00:02:52.530
in each class, they're all
sort of in the same league.

00:02:52.530 --> 00:02:56.480
Obviously, some of the classes
do better consistently.

00:02:56.480 --> 00:02:58.270
Now their point
in their analysis

00:02:58.270 --> 00:02:59.960
is about the wisdom
of the crowds, that

00:02:59.960 --> 00:03:01.970
taking all these data
together, even including

00:03:01.970 --> 00:03:04.265
some of the bad
ones, is beneficial.

00:03:04.265 --> 00:03:05.890
That's not the main
thing that I wanted

00:03:05.890 --> 00:03:07.640
to get out of these
data for our purposes.

00:03:07.640 --> 00:03:10.460
So these E. coli data,
notice though that the errant

00:03:10.460 --> 00:03:14.790
to the curve, it's about
30 something percent.

00:03:14.790 --> 00:03:17.787
Now this is, oh, sorry,
this is in silico data.

00:03:17.787 --> 00:03:19.620
Now this is the first
real experimental data

00:03:19.620 --> 00:03:21.567
we'll look at, so
this is E. coli data.

00:03:21.567 --> 00:03:24.150
And notice the change of scale,
that the best performer's only

00:03:24.150 --> 00:03:28.845
doing under less than 10% of
the possible objective optimal

00:03:28.845 --> 00:03:29.345
results.

00:03:29.345 --> 00:03:31.870
So you can see that the real
data are much, much harder

00:03:31.870 --> 00:03:33.467
than the in silico data.

00:03:33.467 --> 00:03:35.300
And here the performance
varies quite a lot.

00:03:35.300 --> 00:03:37.230
You can see that the Bayesian
networks are struggling,

00:03:37.230 --> 00:03:38.970
compared to some of
the other techniques.

00:03:38.970 --> 00:03:41.540
The best of those
doesn't really get

00:03:41.540 --> 00:03:45.231
close to the best of some
of these other approaches.

00:03:45.231 --> 00:03:47.730
So what they did next, was they
took some of the predictions

00:03:47.730 --> 00:03:53.454
from their community
predictions that were built off

00:03:53.454 --> 00:03:55.870
of all these other data, and
they went and actually tested

00:03:55.870 --> 00:03:56.453
some of these.

00:03:56.453 --> 00:03:59.010
So they built regulatory
networks for E. coli

00:03:59.010 --> 00:04:00.476
and for aureus.

00:04:00.476 --> 00:04:02.850
And then they actually did
some experiments to test them.

00:04:02.850 --> 00:04:04.350
I think the results
overall are kind

00:04:04.350 --> 00:04:07.050
of encouraging, in the
sense that if you focus

00:04:07.050 --> 00:04:09.749
on the top pie chart
here, of all the things

00:04:09.749 --> 00:04:11.290
that they tested,
about half of them,

00:04:11.290 --> 00:04:12.920
they could get some support.

00:04:12.920 --> 00:04:15.170
In some cases, it was
very strong support.

00:04:15.170 --> 00:04:18.370
In other cases, it
wasn't quite as good.

00:04:18.370 --> 00:04:20.745
So the glass is half
empty or half full.

00:04:20.745 --> 00:04:22.370
But also, one of the
interesting things

00:04:22.370 --> 00:04:24.260
is that the data
are quite variable

00:04:24.260 --> 00:04:26.310
over the different
predictions that they make.

00:04:26.310 --> 00:04:29.990
So each one of these circles
represents a regulator,

00:04:29.990 --> 00:04:33.450
and the things that they claim
are targets of that regulator.

00:04:33.450 --> 00:04:35.180
And things that are
in blue are things

00:04:35.180 --> 00:04:37.510
that were confirmed
by their experiments.

00:04:37.510 --> 00:04:41.000
The things with black outlines
and blue are the controls.

00:04:41.000 --> 00:04:42.860
So they knew that
these would be right.

00:04:42.860 --> 00:04:46.150
So you could see that for
pure R, they do very well.

00:04:46.150 --> 00:04:49.385
For some of these
others, they do mediocre.

00:04:49.385 --> 00:04:51.760
But there are some, which
they're honest enough to admit,

00:04:51.760 --> 00:04:52.760
they do very poorly on.

00:04:52.760 --> 00:04:54.551
So they didn't get any
of their predictions

00:04:54.551 --> 00:04:55.634
right for this regulator.

00:04:55.634 --> 00:04:58.050
And this probably reflects the
kind of data that they had,

00:04:58.050 --> 00:05:01.380
in terms of what conditions
were being tested.

00:05:01.380 --> 00:05:03.470
So, so far, things
look reasonable.

00:05:03.470 --> 00:05:05.330
I think the real
shocker of this paper

00:05:05.330 --> 00:05:07.610
does not appear in the
abstract or the title.

00:05:07.610 --> 00:05:10.430
But it is in one of the main
figures, if you pay attention.

00:05:10.430 --> 00:05:12.810
So these were the results
for in silico data.

00:05:12.810 --> 00:05:14.230
Everything looked pretty good.

00:05:14.230 --> 00:05:17.000
Change of scale to E. coli,
there's some variation.

00:05:17.000 --> 00:05:19.050
But you can make arguments.

00:05:19.050 --> 00:05:21.450
These are the results for
Saccharomyces cerevisiae.

00:05:21.450 --> 00:05:24.090
So this is the organism,
yeast, on which

00:05:24.090 --> 00:05:25.757
most of the gene
regulatory algorithms

00:05:25.757 --> 00:05:26.840
were originally developed.

00:05:26.840 --> 00:05:28.214
And people actually
built careers

00:05:28.214 --> 00:05:30.370
off of saying how great
their algorithms were

00:05:30.370 --> 00:05:32.760
in reconstructing these
regulatory networks.

00:05:32.760 --> 00:05:35.299
And we look at these
completely blinded data,

00:05:35.299 --> 00:05:37.340
where people don't know
what they're looking for.

00:05:37.340 --> 00:05:40.510
You could see that the actual
results are rather terrible.

00:05:40.510 --> 00:05:43.820
So the area under the curve
is in the single digits

00:05:43.820 --> 00:05:44.404
of percentage.

00:05:44.404 --> 00:05:46.861
And it doesn't seem to matter
what algorithm they're using.

00:05:46.861 --> 00:05:48.380
They're all doing very badly.

00:05:48.380 --> 00:05:51.990
And the community predictions
are no better-- in some cases,

00:05:51.990 --> 00:05:54.360
worse-- than the
individual ones.

00:05:54.360 --> 00:05:56.410
So this is really
a stunning result.

00:05:56.410 --> 00:05:57.372
It's there in the data.

00:05:57.372 --> 00:05:58.830
And if you dig into
the supplement,

00:05:58.830 --> 00:06:01.010
they actually explain
what's going on, I think,

00:06:01.010 --> 00:06:03.110
pretty clearly.

00:06:03.110 --> 00:06:04.840
Remember that all
of these predictions

00:06:04.840 --> 00:06:09.240
are being made by looking for a
transcriptional regulator that

00:06:09.240 --> 00:06:11.750
increases in its own
expression or decreases

00:06:11.750 --> 00:06:13.030
in its own expression.

00:06:13.030 --> 00:06:14.610
And that change in
its own expression

00:06:14.610 --> 00:06:16.480
is predictive of its targets.

00:06:16.480 --> 00:06:19.460
So the hypothesis is when you
have more of an activator,

00:06:19.460 --> 00:06:21.410
you'll have more of
its targets coming on.

00:06:21.410 --> 00:06:22.250
If you have less
of an activator,

00:06:22.250 --> 00:06:23.610
you'll have less of the targets.

00:06:23.610 --> 00:06:25.026
And you look through
all the data,

00:06:25.026 --> 00:06:27.370
whether it's by Bayesian
networks or regression,

00:06:27.370 --> 00:06:30.880
to find those kinds
of relationships.

00:06:30.880 --> 00:06:32.970
Now what if those
relationships don't actually

00:06:32.970 --> 00:06:34.550
exist in the data?

00:06:34.550 --> 00:06:36.540
And that's what
this chart shows.

00:06:36.540 --> 00:06:40.034
So the green are genes
that have no relationship

00:06:40.034 --> 00:06:40.700
with each other.

00:06:40.700 --> 00:06:44.340
And they're measuring here the
correlation across all the data

00:06:44.340 --> 00:06:46.360
sets, between two
pairs of genes,

00:06:46.360 --> 00:06:48.360
for which have no known
regulatory relationship.

00:06:52.360 --> 00:06:54.116
The purple are ones
that are targets

00:06:54.116 --> 00:06:55.490
of the same
transcription factor.

00:06:55.490 --> 00:06:56.950
And the orange
are ones where one

00:06:56.950 --> 00:06:59.920
is the activator or
repressor of the other.

00:06:59.920 --> 00:07:03.000
And in the in silico data,
they give a very nice spread

00:07:03.000 --> 00:07:05.450
between the green, the
orange, and the purple.

00:07:05.450 --> 00:07:07.670
So the co-regulator are
very highly correlated

00:07:07.670 --> 00:07:08.860
with each other.

00:07:08.860 --> 00:07:12.700
The ones that are parent-child
relationships-- a regulator

00:07:12.700 --> 00:07:15.650
and its target-- have a
pretty good correlation,

00:07:15.650 --> 00:07:17.600
much, much different
from the distribution

00:07:17.600 --> 00:07:20.690
that you see for the things
that are not interacting.

00:07:20.690 --> 00:07:23.590
And on these data, the
algorithms do their best.

00:07:23.590 --> 00:07:25.572
Then you look at
the E. coli data,

00:07:25.572 --> 00:07:27.530
and you can see that in
E. Coli, the curves are

00:07:27.530 --> 00:07:30.430
much closer to each other,
but there's still some spread.

00:07:30.430 --> 00:07:32.376
But when you look
at yeast-- again,

00:07:32.376 --> 00:07:34.000
this is where a lot
of these algorithms

00:07:34.000 --> 00:07:35.583
were developed-- you
could see there's

00:07:35.583 --> 00:07:38.760
almost no difference between the
correlation between the things

00:07:38.760 --> 00:07:40.940
that have no relationship
to each other,

00:07:40.940 --> 00:07:44.580
things that are co-regulated
by the same regulatory protein,

00:07:44.580 --> 00:07:47.040
or those parent-child
relationships.

00:07:47.040 --> 00:07:48.230
They're all quite similar.

00:07:48.230 --> 00:07:49.605
And it doesn't
matter whether you

00:07:49.605 --> 00:07:52.380
use correlation analysis
or mutual information.

00:07:52.380 --> 00:07:54.480
Over here and in this
right-hand panel,

00:07:54.480 --> 00:07:56.990
they've blown up the
bottom part of this curve,

00:07:56.990 --> 00:07:58.730
and you can see how
similar these are.

00:07:58.730 --> 00:08:00.720
So again, this is a
mutual information

00:08:00.720 --> 00:08:06.905
spread for in silico data for
E. coli and then for yeast.

00:08:09.480 --> 00:08:09.980
OK.

00:08:09.980 --> 00:08:14.390
So what I think we can say
about the expression analysis

00:08:14.390 --> 00:08:16.840
is that expression data
are very, very powerful

00:08:16.840 --> 00:08:18.590
for some things and
are going to be rather

00:08:18.590 --> 00:08:20.690
poor for some
other applications.

00:08:20.690 --> 00:08:23.360
So they're very powerful for
classification and clustering.

00:08:23.360 --> 00:08:25.560
We saw that earlier.

00:08:25.560 --> 00:08:27.250
Now what those
clusters mean, that's

00:08:27.250 --> 00:08:29.777
this inference problem
they're trying to solve now.

00:08:29.777 --> 00:08:32.110
And the expression data are
not sufficient to figure out

00:08:32.110 --> 00:08:34.539
what the regulatory proteins
are that are causing

00:08:34.539 --> 00:08:37.539
those sets of genes to be
co-expressed-- at least

00:08:37.539 --> 00:08:38.498
not in yeast.

00:08:38.498 --> 00:08:40.039
And I think there's
every expectation

00:08:40.039 --> 00:08:41.747
that if you did the
same thing in humans,

00:08:41.747 --> 00:08:43.830
you would have the same result.

00:08:43.830 --> 00:08:45.590
So the critical
question then is if you

00:08:45.590 --> 00:08:48.820
do want to build models of
how regulation is taking place

00:08:48.820 --> 00:08:51.110
in organisms, what do you do?

00:08:51.110 --> 00:08:54.060
And the answer is that you
need some other kind of data.

00:08:54.060 --> 00:08:56.960
So one thing you might
think, if we go back

00:08:56.960 --> 00:08:58.840
to this core analysis,
like what's wrong?

00:08:58.840 --> 00:09:02.870
Why is it that these gene
expression levels cannot be

00:09:02.870 --> 00:09:05.470
used to predict the
regulatory networks?

00:09:05.470 --> 00:09:08.540
And it comes down to
whether gene levels are

00:09:08.540 --> 00:09:09.790
predictive approaching levels.

00:09:09.790 --> 00:09:12.260
And a couple of groups
have looked into this.

00:09:12.260 --> 00:09:17.270
One of the earlier studies
was this one, now 2009,

00:09:17.270 --> 00:09:20.320
where they used microarray data
and looked at mRNA expression

00:09:20.320 --> 00:09:22.794
levels versus protein levels.

00:09:22.794 --> 00:09:23.960
And what do you see in this?

00:09:23.960 --> 00:09:25.740
You see that there is a trend.

00:09:25.740 --> 00:09:28.440
Right there, R
squared is around 0.2,

00:09:28.440 --> 00:09:30.170
but that there's a huge spread.

00:09:30.170 --> 00:09:32.390
So that for any
position on the x-axis,

00:09:32.390 --> 00:09:36.470
a particular level of mRNA, you
can have 1,000-fold variation

00:09:36.470 --> 00:09:38.769
in the protein levels.

00:09:38.769 --> 00:09:40.310
So a lot of people
saw this and said,

00:09:40.310 --> 00:09:42.890
well, we know there are
problems with microarrays.

00:09:42.890 --> 00:09:46.640
They're not really great
at predicting mRNA levels

00:09:46.640 --> 00:09:48.030
or low in protein levels.

00:09:48.030 --> 00:09:52.497
So maybe this will all get
better if we use mRNA-Seq.

00:09:52.497 --> 00:09:54.080
Now that turns out
not to be the case.

00:09:54.080 --> 00:09:58.090
So there was a very careful
study published in 2012,

00:09:58.090 --> 00:10:02.110
where the group used
microarray data, RNA-Seq data,

00:10:02.110 --> 00:10:05.490
and a number of different ways
of calling the proteomics data.

00:10:05.490 --> 00:10:06.676
So you might say, well,
maybe some of the problem

00:10:06.676 --> 00:10:09.217
is that you're not doing a very
good job of inferring protein

00:10:09.217 --> 00:10:11.360
levels from mass spec data.

00:10:11.360 --> 00:10:13.645
And so they try a whole
bunch of these different ways

00:10:13.645 --> 00:10:15.070
of pulling mass spec data.

00:10:15.070 --> 00:10:16.570
And then they look,
you should focus

00:10:16.570 --> 00:10:18.570
on the numbers in these
columns for the average

00:10:18.570 --> 00:10:22.510
and the best correlations
between the RNA

00:10:22.510 --> 00:10:27.470
data in these columns and the
proteomic data in the rows.

00:10:27.470 --> 00:10:29.700
And you could see the
best case scenario--

00:10:29.700 --> 00:10:35.730
you can get these up to 0.54
correlation, still pretty weak.

00:10:35.730 --> 00:10:38.029
So what's going on?

00:10:38.029 --> 00:10:39.820
What we've been focusing
on now is the idea

00:10:39.820 --> 00:10:42.700
that the RNA levels are going
to be very well correlated

00:10:42.700 --> 00:10:43.600
with protein levels.

00:10:43.600 --> 00:10:45.016
And I think a lot
of literature is

00:10:45.016 --> 00:10:47.760
based on hypotheses that
are almost identical.

00:10:47.760 --> 00:10:49.350
But in reality, of
course, there are

00:10:49.350 --> 00:10:50.516
a lot of processes involved.

00:10:50.516 --> 00:10:52.051
There's the process
of translation,

00:10:52.051 --> 00:10:53.550
which has a rate
associated with it.

00:10:53.550 --> 00:10:55.664
It has regulatory steps
associated with it.

00:10:55.664 --> 00:10:57.330
And then there are
degradatory pathways.

00:10:57.330 --> 00:10:59.580
So the RNA gets
degraded at some rate,

00:10:59.580 --> 00:11:01.430
and the protein gets
degraded at some rate.

00:11:01.430 --> 00:11:03.820
And sometimes those
rates are regulated,

00:11:03.820 --> 00:11:04.840
sometimes they're not.

00:11:04.840 --> 00:11:06.790
Sometimes it depends
on the sequence.

00:11:06.790 --> 00:11:08.430
So what would happen
if you actually

00:11:08.430 --> 00:11:09.610
measured what's going on?

00:11:09.610 --> 00:11:13.640
And that was done recently
in this paper in 2011,

00:11:13.640 --> 00:11:17.970
where the group used a
labeling technique for proteins

00:11:17.970 --> 00:11:20.660
to [INAUDIBLE] and measure
steady state levels of proteins

00:11:20.660 --> 00:11:23.620
and then label the
proteins at specific times

00:11:23.620 --> 00:11:25.700
and see how much
newly synthesized

00:11:25.700 --> 00:11:28.130
their protein was
at various times.

00:11:28.130 --> 00:11:30.780
And similarly, for RNA, using
a technology that allowed them

00:11:30.780 --> 00:11:35.197
to separate newly synthesized
transcripts from the bulk RNA.

00:11:35.197 --> 00:11:36.780
And once you have
those data, then you

00:11:36.780 --> 00:11:40.650
can find out what the spread is
in the half lives of proteins

00:11:40.650 --> 00:11:43.200
and the abundance of proteins.

00:11:43.200 --> 00:11:46.010
So if you focus on
the left-hand side,

00:11:46.010 --> 00:11:51.280
these are the
determined half lives

00:11:51.280 --> 00:11:54.890
for various RNAs in blue
and proteins in red.

00:11:54.890 --> 00:11:56.955
If you look at the
spread in the red ones,

00:11:56.955 --> 00:11:59.205
you've got at least three
orders of magnitude of range

00:11:59.205 --> 00:12:03.650
in stability in half
lives for proteins.

00:12:03.650 --> 00:12:06.201
So that's really at the
heart of why RNA levels are

00:12:06.201 --> 00:12:07.950
very poorly predictive
approaching levels,

00:12:07.950 --> 00:12:10.920
because there's such a range
of the stability proteins.

00:12:10.920 --> 00:12:13.130
And the RNAs also, they
spread over probably

00:12:13.130 --> 00:12:16.780
about one or two orders of
magnitude in the RNA stability.

00:12:16.780 --> 00:12:18.590
And then here are
the abundances.

00:12:18.590 --> 00:12:21.040
So you can see that
the range of abundance

00:12:21.040 --> 00:12:24.330
for average copies
per cell of proteins

00:12:24.330 --> 00:12:26.880
is extremely large,
from 100 to 10

00:12:26.880 --> 00:12:30.000
to the eighth copies per cell.

00:12:30.000 --> 00:12:34.570
Now if you look at the
degradation rates for protein

00:12:34.570 --> 00:12:36.170
half lives and RNA
half lives, you

00:12:36.170 --> 00:12:38.640
can see there's no correlation.

00:12:38.640 --> 00:12:40.575
So these are completely
independent processes

00:12:40.575 --> 00:12:42.825
that determine whether an
RNA is degraded or a protein

00:12:42.825 --> 00:12:44.292
is degraded.

00:12:44.292 --> 00:12:46.750
So then when you try to figure
out what the relationship is

00:12:46.750 --> 00:12:48.590
between RNA levels
and protein levels,

00:12:48.590 --> 00:12:51.350
you really have to resort to a
set of differential equations

00:12:51.350 --> 00:12:53.542
to map out what
all the rates are.

00:12:53.542 --> 00:12:55.250
And if you know all
those rates, then you

00:12:55.250 --> 00:12:57.200
can estimate what the
relationships will be.

00:12:57.200 --> 00:12:59.400
And so they did exactly that.

00:12:59.400 --> 00:13:01.750
And these charts show
what they inferred

00:13:01.750 --> 00:13:05.680
to be the contribution of
each of these components

00:13:05.680 --> 00:13:08.140
to protein levels.

00:13:08.140 --> 00:13:10.540
So on the left-hand
side, these are

00:13:10.540 --> 00:13:12.964
from cells which
had the most data.

00:13:12.964 --> 00:13:14.630
And they build a model
on the same cells

00:13:14.630 --> 00:13:16.088
from which they
collected the data.

00:13:16.088 --> 00:13:19.740
And in these cells, the RNA
levels account for about 40%

00:13:19.740 --> 00:13:22.320
of the protein
levels, the variance.

00:13:22.320 --> 00:13:24.080
And the biggest
thing that affects

00:13:24.080 --> 00:13:27.130
the abundance of proteins
is rates of translation.

00:13:27.130 --> 00:13:30.460
And then they took the data
built from one set of cells

00:13:30.460 --> 00:13:32.330
and tried to use it
to predict outcomes

00:13:32.330 --> 00:13:34.160
in another set of
cells in replicate.

00:13:34.160 --> 00:13:36.267
And the results are
kind of similar.

00:13:36.267 --> 00:13:38.850
They also did it for an entirely
different kind of cell types.

00:13:38.850 --> 00:13:40.741
In all of these cases,
the precise amounts

00:13:40.741 --> 00:13:41.490
are going to vary.

00:13:41.490 --> 00:13:43.250
But you can see that
the red bars, which

00:13:43.250 --> 00:13:46.720
represent the amount of
information content in the RNA,

00:13:46.720 --> 00:13:50.260
is less than about half of what
you can get from other sources.

00:13:50.260 --> 00:13:52.120
So this gets back
to why it's so hard

00:13:52.120 --> 00:13:56.740
to infer regulatory networks
solely from RNA levels.

00:13:56.740 --> 00:13:58.684
So this is the
plot that they get

00:13:58.684 --> 00:14:00.350
when they compare
protein levels and RNA

00:14:00.350 --> 00:14:02.270
levels at the
experimental level.

00:14:02.270 --> 00:14:04.040
And again, you see
that big spread and R

00:14:04.040 --> 00:14:06.650
squared at about 0.4,
which at the time,

00:14:06.650 --> 00:14:07.650
they were very proud of.

00:14:07.650 --> 00:14:09.316
They write several
times in the article,

00:14:09.316 --> 00:14:11.730
this is the best anyone
has seen to date.

00:14:11.730 --> 00:14:14.230
But if you incorporate all these
other pieces of information

00:14:14.230 --> 00:14:16.340
about RNA stability
and protein stability,

00:14:16.340 --> 00:14:18.670
you can actually get a
very, very good correlation.

00:14:18.670 --> 00:14:22.220
So once you know the variation
in the protein stability

00:14:22.220 --> 00:14:25.529
and the RNA stability for each
and every protein and RNA,

00:14:25.529 --> 00:14:27.820
then you can do a good job
of predicting protein levels

00:14:27.820 --> 00:14:28.690
from RNA levels.

00:14:28.690 --> 00:14:32.280
But without all that
data, you can't.

00:14:32.280 --> 00:14:33.370
Any questions on this?

00:14:36.627 --> 00:14:37.960
So what are we going to do then?

00:14:37.960 --> 00:14:41.000
So we really have two primary
things that we can do.

00:14:41.000 --> 00:14:44.600
We can try to explicitly model
all of these regulatory steps

00:14:44.600 --> 00:14:47.000
and include those in
our predictive models

00:14:47.000 --> 00:14:50.150
and try to build up gene
regulatory networks, protein

00:14:50.150 --> 00:14:53.500
models that actually include all
those different kinds of data.

00:14:53.500 --> 00:14:56.549
And we'll see that
in just a minute.

00:14:56.549 --> 00:14:58.590
And the other thing we
can try to do is actually,

00:14:58.590 --> 00:15:00.190
rather than try
to focus on what's

00:15:00.190 --> 00:15:03.930
downstream of RNA synthesis,
the protein levels,

00:15:03.930 --> 00:15:06.950
we can try to focus on what's
upstream of RNA synthesis

00:15:06.950 --> 00:15:09.800
and look at what the production
of RNAs-- which RNAs are

00:15:09.800 --> 00:15:11.370
getting turned on
and off-- tell us

00:15:11.370 --> 00:15:13.940
about the signaling pathways
and the transcription factors.

00:15:13.940 --> 00:15:16.148
And that's going to be a
topic of one of the upcoming

00:15:16.148 --> 00:15:22.290
lectures in which Professor
Gifford will look at variations

00:15:22.290 --> 00:15:24.780
in epigenomic data and
using those variations

00:15:24.780 --> 00:15:27.630
in epigenomic data to identify
sequences that represent which

00:15:27.630 --> 00:15:29.990
regulatory proteins are bound
under certain conditions

00:15:29.990 --> 00:15:31.090
and not others.

00:15:31.090 --> 00:15:31.682
Questions?

00:15:31.682 --> 00:15:32.182
Yeah?

00:15:32.182 --> 00:15:33.598
AUDIENCE: In a
typical experiment,

00:15:33.598 --> 00:15:36.761
the rate constants for how
many mRNAs or proteins can

00:15:36.761 --> 00:15:37.957
be estimated?

00:15:37.957 --> 00:15:40.540
PROFESSOR: So the question was
how many rate constants can you

00:15:40.540 --> 00:15:41.430
estimate in a
typical experiment?

00:15:41.430 --> 00:15:42.440
So I should say,
first of all, they're

00:15:42.440 --> 00:15:43.480
not typical experiments.

00:15:43.480 --> 00:15:45.580
Very few people do
this kind of analysis.

00:15:45.580 --> 00:15:47.910
It's actually very time
consuming, very expensive.

00:15:47.910 --> 00:15:50.802
So I think in this one, it was--
I'll get the numbers roughly

00:15:50.802 --> 00:15:52.010
wrong-- but it was thousands.

00:15:52.010 --> 00:15:55.410
It was some decent fraction
of the proteome, but not

00:15:55.410 --> 00:15:56.520
the entire one.

00:15:56.520 --> 00:15:59.200
But most of the data
set's papers you'll read

00:15:59.200 --> 00:16:02.670
do not include any analysis of
stability rates, degradation

00:16:02.670 --> 00:16:03.930
rates.

00:16:03.930 --> 00:16:08.459
They only look at the bulk
abundance of the RNAs.

00:16:08.459 --> 00:16:09.125
Other questions?

00:16:13.110 --> 00:16:13.610
OK.

00:16:13.610 --> 00:16:15.140
So this is an
upcoming lecture where

00:16:15.140 --> 00:16:17.620
we're going to actually
try to go backwards.

00:16:17.620 --> 00:16:19.896
We're going to say, we
see these changes in RNA.

00:16:19.896 --> 00:16:21.270
What does that
tell us about what

00:16:21.270 --> 00:16:24.280
regulatory regions of the
genome were active or not?

00:16:24.280 --> 00:16:26.065
And then you could
go upstream from that

00:16:26.065 --> 00:16:27.940
and try to figure out
the signaling pathways.

00:16:27.940 --> 00:16:30.750
So if I know
changes in RNA, I'll

00:16:30.750 --> 00:16:32.740
deduce, as we'll see in
that upcoming lecture--

00:16:32.740 --> 00:16:36.090
the sequences-- the identity
of the DNA binding proteins.

00:16:36.090 --> 00:16:37.580
And then I could
try to figure out

00:16:37.580 --> 00:16:40.270
what the signaling
pathways were that drove

00:16:40.270 --> 00:16:42.980
those changes in
gene expression.

00:16:42.980 --> 00:16:46.910
Now later in this lecture, we'll
talk about the network modeling

00:16:46.910 --> 00:16:47.870
problem.

00:16:47.870 --> 00:16:50.270
If assuming you knew these
transcription factors,

00:16:50.270 --> 00:16:53.330
what could you do to
infer this network?

00:16:53.330 --> 00:16:54.920
But before we go
to that, I'd like

00:16:54.920 --> 00:16:57.230
to talk about an interesting
modeling approach that

00:16:57.230 --> 00:16:59.970
tries to take into account
all these degradatory pathways

00:16:59.970 --> 00:17:02.470
and look specifically at
each kind of regulation

00:17:02.470 --> 00:17:05.530
as an explicit step in
the model and see how that

00:17:05.530 --> 00:17:08.369
copes with some of these issues.

00:17:08.369 --> 00:17:16.520
So this is work
from Josh Stewart.

00:17:16.520 --> 00:17:18.427
And one of the first
papers is here.

00:17:18.427 --> 00:17:20.010
We'll look at some
later ones as well.

00:17:20.010 --> 00:17:22.720
And the idea here is to
explicitly, as I said,

00:17:22.720 --> 00:17:25.130
deal with many, many
different steps in regulation

00:17:25.130 --> 00:17:28.860
and try to be quite specific
about what kinds of data

00:17:28.860 --> 00:17:32.750
are informing about what
step in the process.

00:17:32.750 --> 00:17:35.120
So we measure the
things in the bottom

00:17:35.120 --> 00:17:39.300
here-- arrays that tell us
how many copies of a gene

00:17:39.300 --> 00:17:41.282
there are in the genome,
especially in cancer.

00:17:41.282 --> 00:17:42.740
And you can get
big changes of what

00:17:42.740 --> 00:17:45.510
are called copy number,
amplifications, or deletions

00:17:45.510 --> 00:17:46.970
of large chunks of chromosomes.

00:17:46.970 --> 00:17:49.250
You need to take
that into account.

00:17:49.250 --> 00:17:52.120
All the RNA-Seq and microarrays
that we were talking about

00:17:52.120 --> 00:17:53.620
in measuring
transcription levels--

00:17:53.620 --> 00:17:54.870
what do they actually tell us?

00:17:54.870 --> 00:17:56.590
Well, they give us
some information

00:17:56.590 --> 00:17:58.370
about what they're
directly connected to.

00:17:58.370 --> 00:18:01.195
So the transcriptomic data tells
something about the expression

00:18:01.195 --> 00:18:01.930
state.

00:18:01.930 --> 00:18:04.380
But notice they have explicitly
separated the expression

00:18:04.380 --> 00:18:07.370
state of the RNA from
the protein level.

00:18:07.370 --> 00:18:08.870
And they separated
the protein level

00:18:08.870 --> 00:18:10.560
from the protein activity.

00:18:10.560 --> 00:18:12.657
And they have these
little black boxes in here

00:18:12.657 --> 00:18:14.740
that represent the different
kinds of regulations.

00:18:14.740 --> 00:18:18.504
So however many copies of a
gene you have in the genome,

00:18:18.504 --> 00:18:20.920
there's some regulatory event,
transcriptional regulation,

00:18:20.920 --> 00:18:22.545
that determines how
much expression you

00:18:22.545 --> 00:18:24.219
get at the mRNA level.

00:18:24.219 --> 00:18:25.760
There's another
regulatory event here

00:18:25.760 --> 00:18:28.100
that determines at what
rate those RNAs are

00:18:28.100 --> 00:18:29.890
turned into proteins.

00:18:29.890 --> 00:18:31.674
And there are other
regulatory steps here

00:18:31.674 --> 00:18:33.340
that have to do with
signaling pathways,

00:18:33.340 --> 00:18:35.589
for example, that determine
whether those proteins are

00:18:35.589 --> 00:18:36.650
active or not.

00:18:36.650 --> 00:18:39.108
So we're going to treat each
of those as separate variables

00:18:39.108 --> 00:18:41.490
in our model that
are going to be

00:18:41.490 --> 00:18:43.410
connected by these black boxes.

00:18:46.254 --> 00:18:47.920
So they call their
algorithm "Paradigm,"

00:18:47.920 --> 00:18:49.590
and they developed
it in the context

00:18:49.590 --> 00:18:51.059
of looking at cancer data.

00:18:51.059 --> 00:18:53.600
In cancer data, the two primary
kinds of information they had

00:18:53.600 --> 00:18:58.100
were the RNA levels from either
microarray or RNA-Seq and then

00:18:58.100 --> 00:18:59.710
these copy number
variations, again,

00:18:59.710 --> 00:19:01.990
representing
amplifications or deletions

00:19:01.990 --> 00:19:03.820
of chunks of the genome.

00:19:03.820 --> 00:19:05.780
And what they're trying
to infer from that

00:19:05.780 --> 00:19:07.870
is how active different
components are of

00:19:07.870 --> 00:19:10.706
known signaling pathways.

00:19:10.706 --> 00:19:12.580
Now the approach that
they used that involved

00:19:12.580 --> 00:19:14.413
all of those little
black boxes is something

00:19:14.413 --> 00:19:16.060
called a factor graph.

00:19:16.060 --> 00:19:18.855
And factor graphs can be
thought of in the same context

00:19:18.855 --> 00:19:19.730
as Bayesian networks.

00:19:19.730 --> 00:19:23.540
In fact, Bayesian networks
are a type of factor graph.

00:19:23.540 --> 00:19:25.670
So if I have a
Bayesian network that

00:19:25.670 --> 00:19:27.730
represents these three
variables, where they're

00:19:27.730 --> 00:19:29.770
directly connected by
edges, in a factor graph,

00:19:29.770 --> 00:19:32.700
there would be this extra
kind of node-- this black box

00:19:32.700 --> 00:19:35.960
or red box-- that's the factor
that's going to connect them.

00:19:35.960 --> 00:19:37.467
So what do these things do?

00:19:37.467 --> 00:19:39.050
Well, again, they're
bipartite graphs.

00:19:39.050 --> 00:19:40.800
They always have these
two different kinds

00:19:40.800 --> 00:19:43.696
of nodes-- the random
variables and the factors.

00:19:43.696 --> 00:19:45.820
And the reason they're
called factor graphs is they

00:19:45.820 --> 00:19:48.460
describe how the global
function-- in our case,

00:19:48.460 --> 00:19:51.120
it's going to be the global
probability distribution--

00:19:51.120 --> 00:19:53.390
can be broken down into
factorable components.

00:19:53.390 --> 00:19:56.330
It can be combined
in a product to look

00:19:56.330 --> 00:20:02.090
at what the global
probability function is.

00:20:02.090 --> 00:20:04.386
So if I have some
global function over all

00:20:04.386 --> 00:20:06.760
the variables, you can think
of this again, specifically,

00:20:06.760 --> 00:20:09.218
as the probability function--
the joint probability for all

00:20:09.218 --> 00:20:11.910
the variables in my system--
I want to be able to divide it

00:20:11.910 --> 00:20:14.230
into a product of
individual terms,

00:20:14.230 --> 00:20:16.900
where I don't have all the
variables in each of these f's.

00:20:16.900 --> 00:20:19.382
They're just some
subset of variables.

00:20:19.382 --> 00:20:23.150
And each of these represents
one of these terms

00:20:23.150 --> 00:20:25.990
in that global product.

00:20:25.990 --> 00:20:27.837
The only things that
are in this function,

00:20:27.837 --> 00:20:29.670
are things to which
it's directly connected.

00:20:29.670 --> 00:20:32.540
So these edges exist
solely between a factor

00:20:32.540 --> 00:20:35.500
and the variables that are
terms in that equation.

00:20:35.500 --> 00:20:36.450
Is that clear?

00:20:43.710 --> 00:20:45.130
So in this context,
the variables

00:20:45.130 --> 00:20:46.464
are going to be nodes.

00:20:46.464 --> 00:20:47.880
And their allowed
values are going

00:20:47.880 --> 00:20:51.770
to be whether they're
activated or not activated.

00:20:51.770 --> 00:20:54.044
The factors are going to
describe the relationships

00:20:54.044 --> 00:20:54.960
among those variables.

00:20:54.960 --> 00:20:57.970
We previously saw those as
being cases of regulation.

00:20:57.970 --> 00:20:59.689
Is the RNA turned into protein?

00:20:59.689 --> 00:21:00.730
Is the protein activated?

00:21:04.282 --> 00:21:05.740
And what we'd like
to be able do is

00:21:05.740 --> 00:21:07.410
compute marginal probabilities.

00:21:07.410 --> 00:21:09.650
So we've got some
big network that

00:21:09.650 --> 00:21:12.580
represents our understanding
of all the signaling pathways

00:21:12.580 --> 00:21:15.610
and all the transcriptional
regulatory networks in a cancer

00:21:15.610 --> 00:21:16.110
cell.

00:21:16.110 --> 00:21:18.830
And we want to ask about
a particular pathway

00:21:18.830 --> 00:21:21.980
or a particular protein,
what's the probability

00:21:21.980 --> 00:21:24.040
that this protein or this
pathway is activated,

00:21:24.040 --> 00:21:27.669
marginalized over all
the other variables?

00:21:27.669 --> 00:21:28.460
So that's our goal.

00:21:28.460 --> 00:21:30.220
Our goal is to find
a way to compute

00:21:30.220 --> 00:21:32.844
these marginal
probabilities efficiently.

00:21:32.844 --> 00:21:34.260
And how do you
compute a marginal?

00:21:34.260 --> 00:21:37.430
Well, obviously you need to
sum over all the configurations

00:21:37.430 --> 00:21:41.190
of all the variables that
have your particular variable

00:21:41.190 --> 00:21:41.820
at its value.

00:21:41.820 --> 00:21:44.390
So if I want to know if
MYC and MAX are active,

00:21:44.390 --> 00:21:47.300
I set MYC and MAX
equal to active.

00:21:47.300 --> 00:21:49.200
And then I sum over
all the configurations

00:21:49.200 --> 00:21:50.882
that are consistent with that.

00:21:50.882 --> 00:21:52.590
And in general, that
would be hard to do.

00:21:52.590 --> 00:21:54.600
But the factor graph
gives us an efficient way

00:21:54.600 --> 00:21:55.897
of figuring out how to do that.

00:21:55.897 --> 00:21:56.980
I'll show you in a second.

00:22:00.210 --> 00:22:01.610
So I have some global function.

00:22:01.610 --> 00:22:04.220
In this case, this little
factor graph over here,

00:22:04.220 --> 00:22:07.450
this is the global function.

00:22:07.450 --> 00:22:10.850
Now remember, these
represent the factors,

00:22:10.850 --> 00:22:12.330
and they only have
edges to things

00:22:12.330 --> 00:22:14.850
that are terms in
their equations.

00:22:14.850 --> 00:22:18.850
So over here, is a
function of x3 and x5.

00:22:18.850 --> 00:22:24.920
And so it has edges to x3 and
x5, and so on for all of them.

00:22:24.920 --> 00:22:27.730
And if I want to explicitly
compute the marginal

00:22:27.730 --> 00:22:29.590
with respect to a
particular variable,

00:22:29.590 --> 00:22:32.160
so the marginal
with respect to x1

00:22:32.160 --> 00:22:36.550
set equal to a, so I'd
have this function with x1

00:22:36.550 --> 00:22:40.900
equal to a times the sum over
all possible states of x2,

00:22:40.900 --> 00:22:45.137
the sum over all possible
states of x3, x4, and x5.

00:22:45.137 --> 00:22:45.720
Is that clear?

00:22:45.720 --> 00:22:47.428
That's just the
definition of a marginal.

00:22:51.190 --> 00:22:54.850
They introduced a notation in
factor graphs that's called

00:22:54.850 --> 00:22:55.985
a "not-sum."

00:22:55.985 --> 00:22:59.679
It's rather terrible, but
the not-sum or summary.

00:22:59.679 --> 00:23:01.220
So I like this term,
summary, better.

00:23:01.220 --> 00:23:02.870
The summary over
all the variables.

00:23:02.870 --> 00:23:05.540
So if I want to figure
out the summary for x1,

00:23:05.540 --> 00:23:07.970
that's the sum over
all the other variables

00:23:07.970 --> 00:23:09.600
of all their
possible states when

00:23:09.600 --> 00:23:13.290
I set x1 equal to
a, in this case.

00:23:13.290 --> 00:23:14.510
So it's purely a definition.

00:23:14.510 --> 00:23:19.140
So then I can rewrite-- and you
can work this through by hand

00:23:19.140 --> 00:23:21.340
after class-- but
I can rewrite this,

00:23:21.340 --> 00:23:24.220
which is this intuitive way
of thinking of the marginal,

00:23:24.220 --> 00:23:27.840
in terms of these not-sums,
where each one of these

00:23:27.840 --> 00:23:30.940
is over all the other
variables that are not

00:23:30.940 --> 00:23:33.380
the one that's in the brackets.

00:23:33.380 --> 00:23:35.310
So that's just the definition.

00:23:35.310 --> 00:23:37.112
OK, this hasn't really
helped us very much,

00:23:37.112 --> 00:23:38.570
if we don't have
some efficient way

00:23:38.570 --> 00:23:39.820
of computing these marginals.

00:23:39.820 --> 00:23:41.470
And that's what the
factor graph does.

00:23:41.470 --> 00:23:43.650
So we've got some factor graph.

00:23:43.650 --> 00:23:46.760
We have this
representation, either

00:23:46.760 --> 00:23:50.020
in terms of graph or equation,
of how the global function can

00:23:50.020 --> 00:23:51.340
be partitioned.

00:23:51.340 --> 00:23:54.280
Now if I take any one
of these factor graphs,

00:23:54.280 --> 00:23:56.210
and I want to compute
a marginal over a node,

00:23:56.210 --> 00:24:00.180
I can re-draw the factor graph
so that variable of interest

00:24:00.180 --> 00:24:01.620
is the root node.

00:24:01.620 --> 00:24:02.120
Right?

00:24:02.120 --> 00:24:04.380
Everyone see that these
two representations

00:24:04.380 --> 00:24:06.206
are completely equivalent?

00:24:06.206 --> 00:24:08.722
I've just yanked
x1 up to the top.

00:24:08.722 --> 00:24:10.055
So now this is a tree structure.

00:24:12.482 --> 00:24:14.190
So this is that factor
graph that we just

00:24:14.190 --> 00:24:15.546
saw drawn as a tree.

00:24:15.546 --> 00:24:17.670
And this is what's called
an expression tree, which

00:24:17.670 --> 00:24:19.470
is going to tell
us how to compute

00:24:19.470 --> 00:24:22.575
the marginal over the
structure of the graph.

00:24:25.330 --> 00:24:28.880
So this is just copied
from the previous picture.

00:24:28.880 --> 00:24:32.650
And now we're going to come up
with a program for computing

00:24:32.650 --> 00:24:35.930
these marginals, using
this tree structure.

00:24:35.930 --> 00:24:39.520
So first I'm going to compute
that summary function--

00:24:39.520 --> 00:24:43.430
the sum over all sets of the
other variables for everything

00:24:43.430 --> 00:24:46.300
below this point, starting with
the lowest point in the graph.

00:24:46.300 --> 00:24:48.870
And we can compute the
summary function there.

00:24:48.870 --> 00:24:55.590
And that's this term, the
summary for x3 of just this fE.

00:24:55.590 --> 00:25:00.440
I do the same thing for
fD, the summary for it.

00:25:00.440 --> 00:25:02.720
And then I go up a
level in the tree,

00:25:02.720 --> 00:25:06.327
and I multiply the summary
for everything below it.

00:25:06.327 --> 00:25:08.410
So I'm going to compute
the product of the summary

00:25:08.410 --> 00:25:09.510
functions.

00:25:09.510 --> 00:25:12.010
And I always compute the summary
with respect to the parent.

00:25:12.010 --> 00:25:15.550
So here the parent was
x3, for both of these.

00:25:15.550 --> 00:25:19.240
So these are summaries
with respect to x3.

00:25:19.240 --> 00:25:20.260
Here who's the parent?

00:25:20.260 --> 00:25:20.760
x1.

00:25:20.760 --> 00:25:23.045
And so the summary is to x1.

00:25:27.280 --> 00:25:28.100
Yes?

00:25:28.100 --> 00:25:29.558
AUDIENCE: Are there
directed edges?

00:25:29.558 --> 00:25:33.860
In the sense that in f, in
the example on the right,

00:25:33.860 --> 00:25:37.557
is fD just relating
how x4 relates to x3?

00:25:37.557 --> 00:25:38.890
PROFESSOR: That's exactly right.

00:25:38.890 --> 00:25:44.210
So the edges represent which
factor you're related to.

00:25:44.210 --> 00:25:47.040
So that's why I can
redraw it in any way.

00:25:47.040 --> 00:25:49.872
I'm always going to
go from the leaves up.

00:25:49.872 --> 00:25:54.274
I don't have to worry about any
directed edges in the graph.

00:25:54.274 --> 00:25:54.940
Other questions.

00:25:58.540 --> 00:26:01.080
So what this does
is it gives us a way

00:26:01.080 --> 00:26:04.310
to officially, overall a
complicated graph structure,

00:26:04.310 --> 00:26:07.390
compute marginals.

00:26:07.390 --> 00:26:09.580
And they're typically
thought of in terms

00:26:09.580 --> 00:26:12.430
of messages that are being sent
from the bottom of the graph up

00:26:12.430 --> 00:26:13.160
to the top.

00:26:13.160 --> 00:26:15.451
And you can have a rule from
computing these marginals.

00:26:15.451 --> 00:26:17.260
And the rule is as follows.

00:26:17.260 --> 00:26:19.580
Each vertex waits
for the messages

00:26:19.580 --> 00:26:21.270
from all of its
children, until it

00:26:21.270 --> 00:26:23.820
gets its-- the messages are
accumulating their way up

00:26:23.820 --> 00:26:24.564
the graph.

00:26:24.564 --> 00:26:25.980
And every node is
waiting until it

00:26:25.980 --> 00:26:29.900
hears from all of its progeny
about what's going on.

00:26:29.900 --> 00:26:33.790
And then it sends the signal
up above it to its parent,

00:26:33.790 --> 00:26:35.110
based on the following rules.

00:26:35.110 --> 00:26:38.380
A variable node just takes
the product of the children.

00:26:38.380 --> 00:26:41.120
And a factor node-- one of
those little black boxes--

00:26:41.120 --> 00:26:44.017
computes the summary
for the children

00:26:44.017 --> 00:26:45.350
and sends that up to the parent.

00:26:45.350 --> 00:26:47.540
And it's the summary with
respect to the parent,

00:26:47.540 --> 00:26:50.924
just like in the
examples before.

00:26:50.924 --> 00:26:53.090
So this is a formula for
computing single marginals.

00:26:53.090 --> 00:26:55.780
Now it turns out-- I'm not going
to go into details of this.

00:26:55.780 --> 00:26:57.080
It's kind of complicated.

00:26:57.080 --> 00:27:00.780
But you actually can,
based on this core idea,

00:27:00.780 --> 00:27:04.950
come up with an efficient way of
computing all of the marginals

00:27:04.950 --> 00:27:06.450
without having to
do this separately

00:27:06.450 --> 00:27:07.325
for every single one.

00:27:07.325 --> 00:27:09.370
And that's called a
message passing algorithm.

00:27:09.370 --> 00:27:10.870
And if you're really
interested, you

00:27:10.870 --> 00:27:14.730
can look into the citation
for how that's done.

00:27:14.730 --> 00:27:18.960
So the core idea is that we
can take a representation

00:27:18.960 --> 00:27:22.185
of our belief of how this
global function-- in our case,

00:27:22.185 --> 00:27:24.560
it's going to be the joint
probability-- factors in terms

00:27:24.560 --> 00:27:26.670
of particular
biological processes.

00:27:26.670 --> 00:27:30.040
We can encode what we know about
the regulation in that factor

00:27:30.040 --> 00:27:31.457
graph, the structure
of the graph.

00:27:31.457 --> 00:27:33.081
And then we could
have an efficient way

00:27:33.081 --> 00:27:35.110
of computing the marginals,
which will tell us,

00:27:35.110 --> 00:27:37.010
given the data,
what's the probability

00:27:37.010 --> 00:27:38.960
that this particular
pathway is active?

00:27:41.510 --> 00:27:44.010
So in this particular case,
in this paradigm model,

00:27:44.010 --> 00:27:45.930
the variables can
take three states--

00:27:45.930 --> 00:27:49.130
activated, deactivated,
or unchanged.

00:27:49.130 --> 00:27:51.800
And this is, in a tumor
setting, for example,

00:27:51.800 --> 00:27:55.024
you might say the tumor is
just like the wild type cell,

00:27:55.024 --> 00:27:57.440
or the tumor has activation
with respect to the wild type,

00:27:57.440 --> 00:27:59.648
or it has a repression with
respect to the wild type.

00:28:03.010 --> 00:28:06.510
Again, this is the structure of
the factor graph that they're

00:28:06.510 --> 00:28:09.450
using and the different kinds
of information that they have.

00:28:09.450 --> 00:28:11.230
The primary experimental
data are just

00:28:11.230 --> 00:28:15.320
these arrays that tell us about
SNiPs and copy number variation

00:28:15.320 --> 00:28:18.310
and then arrays or RNA-Seq to
tell us about the transcript

00:28:18.310 --> 00:28:19.910
levels.

00:28:19.910 --> 00:28:21.290
But now they can
encode all sorts

00:28:21.290 --> 00:28:23.310
of rather complicated
biological functions

00:28:23.310 --> 00:28:25.510
in the graph structure itself.

00:28:25.510 --> 00:28:28.750
So transcription
regulation is shown here.

00:28:28.750 --> 00:28:31.016
Why is the edge from
activity to here?

00:28:34.984 --> 00:28:36.990
Because we don't
want to just infer

00:28:36.990 --> 00:28:40.990
that if there's more of the
protein, there's more activity.

00:28:40.990 --> 00:28:42.760
So we're actually,
explicitly computing

00:28:42.760 --> 00:28:45.040
the activity of each protein.

00:28:45.040 --> 00:28:48.440
So if an RNA gets
transcribed, it's

00:28:48.440 --> 00:28:51.400
because some transcription
factor was active.

00:28:51.400 --> 00:28:53.570
And the transcription
factor might not

00:28:53.570 --> 00:28:56.590
be active, even if the levels
of the transcription factor

00:28:56.590 --> 00:28:57.790
are high.

00:28:57.790 --> 00:29:00.322
That's one of the
pieces that's not

00:29:00.322 --> 00:29:02.530
encoded in all of those
things that were in the dream

00:29:02.530 --> 00:29:05.560
challenge, that are really
critical for representing

00:29:05.560 --> 00:29:07.240
the regulatory structure.

00:29:07.240 --> 00:29:09.140
Similarly, protein
activation-- I

00:29:09.140 --> 00:29:11.810
can have protein that goes from
being present to being active.

00:29:11.810 --> 00:29:14.350
So think of a
kinase, that itself

00:29:14.350 --> 00:29:16.855
needs to be phosphorylated
to be active.

00:29:16.855 --> 00:29:18.230
So that would be
that transition.

00:29:18.230 --> 00:29:20.060
Some other kinase comes in.

00:29:20.060 --> 00:29:22.240
And if that other
kinase1 is active,

00:29:22.240 --> 00:29:24.210
then it can
phosphorylate kinase2

00:29:24.210 --> 00:29:26.392
and make that one active.

00:29:26.392 --> 00:29:27.850
And so it's pretty
straightforward.

00:29:27.850 --> 00:29:30.179
You can also represent the
formation of a complex.

00:29:30.179 --> 00:29:32.220
So the fact that all the
proteins are in the cell

00:29:32.220 --> 00:29:34.678
doesn't necessarily mean they're
forming an active complex.

00:29:34.678 --> 00:29:37.590
So the next step
then can be here.

00:29:37.590 --> 00:29:39.190
Only when I have
all of them, would I

00:29:39.190 --> 00:29:40.470
have activity of the complex.

00:29:40.470 --> 00:29:43.730
We'll talk about how AND-like
connections are formed.

00:29:43.730 --> 00:29:46.060
And then they also
can incorporate OR.

00:29:46.060 --> 00:29:47.160
So what does that mean?

00:29:47.160 --> 00:29:50.290
So if I know that all
members of the gene family

00:29:50.290 --> 00:29:52.790
can do something, I might
want to explicitly represent

00:29:52.790 --> 00:29:57.230
that gene family as an element
to the graph-- a variable.

00:29:57.230 --> 00:29:59.400
Is any member of
this family active?

00:29:59.400 --> 00:30:01.140
And so that would be
done this way, where

00:30:01.140 --> 00:30:03.340
if you have an OR-like
function here, then

00:30:03.340 --> 00:30:07.110
this factor would make this gene
active if any of the parents

00:30:07.110 --> 00:30:07.720
are active.

00:30:11.290 --> 00:30:13.250
So there, they
give a toy example,

00:30:13.250 --> 00:30:15.630
where they're trying to figure
out if the P53 pathway is

00:30:15.630 --> 00:30:18.820
active, so MDM2 is
an inhibitor of P53.

00:30:18.820 --> 00:30:21.430
P53 can be an
activator-related apoptosis.

00:30:21.430 --> 00:30:24.260
And so for separately,
for MDM2 and for P53,

00:30:24.260 --> 00:30:27.390
they have the factor graphs
that show the relationship

00:30:27.390 --> 00:30:29.560
between copy number
variation and transcript

00:30:29.560 --> 00:30:32.230
level and protein
level and activity.

00:30:32.230 --> 00:30:33.610
And those relate to each other.

00:30:33.610 --> 00:30:35.750
And then those relate to
the apoptotic pathway.

00:30:39.035 --> 00:30:40.910
So what they want to do
then is take the data

00:30:40.910 --> 00:30:43.300
that they have, in
terms of these pathways,

00:30:43.300 --> 00:30:45.322
and they want to compute
the likelihood ratios.

00:30:45.322 --> 00:30:46.780
What's the probability
of observing

00:30:46.780 --> 00:30:52.250
the data, given a hypothesis
that this pathway is active

00:30:52.250 --> 00:30:54.250
and all my other settings
of the parameters?

00:30:54.250 --> 00:30:55.708
And compare that
to the probability

00:30:55.708 --> 00:30:58.902
of the data, given that
that pathway is not active.

00:30:58.902 --> 00:31:00.610
So this is the kinds
of likelihood ratios

00:31:00.610 --> 00:31:02.526
we've been seeing now
in a couple of lectures.

00:31:04.631 --> 00:31:07.130
So now it gets into the details
of how you actually do this.

00:31:07.130 --> 00:31:09.400
So there's a lot of manual
steps involved here.

00:31:09.400 --> 00:31:12.840
So if I want to encode a
regulatory pathway as a factor

00:31:12.840 --> 00:31:18.052
graph, it's currently done in a
manual way or semi-manual way.

00:31:18.052 --> 00:31:19.510
You convert what's
in the databases

00:31:19.510 --> 00:31:20.968
into the structure
or factor graph.

00:31:20.968 --> 00:31:23.420
And you make a
series of decisions

00:31:23.420 --> 00:31:25.100
about exactly how
you want to do that.

00:31:25.100 --> 00:31:26.891
You can argue with the
particular decisions

00:31:26.891 --> 00:31:29.620
they made, but the
reasonable ones.

00:31:29.620 --> 00:31:31.390
People could do
things differently.

00:31:31.390 --> 00:31:37.757
So they convert the regulatory
networks into graphs.

00:31:37.757 --> 00:31:39.840
And then they have to
define some of the functions

00:31:39.840 --> 00:31:41.050
on this graph.

00:31:41.050 --> 00:31:44.810
So they define the expected
state of a variable,

00:31:44.810 --> 00:31:47.260
based on the state
of its parents.

00:31:47.260 --> 00:31:50.879
And they take a majority
vote of the parents.

00:31:50.879 --> 00:31:53.420
So a parent that's connected by
a positive edge, meaning it's

00:31:53.420 --> 00:31:55.860
an activator, if the
parent is active,

00:31:55.860 --> 00:31:59.020
then it contributes a
plus 1 to the child.

00:31:59.020 --> 00:32:01.360
If it's connected by
a repressive edge,

00:32:01.360 --> 00:32:04.200
then the parenting active
would make a vote of minus 1

00:32:04.200 --> 00:32:05.040
for the child.

00:32:05.040 --> 00:32:10.580
And you take the majority
vote of all those votes.

00:32:10.580 --> 00:32:11.990
So that's what this says.

00:32:11.990 --> 00:32:14.950
But the nice thing is that you
can also incorporate logic.

00:32:14.950 --> 00:32:17.000
So for example, when
we said, is any member

00:32:17.000 --> 00:32:18.250
of this pathway active?

00:32:18.250 --> 00:32:20.630
And you have a
family member node.

00:32:20.630 --> 00:32:23.570
So that can be done
with an OR function.

00:32:23.570 --> 00:32:25.700
And there, it's
these same factors

00:32:25.700 --> 00:32:28.480
that will determine--
so some of these edges

00:32:28.480 --> 00:32:29.960
are going to get
labeled "maximum"

00:32:29.960 --> 00:32:33.130
or "minimum," that
tell you what's

00:32:33.130 --> 00:32:35.420
the expected value of the
child, based on the parent.

00:32:35.420 --> 00:32:38.129
So if it's an OR, then if any
of the parents are active,

00:32:38.129 --> 00:32:39.170
then the child is active.

00:32:39.170 --> 00:32:40.753
And if it's AND, you
need all of them.

00:32:43.150 --> 00:32:45.906
So you could have described
all of these networks

00:32:45.906 --> 00:32:46.780
by Bayesian networks.

00:32:46.780 --> 00:32:48.700
But the advantage
of a factor graph

00:32:48.700 --> 00:32:50.580
is that your explicitly
able to include

00:32:50.580 --> 00:32:54.386
all these steps to describe this
regulation in an intuitive way.

00:32:54.386 --> 00:32:55.760
So you can go back
to your models

00:32:55.760 --> 00:32:57.730
and understand what
you've done, and change it

00:32:57.730 --> 00:32:59.820
in an obvious way.

00:32:59.820 --> 00:33:01.830
Now critically, we're
not trying to learn

00:33:01.830 --> 00:33:03.750
the structure of the
graph from the data.

00:33:03.750 --> 00:33:05.870
We're imposing the
structure of the graph.

00:33:05.870 --> 00:33:07.900
We still need to learn
a lot of variables,

00:33:07.900 --> 00:33:10.550
and that's done using
expectation maximization,

00:33:10.550 --> 00:33:13.620
as we saw in the
Bayesian networks.

00:33:13.620 --> 00:33:15.300
And then, again,
it's a factor graph,

00:33:15.300 --> 00:33:17.970
which primarily means that we
can factor the global function

00:33:17.970 --> 00:33:20.550
into all of these factor nodes.

00:33:20.550 --> 00:33:23.850
So the total probability
is normalized,

00:33:23.850 --> 00:33:27.500
but it's the product
of these factors which

00:33:27.500 --> 00:33:29.720
have to do with just the
variables that are connected

00:33:29.720 --> 00:33:33.830
to that factor
node in the graph.

00:33:33.830 --> 00:33:35.230
And this notation
that you'll see

00:33:35.230 --> 00:33:37.410
if you look through
this, this notation

00:33:37.410 --> 00:33:39.060
means the setting
of all the variables

00:33:39.060 --> 00:33:40.650
consistent with something.

00:33:40.650 --> 00:33:44.020
So let's see that-- here we go.

00:33:44.020 --> 00:33:47.220
So this here, this is the
setting of all the variables

00:33:47.220 --> 00:33:49.800
X, consistent with the data
that we have-- so the data

00:33:49.800 --> 00:33:53.870
being the arrays, the
RNA-Seq, if you had it.

00:33:53.870 --> 00:33:56.630
And so we want to compute
the marginal probability

00:33:56.630 --> 00:33:59.680
of some particular variable
being at a particular setting,

00:33:59.680 --> 00:34:02.320
given the fully
specified factor graph.

00:34:02.320 --> 00:34:06.490
And we just take the product
of all of these marginals.

00:34:06.490 --> 00:34:07.550
Is that clear?

00:34:07.550 --> 00:34:10.050
Consistent with all
the settings where

00:34:10.050 --> 00:34:15.639
that variable is
set to x equals a.

00:34:15.639 --> 00:34:16.139
Questions?

00:34:18.850 --> 00:34:19.350
OK.

00:34:19.350 --> 00:34:21.099
And we can compute the
likelihood function

00:34:21.099 --> 00:34:21.870
in the same way.

00:34:21.870 --> 00:34:24.400
So then what actually happens
when you try to do this?

00:34:24.400 --> 00:34:28.469
So they give an example here in
this more recent paper, where

00:34:28.469 --> 00:34:30.250
it's basically a toy example.

00:34:30.250 --> 00:34:32.469
But they're modeling all
of these different states

00:34:32.469 --> 00:34:33.320
in the cells.

00:34:33.320 --> 00:34:35.550
So G are the number
of genomic copies,

00:34:35.550 --> 00:34:38.060
T, the level of transcripts.

00:34:38.060 --> 00:34:41.061
Those are connected by a factor
to what you actually measure.

00:34:41.061 --> 00:34:42.810
So there is some true
change in the number

00:34:42.810 --> 00:34:44.050
of copies in the cell.

00:34:44.050 --> 00:34:46.370
And then there's what
appears in your array.

00:34:46.370 --> 00:34:49.650
There's some true number of
copies of RNA in the cell.

00:34:49.650 --> 00:34:52.550
And then there's what you
get out of your RNA-Seq.

00:34:52.550 --> 00:34:54.300
So that's what these
factors are present--

00:34:54.300 --> 00:34:55.799
and then these are
regulatory terms.

00:34:55.799 --> 00:34:59.460
So how much transcript you get
depends on these two variables,

00:34:59.460 --> 00:35:03.390
the epigenetic state
of the promoter

00:35:03.390 --> 00:35:05.880
and the regulatory proteins
that interact with it.

00:35:05.880 --> 00:35:07.730
How much transcript
gets turned into protein

00:35:07.730 --> 00:35:10.480
depends on regulatory proteins.

00:35:10.480 --> 00:35:12.910
And those are determined by
upstream signaling events.

00:35:12.910 --> 00:35:14.410
And how much protein
becomes active,

00:35:14.410 --> 00:35:16.860
again, is determined by the
upstream signaling events.

00:35:16.860 --> 00:35:21.757
And then those can have effects
on downstream pathways as well.

00:35:21.757 --> 00:35:24.090
So then in this toy example,
they're looking at MYC/MAX.

00:35:24.090 --> 00:35:28.390
They're trying to figure out
whether it's active or not.

00:35:28.390 --> 00:35:30.170
So we've got this pathway.

00:35:30.170 --> 00:35:32.470
PAK2 represses MYC/MAX.

00:35:32.470 --> 00:35:38.030
MYC/MAX activates these two
genes and represses this one.

00:35:38.030 --> 00:35:39.710
And so if these were
the data that we

00:35:39.710 --> 00:35:42.830
had coming from copy number
variation, DNA methylation,

00:35:42.830 --> 00:35:47.940
and RNA expression, then I'd
see that the following states

00:35:47.940 --> 00:35:53.240
of the downstream genes--
this one's active.

00:35:53.240 --> 00:35:55.220
This one's repressed.

00:35:55.220 --> 00:35:55.970
This one's active.

00:35:55.970 --> 00:35:57.130
This one's repressed.

00:35:57.130 --> 00:36:00.330
They infer that
MYC/MAX is active.

00:36:00.330 --> 00:36:03.510
Oh, but what about the fact
that this one should also

00:36:03.510 --> 00:36:05.030
be activated?

00:36:05.030 --> 00:36:06.720
That can be explained
away by the fact

00:36:06.720 --> 00:36:11.940
that there's a difference in the
epigenetic state between ENO1

00:36:11.940 --> 00:36:14.880
and the other two.

00:36:14.880 --> 00:36:18.360
And then the belief
propagation allows

00:36:18.360 --> 00:36:21.010
us to transfer that information
upward through the graph

00:36:21.010 --> 00:36:25.030
to figure out, OK, so now we've
decided that MYC/MAX is active.

00:36:25.030 --> 00:36:28.430
And that gives us information
about the state of the proteins

00:36:28.430 --> 00:36:33.030
upstream of it and the
activity then of PAK2,

00:36:33.030 --> 00:36:34.915
which is a repressor of MYC/MAX.

00:36:39.220 --> 00:36:41.650
Questions on the factor
graphs specifically

00:36:41.650 --> 00:36:43.559
or anything's that
come up until now?

00:36:48.740 --> 00:36:52.810
So this has all been
reasoning on known pathways.

00:36:52.810 --> 00:36:56.150
One of the big promises of
these systematic approaches

00:36:56.150 --> 00:36:58.390
is the hope that we can
discover new pathways.

00:36:58.390 --> 00:37:01.330
Can we discover things we
don't already know about?

00:37:01.330 --> 00:37:04.040
And for this, we're going to
look at interactome graphs,

00:37:04.040 --> 00:37:06.251
so graphs that are
built primarily

00:37:06.251 --> 00:37:08.250
from high throughput
protein-protein interaction

00:37:08.250 --> 00:37:10.083
data, but could also
be built, as we'll see,

00:37:10.083 --> 00:37:14.514
from other kinds of
large-scale connections.

00:37:14.514 --> 00:37:16.430
And we're going to look
at what the underlying

00:37:16.430 --> 00:37:17.971
structure of these
networks could be.

00:37:17.971 --> 00:37:19.686
And so they could
arise from a graph

00:37:19.686 --> 00:37:21.310
where you put an edge
between two nodes

00:37:21.310 --> 00:37:25.730
if their co-expressed, if they
have high mutual information.

00:37:25.730 --> 00:37:27.620
That's what we saw
in say, ARACHNE,

00:37:27.620 --> 00:37:31.100
which we talked
about a lecture ago.

00:37:31.100 --> 00:37:35.050
Or, if say, the two hybrids
and affinity capture mass spec

00:37:35.050 --> 00:37:37.210
indicated direct
physical interaction

00:37:37.210 --> 00:37:39.020
or say a high throughput
genetic screen

00:37:39.020 --> 00:37:40.394
indicated a genetic interaction.

00:37:40.394 --> 00:37:42.310
These are going to be
very, very large graphs.

00:37:42.310 --> 00:37:44.860
And we're going to look at some
of the algorithmic problems

00:37:44.860 --> 00:37:46.500
that we have dealing
with huge graphs

00:37:46.500 --> 00:37:49.690
and how to compress the
information down so we get

00:37:49.690 --> 00:37:52.795
some piece of the network
that's quite interpretable.

00:37:52.795 --> 00:37:54.420
And we'll look at
various kinds of ways

00:37:54.420 --> 00:37:59.490
of analyzing these graphs
that are listed here.

00:37:59.490 --> 00:38:03.654
So one of the advantages of
dealing with data in the graph

00:38:03.654 --> 00:38:06.070
formulation is that we can
leverage the fact that computer

00:38:06.070 --> 00:38:08.760
science has dealt with large
graphs for quite a while

00:38:08.760 --> 00:38:11.580
now, often in the context
of telecommunications.

00:38:11.580 --> 00:38:14.460
Now big data, Facebook,
Google-- they're always

00:38:14.460 --> 00:38:16.260
dealing with things in
a graph formulation.

00:38:16.260 --> 00:38:20.430
So there are a lot of algorithms
that we can take advantage of.

00:38:20.430 --> 00:38:23.270
We're going to look at
how to use quick distance

00:38:23.270 --> 00:38:24.459
calculations on graphs.

00:38:24.459 --> 00:38:26.500
And we'll look at that
specifically in an example

00:38:26.500 --> 00:38:29.570
of how to find the kinase
target relationships.

00:38:29.570 --> 00:38:31.630
Then we'll look at how
to cluster large graphs

00:38:31.630 --> 00:38:33.640
to find subgraphs
that either represents

00:38:33.640 --> 00:38:35.150
an interesting
topological feature

00:38:35.150 --> 00:38:37.160
of the inherent
structure of the graph

00:38:37.160 --> 00:38:40.830
or perhaps to represent
active pieces of the network.

00:38:40.830 --> 00:38:43.006
And then we'll look at
other kinds of optimization

00:38:43.006 --> 00:38:45.380
techniques to help us find
the part of the network that's

00:38:45.380 --> 00:38:50.390
most relevant to our particular
experimental setting.

00:38:50.390 --> 00:38:54.080
So let's start with
ostensibly a simple problem.

00:38:54.080 --> 00:38:57.140
I know a lot about-- I have a
lot of protein phosphorylation

00:38:57.140 --> 00:38:57.640
data.

00:38:57.640 --> 00:38:59.600
I'd like to figure
out what kinase

00:38:59.600 --> 00:39:03.060
was that phosphorylated
a particular protein.

00:39:03.060 --> 00:39:05.270
So let's say I have
this protein that's

00:39:05.270 --> 00:39:08.560
involved in cancer
signaling, Rad50.

00:39:08.560 --> 00:39:10.750
And I know it's phosphorylated
these two sites.

00:39:10.750 --> 00:39:12.660
And I have the sequences
of those sites.

00:39:12.660 --> 00:39:14.970
So what kinds of tools do
we have at our disposal

00:39:14.970 --> 00:39:16.900
if I have a set of
sequences that I believe

00:39:16.900 --> 00:39:18.570
are phosphorylated,
that would help

00:39:18.570 --> 00:39:21.481
me try to figure out what
kinase did the phosphorylation?

00:39:21.481 --> 00:39:21.980
Any ideas?

00:39:26.910 --> 00:39:29.740
So if I know the specificity of
the kinases, what could I do?

00:39:32.320 --> 00:39:34.260
I could look for
a sequence match

00:39:34.260 --> 00:39:36.440
between the specificity
of the kinase

00:39:36.440 --> 00:39:38.417
and the sequence of
the protein, right?

00:39:38.417 --> 00:39:40.250
In the same way that
we can look for a match

00:39:40.250 --> 00:39:42.790
between the specificity
of a transcription factor

00:39:42.790 --> 00:39:46.330
and the region of the
genome to which it binds.

00:39:46.330 --> 00:39:49.470
So if I have a library
of specificity motifs

00:39:49.470 --> 00:39:51.580
for different kinases,
where every position here

00:39:51.580 --> 00:39:53.859
represents a piece of
the recognition element,

00:39:53.859 --> 00:39:56.150
and the height of the letters
represent the information

00:39:56.150 --> 00:39:57.900
content, I can scan those.

00:39:57.900 --> 00:40:00.310
And I can see what
family of kinases

00:40:00.310 --> 00:40:03.440
are most likely to be
responsible for phosphorylating

00:40:03.440 --> 00:40:04.482
these sites.

00:40:04.482 --> 00:40:06.190
But again, those are
families of kinases.

00:40:06.190 --> 00:40:07.564
There are many
individual members

00:40:07.564 --> 00:40:08.860
of each of those families.

00:40:08.860 --> 00:40:10.380
So how to I find
the specific member

00:40:10.380 --> 00:40:12.330
of that family that's
most likely to carry out

00:40:12.330 --> 00:40:13.620
the regulation?

00:40:13.620 --> 00:40:15.120
So here, what happens
in this paper.

00:40:15.120 --> 00:40:17.244
It's called [? "Network." ?]
And as they say, well,

00:40:17.244 --> 00:40:18.570
let's use the graph properties.

00:40:18.570 --> 00:40:23.290
Let's try to figure out which
proteins are physically linked

00:40:23.290 --> 00:40:26.390
relatively closely in the
network to the target.

00:40:26.390 --> 00:40:29.080
So in this case, they've
got Rad50 over here.

00:40:29.080 --> 00:40:33.620
And they're trying to figure out
which kinase is regulating it.

00:40:33.620 --> 00:40:35.980
So here are two kinases that
have similar specificity.

00:40:35.980 --> 00:40:37.669
But this one's
directly connected

00:40:37.669 --> 00:40:39.210
in the interaction
that works so it's

00:40:39.210 --> 00:40:41.870
more likely to be responsible.

00:40:41.870 --> 00:40:44.430
And here's the member
of the kinase that

00:40:44.430 --> 00:40:47.010
seems to be consistent with the
sequence being phosphorylated

00:40:47.010 --> 00:40:48.130
over here.

00:40:48.130 --> 00:40:50.910
It's not directly connected,
but it's relatively close.

00:40:50.910 --> 00:40:53.530
And so that's also a
highly probable member,

00:40:53.530 --> 00:40:56.110
compared to one that's
more distantly related.

00:40:56.110 --> 00:40:58.870
So in general, if I've
got a set of kinases

00:40:58.870 --> 00:41:02.410
that are all of equally good
sequence matches to the target

00:41:02.410 --> 00:41:05.560
sequence, represented by these
dash lines, but one of them

00:41:05.560 --> 00:41:08.780
is physically linked as well,
perhaps directly and perhaps

00:41:08.780 --> 00:41:10.610
indirectly, I have
higher confidence

00:41:10.610 --> 00:41:13.360
in this kinase because
of its physical links

00:41:13.360 --> 00:41:15.857
than I do in these.

00:41:15.857 --> 00:41:18.190
So that's fine if you want
to look at things one by one.

00:41:18.190 --> 00:41:19.690
But if you want to look
at this at a global scale,

00:41:19.690 --> 00:41:21.110
we need very
efficient algorithms

00:41:21.110 --> 00:41:23.980
for figuring out what the
distance is in this interaction

00:41:23.980 --> 00:41:29.190
network between any
kinase and any target.

00:41:29.190 --> 00:41:31.590
So how do you go about
officially computing distances?

00:41:31.590 --> 00:41:34.030
Well that's where converting
things into a graph structure

00:41:34.030 --> 00:41:35.180
is helpful.

00:41:35.180 --> 00:41:37.070
So when we talk
about graphs here,

00:41:37.070 --> 00:41:40.654
we mean sets of vertices and
the edges that connect them.

00:41:40.654 --> 00:41:42.820
The vertices, in our case,
are going to be proteins.

00:41:42.820 --> 00:41:44.290
The edges are going
to perhaps represent

00:41:44.290 --> 00:41:46.789
physical interactions or some
of these other kinds of graphs

00:41:46.789 --> 00:41:49.520
we talked about.

00:41:49.520 --> 00:41:52.049
These graphs can be directed,
or they can the undirected.

00:41:52.049 --> 00:41:53.090
Undirected would be what?

00:41:53.090 --> 00:41:54.950
For example, say two hybrid.

00:41:54.950 --> 00:41:57.150
I don't know which one's
doing what to which.

00:41:57.150 --> 00:41:59.280
I just know that two
proteins can come together.

00:41:59.280 --> 00:42:01.260
Whereas a directed edge
might be this kinase

00:42:01.260 --> 00:42:02.460
phosphorylates this target.

00:42:02.460 --> 00:42:05.130
And so it's a directed edge.

00:42:05.130 --> 00:42:07.091
I can have weights
associated with these edges.

00:42:07.091 --> 00:42:08.590
We'll see in a
second how we can use

00:42:08.590 --> 00:42:11.240
that to encode our confidence
that the edge represents

00:42:11.240 --> 00:42:14.680
a true physical interaction.

00:42:14.680 --> 00:42:17.580
We can also talk about the
degree, the number of edges

00:42:17.580 --> 00:42:20.770
that come into a
node or leave a node.

00:42:20.770 --> 00:42:22.740
And for our point,
it's rather important

00:42:22.740 --> 00:42:25.170
to talk about the path,
the set of vertices

00:42:25.170 --> 00:42:27.840
that can get me from one
node to another node,

00:42:27.840 --> 00:42:31.476
without ever retracing my steps.

00:42:31.476 --> 00:42:34.100
And we're going to
talk about path length,

00:42:34.100 --> 00:42:35.600
so if my graph is
unweighted, that's

00:42:35.600 --> 00:42:39.000
just the number of
edges along the path.

00:42:39.000 --> 00:42:40.990
But if my graph
has edge weights,

00:42:40.990 --> 00:42:43.675
it's going to be the sum of the
edge weights along that path.

00:42:43.675 --> 00:42:44.290
Is that clear?

00:42:48.040 --> 00:42:50.640
And then we're going to
use an adjacency matrix

00:42:50.640 --> 00:42:51.640
to represent the graphs.

00:42:51.640 --> 00:42:53.400
So I have two completely
equivalent formulations

00:42:53.400 --> 00:42:53.941
of the graph.

00:42:53.941 --> 00:42:56.190
One is the picture on
the left-hand side,

00:42:56.190 --> 00:42:59.310
and the other one is the matrix
on the right-hand side, where

00:42:59.310 --> 00:43:02.820
a 1 between any row
and column represents

00:43:02.820 --> 00:43:03.820
the presence of an edge.

00:43:03.820 --> 00:43:10.500
So the only edge connecting
node 1 goes to node 2.

00:43:10.500 --> 00:43:13.610
Whereas, node 2 is connected
both to node 1 and to node 3.

00:43:13.610 --> 00:43:14.720
Hopefully, that agrees.

00:43:14.720 --> 00:43:15.365
OK.

00:43:15.365 --> 00:43:16.170
Is that clear?

00:43:20.537 --> 00:43:22.370
And if I have a weighted
graph, then instead

00:43:22.370 --> 00:43:23.910
of putting zeros or
ones in the matrix,

00:43:23.910 --> 00:43:25.868
I'll put the actual edge
weights in the matrix.

00:43:28.620 --> 00:43:31.950
So there are algorithms that
exist for officially finding

00:43:31.950 --> 00:43:35.740
shortest paths in large graphs.

00:43:35.740 --> 00:43:37.910
So we can very
rapidly, for example,

00:43:37.910 --> 00:43:40.130
compute the shortest path
between any two nodes,

00:43:40.130 --> 00:43:43.720
based solely on that
adjacency matrix.

00:43:43.720 --> 00:43:46.170
Now why are we going to
look at weighted graphs?

00:43:46.170 --> 00:43:48.650
Because that gives us the
way to encode our confidence

00:43:48.650 --> 00:43:50.040
in the underlying data.

00:43:50.040 --> 00:43:54.350
So because the total
distance in network

00:43:54.350 --> 00:43:57.230
is the sum of the edge weights,
if I set my edge weights

00:43:57.230 --> 00:44:01.150
to be negative log
of a probability,

00:44:01.150 --> 00:44:03.302
then if I sum all
the edge weights,

00:44:03.302 --> 00:44:05.385
I'm taking the product of
all those probabilities.

00:44:08.560 --> 00:44:10.800
And so the shortest
path is going

00:44:10.800 --> 00:44:14.390
to be the most probable
path as well, because it's

00:44:14.390 --> 00:44:18.900
going to be the minimum of
the sum of the negative log.

00:44:18.900 --> 00:44:21.475
So it's going to be the maximum
of the joint probability.

00:44:21.475 --> 00:44:24.270
Is that clear?

00:44:24.270 --> 00:44:24.770
OK.

00:44:24.770 --> 00:44:25.470
Very good.

00:44:25.470 --> 00:44:30.200
So by encoding our network as a
weighted graph, where the edge

00:44:30.200 --> 00:44:31.932
weights are minus log
of the probability,

00:44:31.932 --> 00:44:34.140
then when I use these standard
algorithms for finding

00:44:34.140 --> 00:44:35.806
the shortest path
between any two nodes,

00:44:35.806 --> 00:44:38.870
I'm also getting the most
probable path between these two

00:44:38.870 --> 00:44:41.160
proteins.

00:44:41.160 --> 00:44:44.210
So where these edge
weights come from?

00:44:44.210 --> 00:44:47.090
So if my network consists
say of affinity capture mass

00:44:47.090 --> 00:44:48.530
spec and two hybrid
interactions,

00:44:48.530 --> 00:44:51.700
how would I compute the edge
of weights for that network?

00:45:01.524 --> 00:45:03.190
We actually explicitly
talked about this

00:45:03.190 --> 00:45:04.468
just a lecture or two ago.

00:45:08.950 --> 00:45:10.960
So I have all this
affinity capture mass spec,

00:45:10.960 --> 00:45:12.150
two hybrid data.

00:45:12.150 --> 00:45:13.620
And I want to
assign a probability

00:45:13.620 --> 00:45:18.350
to every edge that tells me how
confident I am that it's real.

00:45:18.350 --> 00:45:21.050
So we already saw that in
the context of this paper

00:45:21.050 --> 00:45:23.970
where we use Bayesian networks
and gold standards to compute

00:45:23.970 --> 00:45:26.345
the probability for every
single edge in the interactome.

00:45:28.800 --> 00:45:29.300
OK.

00:45:29.300 --> 00:45:32.520
So that works pretty well if you
can define the gold standards.

00:45:32.520 --> 00:45:35.630
It turns out that that has
not been the most popular way

00:45:35.630 --> 00:45:38.270
of dealing with mammalian data.

00:45:38.270 --> 00:45:40.160
It works pretty well
for yeast, but it's not

00:45:40.160 --> 00:45:42.400
what's used primarily
in mammalian data.

00:45:42.400 --> 00:45:45.830
So in mammalian data, the
databases are much larger.

00:45:45.830 --> 00:45:48.280
The number of gold
standards are fewer.

00:45:48.280 --> 00:45:51.270
People rely on more
ad hoc methods.

00:45:51.270 --> 00:45:54.370
One of the big advances,
technically, for the field

00:45:54.370 --> 00:45:57.060
was the development of a common
way for all these databases

00:45:57.060 --> 00:45:59.580
of protein-protein interactions
to report their data,

00:45:59.580 --> 00:46:01.150
to be able to interchange them.

00:46:01.150 --> 00:46:05.580
There's something called
PSICQUIC and PSISCORE, that

00:46:05.580 --> 00:46:09.485
allow a client to
pull information

00:46:09.485 --> 00:46:11.610
from all the different
databases of protein-protein

00:46:11.610 --> 00:46:12.720
interactions.

00:46:12.720 --> 00:46:15.810
And because you can get all
the data in a common format

00:46:15.810 --> 00:46:18.500
where it's traceable back to
the underlying experiment,

00:46:18.500 --> 00:46:21.510
then you can start
computing confidence scores

00:46:21.510 --> 00:46:23.110
based on these
properties, what we

00:46:23.110 --> 00:46:26.450
know about where the data came
from in a high throughput way.

00:46:26.450 --> 00:46:28.510
Different people have
different approaches

00:46:28.510 --> 00:46:30.620
to computing those scores.

00:46:30.620 --> 00:46:32.380
So there's a common
format for that

00:46:32.380 --> 00:46:34.790
as well, which is
this PSISCORE where

00:46:34.790 --> 00:46:38.150
you can build your interaction
database from whichever

00:46:38.150 --> 00:46:40.390
one of these underlying
databases you want,

00:46:40.390 --> 00:46:41.740
filter it however you want.

00:46:41.740 --> 00:46:45.780
And then send your database to
one of these scoring servers.

00:46:45.780 --> 00:46:47.690
And they'll send
you back the scores

00:46:47.690 --> 00:46:50.130
according to their algorithm.

00:46:50.130 --> 00:46:52.940
One that I kind of like this
is this Miscore algorithm.

00:46:52.940 --> 00:46:54.509
It digs down into
the underlying data

00:46:54.509 --> 00:46:56.050
of what kind of
experiments were done

00:46:56.050 --> 00:46:58.139
and how many
experiments were done.

00:46:58.139 --> 00:47:00.180
Again, they make all sorts
of arbitrary decisions

00:47:00.180 --> 00:47:01.013
in how they do that.

00:47:01.013 --> 00:47:03.400
But the arbitrary
decisions seem reasonable

00:47:03.400 --> 00:47:05.790
in the absence of
any other data.

00:47:05.790 --> 00:47:10.260
So their scores are based on
these three kinds of terms--

00:47:10.260 --> 00:47:12.180
how many publications
there are associated

00:47:12.180 --> 00:47:17.130
with any interaction, what
experimental method was used,

00:47:17.130 --> 00:47:19.434
and then also, if there's an
annotation in the database

00:47:19.434 --> 00:47:21.725
saying that we know that this
is a genetic interaction,

00:47:21.725 --> 00:47:23.559
or we know that it's a
physical interaction.

00:47:23.559 --> 00:47:25.599
And then they put weights
on all of these things.

00:47:25.599 --> 00:47:27.280
So people can argue
about what the best

00:47:27.280 --> 00:47:28.924
way of approaching this is.

00:47:28.924 --> 00:47:30.590
The fundamental point
is that we can now

00:47:30.590 --> 00:47:33.030
have a very, very
large database of known

00:47:33.030 --> 00:47:34.630
interactions as weighted.

00:47:34.630 --> 00:47:37.360
So by last count,
there are about 250,000

00:47:37.360 --> 00:47:40.352
protein-protein interactions
for humans in these databases.

00:47:40.352 --> 00:47:41.810
So you have that
giant interactome.

00:47:41.810 --> 00:47:44.380
It's got all these scores
associated with it.

00:47:44.380 --> 00:47:46.390
And now we can dive
into that and say,

00:47:46.390 --> 00:47:51.880
these data are somewhat largely
unbiased by our prior notions

00:47:51.880 --> 00:47:53.750
about what's important.

00:47:53.750 --> 00:47:55.560
They're built up from
high throughput data.

00:47:55.560 --> 00:47:57.910
So unlike the carefully
curated pathways

00:47:57.910 --> 00:48:00.077
that are what everybody's
been studying for decades,

00:48:00.077 --> 00:48:01.993
there might be information
here about pathways

00:48:01.993 --> 00:48:02.830
no one knows about.

00:48:02.830 --> 00:48:05.250
Can we find those pathways
in different contexts?

00:48:05.250 --> 00:48:07.050
What can we learn from that?

00:48:07.050 --> 00:48:09.127
So one early thing
people can do is just

00:48:09.127 --> 00:48:10.710
try to find pieces
of the network that

00:48:10.710 --> 00:48:12.168
seem to be modular,
where there are

00:48:12.168 --> 00:48:15.650
more interactions among the
components of that module

00:48:15.650 --> 00:48:17.990
than they are to other
pieces of the network.

00:48:17.990 --> 00:48:20.700
And you can find those
modules in two different ways.

00:48:20.700 --> 00:48:24.240
One is just based on
the underlying network.

00:48:24.240 --> 00:48:27.540
And one is based on the
network, plus some external data

00:48:27.540 --> 00:48:28.290
you have.

00:48:28.290 --> 00:48:29.920
So one would be
to say, are there

00:48:29.920 --> 00:48:32.790
proteins that fundamentally
interact with each other

00:48:32.790 --> 00:48:34.531
under all possible settings?

00:48:34.531 --> 00:48:36.780
And then we would say, in
my particular patient sample

00:48:36.780 --> 00:48:40.290
or my disease or
my microorganism,

00:48:40.290 --> 00:48:42.370
which proteins seem
to be functioning

00:48:42.370 --> 00:48:44.810
in this particular condition?

00:48:44.810 --> 00:48:47.230
So one is the topological model.

00:48:47.230 --> 00:48:48.740
That's just the network itself.

00:48:48.740 --> 00:48:51.534
And one is the functional model,
where I layer onto information

00:48:51.534 --> 00:48:53.950
that the dark nodes are active
in my particular condition.

00:48:56.590 --> 00:49:00.000
So an early use of
this kind of approach

00:49:00.000 --> 00:49:02.995
was to try to annotate
nodes-- a large fraction

00:49:02.995 --> 00:49:05.130
of even well studied
genomes that we don't know

00:49:05.130 --> 00:49:07.500
the function of
any of those genes.

00:49:07.500 --> 00:49:09.490
So what if I use the
structure of the network

00:49:09.490 --> 00:49:13.060
to infer that if some protein
is close to another protein

00:49:13.060 --> 00:49:14.750
in this interaction
network, it is

00:49:14.750 --> 00:49:16.670
likely to have similar function?

00:49:16.670 --> 00:49:19.280
And statistically,
that's definitely true.

00:49:19.280 --> 00:49:24.410
So this graph shows, for things
for where we know the function,

00:49:24.410 --> 00:49:26.590
the semantic similarity
in the y-axis,

00:49:26.590 --> 00:49:28.427
the distance in the
network in the x-axis,

00:49:28.427 --> 00:49:30.510
things that are close to
each other in the network

00:49:30.510 --> 00:49:32.930
of interactions, are
also more likely to be

00:49:32.930 --> 00:49:35.645
similar in terms of function.

00:49:35.645 --> 00:49:37.020
So how do we go
about doing that?

00:49:37.020 --> 00:49:38.520
So let's say we
have got this graph.

00:49:38.520 --> 00:49:40.850
We've got some unknown
node labeled u.

00:49:40.850 --> 00:49:43.810
And we've got two
known nodes in black.

00:49:43.810 --> 00:49:46.250
And we want to systematically
deduce for every example

00:49:46.250 --> 00:49:50.170
like this, every u, what
its annotation should be.

00:49:50.170 --> 00:49:52.710
So I could just look
at its neighbors,

00:49:52.710 --> 00:49:54.949
and depending on how I
set the window around it,

00:49:54.949 --> 00:49:56.490
do I look at the
immediate neighbors?

00:49:56.490 --> 00:49:57.410
Do I go two out?

00:49:57.410 --> 00:49:58.470
Do I go three out?

00:49:58.470 --> 00:50:00.430
I could get different answers.

00:50:00.430 --> 00:50:03.070
So if I set K equal to 1,
I've got the unknown node,

00:50:03.070 --> 00:50:04.710
but all the neighbors
are also unknown.

00:50:04.710 --> 00:50:07.570
If I go two steps out,
then I pick up two knowns.

00:50:10.380 --> 00:50:13.940
Now there's a fundamental
assumption going on here

00:50:13.940 --> 00:50:17.300
that the node has the same
function as its neighbors,

00:50:17.300 --> 00:50:20.430
which is fine when the
neighbors are homogeneous.

00:50:20.430 --> 00:50:23.970
But what do you do when the
neighbors are heterogeneous?

00:50:23.970 --> 00:50:27.440
So in this case, I've
got two unknowns u and v.

00:50:27.440 --> 00:50:30.260
And if I just were to take
the K nearest neighbors,

00:50:30.260 --> 00:50:32.390
they would have the same
neighborhood, right?

00:50:32.390 --> 00:50:34.760
But I might have a prior
expectation that u is more like

00:50:34.760 --> 00:50:39.460
the black nodes, and v is
more like the grey nodes.

00:50:39.460 --> 00:50:42.270
So how do you choose
the best annotation?

00:50:42.270 --> 00:50:45.290
The K nearest neighbors is
OK, but it's not the optimal.

00:50:45.290 --> 00:50:48.530
So here's one approach,
which says the following.

00:50:48.530 --> 00:50:51.750
I'm going to go through
for every function,

00:50:51.750 --> 00:50:54.094
every annotation in my
database, separately.

00:50:54.094 --> 00:50:56.260
And for each annotation,
I'll set all the nodes that

00:50:56.260 --> 00:50:59.180
have that annotation to
plus 1 and every node

00:50:59.180 --> 00:51:01.680
that doesn't have that
annotation, either it's unknown

00:51:01.680 --> 00:51:04.890
or it's got some other
annotation, to minus 1.

00:51:04.890 --> 00:51:06.570
And then for every
unknown, I'm going

00:51:06.570 --> 00:51:09.960
to try to find the
setting which is going

00:51:09.960 --> 00:51:12.550
to maximize the sum of products.

00:51:12.550 --> 00:51:15.570
So we're going to take the
sum of the products of u

00:51:15.570 --> 00:51:18.200
and all of its neighbors.

00:51:18.200 --> 00:51:21.742
So in this setting,
if I set u to plus 1,

00:51:21.742 --> 00:51:23.908
then I do better than if I
set it to minus 1, right?

00:51:27.260 --> 00:51:29.870
Because I'll get plus
1 plus 1 minus 1.

00:51:29.870 --> 00:51:32.760
So that will be better
than setting it to minus 1.

00:51:32.760 --> 00:51:33.260
Yes.

00:51:33.260 --> 00:51:35.627
AUDIENCE: Are we ignoring
all the end weights?

00:51:35.627 --> 00:51:37.960
PROFESSOR: In this case, we're
ignoring the end weights.

00:51:37.960 --> 00:51:40.090
We'll come back to using
the end weights later.

00:51:40.090 --> 00:51:42.052
But this was done with
an unweighted graph.

00:51:42.052 --> 00:51:44.052
AUDIENCE: [INAUDIBLE]
[? nearest neighborhood ?]

00:51:44.052 --> 00:51:45.489
they're using it then?

00:51:45.489 --> 00:51:47.780
PROFESSOR: So here they're
using the nearest neighbors.

00:51:47.780 --> 00:51:50.004
That's right, with
no cutoff, right?

00:51:50.004 --> 00:51:50.795
So any interaction.

00:51:56.800 --> 00:51:59.982
So then we could iterate
this into convergence.

00:51:59.982 --> 00:52:01.190
That's one problem with this.

00:52:01.190 --> 00:52:02.840
But maybe a more
fundamental problem

00:52:02.840 --> 00:52:05.880
is that you're never going to
get the best overall solution

00:52:05.880 --> 00:52:08.730
by this local
optimization procedure.

00:52:08.730 --> 00:52:10.950
So consider a setting like this.

00:52:10.950 --> 00:52:13.700
Remember, I'm trying
to maximize the sum

00:52:13.700 --> 00:52:16.950
of the product of the
settings for neighbors.

00:52:16.950 --> 00:52:21.330
So how could I ever-- it seems
plausible that all A, B, and C

00:52:21.330 --> 00:52:24.280
here, should have the
red annotation, right?

00:52:24.280 --> 00:52:27.000
But if I set C to red,
that doesn't help me.

00:52:27.000 --> 00:52:29.250
If I set A to red,
that doesn't help me.

00:52:29.250 --> 00:52:32.190
If I set B to red, it
makes things worse.

00:52:32.190 --> 00:52:34.820
So no local change is going
to get me where I want to go.

00:52:37.374 --> 00:52:38.540
So let's think for a second.

00:52:38.540 --> 00:52:40.340
What algorithms
have we already seen

00:52:40.340 --> 00:52:42.540
that could help us get
to the right answer?

00:52:42.540 --> 00:52:45.320
We can't get here by
local optimization.

00:52:45.320 --> 00:52:48.170
We need to find the global
minimum, not the local minimum.

00:52:48.170 --> 00:52:49.670
So what algorithms
have we seen that

00:52:49.670 --> 00:52:51.140
help us find that
global minimum?

00:52:54.612 --> 00:52:58.180
Yeah, sorry, so a video
simulated annealing.

00:52:58.180 --> 00:53:01.040
So the simulated annealing
version in this setting

00:53:01.040 --> 00:53:02.700
is as follows.

00:53:02.700 --> 00:53:04.280
I initialize the graph.

00:53:04.280 --> 00:53:06.850
I pick a neighboring node,
v, that I'm going to add.

00:53:06.850 --> 00:53:09.830
Say we'll turn one of these red.

00:53:09.830 --> 00:53:16.370
I check the value of that sum of
the products for this new one.

00:53:16.370 --> 00:53:19.864
And if it's improving
things, I keep it.

00:53:19.864 --> 00:53:21.905
But the critical thing
is, if it doesn't improve,

00:53:21.905 --> 00:53:23.530
if it makes things
worse, I still

00:53:23.530 --> 00:53:24.780
keep it with some probability.

00:53:24.780 --> 00:53:27.480
It's based on how bad
things have gotten.

00:53:27.480 --> 00:53:29.295
And by doing this,
we can climb the hill

00:53:29.295 --> 00:53:33.630
and get over to
some global optimum.

00:53:33.630 --> 00:53:35.660
So we saw simulating before.

00:53:35.660 --> 00:53:36.490
In what context?

00:53:36.490 --> 00:53:38.386
When in the side chain
placement problem.

00:53:38.386 --> 00:53:39.510
Here we're seeing it again.

00:53:39.510 --> 00:53:40.370
It's quite broad.

00:53:40.370 --> 00:53:42.299
Any time you've got a
local optimization that

00:53:42.299 --> 00:53:43.840
doesn't get you
where you need to go,

00:53:43.840 --> 00:53:45.114
you need global optimization.

00:53:45.114 --> 00:53:46.530
You can think
simulated annealing.

00:53:46.530 --> 00:53:49.761
It's quite often the
plausible way to go.

00:53:49.761 --> 00:53:50.260
All right.

00:53:50.260 --> 00:53:53.092
So this is one approach
for annotation.

00:53:53.092 --> 00:53:55.050
We also wanted to see
whether we could discover

00:53:55.050 --> 00:53:56.890
inherent structure
in these graphs.

00:53:56.890 --> 00:53:58.600
So often, we'll be
interested in trying

00:53:58.600 --> 00:54:00.600
to find clusters in a graph.

00:54:00.600 --> 00:54:03.680
Some graphs have obvious
structures in them.

00:54:03.680 --> 00:54:05.940
Other graphs, it's a
little less obvious.

00:54:05.940 --> 00:54:07.780
What algorithms exist
for trying to do this?

00:54:07.780 --> 00:54:10.521
We're going to look at two
relatively straightforward

00:54:10.521 --> 00:54:11.020
ways.

00:54:11.020 --> 00:54:13.010
One is called edge
betweenness clustering

00:54:13.010 --> 00:54:16.730
and the other one
is a Markov process.

00:54:16.730 --> 00:54:19.160
Edge betweenness, I think,
is the most intuitive.

00:54:19.160 --> 00:54:25.860
So I look at each
edge, and I ask

00:54:25.860 --> 00:54:28.370
for all pairs of
nodes in the graph,

00:54:28.370 --> 00:54:30.360
does the shortest path
between those nodes

00:54:30.360 --> 00:54:31.395
pass through this edge?

00:54:35.270 --> 00:54:38.790
So if I look at this edge,
very few shortest paths

00:54:38.790 --> 00:54:40.240
go through this edge, right?

00:54:40.240 --> 00:54:42.640
Just the shortest path
for those two nodes.

00:54:42.640 --> 00:54:47.759
But if I look at this edge,
all of the shortest paths

00:54:47.759 --> 00:54:50.050
between any node on this side
and any node on this side

00:54:50.050 --> 00:54:51.325
have to pass through there.

00:54:51.325 --> 00:54:55.090
So that has a high betweenness.

00:54:55.090 --> 00:54:58.750
So if I want a cluster, I
can go through my graph.

00:54:58.750 --> 00:55:01.400
I can compute betweenness.

00:55:01.400 --> 00:55:03.470
I take the edge that has
the highest betweenness,

00:55:03.470 --> 00:55:05.330
and I remove it from my graph.

00:55:05.330 --> 00:55:07.720
And then I repeat.

00:55:07.720 --> 00:55:09.960
And I'll be slowly
breaking my graph down

00:55:09.960 --> 00:55:14.050
into chunks that are relatively
more connected internally

00:55:14.050 --> 00:55:15.890
than they are to
things in other pieces.

00:55:19.430 --> 00:55:20.360
Any questions?

00:55:20.360 --> 00:55:21.860
So that's an entire
edge betweenness

00:55:21.860 --> 00:55:22.860
clustering algorithm.

00:55:22.860 --> 00:55:23.840
Pretty straightforward.

00:55:27.480 --> 00:55:32.590
Now an alternative is a
Markov clustering method.

00:55:32.590 --> 00:55:34.430
And the Markov
clustering method is

00:55:34.430 --> 00:55:37.780
based on the idea of
random walks in the graph.

00:55:37.780 --> 00:55:41.070
So again, let's try to
develop some intuition here.

00:55:41.070 --> 00:55:42.610
If I start at some
node over here,

00:55:42.610 --> 00:55:46.220
and I randomly wander
across this graph,

00:55:46.220 --> 00:55:48.991
I'm more likely to stay
on the left-hand side

00:55:48.991 --> 00:55:51.490
than I am to move all the way
across to the right-hand side,

00:55:51.490 --> 00:55:54.060
correct?

00:55:54.060 --> 00:55:56.090
So can I formalize that
and actually come up

00:55:56.090 --> 00:55:58.500
with a measure of how
often any node will visit

00:55:58.500 --> 00:56:01.410
any other and then use
that to cluster the graph?

00:56:05.990 --> 00:56:07.720
So remember our
adjacency matrix,

00:56:07.720 --> 00:56:12.020
which just represented which
nodes were connected to which.

00:56:12.020 --> 00:56:16.280
And what happens if I multiply
the adjacency matrix by itself?

00:56:16.280 --> 00:56:19.290
So I raise it to some power.

00:56:19.290 --> 00:56:23.750
Well, if I multiply the
adjacency matrix by itself

00:56:23.750 --> 00:56:27.770
just once, the squared adjacency
matrix of the property,

00:56:27.770 --> 00:56:30.760
that it tells me how
many paths of length 2

00:56:30.760 --> 00:56:33.150
exists between any two nodes.

00:56:33.150 --> 00:56:36.160
So the adjacency matrix told
me how many paths of length 1

00:56:36.160 --> 00:56:36.851
exist.

00:56:36.851 --> 00:56:37.350
Right?

00:56:37.350 --> 00:56:38.704
You're directly connected.

00:56:38.704 --> 00:56:40.120
If I squared the
adjacency matrix,

00:56:40.120 --> 00:56:43.510
it tells me how many
paths of length 2 exist.

00:56:43.510 --> 00:56:46.790
N-th power tells me how many
paths of length N exist.

00:56:46.790 --> 00:56:48.150
So let's see if that works.

00:56:48.150 --> 00:56:49.710
This claims that
there are exactly

00:56:49.710 --> 00:56:53.334
two paths that connect
node 2 to node 2.

00:56:53.334 --> 00:56:54.375
What are those two paths?

00:56:59.060 --> 00:57:00.180
Connect node 2 to node 2.

00:57:00.180 --> 00:57:01.930
I go here, and I go back.

00:57:01.930 --> 00:57:06.030
That's the path of length 2, and
this is the path of length 2.

00:57:06.030 --> 00:57:08.200
And there are zero
paths of length 2

00:57:08.200 --> 00:57:13.210
that connect node 2 to
node three, because 1, 2.

00:57:13.210 --> 00:57:15.110
I'm not back at 3.

00:57:15.110 --> 00:57:18.390
So that's from general
A to the N equals m,

00:57:18.390 --> 00:57:23.220
if there exists exactly m paths
of length N between those two

00:57:23.220 --> 00:57:24.020
nodes.

00:57:24.020 --> 00:57:25.160
So how does this help me?

00:57:25.160 --> 00:57:28.830
Well, when you take that idea of
the N-th power of the adjacency

00:57:28.830 --> 00:57:32.360
matrix and convert it to a
transition probability matrix,

00:57:32.360 --> 00:57:34.510
simply by normalizing.

00:57:34.510 --> 00:57:36.969
So if I were to do a
random walk in this graph,

00:57:36.969 --> 00:57:39.010
what's the probability
that I'll move from node i

00:57:39.010 --> 00:57:41.420
to node j in a certain
number of steps?

00:57:41.420 --> 00:57:43.330
That's what I want to compute.

00:57:43.330 --> 00:57:45.779
So I need to have a
stochastic matrix,

00:57:45.779 --> 00:57:47.195
where the sum of
the probabilities

00:57:47.195 --> 00:57:50.426
for any transition is 1.

00:57:50.426 --> 00:57:51.550
I have to end up somewhere.

00:57:51.550 --> 00:57:53.370
I either end up back
in myself, or I end up

00:57:53.370 --> 00:57:54.203
at some other nodes.

00:57:54.203 --> 00:57:56.810
I'm just going to take
that adjacency matrix

00:57:56.810 --> 00:57:59.370
and normalize the columns.

00:57:59.370 --> 00:58:03.140
And then that gives me
the stochastic matrix.

00:58:03.140 --> 00:58:05.460
And then I can exponentiate
the stochastic matrix

00:58:05.460 --> 00:58:08.290
to figure out my probability
of moving from any node

00:58:08.290 --> 00:58:11.594
to any other in a
certain number of steps.

00:58:11.594 --> 00:58:12.510
Any questions on that?

00:58:15.201 --> 00:58:15.700
OK.

00:58:18.790 --> 00:58:23.830
So if we simply keep multiplying
this stochasticity matrix,

00:58:23.830 --> 00:58:26.450
we'll get the probability of
increasing numbers of moves.

00:58:26.450 --> 00:58:28.700
But it doesn't give us sharp
partitions of the matrix.

00:58:28.700 --> 00:58:31.034
So to do a Markov clustering,
we do an exponentiation

00:58:31.034 --> 00:58:32.950
of this matrix with
what's called an inflation

00:58:32.950 --> 00:58:35.720
operator, which
is the following.

00:58:38.500 --> 00:58:43.930
This inflation operator
takes the r-th power

00:58:43.930 --> 00:58:48.100
of the adjacency matrix
and puts a denominator,

00:58:48.100 --> 00:58:51.945
the sum of the powers
of the transition.

00:58:51.945 --> 00:58:52.820
So here's an example.

00:58:52.820 --> 00:58:57.275
Let's say I've got two
probabilities-- 0.9 and 0.1.

00:58:57.275 --> 00:59:01.416
When I inflate it, I
square the numerator,

00:59:01.416 --> 00:59:03.290
and I square each element
of the denominator.

00:59:03.290 --> 00:59:09.210
Now I've gone from 0.9
to 0.99 and 0.1 to 0.01.

00:59:09.210 --> 00:59:11.380
So this inflation
operator exaggerates

00:59:11.380 --> 00:59:14.314
all my probabilities and makes
the higher probabilities more

00:59:14.314 --> 00:59:16.480
probable and makes the lower
probabilities even less

00:59:16.480 --> 00:59:18.910
probable.

00:59:18.910 --> 00:59:20.430
So I take this
adjacency matrix that

00:59:20.430 --> 00:59:22.280
represents the number
of steps in my matrix,

00:59:22.280 --> 00:59:24.510
and I exaggerate it with
the inflation operator.

00:59:24.510 --> 00:59:27.310
And that takes the
basic clustering,

00:59:27.310 --> 00:59:30.990
and it makes it more compact.

00:59:30.990 --> 00:59:35.540
So the algorithm for this
Markov clustering is as follows.

00:59:35.540 --> 00:59:37.220
I start with a graph.

00:59:37.220 --> 00:59:38.370
I add loops to the graph.

00:59:38.370 --> 00:59:39.237
Why do I add loops?

00:59:39.237 --> 00:59:41.820
Because I need some probability
that I stay in the same place,

00:59:41.820 --> 00:59:42.910
right?

00:59:42.910 --> 00:59:44.425
And in a normal
adjacency matrix,

00:59:44.425 --> 00:59:45.800
you can't stay in
the same place.

00:59:45.800 --> 00:59:47.734
You have to go somewhere.

00:59:47.734 --> 00:59:48.400
So I add a loop.

00:59:48.400 --> 00:59:51.510
So there's always a self loop.

00:59:51.510 --> 00:59:56.680
Then I set the inflation
parameter to some value.

00:59:56.680 --> 01:00:01.176
M_1 is the matrix of random
walks in the original graph.

01:00:01.176 --> 01:00:02.720
I multiply that.

01:00:02.720 --> 01:00:05.110
I inflate it.

01:00:05.110 --> 01:00:07.550
And then I find the difference.

01:00:07.550 --> 01:00:11.480
And I do that until the
difference in this--

01:00:11.480 --> 01:00:15.000
because this matrix
gets below some value.

01:00:15.000 --> 01:00:17.770
And what I end up with then
are relatively sharp partitions

01:00:17.770 --> 01:00:20.734
of the overall structure.

01:00:20.734 --> 01:00:24.710
So I'll show you an
example of how that works.

01:00:24.710 --> 01:00:26.260
So in this case,
the authors were

01:00:26.260 --> 01:00:32.210
using a matrix where the
nodes represented proteins.

01:00:32.210 --> 01:00:34.225
The edges represented
BLAST hits.

01:00:34.225 --> 01:00:35.990
And what they wanted
to do was find

01:00:35.990 --> 01:00:39.610
families of proteins
that had similar sequence

01:00:39.610 --> 01:00:40.770
similarity to each other.

01:00:40.770 --> 01:00:44.152
But they didn't want it to be
entirely dominated by domains.

01:00:44.152 --> 01:00:46.610
So they figured that this graph
structure would be helpful,

01:00:46.610 --> 01:00:48.630
because you'd get--
for any protein,

01:00:48.630 --> 01:00:53.262
there'd be edges,
not just things

01:00:53.262 --> 01:00:55.470
that had similar common
domains, but also things that

01:00:55.470 --> 01:00:59.670
had edges connecting it
to other proteins as well.

01:00:59.670 --> 01:01:03.750
So in the original graph, the
edges are these BLAST values.

01:01:03.750 --> 01:01:05.980
They come up with the
transition matrix.

01:01:05.980 --> 01:01:08.170
They convert into
the Markov matrix,

01:01:08.170 --> 01:01:10.540
and they carry out
that exponentiation.

01:01:10.540 --> 01:01:12.680
And what they end
up with are clusters

01:01:12.680 --> 01:01:17.190
where any individual domain
can appear multiple clusters.

01:01:17.190 --> 01:01:20.060
The domains are dominated not
just by the highest BLAST hit,

01:01:20.060 --> 01:01:22.760
but by the whole network
property of what other proteins

01:01:22.760 --> 01:01:24.940
they're connected to.

01:01:24.940 --> 01:01:28.300
And it's also been done with a
network, where the underlying

01:01:28.300 --> 01:01:30.120
network represents
gene expression,

01:01:30.120 --> 01:01:33.330
and edges between two
genes represent the degree

01:01:33.330 --> 01:01:37.480
of correlation of the expression
across a very large data

01:01:37.480 --> 01:01:39.980
set for 61 mouse tissues.

01:01:39.980 --> 01:01:42.050
And once again, you
take the overall graph,

01:01:42.050 --> 01:01:44.140
and you can break it
down into clusters,

01:01:44.140 --> 01:01:46.540
where you can find
functional annotations

01:01:46.540 --> 01:01:47.570
for specific clusters.

01:01:50.320 --> 01:01:54.210
Any questions then on
the Markov clustering?

01:01:54.210 --> 01:01:55.930
So these are two
separate ways of looking

01:01:55.930 --> 01:01:57.962
at the underlying
structure of a graph.

01:01:57.962 --> 01:02:00.170
We had the edge betweenness
clustering and the Markov

01:02:00.170 --> 01:02:00.862
clustering.

01:02:00.862 --> 01:02:03.070
Now when you do this, you
have to make some decision,

01:02:03.070 --> 01:02:04.500
as I found this cluster.

01:02:04.500 --> 01:02:06.260
Now how do I decide
what it's doing?

01:02:06.260 --> 01:02:08.350
So you need to do some
sort of annotation.

01:02:08.350 --> 01:02:09.940
So once I have a
cluster, how am I

01:02:09.940 --> 01:02:13.840
going to assign a
function to that cluster?

01:02:13.840 --> 01:02:16.220
So one thing I could
do would be to look

01:02:16.220 --> 01:02:18.590
at things that already
have an annotation.

01:02:18.590 --> 01:02:19.680
So I got some cluster.

01:02:19.680 --> 01:02:21.110
Maybe two members
of this cluster

01:02:21.110 --> 01:02:23.540
have an annotation and
two members of this one.

01:02:23.540 --> 01:02:25.110
And that's fine.

01:02:25.110 --> 01:02:26.910
But what do I do
when a cluster has

01:02:26.910 --> 01:02:29.840
a whole bunch of
different annotations?

01:02:29.840 --> 01:02:31.885
So I could be arbitrary.

01:02:31.885 --> 01:02:33.930
I could just take the one
that's the most common.

01:02:33.930 --> 01:02:36.471
But a nice way to do it is by
the hypergeometric distribution

01:02:36.471 --> 01:02:38.620
that you saw in the earlier
part of the semester.

01:02:43.950 --> 01:02:46.790
So these are all ways of
clustering the underlying graph

01:02:46.790 --> 01:02:48.420
without any reference
to specific data

01:02:48.420 --> 01:02:50.540
for a particular condition
that you're interested in.

01:02:50.540 --> 01:02:51.915
A slightly harder
problem is when

01:02:51.915 --> 01:02:53.680
I do have those
specific data, and I'd

01:02:53.680 --> 01:02:55.620
like to find a piece
of the network that's

01:02:55.620 --> 01:02:57.732
most relevant to
those specific data.

01:02:57.732 --> 01:02:59.690
So it could be different
in different settings.

01:02:59.690 --> 01:03:01.190
Maybe the part of
the network that's

01:03:01.190 --> 01:03:02.869
relevant in the
cancer setting is not

01:03:02.869 --> 01:03:05.160
the part of the network that's
relevant in the diabetes

01:03:05.160 --> 01:03:07.640
setting.

01:03:07.640 --> 01:03:10.160
So one way to think about this
is that I have the network,

01:03:10.160 --> 01:03:12.000
and I paint onto
it my expression

01:03:12.000 --> 01:03:13.620
data or my proteomic data.

01:03:13.620 --> 01:03:16.850
And then I want to find
chunks of the network that

01:03:16.850 --> 01:03:18.640
are enriched in activity.

01:03:18.640 --> 01:03:21.600
So this is sometimes called
the active subgraph problem.

01:03:21.600 --> 01:03:23.530
And how do we find
the active subgraph?

01:03:23.530 --> 01:03:25.510
Well, it's not that
different from the problem

01:03:25.510 --> 01:03:27.190
that we just looked at.

01:03:27.190 --> 01:03:31.030
So if I want to figure out a
piece of the network that's

01:03:31.030 --> 01:03:33.790
active, I could just take the
things that are immediately

01:03:33.790 --> 01:03:35.160
connected to each other.

01:03:35.160 --> 01:03:37.040
That doesn't give me
the global picture.

01:03:37.040 --> 01:03:38.870
So instead why
don't I try to find

01:03:38.870 --> 01:03:40.350
larger chunks of
the network where

01:03:40.350 --> 01:03:43.300
I can include some
nodes for which I do not

01:03:43.300 --> 01:03:45.240
have specific data?

01:03:45.240 --> 01:03:46.877
And one way that's
been done for that

01:03:46.877 --> 01:03:48.710
is, again, the simulated
annealing approach.

01:03:48.710 --> 01:03:51.880
So you can try to find
pieces of the network that

01:03:51.880 --> 01:03:54.700
maximize the
probability that all

01:03:54.700 --> 01:03:57.790
the things in the
subnetwork are active.

01:04:00.302 --> 01:04:01.760
Another formulation
of this problem

01:04:01.760 --> 01:04:04.126
is something that's called
the Steiner tree problem.

01:04:04.126 --> 01:04:06.000
And in the Steiner tree,
I want to find trees

01:04:06.000 --> 01:04:10.060
in the network that consist of
all the nodes that are active,

01:04:10.060 --> 01:04:15.494
plus some nodes that are not,
for which I have no data.

01:04:15.494 --> 01:04:17.160
And those nodes for
which I have no data

01:04:17.160 --> 01:04:18.712
are called Steiner nodes.

01:04:18.712 --> 01:04:20.920
And this was a problem that
was looked at extensively

01:04:20.920 --> 01:04:21.836
in telecommunications.

01:04:21.836 --> 01:04:25.090
So if I want to wire up
a bunch of buildings--

01:04:25.090 --> 01:04:29.130
back when people used wires--
say to give telephone service,

01:04:29.130 --> 01:04:31.552
so I need to figure out
what the minimum cost is

01:04:31.552 --> 01:04:32.510
for wiring them all up.

01:04:32.510 --> 01:04:35.580
And sometimes, that involves
sticking a pole in the ground,

01:04:35.580 --> 01:04:37.850
then having everybody
communicate to that pole.

01:04:37.850 --> 01:04:43.000
So if I've got paying
customers over here,

01:04:43.000 --> 01:04:45.110
and I want to wire
them to each other,

01:04:45.110 --> 01:04:51.160
I could run wires
between everybody.

01:04:51.160 --> 01:04:52.330
But I don't have to.

01:04:52.330 --> 01:04:55.290
If I stick a pole over here,
then I don't need this wire,

01:04:55.290 --> 01:04:58.194
and I don't need this wire,
and I don't need this wire.

01:04:58.194 --> 01:04:59.860
So this is what's
called a Steiner node.

01:05:06.940 --> 01:05:11.470
And so in graph theory, there
are pretty efficient algorithms

01:05:11.470 --> 01:05:16.030
for finding a Steiner graph--
the Steiner tree-- the smallest

01:05:16.030 --> 01:05:17.600
tree that connects
all of the nodes.

01:05:17.600 --> 01:05:20.640
Now the problem in our setting
is that we don't necessarily

01:05:20.640 --> 01:05:22.371
want to connect every
node, because we're

01:05:22.371 --> 01:05:24.120
going to have in our
data some things that

01:05:24.120 --> 01:05:25.640
are false positives.

01:05:25.640 --> 01:05:27.660
And if we connect too
many things in our graph,

01:05:27.660 --> 01:05:31.050
we end up with what are
lovingly called "hairballs."

01:05:31.050 --> 01:05:33.100
So I'll give you a
specific example of that.

01:05:33.100 --> 01:05:34.891
Here's some data that
we were working with.

01:05:34.891 --> 01:05:37.830
We had a relatively small
number of experimental hits that

01:05:37.830 --> 01:05:39.460
were detected as
changing in a cancer

01:05:39.460 --> 01:05:42.240
setting and the
interactome graph.

01:05:42.240 --> 01:05:46.399
And if you simply look
for the shortest path,

01:05:46.399 --> 01:05:48.190
I should say, between
the experimental hits

01:05:48.190 --> 01:05:49.640
across the
interactome, you end up

01:05:49.640 --> 01:05:53.590
with something that looks very
similar to the interactome.

01:05:53.590 --> 01:05:56.286
So you start off with a
relatively small set of nodes,

01:05:56.286 --> 01:05:57.910
and you try to find
the subnetwork that

01:05:57.910 --> 01:05:59.060
includes everything.

01:05:59.060 --> 01:06:02.290
And you get a giant graph.

01:06:02.290 --> 01:06:04.060
And it's very hard
to figure out what

01:06:04.060 --> 01:06:06.179
to do with a graph
that's this big.

01:06:06.179 --> 01:06:07.970
I mean, there may be
some information here,

01:06:07.970 --> 01:06:09.939
but you've taken a
relatively simple problem

01:06:09.939 --> 01:06:12.230
to try to understand the
relationship among these hits.

01:06:12.230 --> 01:06:13.896
And you've turned it
into a problem that

01:06:13.896 --> 01:06:18.070
now involves hundreds
and hundreds of nodes.

01:06:18.070 --> 01:06:20.530
So these kinds of
problems arise, as I said,

01:06:20.530 --> 01:06:22.400
in part, because of
noise in the data.

01:06:22.400 --> 01:06:25.060
So some of these
hits are not real.

01:06:25.060 --> 01:06:26.740
And incorporating
those, obviously,

01:06:26.740 --> 01:06:30.470
makes me take very long
paths in the interactome,

01:06:30.470 --> 01:06:33.160
but also arises because of
the noise in the interactome--

01:06:33.160 --> 01:06:35.910
both false positives
and false negatives.

01:06:35.910 --> 01:06:38.710
So I have two proteins
that I'm trying to connect,

01:06:38.710 --> 01:06:40.710
and there's a false
positive in the interactome.

01:06:40.710 --> 01:06:42.582
It's going to draw
a line between them.

01:06:42.582 --> 01:06:44.540
If there's a false negative
in the interactome,

01:06:44.540 --> 01:06:47.780
maybe these things really do
interact, but there's no edge.

01:06:47.780 --> 01:06:49.720
If I force the algorithm
to find a connection,

01:06:49.720 --> 01:06:51.720
it probably can, because
most of the interactome

01:06:51.720 --> 01:06:54.400
is one giant
connected component.

01:06:54.400 --> 01:06:56.630
But it could be a
very, very long edge.

01:06:56.630 --> 01:06:58.619
It goes through
many other proteins.

01:06:58.619 --> 01:07:00.910
And so in the process of
trying to connect all my data,

01:07:00.910 --> 01:07:02.555
I can get extremely
large graphs.

01:07:05.230 --> 01:07:07.304
So to avoid having
giant networks--

01:07:07.304 --> 01:07:08.970
so on this projector,
unfortunately, you

01:07:08.970 --> 01:07:10.190
can't see this very well.

01:07:10.190 --> 01:07:13.947
But there are a lot of edges
among all the nodes here.

01:07:13.947 --> 01:07:15.280
Most of you have your computers.

01:07:15.280 --> 01:07:16.321
You can look at it there.

01:07:16.321 --> 01:07:20.150
So in a Steiner tree
approach, if my data

01:07:20.150 --> 01:07:24.220
are the ones that are yellow,
they're called terminals.

01:07:24.220 --> 01:07:26.450
And the grey ones,
I have no data.

01:07:26.450 --> 01:07:30.770
And I ask to try to solve
the Steiner tree problem,

01:07:30.770 --> 01:07:33.625
it's going to have to find a
way to connect this node up

01:07:33.625 --> 01:07:34.750
to the rest of the network.

01:07:37.852 --> 01:07:39.310
But if this one's
a false positive,

01:07:39.310 --> 01:07:41.760
that's not the desired outcome.

01:07:41.760 --> 01:07:43.620
So there are
optimization techniques

01:07:43.620 --> 01:07:46.110
that actually allow me to
tell the algorithm that it's

01:07:46.110 --> 01:07:49.615
OK to leave out some of the data
to get a more compact network.

01:07:52.497 --> 01:07:54.330
So one of those approaches
is called a prize

01:07:54.330 --> 01:07:56.130
collecting Steiner tree problem.

01:07:56.130 --> 01:07:58.580
And the idea here
is the following.

01:07:58.580 --> 01:08:01.370
For every node for which
I have experimental data,

01:08:01.370 --> 01:08:05.410
I associate with
that node a prize.

01:08:05.410 --> 01:08:07.590
The prize is larger,
the more confident

01:08:07.590 --> 01:08:10.640
I am that that node is
relevant in the experiment.

01:08:10.640 --> 01:08:12.900
And for every edge,
I take the edge away,

01:08:12.900 --> 01:08:15.400
and I convert it into a cost.

01:08:15.400 --> 01:08:19.439
If I have a high confidence
edge, there's a low cost.

01:08:19.439 --> 01:08:20.810
It's cheap.

01:08:20.810 --> 01:08:24.520
Low confidence edges are
going to be very expensive.

01:08:24.520 --> 01:08:26.210
And now I ask the
algorithm to try

01:08:26.210 --> 01:08:28.790
to connect up all
the things it can.

01:08:28.790 --> 01:08:31.540
Every time it includes a
node for which the zeta keeps

01:08:31.540 --> 01:08:35.040
the prize, but it had to add
an edge, so it pays the cost.

01:08:35.040 --> 01:08:37.250
So there's a trade-off
for every node.

01:08:37.250 --> 01:08:41.260
So if the algorithm wants
to include this node,

01:08:41.260 --> 01:08:44.642
then it's going to pay the
price for all the edges,

01:08:44.642 --> 01:08:45.850
but it gets to keep the node.

01:08:45.850 --> 01:08:47.766
So the optimization
function is the following.

01:08:47.766 --> 01:08:53.220
For every vertex that's not in
the tree, there's a penalty.

01:08:53.220 --> 01:08:55.319
And for every edge in
the tree, there's a cost.

01:08:55.319 --> 01:08:57.810
And you want to minimize
the sum of these two terms.

01:08:57.810 --> 01:09:01.282
You want to minimize the number
of edge costs you pay for.

01:09:01.282 --> 01:09:02.740
And you want to
minimize the number

01:09:02.740 --> 01:09:04.705
of prizes you leave behind.

01:09:04.705 --> 01:09:05.695
Is that clear?

01:09:12.140 --> 01:09:15.439
So then the algorithm then can,
depending on the optimization

01:09:15.439 --> 01:09:19.492
terms, figure out is it more of
a benefit to include this node,

01:09:19.492 --> 01:09:21.950
keep the prize, and pay all
the edge costs or the opposite?

01:09:21.950 --> 01:09:23.044
Throw it out.

01:09:23.044 --> 01:09:24.710
You don't get to keep
the prize, but you

01:09:24.710 --> 01:09:26.560
don't have to pay
the edge costs.

01:09:26.560 --> 01:09:28.810
And so that turns these
very, very large networks

01:09:28.810 --> 01:09:30.350
into relatively compact ones.

01:09:30.350 --> 01:09:32.910
Now solving this problem is
actually rather computationally

01:09:32.910 --> 01:09:33.899
challenging.

01:09:33.899 --> 01:09:36.415
You can do it with integer
linear programming.

01:09:36.415 --> 01:09:38.579
It takes a huge
amount of memory.

01:09:38.579 --> 01:09:40.620
There's also signal and
message passing approach.

01:09:40.620 --> 01:09:42.800
If you're interested in
the underlying algorithms,

01:09:42.800 --> 01:09:45.990
you can look at some
of these papers.

01:09:45.990 --> 01:09:47.750
So what happens when
you actually do this?

01:09:47.750 --> 01:09:49.920
So that hairball that
I showed you before

01:09:49.920 --> 01:09:53.160
consisted of a very
small initial data set.

01:09:53.160 --> 01:09:55.960
If you do a shortest path
search across the network,

01:09:55.960 --> 01:09:59.110
you get thousands
of edges shown here.

01:09:59.110 --> 01:10:02.640
But the prize collecting Steiner
tree solution to this problem

01:10:02.640 --> 01:10:07.210
is actually extremely compact,
and it consists of subnetworks.

01:10:07.210 --> 01:10:08.590
You can cluster
it automatically.

01:10:08.590 --> 01:10:10.620
This was clustered by hand,
but you get more or less

01:10:10.620 --> 01:10:11.160
the same results.

01:10:11.160 --> 01:10:12.409
It's just not quite as pretty.

01:10:12.409 --> 01:10:16.109
If you cluster by hand or by
say, edge betweenness, then

01:10:16.109 --> 01:10:17.650
you get subnetworks
that are enriched

01:10:17.650 --> 01:10:19.910
in various reasonable
cellular processes.

01:10:19.910 --> 01:10:22.030
This was a network
built from cancer data.

01:10:22.030 --> 01:10:25.150
And you can see things that are
highly relevant to cancer-- DNA

01:10:25.150 --> 01:10:29.250
damage, cell cycle, and so on.

01:10:29.250 --> 01:10:30.750
And the really nice
thing about this

01:10:30.750 --> 01:10:32.400
then is it gives you
a very focused way

01:10:32.400 --> 01:10:34.030
to then go and do experiments.

01:10:34.030 --> 01:10:35.570
So you can take the networks
that come out of it.

01:10:35.570 --> 01:10:37.486
And now you're not
operating on a network that

01:10:37.486 --> 01:10:39.510
consists of tens of
thousands of edges.

01:10:39.510 --> 01:10:41.790
You're working on a
network that consists

01:10:41.790 --> 01:10:43.800
of very small sets of proteins.

01:10:43.800 --> 01:10:45.760
So in this particular
case, we actually

01:10:45.760 --> 01:10:48.220
were able to go in and test
the number of the nodes that

01:10:48.220 --> 01:10:50.910
were not detected by
the experimental data,

01:10:50.910 --> 01:10:53.390
but were inferred by the
algorithms of the Steiner

01:10:53.390 --> 01:10:56.610
nodes, which had no
direct experimental data.

01:10:56.610 --> 01:11:00.380
We will test whether blocking
the activities of these nodes

01:11:00.380 --> 01:11:02.950
had any effect on the
growth of these tumor cells.

01:11:02.950 --> 01:11:04.334
We will show that
nodes that were

01:11:04.334 --> 01:11:06.250
very central to the
network that were included

01:11:06.250 --> 01:11:08.710
in the prize collecting
Steiner tree solution,

01:11:08.710 --> 01:11:11.830
had a high probability
of being cancer targets.

01:11:11.830 --> 01:11:14.319
Whereas the ones that were
just slightly more removed

01:11:14.319 --> 01:11:15.610
were much lower in probability.

01:11:18.750 --> 01:11:22.650
So one of the advantages of
these large interaction graphs

01:11:22.650 --> 01:11:24.630
is they give us a
natural way to integrate

01:11:24.630 --> 01:11:26.970
many different kinds of data.

01:11:26.970 --> 01:11:31.310
So we already saw that the
protein levels and the mRNA

01:11:31.310 --> 01:11:35.440
levels agreed very
poorly with each other.

01:11:35.440 --> 01:11:37.254
And we talked about
the fact that one thing

01:11:37.254 --> 01:11:38.670
you could do with
those data would

01:11:38.670 --> 01:11:41.940
be to try to find the
connections between not

01:11:41.940 --> 01:11:44.161
the RNAs and the proteins,
but the connections

01:11:44.161 --> 01:11:45.660
between the RNAs
and the things that

01:11:45.660 --> 01:11:48.040
drove the expression of the RNA.

01:11:48.040 --> 01:11:50.790
And so as I said, we'll see
in one of Professor Gifford's

01:11:50.790 --> 01:11:52.790
lectures, precisely
how to do that.

01:11:52.790 --> 01:11:57.120
But once you are able to do
that, you take epigenetic data,

01:11:57.120 --> 01:12:02.300
look at the regions that are
regulatory around the sites

01:12:02.300 --> 01:12:04.240
of genes that are
changing in transcription.

01:12:04.240 --> 01:12:06.380
You can infer DNA
binding proteins.

01:12:06.380 --> 01:12:07.880
And then you can
pile all those data

01:12:07.880 --> 01:12:09.420
onto an interaction
graph, where you've

01:12:09.420 --> 01:12:10.628
got different kinds of edges.

01:12:10.628 --> 01:12:13.010
So you've got RNA nodes that
represent the transcript

01:12:13.010 --> 01:12:13.700
levels.

01:12:13.700 --> 01:12:15.200
You've got the
transcription factors

01:12:15.200 --> 01:12:16.880
that infer from the
epigenetic data.

01:12:16.880 --> 01:12:18.750
And then you've got the
protein-protein interaction

01:12:18.750 --> 01:12:20.208
data that came from
the two hybrid,

01:12:20.208 --> 01:12:21.820
the affinity capture mass spec.

01:12:21.820 --> 01:12:23.695
And now you can put all
those different kinds

01:12:23.695 --> 01:12:25.860
of data in the same graph.

01:12:25.860 --> 01:12:27.910
And even though
there's no correlation

01:12:27.910 --> 01:12:31.520
between what happens in an RNA
and what happens in the protein

01:12:31.520 --> 01:12:33.704
level-- or very
low correlation--

01:12:33.704 --> 01:12:35.120
there's this
physical process that

01:12:35.120 --> 01:12:37.030
links that RNA up
to the signaling

01:12:37.030 --> 01:12:38.155
pathways that are above it.

01:12:38.155 --> 01:12:40.600
And by using the prize
collecting Steiner tree

01:12:40.600 --> 01:12:42.230
approaches, you can rediscover.

01:12:45.444 --> 01:12:46.860
And these kinds
of networks can be

01:12:46.860 --> 01:12:49.250
very valuable for other kinds
of data that don't agree.

01:12:49.250 --> 01:12:53.330
So it's not unique to transcript
data and proteome data.

01:12:53.330 --> 01:12:55.580
Turns out there are many
different kinds of omic data,

01:12:55.580 --> 01:12:58.420
when looked at individually,
give you very different views

01:12:58.420 --> 01:12:59.800
of what's going on in a cell.

01:12:59.800 --> 01:13:05.200
So if you take knockout data,
so which genes when knocked out,

01:13:05.200 --> 01:13:06.200
affect the phenotype?

01:13:06.200 --> 01:13:09.845
And which genes, in
the same condition,

01:13:09.845 --> 01:13:10.720
change an expression?

01:13:10.720 --> 01:13:12.678
Those give you two
completely different answers

01:13:12.678 --> 01:13:15.930
about which genes are important
in a particular setting.

01:13:15.930 --> 01:13:19.671
So here we're looking at
which genes are differentially

01:13:19.671 --> 01:13:21.670
expressed when you put
cells under a whole bunch

01:13:21.670 --> 01:13:23.810
of these different conditions.

01:13:23.810 --> 01:13:25.900
And which genes
when knocked out,

01:13:25.900 --> 01:13:28.445
affect viability
in that condition.

01:13:28.445 --> 01:13:30.445
And then the right-hand
column shows the overlap

01:13:30.445 --> 01:13:32.010
in the number of genes.

01:13:32.010 --> 01:13:33.995
And you can see the
overlap is small.

01:13:33.995 --> 01:13:35.370
In fact, it's less
than you would

01:13:35.370 --> 01:13:39.190
expect by chance
for most of these.

01:13:39.190 --> 01:13:42.900
So just to drill that home, if
I do two separate experiments

01:13:42.900 --> 01:13:45.580
on exactly the same
experimental system,

01:13:45.580 --> 01:13:48.116
say yeast responding
to DNA damage.

01:13:48.116 --> 01:13:49.490
And in one case,
I read out which

01:13:49.490 --> 01:13:51.652
genes are important by
looking at RNA levels.

01:13:51.652 --> 01:13:53.110
And the other one,
I read out which

01:13:53.110 --> 01:13:55.484
genes are important by knocking
every gene out and seeing

01:13:55.484 --> 01:13:56.700
whether it affects viability.

01:13:56.700 --> 01:13:59.580
We'll get two completely
different sets of genes.

01:13:59.580 --> 01:14:03.700
And we'll also have two
completely different sets

01:14:03.700 --> 01:14:05.750
of gene ontology categories.

01:14:05.750 --> 01:14:07.710
But there is some underlying
biological process

01:14:07.710 --> 01:14:10.284
that gives rise to that, right?

01:14:10.284 --> 01:14:11.700
And one of the
reasons for this is

01:14:11.700 --> 01:14:15.030
different assays are
measuring different things.

01:14:15.030 --> 01:14:18.250
So it turns out, if you
look-- at least in yeast--

01:14:18.250 --> 01:14:21.190
over 156 different
experiments, for which there's

01:14:21.190 --> 01:14:24.280
both transcriptional
data and genetic data,

01:14:24.280 --> 01:14:26.100
the things that come
out in genetic screens

01:14:26.100 --> 01:14:27.880
seem to be master regulators.

01:14:27.880 --> 01:14:30.637
Things that were knocked out
have a big effect in phenotype.

01:14:30.637 --> 01:14:32.470
Whereas the things that
change in expression

01:14:32.470 --> 01:14:35.030
tend to be effector molecules.

01:14:35.030 --> 01:14:37.094
And so in say, the
DNA damage case,

01:14:37.094 --> 01:14:38.510
the proteins that
were knocked out

01:14:38.510 --> 01:14:39.926
and have a big
effect on phenotype

01:14:39.926 --> 01:14:43.056
are ones that detect DNA damage
and signal to the nucleus

01:14:43.056 --> 01:14:44.680
that there's been
changes in DNA damage

01:14:44.680 --> 01:14:47.520
that then goes on and
blocks the cell cycle,

01:14:47.520 --> 01:14:50.780
initiates DNA
response to repair.

01:14:50.780 --> 01:14:52.690
Those things show
up as genetic hits,

01:14:52.690 --> 01:14:55.049
but they don't show up as
differentially expressed.

01:14:55.049 --> 01:14:57.340
The things that do show up
as differentially expressed,

01:14:57.340 --> 01:14:58.160
the repair enzymes.

01:14:58.160 --> 01:14:59.330
Those, when you
knock them out, don't

01:14:59.330 --> 01:15:01.370
have a big effect on
phenotype, because they're

01:15:01.370 --> 01:15:03.364
highly redundant.

01:15:03.364 --> 01:15:05.030
But there are these
underlying pathways.

01:15:05.030 --> 01:15:07.520
And so the idea is well, you
could reconstruct these by,

01:15:07.520 --> 01:15:09.050
again, using the
epigenetic data,

01:15:09.050 --> 01:15:11.010
the tough stuff
Professor Gifford

01:15:11.010 --> 01:15:13.000
will talk about in
upcoming lectures.

01:15:13.000 --> 01:15:15.590
And for the transcription
factors and then the network

01:15:15.590 --> 01:15:19.370
properties, to try to build
up a full network of how those

01:15:19.370 --> 01:15:21.150
relate to upstream
signaling pathways

01:15:21.150 --> 01:15:23.290
that would then include
some of the genetic hits.

01:15:27.490 --> 01:15:32.030
I think I'll skip to
the punchline here.

01:15:49.130 --> 01:15:51.870
So we've looked at a number of
different modeling approaches

01:15:51.870 --> 01:15:54.400
for these large interactomes.

01:15:54.400 --> 01:15:57.670
We've also looked at
ways of identifying

01:15:57.670 --> 01:15:59.670
transcriptional
regulatory networks using

01:15:59.670 --> 01:16:02.252
mutual information,
regression, Bayesian networks.

01:16:02.252 --> 01:16:03.960
And how do all these
things fit together?

01:16:03.960 --> 01:16:05.590
And when would you want to
use one of these techniques,

01:16:05.590 --> 01:16:07.214
and when would you
want to use another?

01:16:07.214 --> 01:16:10.017
So I like to think about the
problem along these two axes.

01:16:10.017 --> 01:16:11.600
On one dimension,
we're thinking about

01:16:11.600 --> 01:16:13.440
whether we have systems of
known components or unknown

01:16:13.440 --> 01:16:14.260
components.

01:16:14.260 --> 01:16:15.759
And the other one
is whether we want

01:16:15.759 --> 01:16:17.490
to identify physical
relationships

01:16:17.490 --> 01:16:19.450
or statistical relationships.

01:16:19.450 --> 01:16:21.830
So clustering, regression,
mutual information-- those

01:16:21.830 --> 01:16:23.510
are very, very
powerful for looking

01:16:23.510 --> 01:16:26.430
at the entire genome,
the entire proteome.

01:16:26.430 --> 01:16:28.800
What they give you are
statistical relationships.

01:16:28.800 --> 01:16:30.880
There's no guarantee of
a functional link, right?

01:16:30.880 --> 01:16:34.200
We saw that in the prediction
that postprandial laughter

01:16:34.200 --> 01:16:36.700
predicts breast cancer
outcome, that there's

01:16:36.700 --> 01:16:38.760
no causal link between those.

01:16:38.760 --> 01:16:40.260
Ultimately, you can
find some reason

01:16:40.260 --> 01:16:42.040
why it's not totally random.

01:16:42.040 --> 01:16:43.960
But it's not as if
that's going to lead you

01:16:43.960 --> 01:16:46.290
to new drug targets.

01:16:46.290 --> 01:16:49.740
But those can be on a
completely hypothesis-free way,

01:16:49.740 --> 01:16:52.630
with no external data.

01:16:52.630 --> 01:16:55.764
Bayesian networks are
somewhat more causal.

01:16:55.764 --> 01:16:57.430
But depending on how
much data you have,

01:16:57.430 --> 01:16:58.850
they may not be
perfectly causal.

01:16:58.850 --> 01:17:01.257
You need a lot of
intervention data.

01:17:01.257 --> 01:17:03.340
We also saw that they did
not perform particularly

01:17:03.340 --> 01:17:06.010
well in discovering
gene regulatory networks

01:17:06.010 --> 01:17:07.464
in the dream challenge.

01:17:07.464 --> 01:17:09.130
These interactome
models that we've just

01:17:09.130 --> 01:17:11.990
been talking about work very
well across giant omic data

01:17:11.990 --> 01:17:12.490
sets.

01:17:15.510 --> 01:17:17.530
And they require
this external data.

01:17:17.530 --> 01:17:18.700
They need the interactome.

01:17:18.700 --> 01:17:20.060
So it works well in
organisms for which

01:17:20.060 --> 01:17:21.680
you have all that
interactome data.

01:17:21.680 --> 01:17:25.310
It's not going to work in an
organism for which you don't.

01:17:25.310 --> 01:17:26.960
What they give you
at the end, though,

01:17:26.960 --> 01:17:30.409
is a graph that tells
you relationships

01:17:30.409 --> 01:17:31.200
among the proteins.

01:17:31.200 --> 01:17:32.700
But it doesn't tell
you what's going

01:17:32.700 --> 01:17:35.040
to happen if you start to
perturb those networks.

01:17:35.040 --> 01:17:39.570
So if I give you the
active subgraph that

01:17:39.570 --> 01:17:42.440
has all the proteins and genes
that are changing expression

01:17:42.440 --> 01:17:45.329
in my tumor sample, now
the question is, OK,

01:17:45.329 --> 01:17:47.120
should you inhibit the
nodes in that graph?

01:17:47.120 --> 01:17:49.220
Or should you activate
the nodes in that graph?

01:17:49.220 --> 01:17:51.160
And the interactome
model doesn't tell you

01:17:51.160 --> 01:17:52.260
the answer to that.

01:17:52.260 --> 01:17:54.676
And so what you're going to
hear about in the next lecture

01:17:54.676 --> 01:17:56.750
from Professor
Lauffenburger are models

01:17:56.750 --> 01:17:58.100
that live up in this space.

01:17:58.100 --> 01:18:00.906
Once you've defined a relatively
small piece of the network,

01:18:00.906 --> 01:18:02.780
you can use other kinds
of approaches-- logic

01:18:02.780 --> 01:18:06.580
based models, differential
equation based models, decision

01:18:06.580 --> 01:18:09.410
trees, and other techniques
that will actually

01:18:09.410 --> 01:18:10.910
make very quantitative
processions.

01:18:10.910 --> 01:18:13.640
What happens if I inhibit
a particular node?

01:18:13.640 --> 01:18:16.267
Does it activate the process,
or does it repress the process?

01:18:16.267 --> 01:18:17.850
And so what you could
think about then

01:18:17.850 --> 01:18:20.420
is going from a completely
unbiased view of what's

01:18:20.420 --> 01:18:24.680
going in a cell, collect all
the various kinds of omic data,

01:18:24.680 --> 01:18:26.370
and go through these
kinds of modeling

01:18:26.370 --> 01:18:28.949
approaches to identify a
subnetwork that's of interest.

01:18:28.949 --> 01:18:31.240
And then use the techniques
that we'll [? be hearing ?]

01:18:31.240 --> 01:18:34.219
about in the next lecture
to figure out quantitatively

01:18:34.219 --> 01:18:36.510
what would happen if I were
to inhibit individual nodes

01:18:36.510 --> 01:18:40.500
or inhibit combinations of
nodes or activate, and so on.

01:18:40.500 --> 01:18:44.300
Any questions on anything
we've talked about so far?

01:18:44.300 --> 01:18:45.578
Yes.

01:18:45.578 --> 01:18:48.392
AUDIENCE: Can you say again
the fundamental difference

01:18:48.392 --> 01:18:51.242
between why you get those two
different results if you're

01:18:51.242 --> 01:18:56.277
just weeding out the gene
expression versus the proteins?

01:18:56.277 --> 01:18:57.110
PROFESSOR: Oh, sure.

01:18:57.110 --> 01:18:57.610
Right.

01:18:57.610 --> 01:19:01.204
So we talked about the fact that
if you look at genetic hits,

01:19:01.204 --> 01:19:02.870
and you look at
differential expression,

01:19:02.870 --> 01:19:05.536
you get two completely different
views of what's going in cells.

01:19:05.536 --> 01:19:06.470
So why is that?

01:19:06.470 --> 01:19:09.060
So the genetic hits to tend to
hit master regulators, things

01:19:09.060 --> 01:19:10.720
that when you knock
out a single gene,

01:19:10.720 --> 01:19:13.097
you have a global
effect on the response.

01:19:13.097 --> 01:19:14.555
So in the case of
DNA damage, those

01:19:14.555 --> 01:19:17.380
are things that
detect the DNA damage.

01:19:17.380 --> 01:19:20.530
Those genes tend often not
to be changing very much

01:19:20.530 --> 01:19:22.630
in expression.

01:19:22.630 --> 01:19:24.650
So transcription factors
are very low abundance.

01:19:24.650 --> 01:19:25.760
They usually don't
change very much.

01:19:25.760 --> 01:19:27.820
A lot of signaling proteins
are kept at a constant level,

01:19:27.820 --> 01:19:29.980
and they're regulated
post-transcriptionally.

01:19:29.980 --> 01:19:32.350
So those don't show up in
the differential expression.

01:19:32.350 --> 01:19:35.160
The things that are
changing in expression--

01:19:35.160 --> 01:19:40.110
say the response regulators,
the DNA damage response--

01:19:40.110 --> 01:19:41.510
those often are redundant.

01:19:41.510 --> 01:19:44.660
So one good analogy is to
think about a smoke detector.

01:19:44.660 --> 01:19:46.370
A smoke detector
is on all the time.

01:19:46.370 --> 01:19:48.139
You don't wait until the fire.

01:19:48.139 --> 01:19:50.180
So that's not going to be
changing in expression,

01:19:50.180 --> 01:19:51.390
if you will.

01:19:51.390 --> 01:19:54.330
But if you knock it out,
you've got a big problem.

01:19:54.330 --> 01:19:56.410
The effectors, say
the sprinklers--

01:19:56.410 --> 01:19:58.540
the sprinklers only come
on when there's a fire.

01:19:58.540 --> 01:20:00.140
So that's like the
response genes.

01:20:00.140 --> 01:20:02.002
They come on only in
certain circumstances,

01:20:02.002 --> 01:20:03.210
but they're highly redundant.

01:20:03.210 --> 01:20:04.835
Any room will have
multiple sprinklers,

01:20:04.835 --> 01:20:06.860
so if one gets
damaged or is blocked,

01:20:06.860 --> 01:20:08.190
you still get a response.

01:20:08.190 --> 01:20:10.550
So that's why you get this
discrepancy between the two

01:20:10.550 --> 01:20:11.551
different kinds of data.

01:20:11.551 --> 01:20:12.924
But again, in both
cases, there's

01:20:12.924 --> 01:20:15.290
an underlying physical process
that gives rise to both.

01:20:15.290 --> 01:20:17.040
And if you do this
properly, you can

01:20:17.040 --> 01:20:19.664
detect that on these
interactome models.

01:20:19.664 --> 01:20:20.330
Other questions?

01:20:22.720 --> 01:20:23.220
OK.

01:20:23.220 --> 01:20:25.000
Very good.

