WEBVTT
Kind: captions
Language: en

00:00:00.040 --> 00:00:02.460
The following content is
provided under a Creative

00:00:02.460 --> 00:00:03.870
Commons license.

00:00:03.870 --> 00:00:06.910
Your support will help MIT
OpenCourseWare continue to

00:00:06.910 --> 00:00:10.560
offer high quality educational
resources for free.

00:00:10.560 --> 00:00:13.460
To make a donation or view
additional materials from

00:00:13.460 --> 00:00:19.290
hundreds of MIT courses, visit
MIT OpenCourseWare at

00:00:19.290 --> 00:00:21.006
ocw.mit.edu.

00:00:21.006 --> 00:00:21.500
PROFESSOR: All right.

00:00:21.500 --> 00:00:26.070
Today I want to spend a few more
minutes on plotting, and

00:00:26.070 --> 00:00:30.390
then return to a subject that
will occupy us for a couple of

00:00:30.390 --> 00:00:32.330
weeks, which is the use of

00:00:32.330 --> 00:00:34.665
randomness in solving problems.

00:00:38.450 --> 00:00:39.700
Don't save.

00:00:41.520 --> 00:00:44.360
All right, so let's
first look at one

00:00:44.360 --> 00:00:46.960
more example of plotting.

00:00:46.960 --> 00:00:48.210
It's simple.

00:00:48.210 --> 00:00:50.620
It's so simple you'll find
it's not in your handout.

00:00:54.330 --> 00:00:56.190
So here it is to start with.

00:01:02.180 --> 00:01:05.590
PROFESSOR: All I'm doing here is
I wrote a little program to

00:01:05.590 --> 00:01:09.800
show the effect of compound
interest, nothing very

00:01:09.800 --> 00:01:10.710
sophisticated.

00:01:10.710 --> 00:01:14.380
We start with some principal and
an interest rate, and then

00:01:14.380 --> 00:01:16.960
we just apply it over
and over again.

00:01:16.960 --> 00:01:22.890
And then we're going to plot to
show what the principal has

00:01:22.890 --> 00:01:25.260
become if we just keep
compounding the interest.

00:01:30.830 --> 00:01:33.940
So it is kind of what
you'd expect.

00:01:33.940 --> 00:01:35.830
Compound interest is
a nice formula.

00:01:35.830 --> 00:01:39.700
You can actually get rich
applying it, and we see this

00:01:39.700 --> 00:01:40.950
nice little graph.

00:01:44.260 --> 00:01:48.040
On the other hand, we can't
really tell what it is.

00:01:48.040 --> 00:01:52.790
And this is the sort of thing
that I see all too often,

00:01:52.790 --> 00:01:54.800
including my graduate
students produce it.

00:01:54.800 --> 00:01:57.440
They come to my office, they
show me a graph, and they

00:01:57.440 --> 00:01:58.570
start explaining it.

00:01:58.570 --> 00:02:03.150
And I usually refuse to look at
it if it looks like this.

00:02:03.150 --> 00:02:07.245
There is no point in ever, and I
mean ever, producing a graph

00:02:07.245 --> 00:02:12.060
that does not have a title
and labeled axes.

00:02:12.060 --> 00:02:15.280
And in particular, you have
to label the axes to

00:02:15.280 --> 00:02:17.500
say what they mean.

00:02:17.500 --> 00:02:22.300
Fortunately, it's easy
enough to do.

00:02:22.300 --> 00:02:25.425
And here, I've just done that.

00:02:29.930 --> 00:02:35.430
So I'm going to run the same
code to compute the interest,

00:02:35.430 --> 00:02:39.130
but I'm going to put a
title on the graph.

00:02:39.130 --> 00:02:40.630
You've seen this before,
I just want to

00:02:40.630 --> 00:02:42.860
remind you how it works.

00:02:42.860 --> 00:02:47.200
PyLab.title And then I'm
going to label the

00:02:47.200 --> 00:02:48.540
x-axis and the y-axis.

00:02:55.010 --> 00:02:57.750
And that gives me a much
more useful graph.

00:03:02.130 --> 00:03:04.860
Nothing magical here, it's
just a reminder that you

00:03:04.860 --> 00:03:07.780
really need to do
these things.

00:03:07.780 --> 00:03:11.150
You'll notice here I've not only
told you that this is the

00:03:11.150 --> 00:03:14.410
years of compounding and that
this is the principal but I've

00:03:14.410 --> 00:03:16.220
measured it in dollars.

00:03:16.220 --> 00:03:19.090
Maybe I should have been even
more explicit and said, well,

00:03:19.090 --> 00:03:21.860
US dollars, whatever.

00:03:21.860 --> 00:03:26.750
One of the things I did want
to point out is you saw the

00:03:26.750 --> 00:03:29.620
two of these various icons that
will let you do things

00:03:29.620 --> 00:03:32.980
like zoom in on a graph
and save a graph.

00:03:32.980 --> 00:03:36.490
Here's this icon that I think
Professor Grimson mentioned,

00:03:36.490 --> 00:03:37.960
in fact, I know he did.

00:03:37.960 --> 00:03:42.780
It's a floppy disk, just in case
you've never seen one, I

00:03:42.780 --> 00:03:44.840
brought a floppy disk
to show you.

00:03:44.840 --> 00:03:48.950
This is one of the older
floppy disks.

00:03:48.950 --> 00:03:54.160
These were invented
in 1971 by IBM.

00:03:54.160 --> 00:03:58.590
They were originally 8 inches in
diameter and held all of 80

00:03:58.590 --> 00:04:02.100
kilobytes of data.

00:04:02.100 --> 00:04:04.900
And as you can see, unlike
later floppy disks, they

00:04:04.900 --> 00:04:07.250
actually flopped.

00:04:07.250 --> 00:04:11.610
Eventually, Apple and others
pioneered a non-floppy floppy

00:04:11.610 --> 00:04:17.600
disk, that was in the '80s.

00:04:17.600 --> 00:04:21.700
The interesting thing today is I
typically carry around a USB

00:04:21.700 --> 00:04:27.030
stick with me about that big
that holds roughly 400,000

00:04:27.030 --> 00:04:29.720
times more data than
this floppy.

00:04:29.720 --> 00:04:33.980
And so it's just quite
incredible how things have

00:04:33.980 --> 00:04:35.230
gone along.

00:04:35.230 --> 00:04:35.980
All right.

00:04:35.980 --> 00:04:40.390
I now want to return to what
will be the main theme for, as

00:04:40.390 --> 00:04:43.630
I said, a couple of weeks
which is randomness.

00:04:43.630 --> 00:04:46.290
And in order to talk about
randomness we have to talk

00:04:46.290 --> 00:04:49.090
about probability.

00:04:49.090 --> 00:04:53.610
And I know that Professor
Grimson started down that path

00:04:53.610 --> 00:04:57.040
just before spring break, but
if you're anything like me

00:04:57.040 --> 00:04:59.970
your mind kind of deteriorated
a little bit over spring

00:04:59.970 --> 00:05:03.200
break, and your head isn't
quite into things.

00:05:03.200 --> 00:05:08.050
And so, I'm just going to back
up a tiny bit and start over

00:05:08.050 --> 00:05:11.690
to get our heads into it, and
then fairly quickly move on to

00:05:11.690 --> 00:05:14.010
new things.

00:05:14.010 --> 00:05:19.070
So let's start by asking
a simple question.

00:05:19.070 --> 00:05:21.890
You can tell my head isn't
quite yet back to things

00:05:21.890 --> 00:05:28.520
because I forgot that I needed
to begin by gathering chalk.

00:05:28.520 --> 00:05:29.770
I've now got it.

00:05:33.760 --> 00:05:36.015
And we'll come over here and
take a look at some examples.

00:05:43.236 --> 00:05:44.110
All right.

00:05:44.110 --> 00:05:49.710
So the first question I want
to ask is, suppose I take a

00:05:49.710 --> 00:05:54.630
6-sided die, a fair one, and I
roll it 10 times, what's the

00:05:54.630 --> 00:06:02.450
probability of not getting a
single 1, out of that die?

00:06:02.450 --> 00:06:07.000
Well, how do we go about
answering this?

00:06:07.000 --> 00:06:12.390
Well, there is a wrong way to
do it, which is sort of the

00:06:12.390 --> 00:06:16.970
obvious way, and many people
will start down this path.

00:06:16.970 --> 00:06:18.480
They'll say, well the
probability of

00:06:18.480 --> 00:06:20.280
rolling a 1 on the--

00:06:20.280 --> 00:06:23.950
not rolling a 1 on the first
try is 1 over 6.

00:06:29.130 --> 00:06:31.030
Right?

00:06:31.030 --> 00:06:32.090
That's true?

00:06:32.090 --> 00:06:32.950
That's not true.

00:06:32.950 --> 00:06:36.230
What's the probability of not
rolling a 1 the first time?

00:06:36.230 --> 00:06:37.510
5 over 6.

00:06:41.812 --> 00:06:43.810
All right.

00:06:43.810 --> 00:06:48.280
What's the probability of not
rolling a 1 on the second try?

00:06:48.280 --> 00:06:50.430
5 over 6.

00:06:50.430 --> 00:06:54.100
Well, the wrong thing to do, of
course, would be to start

00:06:54.100 --> 00:06:56.830
adding them up.

00:06:56.830 --> 00:07:00.950
We say, well, OK, we'll
just add these up.

00:07:00.950 --> 00:07:07.780
Well, one way we can tell that's
wrong is if we add up

00:07:07.780 --> 00:07:11.040
10 of these, we get
more than 1.

00:07:11.040 --> 00:07:15.910
Probabilities can never be
more than 1 as we'll see.

00:07:15.910 --> 00:07:18.050
So let's now try and think
of the right way to

00:07:18.050 --> 00:07:19.300
look at this problem.

00:07:21.500 --> 00:07:24.160
So you can think about it.

00:07:24.160 --> 00:07:27.230
If we roll these--

00:07:27.230 --> 00:07:32.840
a die 10 times, each time
I'll get a number.

00:07:32.840 --> 00:07:38.230
So I might get a 3, and then
a 4, and then a 2.

00:07:38.230 --> 00:07:42.470
How many possible 10-digit
numbers are there?

00:07:42.470 --> 00:07:44.870
On a 6-sided die, if
I roll it 10 times?

00:07:48.860 --> 00:07:51.840
How many?

00:07:51.840 --> 00:07:53.736
AUDIENCE: 6 to the 10th?

00:07:53.736 --> 00:07:55.640
PROFESSOR: 6 to the 10th.

00:07:55.640 --> 00:08:02.870
Exactly Just when we look at
binary numbers, if I take a

00:08:02.870 --> 00:08:06.170
10-digit binary number, and ask
how many different numbers

00:08:06.170 --> 00:08:08.420
can I represent in 10 binary
digits, it's going

00:08:08.420 --> 00:08:10.930
to be 2 to the 10th.

00:08:10.930 --> 00:08:13.460
Here we're base 6.

00:08:13.460 --> 00:08:16.470
So it's going to be
6 to the 10th.

00:08:16.470 --> 00:08:18.660
Pretty big number.

00:08:18.660 --> 00:08:23.840
Now I can say, how many
of those numbers

00:08:23.840 --> 00:08:25.090
don't contain a 1?

00:08:30.300 --> 00:08:30.550
All right.

00:08:30.550 --> 00:08:33.400
So that's really the question
I'm now asking.

00:08:33.400 --> 00:08:36.830
How many of these don't
contain a 1?

00:08:36.830 --> 00:08:43.600
So as we said, if I look at the
first roll the odds of not

00:08:43.600 --> 00:08:50.600
getting a one the first time
is 5 over 6 Now what's the

00:08:50.600 --> 00:08:56.060
odds of not getting 1 the first
or the second time?

00:08:56.060 --> 00:09:01.575
It's 5 over 6 times 5 over 6.

00:09:07.080 --> 00:09:08.750
That makes sense?

00:09:08.750 --> 00:09:13.660
Because these are independent
events.

00:09:13.660 --> 00:09:16.490
And that's a key notion here.

00:09:16.490 --> 00:09:20.490
I'm assuming that whether I get
a 1 on the second roll is

00:09:20.490 --> 00:09:22.885
independent of whether I got
a 1 on the first roll.

00:09:25.920 --> 00:09:30.200
It should be true, assuming
my dice--

00:09:30.200 --> 00:09:32.560
die is fair.

00:09:32.560 --> 00:09:40.920
Similarly, I can do this for
the third roll et cetera.

00:09:40.920 --> 00:09:46.260
So the probability of not
getting a 1 in 10 rolls is

00:09:46.260 --> 00:09:51.595
going to be (5 over
6) to the 10th.

00:09:58.770 --> 00:10:01.170
That makes sense?

00:10:01.170 --> 00:10:03.980
If not, speak up, because things
are going to get more

00:10:03.980 --> 00:10:05.230
complicated quickly.

00:10:08.250 --> 00:10:09.680
All right.

00:10:09.680 --> 00:10:12.115
So that's pretty simple.

00:10:16.570 --> 00:10:17.100
You--

00:10:17.100 --> 00:10:18.690
you all-- are you all
with me on that?

00:10:21.810 --> 00:10:26.360
Now, suppose I ask you
the inverse question.

00:10:26.360 --> 00:10:30.240
What is the probability of
getting at least one 1 if I

00:10:30.240 --> 00:10:31.490
roll the die 10 times?

00:10:35.320 --> 00:10:39.130
So here I've given you how to
compute the probability of not

00:10:39.130 --> 00:10:40.810
getting any 1's.

00:10:40.810 --> 00:10:46.290
Suppose I asked you the
probability of at least one 1?

00:10:46.290 --> 00:10:46.665
Yeah?

00:10:46.665 --> 00:10:48.040
AUDIENCE: [INAUDIBLE]

00:10:48.040 --> 00:10:50.040
1 minus not having a 1?

00:10:50.040 --> 00:10:51.040
PROFESSOR: Exactly.

00:10:51.040 --> 00:10:52.540
Thank you.

00:10:52.540 --> 00:11:02.430
So that would be 1 minus because
we know that the

00:11:02.430 --> 00:11:06.900
probability, the sum of all the
possible things that we

00:11:06.900 --> 00:11:08.680
can do when we do a probability

00:11:08.680 --> 00:11:10.620
always has to be 1.

00:11:16.230 --> 00:11:19.434
It was a good effort.

00:11:19.434 --> 00:11:20.840
That's it.

00:11:20.840 --> 00:11:21.860
If you take--

00:11:21.860 --> 00:11:24.500
if you want to get something
where everything is covered,

00:11:24.500 --> 00:11:28.700
the probabilities always
have to sum to 1.

00:11:28.700 --> 00:11:31.640
And so now, there are only
two possibilities here.

00:11:31.640 --> 00:11:34.760
One possibility is I
don't get any 1's.

00:11:34.760 --> 00:11:39.350
One possibility is I
get at least one 1.

00:11:39.350 --> 00:11:44.550
So if I take all of the
possibilities, and I subtract

00:11:44.550 --> 00:11:48.020
the possibilities of not getting
any 1's, the result

00:11:48.020 --> 00:11:52.800
must be the probability of
getting at least one 1.

00:11:52.800 --> 00:11:58.710
This is a very common trick in
computing probabilities.

00:11:58.710 --> 00:12:02.160
Very often when I ask or
somebody says, what's the

00:12:02.160 --> 00:12:04.550
probability of x?

00:12:04.550 --> 00:12:07.470
The simplest way to compute
it, is to compute the

00:12:07.470 --> 00:12:11.210
probability of not x and
subtract it from 1.

00:12:17.700 --> 00:12:17.980
OK.

00:12:17.980 --> 00:12:22.350
Again, heading down a wrong
track for this, one might have

00:12:22.350 --> 00:12:26.010
said, well all right, the
probability of getting a 1 on

00:12:26.010 --> 00:12:32.020
the first roll is 1 over 6.

00:12:32.020 --> 00:12:33.960
The probability of getting
a 1 on the second

00:12:33.960 --> 00:12:35.235
roll is 1 over 6.

00:12:35.235 --> 00:12:38.850
The probability of getting
a third roll is 1 over 6.

00:12:38.850 --> 00:12:40.640
I'll just add them up, and
that will give me the

00:12:40.640 --> 00:12:44.260
probability of getting
at least one one.

00:12:44.260 --> 00:12:45.080
How do I--

00:12:45.080 --> 00:12:47.320
how can I be sure
that's wrong?

00:12:47.320 --> 00:12:50.840
Well when I'm done, I would
claim the probability is

00:12:50.840 --> 00:12:53.510
something like that.

00:12:53.510 --> 00:12:55.470
And we know that
can't be true.

00:12:55.470 --> 00:12:58.290
Because a probability always
has to be less

00:12:58.290 --> 00:13:00.890
than or equal to 1.

00:13:00.890 --> 00:13:03.820
So this is a good trick to keep
in mind, whenever you're

00:13:03.820 --> 00:13:07.840
given a probability problem, try
and figure out whether you

00:13:07.840 --> 00:13:10.890
have a good way to compute it
directly, or whether it's

00:13:10.890 --> 00:13:17.680
simpler to compute the not of
the probability, and then

00:13:17.680 --> 00:13:20.710
subtract it from 1.

00:13:20.710 --> 00:13:25.840
Probability is really
a fun field.

00:13:25.840 --> 00:13:28.910
It's interesting, it's history,
it's intimately

00:13:28.910 --> 00:13:31.680
connected with the history
of gambling.

00:13:31.680 --> 00:13:36.710
And, in fact, almost all of
early probability theory owes

00:13:36.710 --> 00:13:40.090
its existence to gamblers.

00:13:40.090 --> 00:13:44.550
People like Cardano, Pascal,
Fermat, Bernoulli, de Moivre,

00:13:44.550 --> 00:13:48.990
Laplace, all famous names you've
heard, were motivated

00:13:48.990 --> 00:13:51.760
by desire to understand
games of chance.

00:13:55.260 --> 00:13:57.980
Mostly, it started with dice.

00:13:57.980 --> 00:14:00.100
I've been talking about dice.

00:14:00.100 --> 00:14:03.960
And in fact, dice are probably
the human race's oldest

00:14:03.960 --> 00:14:05.210
gambling implement.

00:14:07.530 --> 00:14:10.600
They date at least,
archaeologically, to about 600

00:14:10.600 --> 00:14:15.960
BC, where a pair of dice was
found in Egyptian tombs,

00:14:15.960 --> 00:14:17.660
actually longer than that.

00:14:17.660 --> 00:14:21.110
Two millennia before the birth
of Christ, people found dice

00:14:21.110 --> 00:14:22.360
in Egyptian tombs.

00:14:24.580 --> 00:14:27.350
Typically, they were made
from animal bones, but

00:14:27.350 --> 00:14:28.960
that doesn't matter.

00:14:28.960 --> 00:14:32.230
Pascal's interest in it, and
Pascal is really considered

00:14:32.230 --> 00:14:35.840
the founder of probability
theory, came when a friend

00:14:35.840 --> 00:14:38.730
asked him to solve the following
problem which I want

00:14:38.730 --> 00:14:42.300
to work out with you.

00:14:42.300 --> 00:14:47.590
Is it profitable to bet that
given 24 rolls of a pair of

00:14:47.590 --> 00:14:51.535
fair dice, you would
roll a double 6?

00:14:54.220 --> 00:14:57.070
He actually had a friend who
was in the business of

00:14:57.070 --> 00:14:59.180
gambling, making these bets.

00:14:59.180 --> 00:15:07.290
So he said, you've got a pair of
dice, you roll it 24 times

00:15:07.290 --> 00:15:12.830
and ask the question, what is
the probability of getting

00:15:12.830 --> 00:15:16.220
what we call today "box cars",
in those days they

00:15:16.220 --> 00:15:19.540
just called two 6's.

00:15:19.540 --> 00:15:23.060
This was considered a really
hard problem in

00:15:23.060 --> 00:15:25.430
the mid-17th century.

00:15:25.430 --> 00:15:31.080
And in fact, Pascal and Fermat,
two pretty smart guys

00:15:31.080 --> 00:15:33.190
as it happens, debated this.

00:15:33.190 --> 00:15:35.540
They exchanged letters with each
other trying to figure

00:15:35.540 --> 00:15:37.490
out how to solve this problem.

00:15:37.490 --> 00:15:40.920
It shows how math has advanced
because, in fact, today, it's

00:15:40.920 --> 00:15:42.830
quite an easy problem.

00:15:42.830 --> 00:15:45.680
So let's work it through
and think how would

00:15:45.680 --> 00:15:50.070
we answer this question.

00:15:50.070 --> 00:16:00.560
So what's the probability of
rolling, of not rolling, a

00:16:00.560 --> 00:16:03.475
double 6 on the first try?

00:16:06.910 --> 00:16:10.650
Well, the probability of
not rolling a 6 on one

00:16:10.650 --> 00:16:12.000
die is a sixth --

00:16:12.000 --> 00:16:13.900
1 over 6.

00:16:13.900 --> 00:16:16.900
The probability of not rolling
a one with the next die is

00:16:16.900 --> 00:16:20.170
also 1 over 6.

00:16:20.170 --> 00:16:23.340
So the probability of not
getting a die in the first

00:16:23.340 --> 00:16:26.760
roll, first double 6's is--

00:16:29.750 --> 00:16:34.170
the probability of getting
a double 6 is 1/36.

00:16:34.170 --> 00:16:38.395
So the probability of not
getting a double 6 is 35/36.

00:16:41.800 --> 00:16:44.350
Right?

00:16:44.350 --> 00:16:48.880
So now we know that the
probability of not

00:16:48.880 --> 00:16:50.170
getting it is that.

00:16:52.690 --> 00:16:57.050
What's the probability of not
getting it 24 times in a row?

00:17:00.310 --> 00:17:01.560
It's that.

00:17:05.960 --> 00:17:12.000
Which is approximately
equal to 0.51.

00:17:12.000 --> 00:17:15.550
So you can see why the answer
was not obvious just by

00:17:15.550 --> 00:17:17.670
experience.

00:17:17.670 --> 00:17:24.960
But there is a slight edge in
betting that you will not get

00:17:24.960 --> 00:17:28.369
a double 6 in 24 times.

00:17:28.369 --> 00:17:30.580
Again, assuming you
have fair dice.

00:17:30.580 --> 00:17:34.220
As old as dice is, people have
built cheater's dice.

00:17:34.220 --> 00:17:37.450
The excavation of Pompeii, for
example, they discovered a

00:17:37.450 --> 00:17:41.050
pair of loaded dice, dice with
a little weight in it so one

00:17:41.050 --> 00:17:45.100
number would come up more
often than it should.

00:17:45.100 --> 00:17:47.210
And in fact, if you look at the
internet today, you will

00:17:47.210 --> 00:17:51.880
find many sites where you can,
let's see, the one I found

00:17:51.880 --> 00:17:55.290
this morning says quote, "Are
you on unusually unlucky when

00:17:55.290 --> 00:17:57.780
it comes to rolling dice?

00:17:57.780 --> 00:18:01.640
Investing in a pair of dice
that's more reliable might be

00:18:01.640 --> 00:18:05.770
just what you need." And then
it says, "Of course for

00:18:05.770 --> 00:18:10.530
amusement only." Yeah,
we believe that.

00:18:10.530 --> 00:18:12.420
All right.

00:18:12.420 --> 00:18:17.050
As much as I trust probability
theory, I don't trust my

00:18:17.050 --> 00:18:18.940
ability to use it.

00:18:18.940 --> 00:18:28.270
And so what I did is wrote a
little simulation to see if

00:18:28.270 --> 00:18:32.240
Pascal was right when
he did this.

00:18:32.240 --> 00:18:38.130
So I've got the first-- just
this little test roll, which

00:18:38.130 --> 00:18:42.205
rolls a dice number of times,
gets the result.

00:18:46.360 --> 00:18:50.310
Now, then I decided
to check Pascal.

00:18:50.310 --> 00:18:55.300
So I was going to run 100,000
trials, and keep track of the

00:18:55.300 --> 00:18:57.940
number of times it worked.

00:18:57.940 --> 00:19:00.880
So what you'll see I'm doing
here is for i in the range

00:19:00.880 --> 00:19:04.400
number of trials, and this is
the way we'll do a lot of

00:19:04.400 --> 00:19:06.760
these simulations.

00:19:06.760 --> 00:19:12.110
And in fact, as we deal with
probability, we'll be dealing

00:19:12.110 --> 00:19:19.580
a lot with the notion of
simulation, as you are doing

00:19:19.580 --> 00:19:22.950
in your current problem set.

00:19:22.950 --> 00:19:28.770
So for i in range number of
trials, for j in range 24,

00:19:28.770 --> 00:19:33.500
because that was Pascal's
friend's game, I'll roll the

00:19:33.500 --> 00:19:36.230
first die, I'll roll
the second die.

00:19:36.230 --> 00:19:39.710
If they're both 6's I'll
say, yes equals 1.

00:19:39.710 --> 00:19:47.190
And I'll break and then I'll
compute the probability of

00:19:47.190 --> 00:19:48.440
winning or losing.

00:19:50.800 --> 00:19:52.050
OK?

00:19:54.010 --> 00:19:57.258
So let's let it rip.

00:19:57.258 --> 00:19:58.970
So now let's let it rip.

00:20:11.150 --> 00:20:12.860
There it is.

00:20:12.860 --> 00:20:15.670
And we can see that it actually
comes out pretty

00:20:15.670 --> 00:20:20.980
close to what Pascal
predicted.

00:20:20.980 --> 00:20:24.560
Should we be surprised that it
didn't come out to exactly?

00:20:24.560 --> 00:20:27.030
Well let's see, is it exactly?

00:20:27.030 --> 00:20:30.330
What is 35/36 to the 24th?

00:20:46.120 --> 00:20:48.482
So that's the--

00:20:48.482 --> 00:20:52.630
well, to 17 digits of precision,
the exact answer.

00:20:52.630 --> 00:20:55.950
And you can see we came up with
something close to that.

00:20:55.950 --> 00:20:59.470
Not exactly that, and we
wouldn't expect to.

00:20:59.470 --> 00:21:01.990
Now I only did 100,000 trials.

00:21:01.990 --> 00:21:05.440
If I did a million trials,
I'd probably come up with

00:21:05.440 --> 00:21:09.410
something even closer, but if I
did 2 trials, who knows what

00:21:09.410 --> 00:21:11.244
I get-- come up with it, right?

00:21:11.244 --> 00:21:13.464
Could be--

00:21:13.464 --> 00:21:19.010
I could get 1, I could get lucky
both times, or unlucky.

00:21:19.010 --> 00:21:22.200
Later on, we'll talk more about
the question, how do we

00:21:22.200 --> 00:21:25.460
know how many trials to run?

00:21:25.460 --> 00:21:31.030
Now, the interesting thing is
I'm sure it took me less time

00:21:31.030 --> 00:21:33.040
to write this program
than it took Pascal

00:21:33.040 --> 00:21:34.820
to solve the problem.

00:21:34.820 --> 00:21:37.410
Now the truth is, I had several
hundred years of other

00:21:37.410 --> 00:21:39.690
people's work to build on.

00:21:39.690 --> 00:21:44.270
But in general, I think one of
the questions you'll find is,

00:21:44.270 --> 00:21:48.365
is it easier sometimes to write
a simulation, than it is

00:21:48.365 --> 00:21:53.230
to do the probabilities?

00:21:53.230 --> 00:21:57.550
What I often do in
practice is both.

00:21:57.550 --> 00:22:00.920
I'll scratch my head and figure
out how to figure out

00:22:00.920 --> 00:22:04.060
the answer analytically, and
then if it's easy, I'll write

00:22:04.060 --> 00:22:08.850
some code to simulate the
problem, and expect to get

00:22:08.850 --> 00:22:12.310
roughly the same answer, giving
me confidence I've done

00:22:12.310 --> 00:22:14.000
it correctly.

00:22:14.000 --> 00:22:16.400
On the other hand, if I've done
the simulation and it had

00:22:16.400 --> 00:22:20.980
come up with something totally
bogus, or totally different,

00:22:20.980 --> 00:22:23.150
then I would have had to work
hard to figure out which was

00:22:23.150 --> 00:22:27.400
right, the code or the math.

00:22:27.400 --> 00:22:29.770
Same sort of thing you saw when
you looked at the random

00:22:29.770 --> 00:22:33.340
walk, and the first time it was
done an answer showed up

00:22:33.340 --> 00:22:36.340
that was just wrong.

00:22:36.340 --> 00:22:40.730
But, you need to have some
intuition about a problem, so

00:22:40.730 --> 00:22:42.940
that you can look at
it and say, yeah,

00:22:42.940 --> 00:22:44.680
that's in the ballpark.

00:22:44.680 --> 00:22:48.790
And if it's not, it's
time to worry.

00:22:48.790 --> 00:22:51.770
This kind of simulation that
I've just done for the dice

00:22:51.770 --> 00:22:54.886
game is what's called a "Monte
Carlo simulation".

00:23:03.660 --> 00:23:11.850
It is the most popular kind of
simulation named after a

00:23:11.850 --> 00:23:21.940
Casino on the Riviera, in the
small principality of Monaco.

00:23:21.940 --> 00:23:24.420
This was back in the time when
it was hard to find a place

00:23:24.420 --> 00:23:26.840
you could gamble, and this
happened to be one of the

00:23:26.840 --> 00:23:30.180
places you could.

00:23:30.180 --> 00:23:35.865
The term was coined in 1949 by
Stanislaw Ulam and Nicholas

00:23:35.865 --> 00:23:42.110
Metropolis, two very well-known
mathematicians.

00:23:42.110 --> 00:23:46.020
Ulam, who later became famous
for designing the hydrogen

00:23:46.020 --> 00:23:53.300
bomb with Teller, invented the
method in 1946, and I'm going

00:23:53.300 --> 00:23:58.130
to quote from his description
of how he invented it.

00:23:58.130 --> 00:24:00.710
"The first thoughts and attempts
I made to practice

00:24:00.710 --> 00:24:04.180
the Monte Carlo method, were
suggested by a question which

00:24:04.180 --> 00:24:08.110
occurred to me in 1946, as I
was convalescing from an

00:24:08.110 --> 00:24:11.010
illness and playing
solitaires.

00:24:11.010 --> 00:24:13.740
The question was, what are the
chances that a canfield

00:24:13.740 --> 00:24:16.990
solitaire laid out with
52 cards will come out

00:24:16.990 --> 00:24:18.990
successfully?

00:24:18.990 --> 00:24:21.880
After spending a lot of time
trying to estimate them by

00:24:21.880 --> 00:24:25.310
pure combinatorial calculations,
I wondered

00:24:25.310 --> 00:24:28.800
whether a more practical method
than quote 'abstract

00:24:28.800 --> 00:24:33.210
thinking' end quote, might not
be to lay it out, say, 100

00:24:33.210 --> 00:24:36.520
times, and simply observe
and count the number

00:24:36.520 --> 00:24:39.500
of successful plays.

00:24:39.500 --> 00:24:42.790
This was already possible to
envision with the beginning of

00:24:42.790 --> 00:24:45.920
the new era of fast computers.

00:24:45.920 --> 00:24:48.780
And I immediately thought of
problems, as you would, I'm

00:24:48.780 --> 00:24:52.010
sure, immediately thought of
problems of neutron diffusion

00:24:52.010 --> 00:24:55.260
and other questions of
mathematical physics.

00:24:55.260 --> 00:24:59.040
And more generally, how to
change processes described by

00:24:59.040 --> 00:25:02.680
certain differential equations
into an equivalent form

00:25:02.680 --> 00:25:07.610
interpretable as a succession
of random operations.

00:25:07.610 --> 00:25:11.480
Later, I described the idea to
John von Neumann, and we began

00:25:11.480 --> 00:25:14.460
to plan actual calculations."

00:25:14.460 --> 00:25:17.880
So as early as 1946, people
were thinking about the

00:25:17.880 --> 00:25:22.820
question of moving away from
solving systems of equations,

00:25:22.820 --> 00:25:28.230
to using randomized techniques
to simulate things and try to

00:25:28.230 --> 00:25:32.600
find out what the actual
answer was that way.

00:25:32.600 --> 00:25:35.810
Now of course "fast"
is a relative term.

00:25:35.810 --> 00:25:39.820
Ulam was probably referring to
the ENIAC computer, which

00:25:39.820 --> 00:25:44.180
could perform about 10 to the
3 additions a second.

00:25:44.180 --> 00:25:47.110
Not very many, 1,000 operations
a second, and

00:25:47.110 --> 00:25:50.760
weighed approximately 25 tons.

00:25:50.760 --> 00:25:53.620
Now today's computers, by
comparison, perform 10 to the

00:25:53.620 --> 00:25:58.490
9th additions and weigh about
10 to the minus 3 tons.

00:25:58.490 --> 00:25:59.090
All right.

00:25:59.090 --> 00:26:02.390
This technique was used during
the Manhattan Project to

00:26:02.390 --> 00:26:04.220
predict what would
happen doing--

00:26:04.220 --> 00:26:07.050
during nuclear fission
and worked.

00:26:09.550 --> 00:26:15.950
Monte Carlo simulations are an
example of what's called

00:26:15.950 --> 00:26:17.200
"inferential statistics".

00:26:28.400 --> 00:26:30.930
In brief, and I'm going to be
brief because this is not a

00:26:30.930 --> 00:26:35.260
statistics, course, inferential
statistics is

00:26:35.260 --> 00:26:39.440
based upon one guiding
principle.

00:26:39.440 --> 00:26:58.200
And that principle is that a
random sample tends to exhibit

00:26:58.200 --> 00:27:09.500
the same properties
as the population

00:27:09.500 --> 00:27:10.750
from which it is drawn.

00:27:18.550 --> 00:27:22.110
So if I try and sample people,
say, for predicting an

00:27:22.110 --> 00:27:26.820
election, the notion is if I go
and I asked a 1,000 people

00:27:26.820 --> 00:27:30.710
at random in Massachusetts who
they're going to vote for, the

00:27:30.710 --> 00:27:34.570
average will be about the same
as if I looked at the whole

00:27:34.570 --> 00:27:35.820
population.

00:27:37.550 --> 00:27:41.940
So whenever we use a statistical
method like this,

00:27:41.940 --> 00:27:46.610
so for example, we assumed here,
is those 100,000 times I

00:27:46.610 --> 00:27:50.730
threw the pair of dice, that
that would be representative

00:27:50.730 --> 00:27:54.460
of all possible throws of the
dice, the infinite number of

00:27:54.460 --> 00:27:55.710
possible throws.

00:27:58.890 --> 00:28:04.210
One always has to ask the
question whether this is true,

00:28:04.210 --> 00:28:06.810
or whether one has a sampling
technique that is, for

00:28:06.810 --> 00:28:09.560
example, giving you
a biased sample.

00:28:09.560 --> 00:28:13.260
Little later in the term, we'll
talk about many ways in

00:28:13.260 --> 00:28:17.730
which you can get fooled here
and think you're doing a fair

00:28:17.730 --> 00:28:21.190
statistical analysis, and get
all the math right, and still

00:28:21.190 --> 00:28:25.190
come up with the wrong answer
because this assumption

00:28:25.190 --> 00:28:27.480
doesn't actually hold.

00:28:27.480 --> 00:28:28.240
All right.

00:28:28.240 --> 00:28:31.360
Let's think about it now in
terms of coins, a little

00:28:31.360 --> 00:28:34.590
simpler than dice, where you
can flip a coin and you get

00:28:34.590 --> 00:28:35.840
either a head or a tail.

00:28:38.250 --> 00:28:42.120
Suppose Harvey Dent, for
example, flipped a coin and it

00:28:42.120 --> 00:28:43.370
came up heads.

00:28:45.500 --> 00:28:48.980
Would you feel good inferring
from that that the next time

00:28:48.980 --> 00:28:51.100
he flipped a coin it would
also come up heads?

00:28:54.510 --> 00:28:56.310
I wouldn't.

00:28:56.310 --> 00:28:58.150
Suppose he flipped it
heads and it came up

00:28:58.150 --> 00:29:00.920
heads twice, in a row.

00:29:00.920 --> 00:29:02.510
Would you feel comfortable
with the third

00:29:02.510 --> 00:29:05.410
flip would be a head?

00:29:05.410 --> 00:29:07.540
Probably not.

00:29:07.540 --> 00:29:11.870
But suppose he flipped it a 100
times in a row, and it was

00:29:11.870 --> 00:29:13.040
a head each time.

00:29:13.040 --> 00:29:15.590
What would you infer?

00:29:15.590 --> 00:29:19.820
I would infer that the coin
two-headed And, in fact, every

00:29:19.820 --> 00:29:22.530
time it was going to come up
heads, because it is so

00:29:22.530 --> 00:29:26.070
improbable that if it
was a fair coin--

00:29:26.070 --> 00:29:28.290
what's the probability of having
a 100 heads in a row

00:29:28.290 --> 00:29:29.540
with a fair coin?

00:29:32.170 --> 00:29:34.525
1 over what?

00:29:34.525 --> 00:29:37.129
AUDIENCE: 1 over 100--

00:29:37.129 --> 00:29:39.021
1 over 2 to the 100th.

00:29:39.021 --> 00:29:39.967
Right?

00:29:39.967 --> 00:29:42.600
PROFESSOR: A half the
first time times a

00:29:42.600 --> 00:29:43.830
half times a half.

00:29:43.830 --> 00:29:47.550
A huge number, a very small
number rather, right?

00:29:47.550 --> 00:29:51.370
So the probability and a fair
coin of getting hundred heads

00:29:51.370 --> 00:29:56.360
in a row is so low with just 100
flips, that I would begin

00:29:56.360 --> 00:30:00.910
to think that the coin
was not fair.

00:30:00.910 --> 00:30:02.940
All right.

00:30:02.940 --> 00:30:10.680
Suppose, however, I flipped it
100 times and I got 52 heads

00:30:10.680 --> 00:30:11.930
and 48 tails.

00:30:15.760 --> 00:30:18.700
Well, I wouldn't assume
anything from that.

00:30:18.700 --> 00:30:22.110
Would I assume that the next
time I flipped it a 100 times

00:30:22.110 --> 00:30:26.380
I'd get the same
52 to 48 ratio?

00:30:26.380 --> 00:30:27.670
Probably not, right?

00:30:27.670 --> 00:30:31.940
Your common sense tells
you you wouldn't.

00:30:31.940 --> 00:30:33.360
All right.

00:30:33.360 --> 00:30:36.010
Probably, it tells you, you
wouldn't even feel comfortable

00:30:36.010 --> 00:30:37.520
guessing that there would
be more heads than

00:30:37.520 --> 00:30:38.770
tails the next time.

00:30:42.860 --> 00:30:47.310
So when we think about these
things, we have to think about

00:30:47.310 --> 00:30:53.830
the number of tests and how
close the answer is to what

00:30:53.830 --> 00:30:57.920
you would get if you did
things at random.

00:30:57.920 --> 00:30:59.210
This is sort of comparing--

00:30:59.210 --> 00:31:01.830
this is technically called
comparing something to the

00:31:01.830 --> 00:31:03.980
null hypothesis.

00:31:03.980 --> 00:31:06.760
The null hypothesis is
what you would get

00:31:06.760 --> 00:31:09.110
with a random event.

00:31:09.110 --> 00:31:16.080
And when you do a simulation,
if you get something that is

00:31:16.080 --> 00:31:19.420
far from that, or when you
sample a population, you get

00:31:19.420 --> 00:31:22.950
something that's distant from
the null hypothesis, you can

00:31:22.950 --> 00:31:26.990
assume that maybe you're
seeing something real.

00:31:26.990 --> 00:31:27.250
All right.

00:31:27.250 --> 00:31:30.130
Let's look at this in a little
less abstract way.

00:31:40.860 --> 00:31:43.600
So let's go look at
some coin flips.

00:31:43.600 --> 00:31:47.590
So I wrote a simple
program, flip.

00:31:47.590 --> 00:31:54.130
Just flip the coin some number
of times and tells me what

00:31:54.130 --> 00:31:55.380
fraction came up heads.

00:32:00.260 --> 00:32:04.460
So we can run it, and
let's look at a--

00:32:04.460 --> 00:32:11.402
suppose I flip a 100,
I get 0.55.

00:32:11.402 --> 00:32:15.440
If I flip 10, I get 0.4.

00:32:15.440 --> 00:32:23.490
If I flip 10 again, I get 0.5.

00:32:23.490 --> 00:32:28.990
Now look at that, the same
thing twice in a row

00:32:28.990 --> 00:32:31.330
but now I get 0.2.

00:32:31.330 --> 00:32:36.330
So obviously, I shouldn't infer
too much from 10 flips

00:32:36.330 --> 00:32:39.030
and even from 100 where
I got 0.55.

00:32:39.030 --> 00:32:45.350
Let's see what happens if I
flip 100 again, 0.41, big

00:32:45.350 --> 00:32:47.870
difference.

00:32:47.870 --> 00:32:51.170
So this is suggesting that we
can't feel very good about

00:32:51.170 --> 00:32:53.610
what happens here.

00:32:53.610 --> 00:33:08.330
Now if I do the following, well
I'm feeling a little bit

00:33:08.330 --> 00:33:13.090
better about this, well
for one bad reason

00:33:13.090 --> 00:33:14.110
and one good reason.

00:33:14.110 --> 00:33:17.900
The bad reason is, I know the
answers 0.5, and these are

00:33:17.900 --> 00:33:20.805
both close to 0.5, so I
feel warm and fuzzy.

00:33:20.805 --> 00:33:21.900
But that's cheating.

00:33:21.900 --> 00:33:23.700
I wouldn't need to write
the simulation

00:33:23.700 --> 00:33:25.820
If I knew the answer.

00:33:25.820 --> 00:33:29.530
But mostly I feel good about it
because I'm getting kind of

00:33:29.530 --> 00:33:31.150
the same answer every time.

00:33:34.060 --> 00:33:37.050
OK, and that's important.

00:33:37.050 --> 00:33:42.540
The more I do, the more stable
it gets with the larger the

00:33:42.540 --> 00:33:45.070
number of trials.

00:33:45.070 --> 00:33:50.470
This is an example of what's
called "the law of large

00:33:50.470 --> 00:34:10.780
numbers", also known as
Bernoulli's Law, after one of

00:34:10.780 --> 00:34:13.210
the Bernoulli family of
mathematicians, and I can't

00:34:13.210 --> 00:34:15.889
for the life of me remember
which Bernoulli.

00:34:15.889 --> 00:34:18.290
There are a whole
bunch of them.

00:34:18.290 --> 00:34:22.199
Anyway the law states, and it's
important to understand

00:34:22.199 --> 00:34:25.270
this because again it underlies
the inferential

00:34:25.270 --> 00:34:38.940
statistics, that in repeated
independent tests, and it's

00:34:38.940 --> 00:34:44.600
important to note the word
"independent", each test has

00:34:44.600 --> 00:34:47.469
to be independent of
the earlier test.

00:34:47.469 --> 00:34:57.070
In this case, the tests are
flips of the coin with the

00:34:57.070 --> 00:35:11.910
same actual probability we'll
call it p, often used to

00:35:11.910 --> 00:35:22.030
represent probability, of an
outcome for each test, the

00:35:22.030 --> 00:35:41.350
chance that the fraction of
times that outcome occurs the

00:35:41.350 --> 00:35:56.380
outcome that with probability,
p, converges to p as number of

00:35:56.380 --> 00:35:57.700
trials goes to infinity.

00:36:09.300 --> 00:36:09.680
All right.

00:36:09.680 --> 00:36:14.240
So if I did an infinite number
of trials, the fraction of

00:36:14.240 --> 00:36:20.850
heads I would get in this case
would be exactly 0.5.

00:36:20.850 --> 00:36:24.370
Of course I can't do an infinite
number of trials.

00:36:24.370 --> 00:36:28.430
But that's the law of large
numbers that says the--

00:36:28.430 --> 00:36:36.120
Now, it's worth noting that this
law does not imply that

00:36:36.120 --> 00:36:42.560
if I start out with deviations
from the expected behavior,

00:36:42.560 --> 00:36:46.350
those deviations are likely to
be quote "evened out" by

00:36:46.350 --> 00:36:48.340
opposite deviations
in the future.

00:36:51.300 --> 00:36:54.210
So if I happen to start by
getting a whole bunch of heads

00:36:54.210 --> 00:36:59.960
in a row, it does not mean that
I'm more likely to get

00:36:59.960 --> 00:37:02.210
tails in a subsequent trial.

00:37:05.570 --> 00:37:06.370
All right.

00:37:06.370 --> 00:37:08.070
Because if I were--

00:37:08.070 --> 00:37:12.110
if that were true, then they
wouldn't be independent.

00:37:12.110 --> 00:37:14.940
Independent means memoryless.

00:37:14.940 --> 00:37:18.580
So if I have an independent
process, what happens in the

00:37:18.580 --> 00:37:22.810
future cannot be affected
by the past.

00:37:22.810 --> 00:37:25.910
And therefore, I don't
get this business

00:37:25.910 --> 00:37:29.740
of "they even out".

00:37:29.740 --> 00:37:32.140
Now people refuse
to believe this.

00:37:32.140 --> 00:37:36.140
If you go to any gambling place,
you'll discover that if

00:37:36.140 --> 00:37:39.080
people threw the roulette wheel,
if black comes up 20

00:37:39.080 --> 00:37:42.440
times in a row, they'll be
a rush to bet on red.

00:37:42.440 --> 00:37:46.910
Because everyone will say, red
is do, red is do, red is do.

00:37:46.910 --> 00:37:50.680
And every psychologist who has
ever done this experiment,

00:37:50.680 --> 00:37:52.920
finds that people don't
believe it.

00:37:52.920 --> 00:37:54.910
That it's not true.

00:37:54.910 --> 00:37:57.940
People just don't get
probability, and it happens so

00:37:57.940 --> 00:38:01.800
often it's got a name called
"the gambler's fallacy".

00:38:01.800 --> 00:38:06.780
And there's been great examples
of people going broke

00:38:06.780 --> 00:38:08.670
doing this.

00:38:08.670 --> 00:38:14.750
Now notice that the law of large
numbers here is about

00:38:14.750 --> 00:38:18.820
the fraction of times
I get an outcome.

00:38:18.820 --> 00:38:24.930
It does not imply for example,
that the absolute difference

00:38:24.930 --> 00:38:27.550
between the number of heads and
the number of tails will

00:38:27.550 --> 00:38:29.315
get smaller as I run
more trials.

00:38:32.300 --> 00:38:33.060
Right?

00:38:33.060 --> 00:38:36.040
It doesn't say anything
at all about that.

00:38:36.040 --> 00:38:40.740
It says the ratio of head to
tails will approach 1, but not

00:38:40.740 --> 00:38:43.360
that the difference
between them.

00:38:43.360 --> 00:38:46.920
All right, let's look at an
example showing that off.

00:38:46.920 --> 00:38:53.430
So what I've got here is this
program called "flip plot".

00:38:53.430 --> 00:38:54.820
This is on your hand out.

00:39:03.990 --> 00:39:08.710
This is just going to run this
business of flipping coins.

00:39:08.710 --> 00:39:10.030
I should point out just--

00:39:10.030 --> 00:39:12.900
I did it this way just
to show you.

00:39:12.900 --> 00:39:16.580
What I'm doing is each flip-- if
random.random is less than

00:39:16.580 --> 00:39:19.260
5, I'll call it a head,
0.5, I'll call it

00:39:19.260 --> 00:39:22.200
heads, otherwise a tails.

00:39:22.200 --> 00:39:25.160
You'll notice that it appears
that maybe I'm biasing a

00:39:25.160 --> 00:39:31.610
little bit, because I'm
giving 0.5 a value.

00:39:31.610 --> 00:39:37.250
But there are so many floating
point numbers between 0 and 1,

00:39:37.250 --> 00:39:41.000
that the probability of getting
exactly 0.5 is so

00:39:41.000 --> 00:39:43.590
small that I can ignore it.

00:39:43.590 --> 00:39:49.590
It isn't going to really make a
difference Random.random is

00:39:49.590 --> 00:39:52.900
the key issue, the key function
that's used to

00:39:52.900 --> 00:39:56.240
implement all the other random
functions that we

00:39:56.240 --> 00:39:58.980
have in that package.

00:39:58.980 --> 00:40:00.150
All right.

00:40:00.150 --> 00:40:03.700
So I'm going to do it, and
I'm going to do it

00:40:03.700 --> 00:40:06.030
over a range of values.

00:40:06.030 --> 00:40:10.030
The minimum exponent to the
maximum exponent and for

00:40:10.030 --> 00:40:15.710
exponent in range min x to max
x plus 1, I'm going to choose

00:40:15.710 --> 00:40:18.560
an x value that is 2 to that.

00:40:18.560 --> 00:40:21.840
So this lets me go
over a big range.

00:40:21.840 --> 00:40:26.090
So I'll see what happens if I
get 1 flip, and 2 flips, and 4

00:40:26.090 --> 00:40:31.320
and 8 and 16 and 32 et cetera.

00:40:31.320 --> 00:40:34.170
And then I'm going to
just do some plots.

00:40:34.170 --> 00:40:37.070
I'm going to plot the absolute
difference between heads and

00:40:37.070 --> 00:40:41.735
tails and the ratio
of heads to tails.

00:40:45.050 --> 00:40:49.210
Let's see what happens
when we run that.

00:40:49.210 --> 00:40:52.080
Actually, probably nothing
because I didn't uncomment the

00:40:52.080 --> 00:40:53.330
run part of it.

00:40:56.230 --> 00:40:57.480
Let's do that.

00:41:06.600 --> 00:41:12.750
So I'm going to call flip plot
with 4 and 20, running from

00:41:12.750 --> 00:41:18.105
four trials 2 to the
4 to 2 to the 20.

00:41:18.105 --> 00:41:19.355
Let's see what we get.

00:41:24.430 --> 00:41:29.120
Now, you may get different
things when you run at

00:41:29.120 --> 00:41:30.190
different times.

00:41:30.190 --> 00:41:31.440
In fact, you will.

00:41:34.270 --> 00:41:39.440
So here we see something
kind of uninteresting.

00:41:42.580 --> 00:41:47.290
Let's cheat and see what we got
the first time I ran it,

00:41:47.290 --> 00:41:52.220
which is on your hand out, and
I have a PowerPoint with it.

00:41:52.220 --> 00:41:53.360
I was--

00:41:53.360 --> 00:41:54.960
I knew this might happen.

00:41:54.960 --> 00:41:57.910
Doesn't usually, but sometimes
when you run it you get

00:41:57.910 --> 00:41:59.160
surprising results.

00:42:02.310 --> 00:42:04.730
So here's what happened
when I first ran it.

00:42:08.600 --> 00:42:10.645
Here was the difference between
heads and tails.

00:42:13.360 --> 00:42:16.620
And it seems that, OK, the
difference was low, it went

00:42:16.620 --> 00:42:20.280
up, it went down, it went
up, it went down.

00:42:20.280 --> 00:42:21.810
It seemed to go down
dramatically.

00:42:26.310 --> 00:42:30.590
If you remember what we just saw
when I ran it, we also saw

00:42:30.590 --> 00:42:33.140
something where it went up a
little bit then it went down

00:42:33.140 --> 00:42:35.790
and then shot up dramatically at
the end, which was why that

00:42:35.790 --> 00:42:37.970
scale is so funny.

00:42:37.970 --> 00:42:41.890
And if we look at the ratio,
what we see is it seems to

00:42:41.890 --> 00:42:47.490
start above 1, drop below
1, and then seems to

00:42:47.490 --> 00:42:50.920
converge towards 1.

00:42:50.920 --> 00:42:53.520
Now, I show this because I
want to make a couple of

00:42:53.520 --> 00:42:54.770
points of plotting.

00:42:57.460 --> 00:43:00.990
Let's look at this out here.

00:43:00.990 --> 00:43:04.590
Looks like we have a pretty
dramatic trend

00:43:04.590 --> 00:43:07.920
of this linear drop.

00:43:07.920 --> 00:43:10.140
Do we?

00:43:10.140 --> 00:43:11.645
Do we actually have
a trend here?

00:43:16.940 --> 00:43:19.950
Well let's think about it.

00:43:19.950 --> 00:43:25.690
The default behavior of the plot
command in PyLab is to

00:43:25.690 --> 00:43:27.100
connect points by lines.

00:43:30.160 --> 00:43:32.580
How many points do I actually
have out here?

00:43:41.770 --> 00:43:43.230
Well, you saw the code.

00:43:43.230 --> 00:43:45.650
You have the code
in the hand out.

00:43:45.650 --> 00:43:47.800
How many points do you think
there are out here?

00:43:50.674 --> 00:43:53.548
A 1,000?

00:43:53.548 --> 00:43:56.430
A 100?

00:43:56.430 --> 00:43:58.110
3?

00:43:58.110 --> 00:44:00.550
2?

00:44:00.550 --> 00:44:00.830
2 to 3.

00:44:00.830 --> 00:44:01.630
Right?

00:44:01.630 --> 00:44:04.360
Depending on what I mean
by "out here".

00:44:04.360 --> 00:44:08.590
So what we see here is something
that happens a lot.

00:44:08.590 --> 00:44:12.120
People plot a small number of
points, connect them by a

00:44:12.120 --> 00:44:17.450
line, and mislead the audience
into thinking there's a trend

00:44:17.450 --> 00:44:19.435
when, in fact, maybe all
you have is an outlier.

00:44:22.940 --> 00:44:31.060
So it's problematical here
to do it that way.

00:44:31.060 --> 00:44:33.480
So let's see what happens
if we change the code.

00:44:58.170 --> 00:45:09.550
And what I'm going to do is
change it in two ways.

00:45:12.830 --> 00:45:15.310
Well, maybe I'll change
it in one way first.

00:45:26.290 --> 00:45:28.950
Uncomment.

00:45:28.950 --> 00:45:30.200
Uncomment.

00:45:44.400 --> 00:45:50.550
So what I'm doing here is I am
plotting in a different way.

00:45:50.550 --> 00:45:56.390
This quote "BO" says, don't
connect it by lines but just

00:45:56.390 --> 00:46:01.190
put a dot as an "O" and
B says, make it blue.

00:46:01.190 --> 00:46:04.030
I used blue because it's
my favorite color.

00:46:04.030 --> 00:46:06.600
So now if we look at these
things, we'll see something

00:46:06.600 --> 00:46:07.850
pretty different.

00:46:17.280 --> 00:46:20.816
So that's the difference between
heads and tails,

00:46:20.816 --> 00:46:23.420
that's the ratio.

00:46:23.420 --> 00:46:27.070
But now, if we look at the
difference between heads and

00:46:27.070 --> 00:46:32.610
tails here, what we see
is it's pretty sparse.

00:46:32.610 --> 00:46:36.850
So yeah, maybe there's a trend,
but maybe not, right?

00:46:36.850 --> 00:46:41.730
Because way out here I'm only
connecting two points, giving

00:46:41.730 --> 00:46:47.065
an illusion that there is a
trend but, in fact, no reason

00:46:47.065 --> 00:46:48.880
to believe it.

00:46:48.880 --> 00:46:51.700
So I always think if you're
plotting a small number of

00:46:51.700 --> 00:46:56.410
points, you're much better off
just plotting the points, than

00:46:56.410 --> 00:47:01.820
you are trying to
connect them.

00:47:01.820 --> 00:47:06.400
Now if we look at this one,
again, maybe we'd feel kind of

00:47:06.400 --> 00:47:08.400
comfortable if there is a trend
here, that there are

00:47:08.400 --> 00:47:10.530
several points on this line.

00:47:10.530 --> 00:47:16.540
We can't see much of what's
going on over here, which gets

00:47:16.540 --> 00:47:27.780
me to the next thing I want
to do is I'm going to use

00:47:27.780 --> 00:47:29.530
logarithmic axes here.

00:47:32.360 --> 00:47:37.900
So PyLab.semilogx says make
the x-axis logarithmic.

00:47:37.900 --> 00:47:41.730
PyLab.semilogy the y-axis.

00:47:41.730 --> 00:47:45.710
And so in the case of the
absolute difference, I'm going

00:47:45.710 --> 00:47:48.460
to make both logarithmic.

00:47:48.460 --> 00:47:50.220
Why am I doing that?

00:47:50.220 --> 00:47:56.270
Because both have a large
range, and by making it

00:47:56.270 --> 00:47:59.450
logarithmic, I can see what's
happening at the left side in

00:47:59.450 --> 00:48:03.570
this case where things
are changing.

00:48:03.570 --> 00:48:07.160
When I look at the ratios, the
y-axis does not have a very

00:48:07.160 --> 00:48:10.645
large range, and so there's no
need to make it logarithmic.

00:48:13.540 --> 00:48:14.790
We'll run it.

00:48:24.840 --> 00:48:27.750
So here, we can see
the difference

00:48:27.750 --> 00:48:29.880
between heads and tails.

00:48:29.880 --> 00:48:33.610
And now we can see what's going
on at the left as we can

00:48:33.610 --> 00:48:34.860
in Figure (4).

00:48:38.690 --> 00:48:42.380
And we can see things
much more clearly.

00:48:42.380 --> 00:48:45.770
So log scales can be
enormously useful.

00:48:48.810 --> 00:48:53.190
And in fact, I use them a lot,
everyone uses them a lot but,

00:48:53.190 --> 00:48:57.920
again, it's very important to
observe the fact that it's

00:48:57.920 --> 00:49:00.180
logarithmic and not
get fooled.

00:49:05.570 --> 00:49:06.820
All right.

00:49:09.120 --> 00:49:14.290
So I talked about linear
scaling, logarithmic scaling

00:49:14.290 --> 00:49:19.340
and we now have charts where I
can, perhaps, actually reach

00:49:19.340 --> 00:49:23.415
some conclusion about
what's going on.

00:49:23.415 --> 00:49:28.500
The next question is, how
certain can I be?

00:49:28.500 --> 00:49:33.090
Can I really be certain that,
indeed, this should be

00:49:33.090 --> 00:49:34.340
converging to 1?

00:49:37.140 --> 00:49:40.330
Here, if I sort of look at it,
it does look like there's kind

00:49:40.330 --> 00:49:45.380
of a linear trend of the
absolute difference growing as

00:49:45.380 --> 00:49:47.400
the number of trials grows.

00:49:47.400 --> 00:49:51.540
How certain can I be of that?

00:49:51.540 --> 00:49:56.700
You can never get absolute
certainty from sampling,

00:49:56.700 --> 00:49:59.140
because you could never be
sure if you haven't been

00:49:59.140 --> 00:50:02.170
vastly lucky or unlucky.

00:50:02.170 --> 00:50:04.630
That's not to say you
can't get the

00:50:04.630 --> 00:50:06.260
absolute correct answer.

00:50:06.260 --> 00:50:12.160
Maybe I could get 0.5, which
is the correct answer.

00:50:12.160 --> 00:50:17.180
But I can't know that that's
the correct answer.

00:50:17.180 --> 00:50:21.290
So now the question I want to
pursue, and it's what we'll

00:50:21.290 --> 00:50:27.520
cover on Thursday, is what
techniques can I use to make a

00:50:27.520 --> 00:50:31.650
statement of the form, I'm
certain within the following

00:50:31.650 --> 00:50:34.105
range that I have the
right answer.

00:50:34.105 --> 00:50:38.450
That I know the right answer
is highly likely to be this

00:50:38.450 --> 00:50:41.450
close to the answer my
simulation is giving me.

00:50:41.450 --> 00:50:44.410
And we'll look at how we can
make those statements and

00:50:44.410 --> 00:50:46.140
actually believe them.

00:50:46.140 --> 00:50:47.390
OK, see you on Thursday.

