WEBVTT
Kind: captions
Language: en

00:00:01.640 --> 00:00:04.040
The following and content
is provided under a Creative

00:00:04.040 --> 00:00:05.580
Commons license.

00:00:05.580 --> 00:00:07.880
Your support will help
MIT OpenCourseWare

00:00:07.880 --> 00:00:12.270
continue to offer high-quality,
educational resources for free.

00:00:12.270 --> 00:00:14.870
To make a donation or
view additional materials

00:00:14.870 --> 00:00:18.830
from hundreds of MIT courses,
visit MIT OpenCourseWare

00:00:18.830 --> 00:00:22.370
at ocw.mit.edu.

00:00:22.370 --> 00:00:24.170
HYNEK HERMANSKY: I'm
basically an engineer.

00:00:24.170 --> 00:00:26.720
And I'm working on
speech recognition.

00:00:26.720 --> 00:00:29.600
And so you may wonder, so
what is there to work on?

00:00:29.600 --> 00:00:32.119
Because you have a cell
phone in your pocket,

00:00:32.119 --> 00:00:37.300
and you speak to it, and Siri
answers you and everything.

00:00:37.300 --> 00:00:40.085
And the whole thing is
working very basic principles.

00:00:40.085 --> 00:00:41.085
You start with a signal.

00:00:41.085 --> 00:00:42.780
It goes to signal processing.

00:00:42.780 --> 00:00:44.840
There's some pattern
classification.

00:00:44.840 --> 00:00:47.040
Of course, deep
neural nets as usual.

00:00:47.040 --> 00:00:50.270
And so this recognizes
the message.

00:00:50.270 --> 00:00:52.850
This recognizes
what you are saying,

00:00:52.850 --> 00:00:56.240
so the question is, what is it
that is fighter of the boat?

00:00:56.240 --> 00:01:00.410
Why not keep going, and try
just to improve the error rates,

00:01:00.410 --> 00:01:04.040
and improve them
basically step by step?

00:01:04.040 --> 00:01:05.730
Because we have a
good thing going.

00:01:05.730 --> 00:01:08.210
We have something which
is already out there.

00:01:08.210 --> 00:01:09.560
And it's working.

00:01:09.560 --> 00:01:11.480
But you may know
the answer, so this

00:01:11.480 --> 00:01:15.500
is, imagine that you are, sort
of, skiing or going on a sled.

00:01:15.500 --> 00:01:17.010
And suddenly, you
come somewhere.

00:01:17.010 --> 00:01:18.260
And you have to start pushing.

00:01:18.260 --> 00:01:21.380
You don't want to do that,
but you do it for the reason,

00:01:21.380 --> 00:01:24.790
because there may be some--
another kind of slope going out

00:01:24.790 --> 00:01:25.400
there.

00:01:25.400 --> 00:01:28.350
And that's the way I feel
about the speech recognition.

00:01:28.350 --> 00:01:31.370
So basically, sometimes we
need to push a little bit up

00:01:31.370 --> 00:01:34.310
and maybe go slightly
out of our comfort zone

00:01:34.310 --> 00:01:36.160
in order to get further.

00:01:38.680 --> 00:01:41.269
Speech is not what we are--

00:01:41.269 --> 00:01:42.810
it's not the thing
which we are using

00:01:42.810 --> 00:01:44.690
for communicating with Siri.

00:01:44.690 --> 00:01:45.710
Speech is this.

00:01:45.710 --> 00:01:47.690
Basically, people
speak the way I do.

00:01:47.690 --> 00:01:49.220
They hesitate.

00:01:49.220 --> 00:01:51.710
There's a lot of
fillers, interruptions.

00:01:51.710 --> 00:01:54.230
And I don't finish
the sentences.

00:01:54.230 --> 00:01:57.620
I speak with a strong
accent, and so on.

00:01:57.620 --> 00:02:00.620
I become excited,
and so on, and so on.

00:02:00.620 --> 00:02:02.780
And we would like to
put a machine there

00:02:02.780 --> 00:02:04.340
instead of the other person.

00:02:04.340 --> 00:02:07.070
Basically, this is what a
speech recognition ultimately

00:02:07.070 --> 00:02:07.640
is, right?

00:02:07.640 --> 00:02:11.380
I mean, and actually, if you see
what government is supporting,

00:02:11.380 --> 00:02:14.000
what the big companies
are working on,

00:02:14.000 --> 00:02:17.110
this is what we
are worried about.

00:02:17.110 --> 00:02:19.940
We are worried about
the real speech

00:02:19.940 --> 00:02:24.770
produced by real people in the
real communications by speech.

00:02:24.770 --> 00:02:29.120
And you know, I didn't mention
all the disturbing things

00:02:29.120 --> 00:02:33.000
like noises, and so on, and so
on, but we will get into that.

00:02:33.000 --> 00:02:35.780
So I believe that
we don't only need

00:02:35.780 --> 00:02:38.300
signal processing, and
information theory,

00:02:38.300 --> 00:02:40.520
and machine
learning, but we also

00:02:40.520 --> 00:02:42.050
need the other disciplines.

00:02:42.050 --> 00:02:44.810
And this is where you
guys are coming in.

00:02:44.810 --> 00:02:46.340
So that's what I believe in.

00:02:46.340 --> 00:02:49.310
We should be working
together, engineering and life

00:02:49.310 --> 00:02:52.820
sciences working together.

00:02:52.820 --> 00:02:54.650
At least we should try.

00:02:54.650 --> 00:02:56.030
We should at least
try to be-- we

00:02:56.030 --> 00:03:00.590
engineers should be try to
inspired by life sciences.

00:03:00.590 --> 00:03:03.500
And as far as
inspiration is concerned,

00:03:03.500 --> 00:03:07.040
I have a story to start with.

00:03:07.040 --> 00:03:11.010
There was a guy who won the
lottery by using numbers 1, 2,

00:03:11.010 --> 00:03:13.250
3, 6, 7, 49.

00:03:13.250 --> 00:03:16.070
And they said, well, this is
of course unusual sequence

00:03:16.070 --> 00:03:18.950
of numbers, so they say, how
did you ever get to that?

00:03:18.950 --> 00:03:20.750
He says, I'm the first child.

00:03:20.750 --> 00:03:23.570
My mother was my
mother's second marriage

00:03:23.570 --> 00:03:25.190
and my father's third marriage.

00:03:25.190 --> 00:03:27.120
And I was born on
the 6th of July.

00:03:27.120 --> 00:03:31.340
And of course, 6 by 7 is 49.

00:03:31.340 --> 00:03:34.010
And that's sometimes I
feel, I'm getting this sort

00:03:34.010 --> 00:03:35.680
of inspiration from you people.

00:03:35.680 --> 00:03:37.040
I may not get it right.

00:03:37.040 --> 00:03:42.480
I may not get it right, but as
long as it works, I'm happy.

00:03:42.480 --> 00:03:45.350
You know, I'm not being
paid for being smart

00:03:45.350 --> 00:03:47.830
and being knowledgeable
about biology.

00:03:47.830 --> 00:03:51.420
I'm being, really, paid for
making something which works.

00:03:51.420 --> 00:03:53.900
Anyways, so this is
just the warm up.

00:03:53.900 --> 00:03:56.270
I thought that you will
still be drinking a coffee,

00:03:56.270 --> 00:03:59.160
so I decided to
start with a joke.

00:03:59.160 --> 00:04:00.820
But anyway, but it's
an inspiring joke.

00:04:00.820 --> 00:04:02.650
I mean, it's about inspiration.

00:04:02.650 --> 00:04:05.390
And I would maybe point out
to some of the inspiration

00:04:05.390 --> 00:04:08.330
points, which I, of course,
didn't get it right,

00:04:08.330 --> 00:04:11.370
but still, it was working.

00:04:11.370 --> 00:04:13.330
Why do we have audition?

00:04:13.330 --> 00:04:14.960
Josh already told
us-- because we

00:04:14.960 --> 00:04:16.829
want to survive in this world.

00:04:16.829 --> 00:04:19.715
I mean, this is a little ferret
or whatever, and there is--

00:04:19.715 --> 00:04:21.360
he's getting something now.

00:04:21.360 --> 00:04:22.670
And there is a object.

00:04:22.670 --> 00:04:25.186
And ferret is worrying,
is this something

00:04:25.186 --> 00:04:26.810
I should be friendly
with or I should--

00:04:26.810 --> 00:04:32.630
it should be something
which I run away.

00:04:32.630 --> 00:04:34.850
So what is the message
in this signal?

00:04:34.850 --> 00:04:37.910
Is it a danger or
is a opportunity?

00:04:37.910 --> 00:04:40.550
Well, the same way,
how do we survive

00:04:40.550 --> 00:04:43.050
in this world as human beings?

00:04:43.050 --> 00:04:47.070
So there is my wife who has
some message in her head.

00:04:47.070 --> 00:04:49.700
And so she wants to
tell me, eat vegetables,

00:04:49.700 --> 00:04:52.340
they are good for you,
so she's using speech.

00:04:52.340 --> 00:04:54.012
And speech is
actually amazing sort

00:04:54.012 --> 00:04:57.759
of mechanism for sharing
the experiences and for--

00:04:57.759 --> 00:04:59.300
actually, without
speech, we wouldn't

00:04:59.300 --> 00:05:01.090
be where we are, I
can guarantee you,

00:05:01.090 --> 00:05:05.720
because that allows us to tell
the other people things what

00:05:05.720 --> 00:05:08.420
they should do without
going through much trouble

00:05:08.420 --> 00:05:12.110
like a ferret with the bird.

00:05:12.110 --> 00:05:14.780
That we may not
have to be eaten,

00:05:14.780 --> 00:05:18.125
maybe we just die a little early
if we don't get this right,

00:05:18.125 --> 00:05:19.830
if we don't get this message.

00:05:19.830 --> 00:05:24.030
So she says this thing, and
hopefully, I get the message.

00:05:24.030 --> 00:05:25.760
So this is what
speech is about, but I

00:05:25.760 --> 00:05:28.550
wanted to say, the speech
is an important thing,

00:05:28.550 --> 00:05:31.790
because it allows us to
communicate abstract ideas

00:05:31.790 --> 00:05:33.470
like good for you.

00:05:33.470 --> 00:05:35.950
And that's sort of not only
vegetable, vegetable is saying,

00:05:35.950 --> 00:05:39.920
but a lot of abstract ideas
can be conveyed by speech.

00:05:39.920 --> 00:05:45.020
And that's why I think
it's kind of exciting.

00:05:45.020 --> 00:05:49.280
Why do we work on machine
recognition of speech?

00:05:49.280 --> 00:05:52.490
Well, first one is just
like Edmund Hillary

00:05:52.490 --> 00:05:54.120
said, because it's there.

00:05:54.120 --> 00:05:56.330
They are asking, why did
you climb Mount Everest?

00:05:56.330 --> 00:05:57.746
He said, well,
because it's there.

00:05:57.746 --> 00:05:59.390
I mean, it's a challenge, right?

00:05:59.390 --> 00:06:02.120
Spoken language is one of
the most amazing things,

00:06:02.120 --> 00:06:05.330
I already told you
before, of human race

00:06:05.330 --> 00:06:07.580
so there would be
hell if we can't build

00:06:07.580 --> 00:06:09.920
a machine which understands it.

00:06:09.920 --> 00:06:12.740
And we don't have an
easy time so far yet.

00:06:12.740 --> 00:06:15.680
In addition, when you
are addressing speech,

00:06:15.680 --> 00:06:18.140
you are really addressing
the generic problems

00:06:18.140 --> 00:06:21.470
which we have in processing
of other cognitive signals.

00:06:21.470 --> 00:06:25.550
And we touched it to some
extent during this panel,

00:06:25.550 --> 00:06:27.890
because, you know, problems
which we have in speech,

00:06:27.890 --> 00:06:30.200
we have the similar
problems in perceiving

00:06:30.200 --> 00:06:32.390
images and perceiving smells.

00:06:32.390 --> 00:06:35.300
All these cognitive
signals, basically, machines

00:06:35.300 --> 00:06:36.697
are not very good at it.

00:06:36.697 --> 00:06:37.280
Let's face it.

00:06:37.280 --> 00:06:40.550
Machines can add 10 billion
numbers very quickly,

00:06:40.550 --> 00:06:43.590
but they cannot tell my
grandmother from the monkey,

00:06:43.590 --> 00:06:44.090
right?

00:06:44.090 --> 00:06:48.670
I mean, so this is
actually important thing.

00:06:48.670 --> 00:06:50.840
There are also practical
applications, obviously--

00:06:50.840 --> 00:06:52.710
access to information.

00:06:52.710 --> 00:06:56.510
Voice interaction with
machines extracting information

00:06:56.510 --> 00:06:59.840
from speech, given how much
speech is out there now with--

00:06:59.840 --> 00:07:02.360
I don't know how
much we are adding

00:07:02.360 --> 00:07:06.897
every second through the
YouTube and that sort of things,

00:07:06.897 --> 00:07:08.480
but there's a lot
of speech out there.

00:07:08.480 --> 00:07:10.400
It would be good if
information can actually

00:07:10.400 --> 00:07:13.270
extract information from that.

00:07:13.270 --> 00:07:16.100
And I tell always the students,
there is a job security.

00:07:16.100 --> 00:07:18.470
It's not going to be solved
during your lifetime,

00:07:18.470 --> 00:07:19.695
certainly not during mine.

00:07:19.695 --> 00:07:22.040
I mean, sort of, if
you get into it--

00:07:22.040 --> 00:07:26.690
in addition, I mean, I know
that this is maybe on YouTube,

00:07:26.690 --> 00:07:30.860
but also, if you don't like
it, you can get fantastic jobs.

00:07:30.860 --> 00:07:33.470
There is a half of
the IBM group ended up

00:07:33.470 --> 00:07:36.920
on the Wall Street making
insane amount of money.

00:07:36.920 --> 00:07:38.550
So I mean, you
know, what skills we

00:07:38.550 --> 00:07:42.290
should get in recognizing
speech, working on the speech,

00:07:42.290 --> 00:07:44.510
can be also applied
in other areas.

00:07:44.510 --> 00:07:49.700
Obviously it can be applied in
vision, and so on, and so on.

00:07:49.700 --> 00:07:52.040
Speech has been produced
to be perceived.

00:07:52.040 --> 00:07:57.020
Here is Roman Jakobson,
the great Harvard, MIT guy,

00:07:57.020 --> 00:07:58.660
passed away unfortunately.

00:07:58.660 --> 00:08:00.710
He would be now a
hundred and something.

00:08:00.710 --> 00:08:02.750
He says, we speak in
order to be heard,

00:08:02.750 --> 00:08:04.430
in order to be understood.

00:08:04.430 --> 00:08:07.680
Speech has been produced
to be perceived.

00:08:07.680 --> 00:08:11.460
And over the millennia
of the human evolution,

00:08:11.460 --> 00:08:14.010
it evolved this way
so that it reflects

00:08:14.010 --> 00:08:16.190
properties of human hearing.

00:08:16.190 --> 00:08:18.620
And so I'm very
much also with Josh.

00:08:18.620 --> 00:08:21.050
If you build up a machine
which recognizes speech,

00:08:21.050 --> 00:08:24.420
you may be verifying some of the
theories of speech perception.

00:08:24.420 --> 00:08:28.230
And I'll point out
that along the way.

00:08:28.230 --> 00:08:32.929
How do I know that the speech
evolved to fit the hearing

00:08:32.929 --> 00:08:34.169
and not the other way around?

00:08:34.169 --> 00:08:37.460
I got some big people arguing
over that, because they say,

00:08:37.460 --> 00:08:40.280
you don't know, I mean,
basically, but I know.

00:08:40.280 --> 00:08:40.970
I think.

00:08:40.970 --> 00:08:43.760
Well, I think that
I know, right?

00:08:43.760 --> 00:08:47.000
Every single organ which is
used for speech production

00:08:47.000 --> 00:08:49.780
is also used for something
much more useful,

00:08:49.780 --> 00:08:52.560
like, sort of typically,
eating and breathing.

00:08:52.560 --> 00:08:57.440
So this is the organs of speech
production-- lungs, the lips,

00:08:57.440 --> 00:09:03.470
teeth, nose, and velum,
and so on, and so on.

00:09:03.470 --> 00:09:07.740
Everything is being used for
some life-sustaining functions,

00:09:07.740 --> 00:09:08.970
including speaking.

00:09:08.970 --> 00:09:12.825
So I know that it's not
the same in hearing.

00:09:12.825 --> 00:09:15.330
Hearing has evolved
to hear, for hearing.

00:09:15.330 --> 00:09:18.290
Maybe there are some
organs of balance,

00:09:18.290 --> 00:09:20.870
and that sort of thing,
but mostly, you do hear.

00:09:20.870 --> 00:09:23.780
In the speech, everything,
what is being used,

00:09:23.780 --> 00:09:25.520
has been used for--

00:09:25.520 --> 00:09:27.310
it's used for
something else also,

00:09:27.310 --> 00:09:30.590
so clearly, we just
learned how to speak

00:09:30.590 --> 00:09:33.530
because we had the
appropriate hardware there,

00:09:33.530 --> 00:09:35.630
and we learned how to use it.

00:09:35.630 --> 00:09:42.410
So in order to get
the message, you

00:09:42.410 --> 00:09:44.030
use some cognitive
aspects, which

00:09:44.030 --> 00:09:45.360
I won't be talking much about.

00:09:45.360 --> 00:09:47.750
So you have to use
the common language.

00:09:47.750 --> 00:09:50.390
You have to have some
context of the conversation.

00:09:50.390 --> 00:09:53.120
You have to have some
common set of priors,

00:09:53.120 --> 00:09:56.450
some common experience, and
so on, and so on, but mainly

00:09:56.450 --> 00:09:58.430
what I will be
talking about, you

00:09:58.430 --> 00:10:02.210
need the reliable signal
which carries the message,

00:10:02.210 --> 00:10:05.620
because the message
is in the signal.

00:10:05.620 --> 00:10:07.760
It's also in your
head, but the signal

00:10:07.760 --> 00:10:11.240
supports what is
happening in your head.

00:10:11.240 --> 00:10:14.370
So how much information
is in speech signal?

00:10:14.370 --> 00:10:18.690
This is, I have stolen I believe
from George Miller, I think.

00:10:18.690 --> 00:10:21.080
So if you look at
Shannon's theory,

00:10:21.080 --> 00:10:24.860
I mean, there will be about
80 kilobytes per second.

00:10:24.860 --> 00:10:27.980
And indeed, you can
generate a reasonable signal

00:10:27.980 --> 00:10:30.030
without being very
smart about it

00:10:30.030 --> 00:10:34.220
just by coding it to 11 bits
at 8 kilohertz per second, 80

00:10:34.220 --> 00:10:35.240
kilobits per second.

00:10:35.240 --> 00:10:36.710
This verifies it.

00:10:36.710 --> 00:10:40.160
So this is how much information
might be in the signal.

00:10:40.160 --> 00:10:42.620
How much is in the
speech is actually very--

00:10:42.620 --> 00:10:46.010
it's, sort of, not very
clear, but at least we

00:10:46.010 --> 00:10:48.590
can estimate it to some extent.

00:10:48.590 --> 00:10:52.220
If you say, I would like to
transcribe the signal in terms

00:10:52.220 --> 00:10:55.250
of the speech sounds,
phonemes, so there is maybe

00:10:55.250 --> 00:11:01.070
about 40 to 49 phonemes,
or something, 41 phonemes.

00:11:01.070 --> 00:11:03.350
So if you look at
the entropy of that,

00:11:03.350 --> 00:11:05.970
it comes to about
80 bits per second.

00:11:05.970 --> 00:11:08.840
So there is three orders
of magnitude difference.

00:11:08.840 --> 00:11:11.610
If you push a
little bit further--

00:11:11.610 --> 00:11:16.430
indeed, I mean, if you speak
with about 150,000 words, that

00:11:16.430 --> 00:11:19.490
means about 80 bits,
30 words per minute,

00:11:19.490 --> 00:11:23.390
again, it comes to
less than 100 bits.

00:11:23.390 --> 00:11:25.130
So there's, as I
said, there's a number

00:11:25.130 --> 00:11:28.340
of ways how you can argue about
this amount of information.

00:11:28.340 --> 00:11:31.300
If you start thinking about
dependencies between phonemes,

00:11:31.300 --> 00:11:35.300
it can go as low as
10, 20 bits per second.

00:11:35.300 --> 00:11:39.980
No question that there is much
more information in the signal

00:11:39.980 --> 00:11:43.830
than it is in useful message
which we would like to get out.

00:11:43.830 --> 00:11:46.520
And we'll get into that.

00:11:46.520 --> 00:11:50.090
Because what is in the
message, there is not only

00:11:50.090 --> 00:11:52.040
information about the
message, but there

00:11:52.040 --> 00:11:54.230
is a lot of other information.

00:11:54.230 --> 00:11:58.808
There's information about health
of the speaker, about which

00:11:58.808 --> 00:12:00.620
language the speaker
is using, what

00:12:00.620 --> 00:12:03.380
are-- what emotions,
there is who is speaking,

00:12:03.380 --> 00:12:07.190
speaker-dependent
information, what is the mood,

00:12:07.190 --> 00:12:09.030
and so on, and so on.

00:12:09.030 --> 00:12:11.360
And there is a lot
of noise coming

00:12:11.360 --> 00:12:14.030
from around, reverberations.

00:12:14.030 --> 00:12:16.610
We talk about it quite
a lot in the morning,

00:12:16.610 --> 00:12:18.080
all kinds of other noises.

00:12:18.080 --> 00:12:21.350
So what I call
noise in general, I

00:12:21.350 --> 00:12:23.430
call everything
what we don't want

00:12:23.430 --> 00:12:26.240
besides the signal, which,
in speech recognition,

00:12:26.240 --> 00:12:27.960
is the message.

00:12:27.960 --> 00:12:30.020
So when I talk
about the noise, it

00:12:30.020 --> 00:12:34.070
can be information about who is
speaking, about their emotions,

00:12:34.070 --> 00:12:37.020
about the fact that
maybe my voice is going,

00:12:37.020 --> 00:12:38.030
and so on, and so on.

00:12:42.020 --> 00:12:44.860
To my mind, purpose
of perception is

00:12:44.860 --> 00:12:50.340
get the information
which carries--

00:12:50.340 --> 00:12:54.520
get the signal which carries
the desired information

00:12:54.520 --> 00:12:58.450
and suppress the noise,
eliminate the noise.

00:12:58.450 --> 00:13:01.130
So the purpose of perception,
being a little bit vulgar

00:13:01.130 --> 00:13:05.620
about it, is how to get rid
of most of the information

00:13:05.620 --> 00:13:09.640
very quickly, because otherwise,
your brain would go bananas.

00:13:09.640 --> 00:13:12.970
So you basically want to focus
on what you want to hear,

00:13:12.970 --> 00:13:17.480
and you want to ignore, if
possible, everything else.

00:13:17.480 --> 00:13:19.690
And it's not, of course,
easy, but we discuss that

00:13:19.690 --> 00:13:23.540
again in the morning, about some
techniques how to go about it.

00:13:23.540 --> 00:13:26.350
And I will mention a
few more techniques

00:13:26.350 --> 00:13:28.960
which we are working on.

00:13:28.960 --> 00:13:33.100
But this a key thing,
is, purpose of perception

00:13:33.100 --> 00:13:37.011
is to get what you need and
not to get what you don't need,

00:13:37.011 --> 00:13:39.010
because otherwise, your
brain would be too busy.

00:13:41.980 --> 00:13:44.710
Speech happens in many--
it's a very simple example.

00:13:44.710 --> 00:13:47.380
Speech happens in many,
many environments.

00:13:47.380 --> 00:13:50.300
And there is a lot of
stuff happening around it,

00:13:50.300 --> 00:13:52.340
so the very simple
example, which I actually

00:13:52.340 --> 00:13:55.310
used when I was giving a talk to
some grandmothers in the Czech

00:13:55.310 --> 00:13:58.580
Republic is that, what
you can already use

00:13:58.580 --> 00:14:02.270
is the fact that things
happen at different levels.

00:14:02.270 --> 00:14:05.810
And they happen at
different frequencies,

00:14:05.810 --> 00:14:09.140
so perception is selective.

00:14:09.140 --> 00:14:12.190
Every perceptual mode is
selective and attends only

00:14:12.190 --> 00:14:15.290
to part of the world.

00:14:15.290 --> 00:14:17.210
You know, we don't
hear the radio--

00:14:17.210 --> 00:14:18.740
we don't see the radio waves.

00:14:18.740 --> 00:14:21.080
And we don't hear
the ultrasound,

00:14:21.080 --> 00:14:25.830
and so does all the lower
elements, and so on, and so on.

00:14:25.830 --> 00:14:27.390
So there are
different frequencies,

00:14:27.390 --> 00:14:30.470
different sound intensities
are in the first approximation.

00:14:30.470 --> 00:14:31.580
This is what you may use.

00:14:31.580 --> 00:14:34.857
If something is too
weak, I don't care.

00:14:34.857 --> 00:14:36.440
If something has too
high frequencies,

00:14:36.440 --> 00:14:38.930
I don't care, and
so on, and so on.

00:14:38.930 --> 00:14:41.960
There are also different
spectral and temporal dynamics

00:14:41.960 --> 00:14:45.260
to speech, which we are
learning about that quite a lot.

00:14:45.260 --> 00:14:48.095
It happens at different
locations of the space.

00:14:48.095 --> 00:14:53.010
Again, this is the reason why
we have a spatial directivity.

00:14:53.010 --> 00:14:54.870
That's why we have two ears.

00:14:54.870 --> 00:14:57.620
That's why we have a
specifically-shaped ears,

00:14:57.620 --> 00:14:59.060
and so on, and so on.

00:14:59.060 --> 00:15:01.540
There are also other
cognitive aspects,

00:15:01.540 --> 00:15:03.610
I mean, sort of, like,
the selective attention.

00:15:03.610 --> 00:15:05.510
Again, we talk about
it, that people

00:15:05.510 --> 00:15:09.230
appear to be able to
modify the properties

00:15:09.230 --> 00:15:11.135
of your cognitive
processing depending

00:15:11.135 --> 00:15:12.660
on what you want to listen to.

00:15:12.660 --> 00:15:16.460
And my friend Nima
Mesgarani with Eddie Chang,

00:15:16.460 --> 00:15:19.010
who was supposed to
be here instead of me,

00:15:19.010 --> 00:15:22.210
just had a major paper in Nature
about that, and so on, and so

00:15:22.210 --> 00:15:22.710
on.

00:15:22.710 --> 00:15:26.200
There's a number of ways how
we can modify the selectivity.

00:15:26.200 --> 00:15:29.490
We talk about this sharpening
the cochlear filters,

00:15:29.490 --> 00:15:32.180
I mean, depending on the
signal from the brain.

00:15:32.180 --> 00:15:35.000
So speech happens like
this, start with a message.

00:15:35.000 --> 00:15:38.130
You have a linguistic code,
maybe 50 bits per second.

00:15:38.130 --> 00:15:39.380
There are some motor controls.

00:15:39.380 --> 00:15:42.110
Speech production comes
to a speech signal,

00:15:42.110 --> 00:15:46.850
which has three orders of
magnitude larger information

00:15:46.850 --> 00:15:47.690
content.

00:15:47.690 --> 00:15:49.880
Through speech perception
and cognitive processes,

00:15:49.880 --> 00:15:52.040
we get, somehow, back
to the linguistic code

00:15:52.040 --> 00:15:55.070
and extract the
message, so this is

00:15:55.070 --> 00:15:58.060
important-- from the small,
low bit-rate, to high bit-rate,

00:15:58.060 --> 00:16:00.200
to the low bit-rate.

00:16:00.200 --> 00:16:01.790
In production,
actually, I don't want

00:16:01.790 --> 00:16:04.780
to pretend it happens
in such a linear way.

00:16:04.780 --> 00:16:07.070
There are also
feedbacks, so there

00:16:07.070 --> 00:16:10.610
is a feedback from you listen to
yourself when you are speaking.

00:16:10.610 --> 00:16:13.060
You can control how you speak.

00:16:13.060 --> 00:16:14.810
And you can also
actually change the code,

00:16:14.810 --> 00:16:16.640
because you realize,
oh, I should have

00:16:16.640 --> 00:16:19.040
said it somehow differently.

00:16:19.040 --> 00:16:23.170
In speech perception, again, we
just talked about it, you can,

00:16:23.170 --> 00:16:24.950
if the message is
not getting through,

00:16:24.950 --> 00:16:28.020
you may be able to tune
the system in some ways.

00:16:28.020 --> 00:16:31.610
You may be changing
the things, you know?

00:16:31.610 --> 00:16:34.820
And you may also use the
very mechanical techniques,

00:16:34.820 --> 00:16:38.360
as I told you, close the
window, or walk away.

00:16:38.360 --> 00:16:41.970
There is also feedback
through the dialogue,

00:16:41.970 --> 00:16:43.790
so from-- between
message and message,

00:16:43.790 --> 00:16:45.980
depending what
I'm hearing, I may

00:16:45.980 --> 00:16:47.960
be asking a different
kind of question,

00:16:47.960 --> 00:16:50.900
so which also modifies
the message of the sender.

00:16:54.030 --> 00:16:56.290
How do we produce speech?

00:16:56.290 --> 00:16:58.510
So we speak in order
to be heard, in order

00:16:58.510 --> 00:16:59.725
to be understood.

00:16:59.725 --> 00:17:02.710
So very quickly,
I want to go back

00:17:02.710 --> 00:17:05.440
to something which people
already forgot a big way, which

00:17:05.440 --> 00:17:06.730
is Homer Dudley.

00:17:06.730 --> 00:17:09.410
He was a great researcher
at Bell Laboratories

00:17:09.410 --> 00:17:10.810
before the Second World War.

00:17:10.810 --> 00:17:13.950
He retired I think
sometime early in '50s.

00:17:13.950 --> 00:17:16.030
He passed away in the '60s.

00:17:16.030 --> 00:17:20.579
He was saying message is in the
movements of the vocal tract

00:17:20.579 --> 00:17:24.069
which modulates the carrier,
so message in the speech

00:17:24.069 --> 00:17:26.020
is not in fundamental
frequency, it's

00:17:26.020 --> 00:17:28.420
not the way you are
exciting your vocal tract.

00:17:28.420 --> 00:17:32.860
Message is how you shape the
organs of speech production.

00:17:32.860 --> 00:17:37.270
Proof for that is that you
can whisper and you can still

00:17:37.270 --> 00:17:39.700
understand, so you don't--

00:17:39.700 --> 00:17:41.980
how you excite the vocal
tract is secondary.

00:17:41.980 --> 00:17:47.900
How do you generate this
audible carrier is secondary.

00:17:47.900 --> 00:17:51.190
You know, you can use
the artificial larynx,

00:17:51.190 --> 00:17:54.160
so there is this idea,
there's a message.

00:17:54.160 --> 00:17:55.420
A message is being--

00:17:55.420 --> 00:18:00.790
goes through modulator into
carrier, comes out as speech.

00:18:00.790 --> 00:18:03.970
So this modulation actually
has been used a long time ago,

00:18:03.970 --> 00:18:06.710
and excuse me for being maybe
a little bit simplistic,

00:18:06.710 --> 00:18:08.920
but it's actually, in
some ways, interesting.

00:18:08.920 --> 00:18:13.030
So this was the speech
production mechanism

00:18:13.030 --> 00:18:17.200
which was developed in some
time in the 18th century

00:18:17.200 --> 00:18:21.760
by the guy Johannes Wolfgang
Ritter von Kempelen.

00:18:21.760 --> 00:18:24.100
And he actually had it right.

00:18:24.100 --> 00:18:26.050
The only problem is
nobody trusted him,

00:18:26.050 --> 00:18:29.170
because he also invented
Mechanical Turk, which

00:18:29.170 --> 00:18:30.970
was playing the chess.

00:18:30.970 --> 00:18:32.680
And so he was
caught as a cheater,

00:18:32.680 --> 00:18:35.830
so when he was showing
his synthesizer,

00:18:35.830 --> 00:18:37.450
nobody believed him.

00:18:37.450 --> 00:18:42.220
But anyways, he was
definitely a smart guy.

00:18:42.220 --> 00:18:45.570
So he used already the
principle which is now used.

00:18:45.570 --> 00:18:49.930
This is a linear model of speech
production developed actually

00:18:49.930 --> 00:18:51.790
before the Second World
War, really, again,

00:18:51.790 --> 00:18:54.280
Bell Laboratories
should get the credit.

00:18:54.280 --> 00:19:00.890
I believe this is stolen
from Dudley's paper.

00:19:00.890 --> 00:19:03.680
So there is a source,
and you can change it.

00:19:03.680 --> 00:19:06.700
It periodic signals
out random noise,

00:19:06.700 --> 00:19:10.540
if you are producing voice
signal or unvoice signal.

00:19:10.540 --> 00:19:13.190
And then there is
a resonance control

00:19:13.190 --> 00:19:17.320
which goes into amplifier,
and it produces the speech.

00:19:17.320 --> 00:19:20.110
So this is the key here,
this a key to the point

00:19:20.110 --> 00:19:23.410
that Dudley developed
for this called a voder.

00:19:23.410 --> 00:19:26.710
And he trained the lady who
spent a year or something

00:19:26.710 --> 00:19:27.370
to play it.

00:19:27.370 --> 00:19:29.650
It was played like a organ.

00:19:29.650 --> 00:19:31.990
And she was changing
the resonance properties

00:19:31.990 --> 00:19:33.715
of this system here.

00:19:33.715 --> 00:19:40.060
And she was creating excitation
by pushing on a pitch pedal

00:19:40.060 --> 00:19:42.220
and switching on the big--

00:19:42.220 --> 00:19:44.110
on the wrist bar.

00:19:44.110 --> 00:19:48.730
And if it works well, we may
even be able to make the sound.

00:19:48.730 --> 00:19:50.878
This is a test.

00:19:50.878 --> 00:19:52.790
[AUDIO PLAYBACK]

00:19:52.790 --> 00:19:55.665
- Will you please make the voder
say, for our Eastern listeners,

00:19:55.665 --> 00:19:56.540
good evening, Radio--

00:19:56.540 --> 00:19:56.982
HYNEK HERMANSKY:
This is a real--

00:19:56.982 --> 00:19:57.125
- --audience.

00:19:57.125 --> 00:19:58.208
HYNEK HERMANSKY: --speech.

00:19:58.208 --> 00:20:00.687
- Good evening, radio audience.

00:20:00.687 --> 00:20:01.770
HYNEK HERMANSKY: This is--

00:20:01.770 --> 00:20:03.780
- And now, for our
Western listeners,

00:20:03.780 --> 00:20:06.731
say, good afternoon,
radio audience.

00:20:06.731 --> 00:20:10.460
- Good afternoon,
radio audience.

00:20:10.460 --> 00:20:11.460
[END PLAYBACK]

00:20:11.460 --> 00:20:12.960
HYNEK HERMANSKY:
Good enough, right?

00:20:12.960 --> 00:20:17.080
I mean, sort of--
so already, 1940s,

00:20:17.080 --> 00:20:20.850
This was the demonstrated
at a trade fair.

00:20:20.850 --> 00:20:24.790
And the lady was trained so
well that, in the '50s, when

00:20:24.790 --> 00:20:27.090
Dudley was retiring,
they brought her in.

00:20:27.090 --> 00:20:29.434
She was already retired
a long time ago.

00:20:29.434 --> 00:20:30.600
And she still could play it.

00:20:34.250 --> 00:20:35.480
How the speech works--

00:20:35.480 --> 00:20:38.090
I mean, maybe-- oh, I wanted
to jump this, but anyways,

00:20:38.090 --> 00:20:39.800
let's go very
quickly through that.

00:20:39.800 --> 00:20:40.925
So this is a speech signal.

00:20:40.925 --> 00:20:43.070
This is a acoustic signal.

00:20:43.070 --> 00:20:46.250
It changes in-- this is a
sinusoid, high pressure,

00:20:46.250 --> 00:20:48.530
low pressure, high
pressure, low pressure.

00:20:48.530 --> 00:20:52.220
If you put somewhere
in the in the path,

00:20:52.220 --> 00:20:56.010
some barrier, what happens is
you generate a standing wave.

00:20:56.010 --> 00:21:00.280
A standing wave
stands in a space.

00:21:00.280 --> 00:21:02.780
And there are high pressures,
low pressures, high pressures,

00:21:02.780 --> 00:21:03.655
low pressures.

00:21:03.655 --> 00:21:06.140
And the frequency depends
on the frequency--

00:21:06.140 --> 00:21:08.990
I mean, the size of
this standing wave

00:21:08.990 --> 00:21:13.620
depends on the
frequency of the signal.

00:21:13.620 --> 00:21:17.680
So if I put it into
something like a vocal tract,

00:21:17.680 --> 00:21:18.960
which we have here--

00:21:18.960 --> 00:21:19.910
so this is a glottis.

00:21:19.910 --> 00:21:21.201
This is where it gets exciting.

00:21:21.201 --> 00:21:23.000
This is a very simple
model of vocal tract.

00:21:23.000 --> 00:21:24.740
And here I have a lips.

00:21:24.740 --> 00:21:28.970
So it takes certain time to
provide this through the tube.

00:21:28.970 --> 00:21:34.310
And the tube will have a maximum
velocity at certain point for--

00:21:34.310 --> 00:21:37.120
so that it will be
resonating in a quarter

00:21:37.120 --> 00:21:41.660
wavelength of the signal, 3/4 of
the wavelength of the signals,

00:21:41.660 --> 00:21:47.100
in 5/4 of the wavelength of the
signal, and so on, and so on.

00:21:47.100 --> 00:21:49.720
So we can compute
which frequencies

00:21:49.720 --> 00:21:53.780
this tube will be resonating.

00:21:53.780 --> 00:21:56.210
This is a very simplistic
way of producing speech,

00:21:56.210 --> 00:22:00.000
but you can generate reasonable
speech sounds with that.

00:22:00.000 --> 00:22:03.350
So if we start putting a
constriction there somewhere,

00:22:03.350 --> 00:22:06.530
which emulates the way,
very simple the way

00:22:06.530 --> 00:22:10.890
how we can speak by moving
the tongue against the palate

00:22:10.890 --> 00:22:13.700
or making of constrictions
in the speech--

00:22:13.700 --> 00:22:16.410
so when the tube
is open like this,

00:22:16.410 --> 00:22:19.556
it resonates at
500, 1,500, 2,500

00:22:19.556 --> 00:22:22.160
if the tube is 17
centimeters long,

00:22:22.160 --> 00:22:26.960
which is a typical length
for the vocal tract.

00:22:26.960 --> 00:22:29.570
So if I put a constriction
here, everything

00:22:29.570 --> 00:22:32.810
moves down because there is
such a thing like perturbation

00:22:32.810 --> 00:22:34.940
theory, which says
that, if you are putting

00:22:34.940 --> 00:22:38.930
a constriction through the point
of the maximum velocity, which

00:22:38.930 --> 00:22:42.760
is, of course, at the opening,
all the modes will go down.

00:22:42.760 --> 00:22:47.960
And as you go on, basically,
the whole thing keeps changing.

00:22:47.960 --> 00:22:51.320
The point is that, almost
in every position of the,

00:22:51.320 --> 00:22:54.530
say, this tongue, all
the resonance frequencies

00:22:54.530 --> 00:22:57.650
are changing, so the whole
spectrum is being affected.

00:22:57.650 --> 00:23:02.280
And that may become useful
to explain something later.

00:23:02.280 --> 00:23:03.350
But we go like this.

00:23:03.350 --> 00:23:08.035
At the end, you end up, again,
in the same frequencies.

00:23:08.035 --> 00:23:09.430
These are called nomograms.

00:23:09.430 --> 00:23:12.620
And they will be heavily
worked on at the Speech Group

00:23:12.620 --> 00:23:16.790
at MIT and at Stockholm.

00:23:16.790 --> 00:23:19.140
So you can see how the
formants are moving.

00:23:19.140 --> 00:23:22.210
And you can see that, for every
position of the [INAUDIBLE],,

00:23:22.210 --> 00:23:25.670
here we have a distance of a
constriction from the lips.

00:23:25.670 --> 00:23:29.840
For every position, we are
having all the formants moving,

00:23:29.840 --> 00:23:34.160
so information about what I'm
doing with my vocal organs

00:23:34.160 --> 00:23:37.100
is actually at all frequencies,
all audible frequencies

00:23:37.100 --> 00:23:39.660
in different ways, but
it's there everywhere.

00:23:39.660 --> 00:23:42.980
It's not a single frequency
which would carry information

00:23:42.980 --> 00:23:44.360
about something.

00:23:44.360 --> 00:23:48.130
All the audible frequencies
carry information about speech.

00:23:48.130 --> 00:23:49.150
That's important.

00:23:49.150 --> 00:23:51.230
You can also look at
it and you can say,

00:23:51.230 --> 00:23:52.620
you know, what is the--

00:23:52.620 --> 00:23:55.140
where the front
cavity resonates,

00:23:55.140 --> 00:23:56.590
the back cavity resonates.

00:23:56.590 --> 00:23:58.190
Again, this front
cavity resonance

00:23:58.190 --> 00:24:01.900
may become interesting a little
bit later if we get to that.

00:24:01.900 --> 00:24:05.720
But this is a very simplistic
model of the speech production,

00:24:05.720 --> 00:24:12.380
but pretty much contains all the
basic elements of the speech.

00:24:12.380 --> 00:24:16.370
Point here is that, depending on
the length of the vocal tract,

00:24:16.370 --> 00:24:18.650
even when you keep the
constriction at the same

00:24:18.650 --> 00:24:22.070
position-- this is how long
is this front part before

00:24:22.070 --> 00:24:23.810
the construction is--

00:24:23.810 --> 00:24:25.670
so all the resonances
are moving,

00:24:25.670 --> 00:24:28.860
but a shorter vocal tract, like
the children's vocal tract,

00:24:28.860 --> 00:24:31.730
or even in a number of
females, they typically

00:24:31.730 --> 00:24:34.940
have a shorter vocal
tract than the males,

00:24:34.940 --> 00:24:37.340
there's a different
number of resonances.

00:24:37.340 --> 00:24:39.500
So if somebody is telling
you the information

00:24:39.500 --> 00:24:42.830
is in the formants of
speech, question it,

00:24:42.830 --> 00:24:49.340
because it's actually impossible
to generate the same speech

00:24:49.340 --> 00:24:52.340
being two different
people, especially

00:24:52.340 --> 00:24:54.690
having two different
lengths of the vocal tract.

00:24:54.690 --> 00:25:00.020
And we get into it when we talk
about the speaker dependencies.

00:25:00.020 --> 00:25:03.420
Second part is-- of the
equation is hearing.

00:25:03.420 --> 00:25:05.870
So we speak in order
to be heard, in order

00:25:05.870 --> 00:25:07.080
to be understood.

00:25:07.080 --> 00:25:12.740
And again, thanks to Josh, he
spent more than sufficient time

00:25:12.740 --> 00:25:15.380
explaining you enough
what I wanted to say.

00:25:15.380 --> 00:25:18.350
I will just add something--
some very, very small things.

00:25:18.350 --> 00:25:21.040
So just to summarize,
Josh was telling you

00:25:21.040 --> 00:25:24.880
the theory works basically
like a bank of bandpass filters

00:25:24.880 --> 00:25:28.220
with a changing
frequency and output

00:25:28.220 --> 00:25:30.530
depending on sound
level intensity.

00:25:30.530 --> 00:25:32.520
There are many
caveats to that, but I

00:25:32.520 --> 00:25:35.720
mean, in a first
approximation, I 100%

00:25:35.720 --> 00:25:38.070
agree this is enough
for us to follow

00:25:38.070 --> 00:25:40.280
for all the rest of the talk.

00:25:40.280 --> 00:25:42.780
Second thing which Josh
mentioned very briefly,

00:25:42.780 --> 00:25:47.750
but I want to stress it, because
it is important, firing rates--

00:25:47.750 --> 00:25:50.240
because you know the
cochlea communicates

00:25:50.240 --> 00:25:58.110
with the rest of the
system through the firings,

00:25:58.110 --> 00:26:00.170
through the impulses.

00:26:00.170 --> 00:26:02.810
Firing rates on
the auditory nerve

00:26:02.810 --> 00:26:07.170
are of the order of 1 kilohertz
every one millisecond.

00:26:07.170 --> 00:26:09.920
But as you go up and
up in the system,

00:26:09.920 --> 00:26:12.691
already here on the colliculus
is maybe order of magnitude

00:26:12.691 --> 00:26:13.190
less.

00:26:13.190 --> 00:26:15.620
And the order in the
level of auditory cortex

00:26:15.620 --> 00:26:19.280
is 2 orders of magnitude less.

00:26:19.280 --> 00:26:22.620
So of course, I mean, you know,
this is how the brain works.

00:26:22.620 --> 00:26:26.780
I mean, so here we have
from periphery up to cortex,

00:26:26.780 --> 00:26:29.320
but also, I think it was
mentioned very briefly,

00:26:29.320 --> 00:26:34.280
if you look at it, number of
neurons increase more than

00:26:34.280 --> 00:26:38.780
actually decrease in the firing
rates, because if we have--

00:26:38.780 --> 00:26:41.360
again, those are just
orders of magnitude--

00:26:41.360 --> 00:26:47.000
100,000 neurons maybe on
the level of auditory nerve,

00:26:47.000 --> 00:26:51.980
or colliculus nucleus, and you
have 100 million neurons maybe

00:26:51.980 --> 00:26:53.450
on the level of the brain.

00:26:53.450 --> 00:26:55.430
And this can become
handy later, when,

00:26:55.430 --> 00:26:58.170
if I get all the way
to the end of the talk,

00:26:58.170 --> 00:27:03.120
I will recall this
piece of information.

00:27:03.120 --> 00:27:05.450
Another thing which was
mentioned a number of times

00:27:05.450 --> 00:27:08.090
is that there are not the
only connections from ear,

00:27:08.090 --> 00:27:10.700
from the periphery to
the brain, but there

00:27:10.700 --> 00:27:13.010
is, by some estimates,
many, many more--

00:27:13.010 --> 00:27:15.290
I mean, again, I mean
the estimates vary,

00:27:15.290 --> 00:27:18.530
but this is something which I
have heard somewhere-- maybe

00:27:18.530 --> 00:27:20.750
there is maybe almost 10
times more connections

00:27:20.750 --> 00:27:23.900
going from brain to the ear
than from the ear to the brain.

00:27:23.900 --> 00:27:26.240
And typically, the
nature hardly ever

00:27:26.240 --> 00:27:28.050
builds anything
without a reason,

00:27:28.050 --> 00:27:30.110
so there must be
some reason for that.

00:27:30.110 --> 00:27:34.050
And perhaps we
will get into that.

00:27:34.050 --> 00:27:37.790
Josh didn't talk much about the
level of the-- on the cortex.

00:27:37.790 --> 00:27:41.610
So what's happening on the
lower levels, on the periphery?

00:27:41.610 --> 00:27:43.430
They are just these
simple increases

00:27:43.430 --> 00:27:48.410
of auditory-- of firing rate.

00:27:48.410 --> 00:27:51.830
There is a certain
enhancement of the changes.

00:27:51.830 --> 00:27:54.710
So at the beginning of the
tone-- this is a tone--

00:27:54.710 --> 00:27:56.990
the beginning of
the tone, there is

00:27:56.990 --> 00:27:58.850
more firing on auditory nerve.

00:27:58.850 --> 00:28:02.840
At the end of the tone,
there is some deflection.

00:28:02.840 --> 00:28:06.080
But when you look at a
higher level of the cortex,

00:28:06.080 --> 00:28:08.060
all these wonderful
curves, which

00:28:08.060 --> 00:28:10.010
are sort of increasing
with intensity

00:28:10.010 --> 00:28:13.720
like it would if you had
a simple bandpass filter,

00:28:13.720 --> 00:28:15.070
start looking quite differently.

00:28:15.070 --> 00:28:16.920
So we measure
majority-- what I heard,

00:28:16.920 --> 00:28:23.810
the majority of the
cortical neurons

00:28:23.810 --> 00:28:25.790
are selective to certain levels.

00:28:25.790 --> 00:28:28.460
Basically, the firing
increases to a certain level,

00:28:28.460 --> 00:28:30.350
and then it decreases again.

00:28:30.350 --> 00:28:34.970
And they are, of course,
selective at different levels.

00:28:34.970 --> 00:28:37.880
Also, I mean, you don't see,
just these simple things

00:28:37.880 --> 00:28:42.160
like here, that your firing
starts as a tone starts.

00:28:42.160 --> 00:28:45.740
But they are neurons like
that, but there are also

00:28:45.740 --> 00:28:48.470
neurons which just are
interested at the beginning

00:28:48.470 --> 00:28:49.580
of the signal.

00:28:49.580 --> 00:28:51.950
There are neurons which
are interested in beginning

00:28:51.950 --> 00:28:52.610
and ends.

00:28:52.610 --> 00:28:54.920
There are neurons which
are interested only

00:28:54.920 --> 00:28:59.280
at the ends of the signals,
and so on, and so on.

00:28:59.280 --> 00:29:02.690
Receptive fields, again, has
been mentioned already before.

00:29:05.360 --> 00:29:08.810
Just as we have a receptive
field in the visual cortex,

00:29:08.810 --> 00:29:11.660
we have also receptive
fields in auditory cortex.

00:29:11.660 --> 00:29:13.540
Here we don't have the--

00:29:13.540 --> 00:29:18.010
here we have a frequency and a
time, unlike x and y, receptive

00:29:18.010 --> 00:29:19.960
fields which are
typical, sort of,

00:29:19.960 --> 00:29:23.080
first thing you are
hearing about when you

00:29:23.080 --> 00:29:25.780
talk about visual perception.

00:29:25.780 --> 00:29:29.430
They come in all
kinds of colors.

00:29:29.430 --> 00:29:32.860
They tend to be
quite long, meaning

00:29:32.860 --> 00:29:35.790
they can be sensitive for
about quarter of the second--

00:29:35.790 --> 00:29:38.410
not all of them,
but certainly, there

00:29:38.410 --> 00:29:42.520
are many, many different
cortical receptive fields.

00:29:42.520 --> 00:29:45.970
So some people are
suggesting, given the richness

00:29:45.970 --> 00:29:49.150
of the neurons in
auditory cortex,

00:29:49.150 --> 00:29:51.310
it's a very legal
thing to suggest

00:29:51.310 --> 00:29:54.780
that maybe the sounds are
processing in following way,

00:29:54.780 --> 00:29:57.100
not only that you
do the frequency

00:29:57.100 --> 00:30:00.460
analysis in the cochlea, but
then, on the higher levels,

00:30:00.460 --> 00:30:05.650
you are creating many
pictures of the outside world.

00:30:05.650 --> 00:30:07.670
And then, of course, only
the question is here,

00:30:07.670 --> 00:30:10.600
if answer, this is Murray
Sachs' paper from their labs,

00:30:10.600 --> 00:30:14.500
from Johns Hopkins in 1988.

00:30:14.500 --> 00:30:16.270
They just simply said
pattern recognition,

00:30:16.270 --> 00:30:18.040
but I believe there
is a mechanism which

00:30:18.040 --> 00:30:21.730
picks up the best streams
and leaves out not so

00:30:21.730 --> 00:30:24.340
useful things, but
the concept was here

00:30:24.340 --> 00:30:27.320
around for a long time.

00:30:27.320 --> 00:30:30.130
So this was physiology 101.

00:30:30.130 --> 00:30:35.760
Psychophysics is saying that you
play the signals to listeners,

00:30:35.760 --> 00:30:38.390
and you ask them what they hear.

00:30:38.390 --> 00:30:41.140
But we want to know what is
the response of the organism

00:30:41.140 --> 00:30:44.860
to the incoming stimulus, so
simply, you play the stimulus

00:30:44.860 --> 00:30:46.781
and you ask what
is the response.

00:30:46.781 --> 00:30:49.280
First thing which you can ask,
do you hear something or not?

00:30:49.280 --> 00:30:52.020
And you already will discover
some interesting stuff.

00:30:52.020 --> 00:30:55.000
Hearing is not
equally-sensitive everywhere.

00:30:55.000 --> 00:30:56.350
It's selective.

00:30:56.350 --> 00:30:58.210
And it's more
sensitive in the area

00:30:58.210 --> 00:31:02.050
somewhere between
1 and 4 kilohertz.

00:31:02.050 --> 00:31:04.690
It's much less sensitive
at the lower frequencies.

00:31:04.690 --> 00:31:06.170
This is a threshold.

00:31:06.170 --> 00:31:07.466
On the threshold level--

00:31:07.466 --> 00:31:08.840
here's another
interesting thing.

00:31:08.840 --> 00:31:18.370
If you just apply the signals
in different ears, as long

00:31:18.370 --> 00:31:21.520
as the signals happen within a
certain period, about a couple

00:31:21.520 --> 00:31:23.760
of hundred millisecond,
and if couple of hundred

00:31:23.760 --> 00:31:27.250
millisecond you hear from
your ear would be more often,

00:31:27.250 --> 00:31:28.420
the thresholds are half.

00:31:28.420 --> 00:31:30.010
Basically, neither
of these signals

00:31:30.010 --> 00:31:32.710
would be heard if you
applied only a single one,

00:31:32.710 --> 00:31:37.990
but when, if you apply both of
them, basically you hear them.

00:31:37.990 --> 00:31:41.560
If you play the signals
of different frequencies,

00:31:41.560 --> 00:31:43.260
if these signals
are close enough,

00:31:43.260 --> 00:31:46.890
close so that, as Josh
mentioned about the beats,

00:31:46.890 --> 00:31:48.640
they happen within
one critical band,

00:31:48.640 --> 00:31:52.390
again, neither blue
or green signal

00:31:52.390 --> 00:31:54.110
would be heard on its own.

00:31:54.110 --> 00:31:56.440
But if you play them
together, you hear them.

00:31:56.440 --> 00:32:00.160
But if they are further in
frequency, you don't hear them.

00:32:00.160 --> 00:32:03.190
Same thing if these guys
are further in time,

00:32:03.190 --> 00:32:04.360
you wouldn't hear them.

00:32:04.360 --> 00:32:07.000
So this subthreshold
perception actually

00:32:07.000 --> 00:32:08.170
is kind of interesting.

00:32:08.170 --> 00:32:09.690
And we will use it.

00:32:09.690 --> 00:32:11.680
Which we didn't
talk much about is

00:32:11.680 --> 00:32:15.160
that there are obvious
ways how you can modify

00:32:15.160 --> 00:32:17.610
the threshold of hearing.

00:32:17.610 --> 00:32:18.789
Here we have a target.

00:32:18.789 --> 00:32:20.830
And since it is higher
than threshold of hearing,

00:32:20.830 --> 00:32:22.010
you hear it.

00:32:22.010 --> 00:32:25.030
But if you play another
sound called masker,

00:32:25.030 --> 00:32:27.640
you will not hear it, because
your threshold basically

00:32:27.640 --> 00:32:28.290
is modified.

00:32:28.290 --> 00:32:30.370
It's called the mask threshold.

00:32:30.370 --> 00:32:32.440
And this part is suddenly not--

00:32:32.440 --> 00:32:34.680
this target is not heard.

00:32:34.680 --> 00:32:37.060
The target can be
something useful,

00:32:37.060 --> 00:32:39.850
but in mp3, it can
be pretty annoying,

00:32:39.850 --> 00:32:41.530
because it's typically noise.

00:32:41.530 --> 00:32:43.960
You try to figure
out how you can mask

00:32:43.960 --> 00:32:45.810
the noise by the useful signal.

00:32:45.810 --> 00:32:49.940
You're computing these
masked thresholds on the fly.

00:32:49.940 --> 00:32:52.210
The initial
experiment with this,

00:32:52.210 --> 00:32:54.950
what is called simultaneous
masking, was following,

00:32:54.950 --> 00:32:58.970
and, again, was Bell Labs,
Fletcher, and his people.

00:32:58.970 --> 00:33:01.060
They would figure out
what is the threshold

00:33:01.060 --> 00:33:02.960
of certain frequency
without the noise.

00:33:02.960 --> 00:33:04.990
But then they would
put noise around it,

00:33:04.990 --> 00:33:08.260
and the threshold had to go
up, because there was a noise,

00:33:08.260 --> 00:33:10.300
so there was masking.

00:33:10.300 --> 00:33:14.627
Then they made a broader noise,
and threshold was going up,

00:33:14.627 --> 00:33:15.460
as you would expect.

00:33:15.460 --> 00:33:19.420
There was more noise, so you
had to make the signal stronger.

00:33:19.420 --> 00:33:21.700
And you made it to
a certain point,

00:33:21.700 --> 00:33:24.940
when you start making the
band of noise too wide,

00:33:24.940 --> 00:33:27.440
suddenly it's not
happening anymore.

00:33:27.440 --> 00:33:29.710
There is no more
masking anymore.

00:33:29.710 --> 00:33:32.770
That's how they came with
this concept of critical band.

00:33:32.770 --> 00:33:37.490
Critical band is what happens
inside the critical band

00:33:37.490 --> 00:33:43.900
matters, basically, influences
the decoding of the signal

00:33:43.900 --> 00:33:45.040
within a critical band.

00:33:45.040 --> 00:33:48.580
But if it happens outside the
critical band, it doesn't.

00:33:48.580 --> 00:33:51.970
So essentially, if the signals
are far away in frequency,

00:33:51.970 --> 00:33:53.746
they don't interact
with each other.

00:33:53.746 --> 00:33:55.120
And again, this
is a useful thing

00:33:55.120 --> 00:33:56.440
for speech recognition people.

00:33:56.440 --> 00:34:01.000
They didn't much realize
that this is the main outcome

00:34:01.000 --> 00:34:02.990
of the masking.

00:34:02.990 --> 00:34:05.870
Critical bands, I mean, again,
I mean, discussions are here,

00:34:05.870 --> 00:34:09.670
but this is a Bark scale which
has been developed in Germany

00:34:09.670 --> 00:34:11.780
by Zwicker and his colleagues.

00:34:11.780 --> 00:34:16.600
It's pretty much regarded to be
from about 600, 700 hertz up.

00:34:16.600 --> 00:34:24.070
And it's approximately
constant up to 600, 700 hertz.

00:34:24.070 --> 00:34:26.549
If you talk to Cambridge people,
Brian Moore, and that sort

00:34:26.549 --> 00:34:28.215
of logarithmic it's
pretty much regarded

00:34:28.215 --> 00:34:30.000
to be pretty much everywhere.

00:34:30.000 --> 00:34:34.000
But not really, but
the critical bands,

00:34:34.000 --> 00:34:37.030
remember, critical bands
from the subthreshold things?

00:34:37.030 --> 00:34:38.740
Again, the critical
band is masking.

00:34:38.740 --> 00:34:40.389
It's starting it
with things happen

00:34:40.389 --> 00:34:42.010
within the critical band.

00:34:42.010 --> 00:34:43.239
They integrate.

00:34:43.239 --> 00:34:44.800
They happen outside the--

00:34:44.800 --> 00:34:46.900
each of them outside
the critical band,

00:34:46.900 --> 00:34:48.590
they don't interact.

00:34:48.590 --> 00:34:52.030
Another masking is
temporal masking.

00:34:52.030 --> 00:34:53.600
So you have a signal--
and of course,

00:34:53.600 --> 00:34:58.010
if you put a mask on it,
it's simultaneous masking.

00:34:58.010 --> 00:34:59.465
You have to make it much--

00:34:59.465 --> 00:35:02.470
the signal much stronger in
order for you to hear it.

00:35:02.470 --> 00:35:05.260
But it also influences
things in time.

00:35:05.260 --> 00:35:07.210
This is what is called
forward masking.

00:35:07.210 --> 00:35:09.460
And this is the one which
is probably more interesting

00:35:09.460 --> 00:35:12.830
and more useful.

00:35:12.830 --> 00:35:15.580
It's also backward masking,
when a masker happens

00:35:15.580 --> 00:35:18.699
after the signal,
but it probably

00:35:18.699 --> 00:35:20.490
has a different origin,
more like cognitive

00:35:20.490 --> 00:35:23.230
rather than earlier.

00:35:23.230 --> 00:35:24.440
So there is still a masker.

00:35:24.440 --> 00:35:28.660
You have to make the signal
stronger up to a certain point.

00:35:28.660 --> 00:35:32.560
When the distance between
masker and the signal

00:35:32.560 --> 00:35:35.260
is more than 200
milliseconds, there

00:35:35.260 --> 00:35:36.990
is like there's
no masker anymore.

00:35:36.990 --> 00:35:39.250
Basically, there is no
temporal masking anymore,

00:35:39.250 --> 00:35:43.820
but it is within this
interval of 200 milliseconds.

00:35:43.820 --> 00:35:47.040
If you make mask stronger,
masking is stronger initially,

00:35:47.040 --> 00:35:49.070
but it also decays faster.

00:35:49.070 --> 00:35:53.140
And again, it decays after
about 200 milliseconds.

00:35:53.140 --> 00:35:57.170
So whatever happens outside
this critical interval,

00:35:57.170 --> 00:35:59.980
about a couple of
hundred millisecond,

00:35:59.980 --> 00:36:01.570
it doesn't integrate.

00:36:01.570 --> 00:36:04.560
But if it happens inside
this critical interval,

00:36:04.560 --> 00:36:06.400
that seems to be influencing--

00:36:06.400 --> 00:36:10.120
these signals seem to be
influencing each other.

00:36:10.120 --> 00:36:11.620
And again, I mean,
you know, I talk

00:36:11.620 --> 00:36:14.620
about the subthreshold
perception--

00:36:14.620 --> 00:36:18.220
if there were two tones which
happen within 200 millisecond,

00:36:18.220 --> 00:36:20.540
neither of them would
be heard in isolation,

00:36:20.540 --> 00:36:24.350
but they are heard if
you play them together.

00:36:24.350 --> 00:36:26.290
Another part which is
kind of interesting

00:36:26.290 --> 00:36:28.730
is that loudness
depends, of course,

00:36:28.730 --> 00:36:31.660
on the intensity of the sound,
but it doesn't depend linearly

00:36:31.660 --> 00:36:32.440
on that.

00:36:32.440 --> 00:36:35.120
It depends with
about cubic root,

00:36:35.120 --> 00:36:38.080
so in order to make a
signal twice as loud,

00:36:38.080 --> 00:36:40.510
you have to make it
about 10 times more

00:36:40.510 --> 00:36:43.190
in intensity for
stimuli which are longer

00:36:43.190 --> 00:36:46.710
than 200 milliseconds.

00:36:46.710 --> 00:36:50.440
Equal loudness curves,
this is a threshold curve,

00:36:50.440 --> 00:36:55.830
but these equal loudness
curve are telling you what

00:36:55.830 --> 00:36:58.260
the intensity of
the sound-- sorry--

00:36:58.260 --> 00:37:02.100
would need to be in order
to hear it equally loud.

00:37:02.100 --> 00:37:06.080
So it's saying that, if you have
a 40 dB signal at 1 kilohertz,

00:37:06.080 --> 00:37:08.750
and you want to make it
equally loud at 100 hertz,

00:37:08.750 --> 00:37:13.800
you have to make it
60 dB, and so on.

00:37:13.800 --> 00:37:16.805
These curves become flatter
and flatter, most pronounced

00:37:16.805 --> 00:37:19.470
at the threshold at lower
levels, but they are there.

00:37:19.470 --> 00:37:22.410
And they are actually kind
of interesting and important.

00:37:22.410 --> 00:37:25.410
Hearing is rather non-linear.

00:37:25.410 --> 00:37:28.915
Properties depend
on the intensity.

00:37:28.915 --> 00:37:31.290
Speech of course is happening
somewhere around here where

00:37:31.290 --> 00:37:32.540
the hearing is more sensitive.

00:37:32.540 --> 00:37:34.350
That was the point here.

00:37:34.350 --> 00:37:36.630
Modulations, again, we
didn't talk much about that,

00:37:36.630 --> 00:37:38.730
but modulations
are very important.

00:37:38.730 --> 00:37:42.460
Since 1923, it's
known that hearing

00:37:42.460 --> 00:37:45.090
is the most sensitive to
certain rate of modulations

00:37:45.090 --> 00:37:48.050
around 4, 5 hertz.

00:37:48.050 --> 00:37:52.740
These are experiments from Bell
Labs repeated number of times.

00:37:52.740 --> 00:37:55.350
So this is this for
a, a modulations.

00:37:55.350 --> 00:37:58.020
This experiment, what you do
is, that you modulate a signal,

00:37:58.020 --> 00:38:00.840
and change the depth,
and change the frequency.

00:38:00.840 --> 00:38:03.090
And you are asking, do
you hear the modulation

00:38:03.090 --> 00:38:05.040
or don't you hear
the modulation?

00:38:05.040 --> 00:38:07.260
Very interesting--
interesting thing

00:38:07.260 --> 00:38:09.780
is, if you look
at-- again, I mean,

00:38:09.780 --> 00:38:12.360
I refer to what Josh was
telling you in the morning.

00:38:12.360 --> 00:38:15.605
If you just take one
trajectory of the spectrum,

00:38:15.605 --> 00:38:18.660
you treat it as a time domain
signal, remove the mean

00:38:18.660 --> 00:38:21.360
and compute its Fourier
components-- frequency

00:38:21.360 --> 00:38:24.810
components, they peak
somewhere around 4 hertz,

00:38:24.810 --> 00:38:28.380
just where the hearing
is the most sensitive.

00:38:28.380 --> 00:38:31.210
So hearing is not very
sensitive, obviously,

00:38:31.210 --> 00:38:34.080
to when the signal
is non-modulated,

00:38:34.080 --> 00:38:37.950
but also there is-- there
are almost no components

00:38:37.950 --> 00:38:40.890
in the signal which would be
non-modulated, because when I

00:38:40.890 --> 00:38:42.490
talk to you, I move the mouth.

00:38:42.490 --> 00:38:44.010
I mean, I change the things.

00:38:44.010 --> 00:38:48.300
And I change the things about
four times a second, mainly.

00:38:51.820 --> 00:38:54.280
When it comes to speech,
you can also compute--

00:38:54.280 --> 00:38:57.210
music, you can also
figure out what

00:38:57.210 --> 00:39:00.370
are the natural
rhythms in the music.

00:39:00.370 --> 00:39:04.210
I stole this from, I
believe, the Munich group,

00:39:04.210 --> 00:39:08.020
from [INAUDIBLE].

00:39:08.020 --> 00:39:10.100
He played 60 pieces of music.

00:39:10.100 --> 00:39:14.230
And then he asked people to
tap to the rhythm of the music.

00:39:14.230 --> 00:39:16.300
And this is the
histogram of tapping.

00:39:16.300 --> 00:39:18.790
Most of the people,
for most of the music,

00:39:18.790 --> 00:39:21.190
tapping was about
four times a second.

00:39:21.190 --> 00:39:23.890
This is where the hearing
is most sensitive.

00:39:23.890 --> 00:39:29.950
And this is modulation
frequency of this music.

00:39:29.950 --> 00:39:33.180
So people play
music in such a way

00:39:33.180 --> 00:39:35.580
that we hear it well,
that it basically

00:39:35.580 --> 00:39:38.510
resonates with the
natural frequency which

00:39:38.510 --> 00:39:40.020
we are perceiving.

00:39:40.020 --> 00:39:42.250
You can also ask
the similar thing.

00:39:42.250 --> 00:39:45.120
So, in speech, you can
play the speech sentences.

00:39:45.120 --> 00:39:47.860
And you ask people to tap in
to the rhythm of the sentences.

00:39:47.860 --> 00:39:49.990
Of course, what gets out
is the syllabic rate.

00:39:49.990 --> 00:39:53.210
And syllabic rate
is about 4 hertz.

00:39:53.210 --> 00:39:55.530
Where is the
information in speech?

00:39:55.530 --> 00:39:58.140
Well, we know what
the ear is doing.

00:39:58.140 --> 00:40:02.940
It analyzes signal into
individual frequency bands.

00:40:02.940 --> 00:40:06.360
We know what Homer
Dudley was telling us.

00:40:06.360 --> 00:40:08.880
When messages and
modulations of these

00:40:08.880 --> 00:40:10.920
frequencies-- as
a matter of fact,

00:40:10.920 --> 00:40:14.250
that was the base
of his vocoder.

00:40:14.250 --> 00:40:17.190
What he also did was that
he designed-- actually,

00:40:17.190 --> 00:40:18.544
it wasn't only him.

00:40:18.544 --> 00:40:19.710
There was another technique.

00:40:19.710 --> 00:40:22.500
This one is, kind of,
somehow cleaner thing,

00:40:22.500 --> 00:40:24.930
which is called the
spectrograph, which tells you

00:40:24.930 --> 00:40:27.480
about the spectrum of
frequency components

00:40:27.480 --> 00:40:30.272
of the acoustic signal.

00:40:30.272 --> 00:40:31.230
So you take the signal.

00:40:31.230 --> 00:40:33.750
You put it through a
bank of bandpass filters.

00:40:33.750 --> 00:40:37.470
And then here, you
basically display,

00:40:37.470 --> 00:40:42.420
on the z-axis, intensity
in each frequency band.

00:40:42.420 --> 00:40:45.200
This was, I heard,
used for listening

00:40:45.200 --> 00:40:48.300
for German submarines,
because they

00:40:48.300 --> 00:40:54.270
wanted to-- they knew that
acoustic signatures were

00:40:54.270 --> 00:40:57.120
different for friendly
submarines and enemy

00:40:57.120 --> 00:40:58.260
submarines.

00:40:58.260 --> 00:41:00.570
People listen to it--
for it, but also people

00:41:00.570 --> 00:41:03.310
realized it may be useful
to look at the signal--

00:41:03.310 --> 00:41:04.335
acoustic signal somehow.

00:41:04.335 --> 00:41:07.000
Waveform, it wasn't making
all that much sense,

00:41:07.000 --> 00:41:09.270
but the spectrogram was.

00:41:09.270 --> 00:41:13.140
Danger there was that the people
who were working in speech

00:41:13.140 --> 00:41:14.850
got hold of it.

00:41:14.850 --> 00:41:17.280
And then they start, sort of,
looking at the spectrograms.

00:41:17.280 --> 00:41:19.950
And they say, haha, we are
seeing the information here.

00:41:19.950 --> 00:41:23.130
We are seeing the
information in waves.

00:41:23.130 --> 00:41:26.910
The spectrum is changing,
because not only that this

00:41:26.910 --> 00:41:28.800
was the way the origin
of the spectrogram

00:41:28.800 --> 00:41:32.880
was developed, that you
were displaying changes

00:41:32.880 --> 00:41:36.710
in energy in individual
frequency bands,

00:41:36.710 --> 00:41:38.000
but you can also look at this.

00:41:38.000 --> 00:41:40.375
This when you get to what is
called a short-term spectrum

00:41:40.375 --> 00:41:42.030
of speech.

00:41:42.030 --> 00:41:44.540
And people said, oh,
this short-term spectrum

00:41:44.540 --> 00:41:46.410
looks different
for R than for E,

00:41:46.410 --> 00:41:49.440
so maybe this is the
way to recognize speech.

00:41:49.440 --> 00:41:51.000
So indeed, I mean,
those are two ways

00:41:51.000 --> 00:41:52.870
of generating the spectrograms.

00:41:52.870 --> 00:41:56.210
I mean, this was the original
one, bank of bandpass filters.

00:41:56.210 --> 00:41:59.790
And you were displaying the
energy as a function of time.

00:41:59.790 --> 00:42:02.130
This is what your ear is doing.

00:42:02.130 --> 00:42:03.330
That's what I'm saying.

00:42:03.330 --> 00:42:05.900
This is not what
your ear is doing,

00:42:05.900 --> 00:42:08.850
that if you take a short
segments of the signal,

00:42:08.850 --> 00:42:10.890
and you compute the
Fourier transform,

00:42:10.890 --> 00:42:15.400
then you display the Fourier
transform one frame at a time,

00:42:15.400 --> 00:42:18.480
but this is the way most of
the speech recognition systems

00:42:18.480 --> 00:42:19.950
work.

00:42:19.950 --> 00:42:24.750
And I'm suggesting that maybe we
should think about other ways.

00:42:27.270 --> 00:42:30.260
So now we have to deal
with all these problems.

00:42:30.260 --> 00:42:35.060
So we have a number
of things coming

00:42:35.060 --> 00:42:40.040
in in the form of the message
with all these chunk around it.

00:42:40.040 --> 00:42:42.030
And machine
recognition of speech

00:42:42.030 --> 00:42:45.170
would like to transcribe the
code which carries the message.

00:42:45.170 --> 00:42:47.870
This is a typical example
of the application

00:42:47.870 --> 00:42:48.830
of speech recognition.

00:42:48.830 --> 00:42:50.690
I'm not saying this
is the only one.

00:42:50.690 --> 00:42:53.840
There are attempts to
recognize just some key words.

00:42:53.840 --> 00:42:55.790
There are attempts
to actually generate

00:42:55.790 --> 00:42:58.580
the understanding of what
people are saying, and so on,

00:42:58.580 --> 00:43:01.340
but we would be
happy, in most cases,

00:43:01.340 --> 00:43:05.340
just to transcribe the speech.

00:43:05.340 --> 00:43:07.860
Speech has been produced
to be perceived.

00:43:07.860 --> 00:43:09.080
We already talked about it.

00:43:09.080 --> 00:43:13.570
It evolved over millennia to
fit the properties of hearing.

00:43:13.570 --> 00:43:16.470
So this is-- I'm sort of
seconding what Josh was saying.

00:43:16.470 --> 00:43:19.350
Josh was saying, you can
learn about the hearing

00:43:19.350 --> 00:43:21.150
by synthesizing stuff.

00:43:21.150 --> 00:43:23.700
I'm saying you of learn
about hearing by trying

00:43:23.700 --> 00:43:25.940
to recognize the stuff.

00:43:25.940 --> 00:43:31.470
So if you put something in
and it works, and it supports

00:43:31.470 --> 00:43:36.090
some theory of hearing, you may
be kind of reasonably confident

00:43:36.090 --> 00:43:38.980
that it was something
which has been useful.

00:43:38.980 --> 00:43:41.730
Actually there's a paper
about that, which, of course,

00:43:41.730 --> 00:43:43.860
I'm co-author, but I
didn't want to show that.

00:43:43.860 --> 00:43:45.270
I thought I would
leave this one,

00:43:45.270 --> 00:43:48.750
but I didn't do
it at last minute.

00:43:48.750 --> 00:43:52.340
Anyways, speech
recognition-- speech signal

00:43:52.340 --> 00:43:54.730
has high bit-rate,
recognizer comes

00:43:54.730 --> 00:43:57.240
in, information, low bit-rate.

00:43:57.240 --> 00:43:59.040
So what you are doing
here, you are trying

00:43:59.040 --> 00:44:01.020
to reorganize your stuff.

00:44:01.020 --> 00:44:04.240
You are trying to
reduce the entropy.

00:44:04.240 --> 00:44:06.510
If you are reducing
the entropy, you better

00:44:06.510 --> 00:44:09.360
know what you are doing,
because otherwise, you

00:44:09.360 --> 00:44:10.990
get real garbage.

00:44:10.990 --> 00:44:13.410
I mean, that's, kind of, like,
one of these common sense

00:44:13.410 --> 00:44:15.210
things, right?

00:44:15.210 --> 00:44:16.910
So you want to use
some knowledge.

00:44:16.910 --> 00:44:18.930
You have plenty of knowledge
in this recognizer.

00:44:18.930 --> 00:44:20.722
Where does this
knowledge come from?

00:44:20.722 --> 00:44:22.180
We keep discussing
it all the time.

00:44:22.180 --> 00:44:27.310
It came from textbooks,
teachers, intuitions, beliefs,

00:44:27.310 --> 00:44:28.530
and so on.

00:44:28.530 --> 00:44:31.110
And its a good thing
about that, that you

00:44:31.110 --> 00:44:34.950
can hardwire this
knowledge so that you

00:44:34.950 --> 00:44:39.630
don't have to learn it, relearn
it next time based on the data.

00:44:39.630 --> 00:44:43.170
Of course, problem is that this
knowledge may be incomplete,

00:44:43.170 --> 00:44:47.550
irrelevant, can be plain
wrong, because, you know,

00:44:47.550 --> 00:44:49.470
who can say that whatever
teachers tell you,

00:44:49.470 --> 00:44:53.880
or textbooks tell, or your
intuitions or beliefs is always

00:44:53.880 --> 00:44:54.840
true?

00:44:54.840 --> 00:44:58.200
Much more often now,
what people are using

00:44:58.200 --> 00:45:01.770
is that they-- that knowledge
comes directly from the data.

00:45:01.770 --> 00:45:05.355
Such knowledge is
relevant and unbiased,

00:45:05.355 --> 00:45:09.020
but the problem is that you
need a lot of training data.

00:45:09.020 --> 00:45:13.650
And it's very hard to get
architecture of the recognizer

00:45:13.650 --> 00:45:16.560
from the data, at
least, I don't know

00:45:16.560 --> 00:45:18.640
quite well how to do it yet.

00:45:18.640 --> 00:45:19.710
So these are two things.

00:45:19.710 --> 00:45:22.680
And again, I mean, let
me go back to '50s.

00:45:22.680 --> 00:45:26.900
First knowledge-based recognizer
was based on the spectrograms.

00:45:26.900 --> 00:45:28.820
There was Richard Galt.

00:45:28.820 --> 00:45:30.870
And he was looking
at spectrograms

00:45:30.870 --> 00:45:33.660
and trying to figure out how
this short-term spectrum looked

00:45:33.660 --> 00:45:35.740
like for different
speech sounds.

00:45:35.740 --> 00:45:38.130
Then he thought he would make
this finite state machine,

00:45:38.130 --> 00:45:41.100
which will generate the text.

00:45:41.100 --> 00:45:43.500
Needless to say, it
didn't work too well.

00:45:43.500 --> 00:45:48.270
He got beaten by data-driven
approach, where people

00:45:48.270 --> 00:45:51.760
took a high-pass filter
speech, low-pass filter speech,

00:45:51.760 --> 00:45:55.200
displayed energies from
these to two channels

00:45:55.200 --> 00:45:58.510
on, at the time it
was oscilloscope.

00:45:58.510 --> 00:46:00.960
And they tried to figure
out what are the patterns.

00:46:00.960 --> 00:46:02.490
They tried to
memorize the patterns,

00:46:02.490 --> 00:46:06.000
make the templates
from the training data.

00:46:06.000 --> 00:46:09.650
And they tried to match it for
the test data was recognized,

00:46:09.650 --> 00:46:11.210
which was recognizing
ten digits.

00:46:11.210 --> 00:46:12.930
And it was working
reasonably well,

00:46:12.930 --> 00:46:16.440
better than 90% of the time for
a single speaker, and so on,

00:46:16.440 --> 00:46:17.500
and so on.

00:46:17.500 --> 00:46:21.200
But it's interesting
that, already in '50s,

00:46:21.200 --> 00:46:25.230
the data-driven
approach got beat

00:46:25.230 --> 00:46:29.250
by the knowledge-based approach,
because knowledge maybe wasn't

00:46:29.250 --> 00:46:31.500
exactly what you needed to use.

00:46:31.500 --> 00:46:34.220
You were looking at the shapes
of the short-term spectra

00:46:34.220 --> 00:46:36.920
basically.

00:46:36.920 --> 00:46:40.460
Of course, now, we are
in 21st century, finally.

00:46:40.460 --> 00:46:43.460
Number of people say,
this is the real way

00:46:43.460 --> 00:46:44.990
of recognizing speech.

00:46:44.990 --> 00:46:48.470
You take the signal as it
comes with the microphone.

00:46:48.470 --> 00:46:50.930
You take the neural net.

00:46:50.930 --> 00:46:53.960
You put a lot of
training data, which

00:46:53.960 --> 00:46:58.000
contain all sources of unwanted
variability, basically, what

00:46:58.000 --> 00:46:58.630
you--

00:46:58.630 --> 00:47:01.490
all possible ways of--

00:47:01.490 --> 00:47:08.700
you can disturb the speech and
comes out the speech message.

00:47:08.700 --> 00:47:11.080
The key thing is, I'm not
saying that this is wrong,

00:47:11.080 --> 00:47:14.330
but I'm saying that, maybe this
is not the most efficient way

00:47:14.330 --> 00:47:17.010
of going about it,
because, in this case,

00:47:17.010 --> 00:47:19.455
you would have to retrain
the recognizer every time.

00:47:19.455 --> 00:47:21.205
It's a little bit like,
sort of, you know,

00:47:21.205 --> 00:47:24.760
if you look at the hearing
system, or the simple animal

00:47:24.760 --> 00:47:25.430
system--

00:47:25.430 --> 00:47:26.780
this is a moth here.

00:47:29.990 --> 00:47:33.610
Here it changes in
acoustic pressure

00:47:33.610 --> 00:47:36.780
to changes in firing rate.

00:47:36.780 --> 00:47:39.590
It goes to very simple
brain, very small one.

00:47:39.590 --> 00:47:42.430
You know, this is not the way
the human hearing is working.

00:47:42.430 --> 00:47:44.870
Human hearing is
much more complex.

00:47:44.870 --> 00:47:47.190
And again, Josh already
told us a lot about it,

00:47:47.190 --> 00:47:48.830
so I won't spend much time.

00:47:48.830 --> 00:47:52.580
The point here is, the human
hearing is frequency-selective.

00:47:52.580 --> 00:47:54.760
It goes through a
number of levels.

00:47:54.760 --> 00:47:58.940
This is very much along the deep
net and that sort of things.

00:47:58.940 --> 00:48:01.220
But still, there is
a lot of structure

00:48:01.220 --> 00:48:04.070
there in the hearing system.

00:48:04.070 --> 00:48:07.370
So it makes at least
some sense to me,

00:48:07.370 --> 00:48:10.470
if you want to do what people
are doing more and more,

00:48:10.470 --> 00:48:13.410
and there will be a whole
special session next week

00:48:13.410 --> 00:48:17.600
at Interspeech on how to
train the things directly

00:48:17.600 --> 00:48:20.750
from the data, probably
you want to have

00:48:20.750 --> 00:48:23.330
highly-structured environment.

00:48:23.330 --> 00:48:26.690
You want to have a convoluted
pre-processing recursive

00:48:26.690 --> 00:48:30.420
structures, and so on, and
long, short-term memory.

00:48:30.420 --> 00:48:32.540
Yeah, here are actually
some, but all these things

00:48:32.540 --> 00:48:33.360
are being used.

00:48:33.360 --> 00:48:37.310
And I think this is
the direction to go.

00:48:37.310 --> 00:48:39.810
But I still argue
that maybe it's

00:48:39.810 --> 00:48:42.060
a better-- there's a
better way to go about it.

00:48:42.060 --> 00:48:44.980
A better way to go
about it is that you

00:48:44.980 --> 00:48:49.270
try first to do some
pre-processing of the signal

00:48:49.270 --> 00:48:53.110
and derive some way of
describing the signal more

00:48:53.110 --> 00:48:58.750
efficiently, using the
features, and so on, and so on.

00:48:58.750 --> 00:49:02.920
Here you put all the
knowledge which you possibly

00:49:02.920 --> 00:49:04.320
may want to--

00:49:04.320 --> 00:49:05.950
you already have.

00:49:05.950 --> 00:49:10.830
This knowledge can be derived
from some development data,

00:49:10.830 --> 00:49:14.020
but you don't want to use
directly the speech signal

00:49:14.020 --> 00:49:15.295
every time you are using--

00:49:19.390 --> 00:49:21.010
you don't want to
retrain, basically,

00:49:21.010 --> 00:49:23.110
every time, directly
from the speech signal.

00:49:23.110 --> 00:49:26.440
You want to reserve
your training

00:49:26.440 --> 00:49:29.860
data, the task-specific
training data,

00:49:29.860 --> 00:49:32.120
to deal with the effects
of the noise which

00:49:32.120 --> 00:49:33.610
you don't understand.

00:49:33.610 --> 00:49:36.462
This is where the
machine learning comes.

00:49:36.462 --> 00:49:38.920
I'm not saying that this is
not a part of machine learning,

00:49:38.920 --> 00:49:40.240
but, I mean, this is--

00:49:40.240 --> 00:49:44.240
there are two different things
which you are going to do.

00:49:44.240 --> 00:49:45.990
I was just looking
for some support.

00:49:45.990 --> 00:49:49.270
This one came from Stu
Geman from Brown University

00:49:49.270 --> 00:49:51.010
and his colleagues.

00:49:51.010 --> 00:49:53.780
Stu Geman is a machine
learning person, definitely,

00:49:53.780 --> 00:49:58.570
but he says, we feel that
meat is in the features

00:49:58.570 --> 00:50:01.150
rather than in the
machine learning,

00:50:01.150 --> 00:50:03.250
because they go
overboard, basically,

00:50:03.250 --> 00:50:06.970
explaining that, if you just
rely on machine learning, sure,

00:50:06.970 --> 00:50:09.040
you have a neural net
which can approximate just

00:50:09.040 --> 00:50:11.380
about any function,
given that you

00:50:11.380 --> 00:50:14.980
have infinite amount of data
an infinitely large neural net.

00:50:14.980 --> 00:50:18.250
And they say, infinite is a
kind of not useful engineering

00:50:18.250 --> 00:50:19.240
concepts.

00:50:19.240 --> 00:50:23.470
So they feel like that, if
representations actually are--

00:50:23.470 --> 00:50:25.030
I hope they still feel the same.

00:50:25.030 --> 00:50:27.040
I didn't talk to
them now, but it

00:50:27.040 --> 00:50:30.340
seems like that there is some
support in this notion, what

00:50:30.340 --> 00:50:31.794
I'm saying.

00:50:31.794 --> 00:50:33.460
But of course, problem
with the features

00:50:33.460 --> 00:50:38.480
is following, whatever you
stripped on the features,

00:50:38.480 --> 00:50:39.820
this is a bottleneck.

00:50:39.820 --> 00:50:43.990
Whatever you decide that is
not important is lost forever.

00:50:43.990 --> 00:50:46.000
You will never recover
from it, right?

00:50:46.000 --> 00:50:48.590
Because I'm asking for
feature extraction.

00:50:48.590 --> 00:50:52.270
I'm asking for this emulation
of the human perception, which

00:50:52.270 --> 00:50:55.360
strips out a lot of
information, but I still

00:50:55.360 --> 00:50:56.830
think that we need
to do it if we

00:50:56.830 --> 00:51:01.150
want to design a useful
engineering representations.

00:51:01.150 --> 00:51:05.410
The other problem, of course,
is whatever you leave in,

00:51:05.410 --> 00:51:09.520
the noise, the information which
is not relevant to your task,

00:51:09.520 --> 00:51:11.950
you will have to
deal with it later.

00:51:11.950 --> 00:51:15.160
You will need to train
the old machine on that,

00:51:15.160 --> 00:51:16.990
so you want to be
very, very careful.

00:51:16.990 --> 00:51:20.272
You are walking
a thin line here.

00:51:20.272 --> 00:51:21.730
What is it that I
should leave out?

00:51:21.730 --> 00:51:23.640
What is it that
I should keep in?

00:51:23.640 --> 00:51:27.390
It's always safer to keep a
little bit more in, obviously.

00:51:27.390 --> 00:51:30.710
But this is the goal
which we have here.

00:51:30.710 --> 00:51:33.130
And I wanted to say,
features can be designed

00:51:33.130 --> 00:51:35.000
using development data.

00:51:35.000 --> 00:51:37.510
And when I'm saying use
the development data,

00:51:37.510 --> 00:51:39.940
design your features
and use them.

00:51:39.940 --> 00:51:42.650
Don't use this
development data anymore.

00:51:42.650 --> 00:51:45.550
We have a lot of data for the
designing of good features.

00:51:45.550 --> 00:51:47.897
And I think that, again,
is happening in the field--

00:51:50.806 --> 00:51:51.306
good.

00:51:54.230 --> 00:51:58.200
How the speech recognition
was done in 20th century,

00:51:58.200 --> 00:52:02.840
this is what I know, maybe, the
most, so we'll spend some time.

00:52:02.840 --> 00:52:06.110
And it's still done largely in--

00:52:06.110 --> 00:52:10.070
there are some variants of this
recognition that's still done.

00:52:10.070 --> 00:52:11.120
You take the signal.

00:52:11.120 --> 00:52:13.700
And you derive the features.

00:52:13.700 --> 00:52:15.620
In the first place,
you derive what

00:52:15.620 --> 00:52:17.810
is called short-term
features, so you

00:52:17.810 --> 00:52:20.000
take short segments of
the signal, about 10

00:52:20.000 --> 00:52:21.560
to 20 milliseconds.

00:52:21.560 --> 00:52:25.040
And you derive some
features from that.

00:52:25.040 --> 00:52:26.670
That was in 20th century.

00:52:26.670 --> 00:52:28.400
Now we are taking
much longer segments,

00:52:28.400 --> 00:52:29.930
but we'll get into that.

00:52:29.930 --> 00:52:32.150
But you derive it
with about 100 hertz

00:52:32.150 --> 00:52:35.060
sampling every 10
millisecond, so you

00:52:35.060 --> 00:52:39.520
turn the one-dimensional signal
into two-dimensional signal.

00:52:39.520 --> 00:52:42.060
And here, typically, the
first step is the frequency,

00:52:42.060 --> 00:52:45.420
so those may be-- imagine
those are frequency vectors,

00:52:45.420 --> 00:52:47.920
or something derived
from frequency vectors,

00:52:47.920 --> 00:52:49.770
gets through or stuff like that.

00:52:49.770 --> 00:52:53.130
Those are just tricks,
signal processing tricks

00:52:53.130 --> 00:52:54.295
which people use--

00:52:54.295 --> 00:52:57.300
but one-dimensional
to two-dimensional.

00:52:57.300 --> 00:53:01.700
Next thing is, you estimate the
likelihood of the sounds each

00:53:01.700 --> 00:53:03.430
10 millisecond.

00:53:03.430 --> 00:53:04.500
So here, what I--

00:53:04.500 --> 00:53:08.490
imagine that here we have
different, say, speech sounds,

00:53:08.490 --> 00:53:13.050
maybe 41 phonemes, maybe 3,000
context-dependent phonemes,

00:53:13.050 --> 00:53:14.370
and so on, depends on--

00:53:14.370 --> 00:53:19.610
but those are parts of speech
which makes some sense.

00:53:19.610 --> 00:53:23.190
And they come, typically,
from phonetics theory.

00:53:23.190 --> 00:53:25.700
And we know that
you can generate

00:53:25.700 --> 00:53:29.550
different words putting phonemes
together in different ways,

00:53:29.550 --> 00:53:30.950
and so on, and so on.

00:53:30.950 --> 00:53:33.200
So suppose for the
simplicity that they

00:53:33.200 --> 00:53:35.060
are-- there's 41 phonemes.

00:53:35.060 --> 00:53:38.960
And so if there
is a red one, red

00:53:38.960 --> 00:53:44.090
means that, probably,
posterior probability of the--

00:53:44.090 --> 00:53:45.440
actually, we need them more.

00:53:45.440 --> 00:53:47.420
We need the likelihoods
rather than posteriors,

00:53:47.420 --> 00:53:52.690
so with your posteriors, we
just divided it by priors

00:53:52.690 --> 00:53:57.470
to get the likelihoods, so
meaning that this phoneme has

00:53:57.470 --> 00:53:59.480
a high likelihood
and white ones don't

00:53:59.480 --> 00:54:02.700
have a likelihood at this time.

00:54:02.700 --> 00:54:07.980
So next step is that
you do the search on it.

00:54:07.980 --> 00:54:09.140
This is a painful part.

00:54:09.140 --> 00:54:10.910
And I won't be spending
much time on that.

00:54:10.910 --> 00:54:13.910
I just want to give you
some flavor of this.

00:54:13.910 --> 00:54:19.790
You try to find the best
path through this lattice

00:54:19.790 --> 00:54:22.260
of the likelihoods.

00:54:22.260 --> 00:54:24.930
And if you are lucky,
the best part, then,

00:54:24.930 --> 00:54:28.240
is going to present
your speech sounds.

00:54:28.240 --> 00:54:32.090
So then the next thing is only
that you look and transcribe

00:54:32.090 --> 00:54:36.420
to go from phonemic
representation from

00:54:36.420 --> 00:54:39.720
into lexical
representation, basically,

00:54:39.720 --> 00:54:41.910
because you know there
is typically one-to-one

00:54:41.910 --> 00:54:43.456
relations--

00:54:43.456 --> 00:54:45.130
Well, should be
careful, one-to-one,

00:54:45.130 --> 00:54:51.810
but it is a relation, known
relation between phonemes

00:54:51.810 --> 00:54:54.250
and the transcription.

00:54:54.250 --> 00:54:56.760
So we know what has been said.

00:54:56.760 --> 00:54:58.878
So this is how the speech
recognition is done.

00:55:03.280 --> 00:55:05.320
Talking about this
part, I mean, here we

00:55:05.320 --> 00:55:09.070
have to deal with one
major problem, which is,

00:55:09.070 --> 00:55:12.790
like, the speech doesn't
come out this way.

00:55:12.790 --> 00:55:16.900
It doesn't come out as a
sequences of individual speech

00:55:16.900 --> 00:55:20.860
sounds, but, since I'm talking
to you, I'm moving the mouse.

00:55:20.860 --> 00:55:23.390
I'm moving the
mouse continuously.

00:55:23.390 --> 00:55:27.580
There is a thing that first I
can make certain sounds longer,

00:55:27.580 --> 00:55:30.070
certain sounds shorter.

00:55:30.070 --> 00:55:33.080
And then I add some noise to it.

00:55:33.080 --> 00:55:37.600
Finally, because of what
is called co-articulation,

00:55:37.600 --> 00:55:43.660
each target phonemes gets spread
in time, so you get a mess.

00:55:43.660 --> 00:55:46.150
But people say--
sometimes, people

00:55:46.150 --> 00:55:49.360
like to say, speech recognition,
this is our biggest problem.

00:55:49.360 --> 00:55:52.670
I claim to say this
is not the problem.

00:55:52.670 --> 00:55:53.690
It is a feature.

00:55:53.690 --> 00:55:58.330
And feature is important,
because it comes quite handy

00:55:58.330 --> 00:55:58.930
later.

00:55:58.930 --> 00:56:01.285
Hopefully, I will
convince you about it.

00:56:01.285 --> 00:56:05.570
But what we get is a mess, so
this is not easy to recognize,

00:56:05.570 --> 00:56:06.070
right?

00:56:06.070 --> 00:56:07.111
We have co-articulations.

00:56:07.111 --> 00:56:11.330
We have speaker dependencies,
noise from the environment,

00:56:11.330 --> 00:56:13.180
and so on, and so on.

00:56:13.180 --> 00:56:15.730
So the way to deal
with it is to recognize

00:56:15.730 --> 00:56:18.970
that different people
may sound different,

00:56:18.970 --> 00:56:21.720
communication and
environment may differ,

00:56:21.720 --> 00:56:24.610
so the features will be
dependent on a number

00:56:24.610 --> 00:56:27.580
of things, on
environmental problems,

00:56:27.580 --> 00:56:29.920
on who are saying
things, and so on.

00:56:29.920 --> 00:56:32.970
People say same things
in different speech.

00:56:32.970 --> 00:56:34.990
I can speak faster,
I can speak slower,

00:56:34.990 --> 00:56:39.790
still, the message is the same.

00:56:39.790 --> 00:56:46.060
So we use what is called the
Hidden Markov Model, where

00:56:46.060 --> 00:56:53.350
you try to find such a sequence
of the phonemes which optimizes

00:56:53.350 --> 00:57:00.250
the conditional probability
of the model, given the data.

00:57:00.250 --> 00:57:02.830
And models you
generate, on the fly,

00:57:02.830 --> 00:57:05.999
as many models as possible,
actually, an infinite number

00:57:05.999 --> 00:57:07.540
of models, but, of
course, again, you

00:57:07.540 --> 00:57:10.810
can't do it infinitely, so
you do it in some smart ways.

00:57:10.810 --> 00:57:15.370
And this is being computed
through modified Bayes' rule.

00:57:15.370 --> 00:57:18.760
Modified is because,
for one, I mean,

00:57:18.760 --> 00:57:22.100
you would need a prior
probability of the signal,

00:57:22.100 --> 00:57:22.600
and so on.

00:57:22.600 --> 00:57:23.840
We don't use that.

00:57:23.840 --> 00:57:29.010
But also, what we are doing, we
are somehow arbitrarily scale

00:57:29.010 --> 00:57:32.020
the things which are called the
language model, because this

00:57:32.020 --> 00:57:35.650
is a prior probability of
the particular utterance.

00:57:35.650 --> 00:57:39.790
This is the likelihoods
coming from the data,

00:57:39.790 --> 00:57:43.960
combining these two things
together, and finding the best

00:57:43.960 --> 00:57:50.000
match, you get the output
which best matches the things.

00:57:50.000 --> 00:57:53.950
Model parameters are typically
derived from the training data.

00:57:53.950 --> 00:57:57.700
Problem is, how to find
the unknown utterance.

00:57:57.700 --> 00:58:00.010
You don't know what is
the form of the model.

00:58:00.010 --> 00:58:03.410
And you don't know
what is the data.

00:58:03.410 --> 00:58:05.740
So we are dealing with
what is called the Doubly

00:58:05.740 --> 00:58:09.140
stochastic model, a
Hidden Markov model.

00:58:09.140 --> 00:58:13.620
Speech is a sequence-- it's
a sequence of hidden states.

00:58:13.620 --> 00:58:15.780
You don't see this hidden state.

00:58:15.780 --> 00:58:20.626
And also, you don't know
what comes from any state.

00:58:20.626 --> 00:58:24.180
So it somehow-- so you don't
know for sure in which state

00:58:24.180 --> 00:58:24.960
you are on.

00:58:24.960 --> 00:58:28.580
You don't know for sure what
comes out, but you know that--

00:58:28.580 --> 00:58:30.540
well, you know, you
assume that this

00:58:30.540 --> 00:58:32.160
is how the speech looks like.

00:58:32.160 --> 00:58:34.950
So here I have a
picture a little bit.

00:58:34.950 --> 00:58:37.080
I apologize for being
trivial about this,

00:58:37.080 --> 00:58:39.630
but imagine that you
have a string of--

00:58:39.630 --> 00:58:40.800
group of people.

00:58:40.800 --> 00:58:43.670
They are-- some are
female, some are male.

00:58:43.670 --> 00:58:46.740
They are groups of
males, groups of females.

00:58:46.740 --> 00:58:48.220
And each of them says something.

00:58:48.220 --> 00:58:49.120
He says, hi.

00:58:49.120 --> 00:58:50.370
And you can measure something.

00:58:50.370 --> 00:58:52.260
This is a fundamental frequency.

00:58:52.260 --> 00:58:56.594
You get some measurement out of
that, but you don't see them.

00:58:56.594 --> 00:58:59.170
But what you know is that
they interleave, basically.

00:58:59.170 --> 00:59:01.420
For a while, there
is a group of males,

00:59:01.420 --> 00:59:04.780
then there is a-- then the
speech is to a group of female.

00:59:04.780 --> 00:59:07.540
And then you stay for a
while in a group of female,

00:59:07.540 --> 00:59:08.650
and so on, and so on.

00:59:08.650 --> 00:59:12.490
So basically-- and
you know what is

00:59:12.490 --> 00:59:14.680
the probability of the
fundamental frequency

00:59:14.680 --> 00:59:16.380
for males, so some distribution.

00:59:16.380 --> 00:59:18.850
So you know what is the path
on the fundamental frequency

00:59:18.850 --> 00:59:20.860
for females.

00:59:20.860 --> 00:59:24.790
You know what is the probability
of the first group being male.

00:59:24.790 --> 00:59:28.790
Subsequently, you also know
what is the probability of the

00:59:28.790 --> 00:59:30.275
[AUDIO OUT]

00:59:34.235 --> 00:59:36.924
Because, to me, the features
are the important as I

00:59:36.924 --> 00:59:41.020
told you, in which
we don't need,

00:59:41.020 --> 00:59:44.476
but we don't want to take
out stuff that you may need.

00:59:47.927 --> 00:59:49.948
I told you that that
one important role

00:59:49.948 --> 00:59:51.820
of the perception
is to eliminate

00:59:51.820 --> 00:59:53.692
some of this information.

00:59:53.692 --> 00:59:57.804
Basically that's to so,
eliminate irrelevant focus

00:59:57.804 --> 01:00:00.600
on irrelevant stuff.

01:00:00.600 --> 01:00:05.250
So this is where I feel the
properties of perception

01:00:05.250 --> 01:00:09.680
can come in very strongly,
because this is what emulates

01:00:09.680 --> 01:00:12.500
this basic process
of the speech,

01:00:12.500 --> 01:00:15.826
of the extraction of
information [INAUDIBLE]..

01:00:15.826 --> 01:00:18.742
Especially about the
Hidden Markov models,

01:00:18.742 --> 01:00:22.180
that speech consists of
the sequences of sounds

01:00:22.180 --> 01:00:24.895
and they can be previously
different speed,

01:00:24.895 --> 01:00:25.970
and other things.

01:00:25.970 --> 01:00:27.010
It's important.

01:00:27.010 --> 01:00:32.830
But here, we can use
a lot of our model.

01:00:32.830 --> 01:00:38.612
Those features which can be
also designed based on the data.

01:00:38.612 --> 01:00:41.057
And what comes out
is probably going

01:00:41.057 --> 01:00:42.640
to be irrelevant to
speech perception,

01:00:42.640 --> 01:00:48.924
so this is my point for how
you can use your engineering

01:00:48.924 --> 01:00:53.326
to verify our theories
of speech perception.

01:00:53.326 --> 01:00:57.240
We use largely,
nowadays, the neural

01:00:57.240 --> 01:01:01.150
nets to derive the features.

01:01:01.150 --> 01:01:04.450
So how we do it is that we
sort of-- because we know

01:01:04.450 --> 01:01:11.123
that best set of features are
posteriors of the class we want

01:01:11.123 --> 01:01:15.175
to recognize our speech sounds,
maybe it's going to be useful.

01:01:15.175 --> 01:01:16.945
If you do a good
job, actually you

01:01:16.945 --> 01:01:19.840
can do the reasonable sound.

01:01:19.840 --> 01:01:22.295
So if you take a signal, you
do something processing--

01:01:22.295 --> 01:01:25.860
and I will be talking about
signal processing quite a lot.

01:01:25.860 --> 01:01:30.625
But then it goes into neural
net, nowadays, deep neural net,

01:01:30.625 --> 01:01:34.150
and you estimate a posterior
use of different speech sounds.

01:01:34.150 --> 01:01:36.350
And then what comes out,
whatever, it's always

01:01:36.350 --> 01:01:39.175
is the high posterior
probability of the phoneme,

01:01:39.175 --> 01:01:44.540
so we have you do it [INAUDIBLE]
sequence of the phoneme.

01:01:48.150 --> 01:01:50.360
As the classes you can
use, directly context

01:01:50.360 --> 01:01:55.020
independent in this
example, small number.

01:01:55.020 --> 01:01:57.880
You can use context
dependent phonemes, which

01:01:57.880 --> 01:02:01.180
I use quite a lot, because
they've tried to optimize this

01:02:01.180 --> 01:02:03.450
despite that, if the
phoneme is produced

01:02:03.450 --> 01:02:07.052
depends on what happened
inside, in the neighborhood,

01:02:07.052 --> 01:02:09.680
[INAUDIBLE]

01:02:13.150 --> 01:02:17.390
These posteriors can be
directly used with our research.

01:02:17.390 --> 01:02:22.949
This is the search through
the lattice of the likelihoods

01:02:22.949 --> 01:02:24.280
in recognition.

01:02:24.280 --> 01:02:26.236
And again, I mean,
it's coming back.

01:02:26.236 --> 01:02:29.750
This was the late 1990,
but this is the way

01:02:29.750 --> 01:02:32.500
that most of this
recognizers work.

01:02:32.500 --> 01:02:35.325
This is the major way now how
you do this feature cognition.

01:02:35.325 --> 01:02:37.820
There's another way, which is
called bottleneck or tandem--

01:02:37.820 --> 01:02:40.420
we were involved in that too--

01:02:40.420 --> 01:02:43.720
which was the way to make the
neural nets friendly to people

01:02:43.720 --> 01:02:48.280
who were used to old
generative HMM models,

01:02:48.280 --> 01:02:50.440
because you
basically convert it,

01:02:50.440 --> 01:02:52.690
your outputs from
the posteriors,

01:02:52.690 --> 01:02:56.520
into some features which
your generative HMM

01:02:56.520 --> 01:02:59.050
model would like for you.

01:02:59.050 --> 01:03:01.270
What you did for you
de- correlated them,

01:03:01.270 --> 01:03:05.410
you coercionized them so that
they have a normal distribution

01:03:05.410 --> 01:03:07.070
and use it as a features.

01:03:07.070 --> 01:03:11.350
And bottom line is, if you
get the good posteriors,

01:03:11.350 --> 01:03:13.110
you will get the good features.

01:03:13.110 --> 01:03:14.700
And we know how to use them.

01:03:14.700 --> 01:03:17.910
And this is pretty much
the mainstream now.

