WEBVTT
Kind: captions
Language: en

00:00:01.640 --> 00:00:04.040
The following content is
provided under a Creative

00:00:04.040 --> 00:00:05.580
Commons license.

00:00:05.580 --> 00:00:07.880
Your support will help
MIT OpenCourseWare

00:00:07.880 --> 00:00:12.270
continue to offer high quality
educational resources for free.

00:00:12.270 --> 00:00:14.870
To make a donation or
view additional materials

00:00:14.870 --> 00:00:18.830
from hundreds of MIT courses,
visit MIT OpenCourseWare

00:00:18.830 --> 00:00:20.000
at ocw.mit.edu.

00:00:22.612 --> 00:00:24.820
AUDE OLIVA: Thank you very
much for the introduction.

00:00:24.820 --> 00:00:26.410
Good morning, everyone.

00:00:26.410 --> 00:00:28.230
So I'm very pleased to be here.

00:00:28.230 --> 00:00:30.420
It's the first time I visit.

00:00:30.420 --> 00:00:34.410
So I would like to give you
a tour during this lecture

00:00:34.410 --> 00:00:38.550
about how you can predict
human visual memory,

00:00:38.550 --> 00:00:41.130
and actually an
interdisciplinary account

00:00:41.130 --> 00:00:44.040
of the methods you can
use together in order

00:00:44.040 --> 00:00:49.030
to have basically a view of what
people can remember or forget.

00:00:49.030 --> 00:00:52.620
So the specific question
we are going to ask today

00:00:52.620 --> 00:00:57.420
is, well, we are experiencing
and seeing all the time

00:00:57.420 --> 00:00:58.950
a lot of digital information.

00:00:58.950 --> 00:01:02.220
First you see in the
real world, but also you

00:01:02.220 --> 00:01:05.640
are exposed to many,
many videos and images.

00:01:05.640 --> 00:01:10.170
So vision and visual memory
is one of the core concept

00:01:10.170 --> 00:01:11.640
of cognition.

00:01:11.640 --> 00:01:14.220
And the question
we ask is, can we

00:01:14.220 --> 00:01:18.900
predict which image or graph
or face or words or videos

00:01:18.900 --> 00:01:21.450
or piece of
information or event is

00:01:21.450 --> 00:01:23.790
going to be memorable
or forgettable

00:01:23.790 --> 00:01:26.700
for a group of people,
and eventually for a given

00:01:26.700 --> 00:01:28.480
individual?

00:01:28.480 --> 00:01:31.710
So let's take a
moment to imagine

00:01:31.710 --> 00:01:37.170
that we will be able
to predict accurately

00:01:37.170 --> 00:01:39.840
the memory of people.

00:01:39.840 --> 00:01:42.750
Well, this will be very
useful to understand

00:01:42.750 --> 00:01:46.110
the mechanism of
human memory, both at

00:01:46.110 --> 00:01:50.640
the cognitive and neuroscience
level, system level,

00:01:50.640 --> 00:01:54.000
as well as possibly
diagnose memory problem,

00:01:54.000 --> 00:01:57.120
short-term visual memory,
long-term visual memory, that

00:01:57.120 --> 00:02:03.210
may arise possibly in an acute
way or developing over time,

00:02:03.210 --> 00:02:06.000
as well as design
mnemonic aids in order

00:02:06.000 --> 00:02:08.699
to recall better informations.

00:02:08.699 --> 00:02:13.140
But beside basic science, if
you could predict information

00:02:13.140 --> 00:02:15.810
that are memorable
or forgettable,

00:02:15.810 --> 00:02:19.020
there is a realm of
application that you

00:02:19.020 --> 00:02:24.780
could work on or basically
propose that lie really

00:02:24.780 --> 00:02:28.890
everywhere between the data
visualization or the slogan,

00:02:28.890 --> 00:02:31.890
make basically slogan
better, as well

00:02:31.890 --> 00:02:35.160
as all the realm of education,
the individual differences that

00:02:35.160 --> 00:02:36.720
may arise between people.

00:02:36.720 --> 00:02:39.330
Someone will not
learn very well.

00:02:39.330 --> 00:02:41.940
Well, what can we do
in order to increase

00:02:41.940 --> 00:02:46.470
the memory of visual information
that this person can grasp?

00:02:46.470 --> 00:02:48.210
As well as various
applications--

00:02:48.210 --> 00:02:51.030
social networking, faces,
retrieve better images,

00:02:51.030 --> 00:02:52.300
and so on.

00:02:52.300 --> 00:02:54.240
So understanding what
make an information

00:02:54.240 --> 00:02:56.370
memorable or
forgettable basically

00:02:56.370 --> 00:02:59.670
is really a very
inter-disciplinary question,

00:02:59.670 --> 00:03:02.040
something very exciting
for us to work on,

00:03:02.040 --> 00:03:04.770
because there's a
lot, a lot of future

00:03:04.770 --> 00:03:08.340
in working in that topic.

00:03:08.340 --> 00:03:11.520
So this is a topic we started
in my lab a few years ago.

00:03:11.520 --> 00:03:14.610
And the best for you
to get a sense of how

00:03:14.610 --> 00:03:17.150
you start working, for
instance, on memory

00:03:17.150 --> 00:03:20.220
is to do the kind of
game and the experiment

00:03:20.220 --> 00:03:21.750
we had people doing.

00:03:21.750 --> 00:03:24.240
So welcome to the
Visual Memory Game.

00:03:24.240 --> 00:03:26.670
A stream of images will
be presented on the screen

00:03:26.670 --> 00:03:28.410
for one second each.

00:03:28.410 --> 00:03:31.020
And your task is to--

00:03:31.020 --> 00:03:33.630
If you are in front of your
computer you will press a key.

00:03:33.630 --> 00:03:36.120
But what you're going to
do is play the game with me

00:03:36.120 --> 00:03:38.310
and clap your hand
anytime you see

00:03:38.310 --> 00:03:40.330
any image that you saw before.

00:03:40.330 --> 00:03:44.040
So you're going to have to
be attentive, because images,

00:03:44.040 --> 00:03:47.960
they go by fast in this
rapid stream of images.

00:03:47.960 --> 00:03:50.740
And so you will be
getting feedback.

00:03:50.740 --> 00:03:54.000
So it's a very straightforward
memory experiment.

00:03:54.000 --> 00:03:55.770
And this is the
first step in order

00:03:55.770 --> 00:04:01.770
to get some score on a
lot of images regarding

00:04:01.770 --> 00:04:04.650
the type of information
that people will naturally

00:04:04.650 --> 00:04:06.720
forget or naturally remember.

00:04:06.720 --> 00:04:08.580
So let's do the game.

00:04:08.580 --> 00:04:09.881
So are you ready?

00:04:09.881 --> 00:04:10.380
All right.

00:04:10.380 --> 00:04:13.260
So clap your hand
whenever you see a repeat.

00:04:16.370 --> 00:04:19.070
So this is what will rerun.

00:04:19.070 --> 00:04:20.329
Very simple images.

00:04:20.329 --> 00:04:20.930
[CLAPPING]

00:04:20.930 --> 00:04:21.769
Fine.

00:04:21.769 --> 00:04:23.420
Excellent.

00:04:23.420 --> 00:04:25.695
So images are shown one second.

00:04:25.695 --> 00:04:26.960
There's a one-second interval.

00:04:30.120 --> 00:04:32.060
[CLAPPING]

00:04:37.890 --> 00:04:39.302
You're good.

00:04:39.302 --> 00:04:42.600
No false alarm.

00:04:42.600 --> 00:04:44.901
Excellent.

00:04:44.901 --> 00:04:45.400
All right.

00:04:45.400 --> 00:04:50.220
So that was one of the level,
level 9, out of 30 complete.

00:04:50.220 --> 00:04:52.680
And so here's the
game that people had.

00:04:52.680 --> 00:04:55.170
They could play this game for
five minutes, had a break,

00:04:55.170 --> 00:04:56.280
and come back.

00:04:56.280 --> 00:04:58.630
And this game had a lot
of success, and we run it.

00:04:58.630 --> 00:05:01.350
I'm going to show many
results about this.

00:05:01.350 --> 00:05:04.380
So you could see your score, the
amount of money you have done.

00:05:04.380 --> 00:05:06.590
This was run on Amazon
Mechanical Turk.

00:05:06.590 --> 00:05:10.110
And this allow us to collect
all around the world a lot

00:05:10.110 --> 00:05:14.010
of data regarding
many, many images.

00:05:14.010 --> 00:05:15.690
So those visual
memory experiment

00:05:15.690 --> 00:05:20.550
were set up by Phillip
Isola from MIT.

00:05:20.550 --> 00:05:24.300
And you can basically
play that game

00:05:24.300 --> 00:05:26.490
for any kind of information
displayed visually.

00:05:26.490 --> 00:05:29.940
So we did it for pictures,
faces, and words.

00:05:29.940 --> 00:05:32.100
And you're going
to see the result.

00:05:32.100 --> 00:05:33.870
So let's start
with the pictures.

00:05:33.870 --> 00:05:36.780
In the first experiment,
we presented 10,000 images,

00:05:36.780 --> 00:05:41.580
and about 2,200 we
repeated many, many time.

00:05:41.580 --> 00:05:45.870
So those were the one where
we collected the score.

00:05:45.870 --> 00:05:48.900
So for a given
subject, those images

00:05:48.900 --> 00:05:50.860
were actually seen only once.

00:05:50.860 --> 00:05:52.020
Twice, sorry.

00:05:52.020 --> 00:05:55.780
So you have the stream
of images at the top.

00:05:55.780 --> 00:06:00.990
So, for instance, an image
will be shown after 90, 100,

00:06:00.990 --> 00:06:03.790
110 images or so.

00:06:03.790 --> 00:06:08.190
And if this image was one
that the subject recognized,

00:06:08.190 --> 00:06:09.990
then he will press a key.

00:06:09.990 --> 00:06:12.600
So it's exactly the
design you just did.

00:06:12.600 --> 00:06:14.910
And when first you look
at the type of images

00:06:14.910 --> 00:06:18.160
that are highly memorable
or forgettable, well,

00:06:18.160 --> 00:06:20.450
there's a trend
that we all expect,

00:06:20.450 --> 00:06:24.840
is images who are kind of
either funny or have something

00:06:24.840 --> 00:06:26.520
distinctive or
something different,

00:06:26.520 --> 00:06:29.310
or if people are
doing various action,

00:06:29.310 --> 00:06:35.250
or of some object that are
kind of out of context,

00:06:35.250 --> 00:06:37.580
those tend to be memorable.

00:06:37.580 --> 00:06:40.710
And, in general,
landscape or images

00:06:40.710 --> 00:06:44.070
that don't have any activity
tend to be forgettable.

00:06:44.070 --> 00:06:47.010
So we have those that score
for more than 2,000 images

00:06:47.010 --> 00:06:48.610
from that experiment.

00:06:48.610 --> 00:06:50.400
So one of the first
thing we need to know

00:06:50.400 --> 00:06:53.560
is, well, everyone
is playing that game,

00:06:53.560 --> 00:06:55.810
and we can have their
own memory score.

00:06:55.810 --> 00:06:58.920
But in order to know if
some images are indeed

00:06:58.920 --> 00:07:02.970
memorable for all of us or a
good amount of the population,

00:07:02.970 --> 00:07:06.670
we need to see if there is
consistency between people.

00:07:06.670 --> 00:07:08.280
So here is a simple
measure we do.

00:07:08.280 --> 00:07:10.650
You have a group of people
looking at those images,

00:07:10.650 --> 00:07:12.450
and we have the memory score.

00:07:12.450 --> 00:07:15.060
And you can split
the group into two,

00:07:15.060 --> 00:07:19.920
and rank according to the
average of the first group,

00:07:19.920 --> 00:07:22.020
the images, from
the most memorable

00:07:22.020 --> 00:07:24.090
to the less memorable.

00:07:24.090 --> 00:07:26.500
And then you can also
rank the same images

00:07:26.500 --> 00:07:28.050
in the second group.

00:07:28.050 --> 00:07:31.620
And if the two group were
identical to each other,

00:07:31.620 --> 00:07:36.150
you will get a correlation
between those two ranking of 1.

00:07:36.150 --> 00:07:39.780
And what we observe when we
split repetitively the group

00:07:39.780 --> 00:07:42.750
into two like this is
we observe a correlation

00:07:42.750 --> 00:07:46.090
of 0.75, which is pretty high.

00:07:46.090 --> 00:07:49.320
And this give us basically
the maximum performances

00:07:49.320 --> 00:07:52.140
that we can expect when
we have a group of people

00:07:52.140 --> 00:07:54.570
that can predict
the rank of images

00:07:54.570 --> 00:07:56.460
of another group of people.

00:07:56.460 --> 00:07:58.630
And actually, here
is the curve that

00:07:58.630 --> 00:08:03.965
shows what the 0.75
consistency look like.

00:08:03.965 --> 00:08:06.930
So on the x-axis, you
have the image rank

00:08:06.930 --> 00:08:09.090
according to the
group number one,

00:08:09.090 --> 00:08:14.505
so the images with the
highest memory score.

00:08:14.505 --> 00:08:17.640
So I have a group of
them that are above 90.

00:08:17.640 --> 00:08:18.800
And it's normal.

00:08:18.800 --> 00:08:23.220
The group number one in
blue decrease as images

00:08:23.220 --> 00:08:26.070
are less and less memorable.

00:08:26.070 --> 00:08:30.090
So that's like basically
your ground truth curve.

00:08:30.090 --> 00:08:32.460
And the green show
the group number

00:08:32.460 --> 00:08:36.840
two, totally independent people,
and also the performances

00:08:36.840 --> 00:08:40.880
that they got for each
bins along the image rank.

00:08:40.880 --> 00:08:44.190
And you can see the curve
are pretty close by.

00:08:44.190 --> 00:08:48.640
So a correlation of
0.75 look like this.

00:08:48.640 --> 00:08:51.750
So it means that, with
independent group of people,

00:08:51.750 --> 00:08:55.320
there are some images that
are going to be systematically

00:08:55.320 --> 00:08:57.250
more memorable or forgettable.

00:08:57.250 --> 00:09:01.260
You can see the full range going
below 40% for the images that

00:09:01.260 --> 00:09:04.860
were forgotten, up to
95 for the one that

00:09:04.860 --> 00:09:07.540
were systematically remembered.

00:09:07.540 --> 00:09:09.390
But, importantly,
a group of person

00:09:09.390 --> 00:09:11.970
is going to predict
another group of person.

00:09:11.970 --> 00:09:14.190
So there's several
ways to test memory.

00:09:14.190 --> 00:09:16.800
The way we tested memory
here to get ground truth

00:09:16.800 --> 00:09:18.870
was an objective measurement.

00:09:18.870 --> 00:09:20.970
You see an image again,
and if you remember it,

00:09:20.970 --> 00:09:21.680
you press a key.

00:09:21.680 --> 00:09:23.580
So that's an
objective measurement.

00:09:23.580 --> 00:09:26.010
But you can also
ask people, do you

00:09:26.010 --> 00:09:27.960
think you will
remember an image?

00:09:27.960 --> 00:09:30.450
Do you think someone else
will remember an image?

00:09:30.450 --> 00:09:34.580
We also run those
subjective memory score.

00:09:34.580 --> 00:09:37.630
And we observed this
very interesting trend

00:09:37.630 --> 00:09:39.410
that the subjective
judgment do not

00:09:39.410 --> 00:09:43.370
predict image memorability,
which means that,

00:09:43.370 --> 00:09:46.970
if you ask yourself, am
I going to remember this?

00:09:46.970 --> 00:09:49.430
Well, basically,
maybe, maybe not.

00:09:49.430 --> 00:09:56.720
So subjective judgment of what
you think your memory will be

00:09:56.720 --> 00:09:59.240
or what you think the memory
of someone else will be

00:09:59.240 --> 00:10:02.480
is not correlated with the true
memory, with whatever you're

00:10:02.480 --> 00:10:04.970
going to remember or not.

00:10:04.970 --> 00:10:08.210
So this was very
interesting, because it

00:10:08.210 --> 00:10:12.050
shows that objective measurement
should be needed here in order

00:10:12.050 --> 00:10:15.560
to really get a sense of what
people will remember or forget.

00:10:15.560 --> 00:10:19.970
We basically have many papers
on this topic since 2010.

00:10:19.970 --> 00:10:21.682
They are all on the web site.

00:10:21.682 --> 00:10:24.140
And then some of them will look
at the correlation existing

00:10:24.140 --> 00:10:26.180
between memory, so
the fact that you're

00:10:26.180 --> 00:10:28.850
going to remember
certain kind of images,

00:10:28.850 --> 00:10:32.000
and other attributes,
for instance, aesthetic.

00:10:32.000 --> 00:10:33.980
And, again, we found
that memorability is

00:10:33.980 --> 00:10:35.990
distinct from image aesthetic.

00:10:35.990 --> 00:10:37.730
This means that
basically you could

00:10:37.730 --> 00:10:40.055
have an image that is
judged very beautiful,

00:10:40.055 --> 00:10:43.940
or, on the contrary,
ugly or boring.

00:10:43.940 --> 00:10:45.830
And in those cases,
you will still

00:10:45.830 --> 00:10:47.680
remember those two images.

00:10:47.680 --> 00:10:51.710
So we found this absence of a
correlation between those two

00:10:51.710 --> 00:10:54.560
attribute in our values studies.

00:10:54.560 --> 00:10:58.110
We replicate this with other
data set and faces as well.

00:10:58.110 --> 00:11:00.950
So it looks like what
you will remember

00:11:00.950 --> 00:11:02.960
is this notion of
distinctiveness,

00:11:02.960 --> 00:11:06.710
but it can be beautiful or ugly.

00:11:06.710 --> 00:11:07.590
It doesn't matter.

00:11:07.590 --> 00:11:10.520
You can still either
remember it or forget it.

00:11:10.520 --> 00:11:14.390
So you had this question
about the notion of the lag.

00:11:14.390 --> 00:11:17.750
So the lag is in that you
could test visual memory

00:11:17.750 --> 00:11:20.880
after a few seconds or a
few intervening images,

00:11:20.880 --> 00:11:23.240
or you could test it
a few minutes or even

00:11:23.240 --> 00:11:25.580
one hour later, or
even days later.

00:11:25.580 --> 00:11:27.620
So because we were
running those experiments

00:11:27.620 --> 00:11:30.350
on Amazon Mechanical Turk,
we did not do the dates.

00:11:30.350 --> 00:11:33.650
However, we did run
some with a larger gap,

00:11:33.650 --> 00:11:37.340
up to 1,000 different
images between the first

00:11:37.340 --> 00:11:39.810
and the second repeat.

00:11:39.810 --> 00:11:42.440
And so here is the
design, the one

00:11:42.440 --> 00:11:46.460
that I show you for the
about 100 images intervening

00:11:46.460 --> 00:11:48.420
between the first and
the second repeat.

00:11:48.420 --> 00:11:52.310
But what about a shorter
and a longer time scale?

00:11:52.310 --> 00:11:55.700
So all that work
is also published.

00:11:55.700 --> 00:11:59.420
You can go and download the
paper and see the details.

00:11:59.420 --> 00:12:04.830
But the basic idea is that
the ranks were conserved.

00:12:04.830 --> 00:12:08.600
So if one image
is very memorable,

00:12:08.600 --> 00:12:11.360
one of the top after,
let's say, a few seconds,

00:12:11.360 --> 00:12:14.750
will still be
memorable after hours.

00:12:14.750 --> 00:12:18.320
And if an image is forgettable
or one of the most forgettable

00:12:18.320 --> 00:12:23.090
after a few seconds, will still
be forgettable after hours.

00:12:23.090 --> 00:12:27.740
So the fact that the magnitude,
the percentage of images

00:12:27.740 --> 00:12:30.140
remembered decrease is normal.

00:12:30.140 --> 00:12:33.690
That is known from memory
research for decades.

00:12:33.690 --> 00:12:35.700
So it is expected.

00:12:35.700 --> 00:12:38.270
However, memorability
here is basically

00:12:38.270 --> 00:12:42.680
the rank, which is an image, is
for a population independently

00:12:42.680 --> 00:12:46.640
basically of the
row, the magnitude.

00:12:46.640 --> 00:12:52.886
One of the most memorable given
this condition is forgettable.

00:12:52.886 --> 00:12:56.670
And we did those
experiment, both on the web

00:12:56.670 --> 00:12:58.970
as well as in the lab,
because in the lab we

00:12:58.970 --> 00:13:01.100
could control for more factors.

00:13:01.100 --> 00:13:06.380
And it was very interesting
to see in the lab experiment

00:13:06.380 --> 00:13:10.940
that only after 20 second there
were images that were totally

00:13:10.940 --> 00:13:15.930
forgotten by a good group,
a good amount of people.

00:13:15.930 --> 00:13:19.990
So some image seems to really--

00:13:19.990 --> 00:13:25.536
basically do not
stick and be gone in I

00:13:25.536 --> 00:13:26.660
don't know how many second.

00:13:26.660 --> 00:13:28.940
We did not go really
to short-term memory.

00:13:28.940 --> 00:13:32.730
We work starting in long-term
memory at 20 seconds and so on.

00:13:32.730 --> 00:13:35.250
But there's this phenomenon.

00:13:35.250 --> 00:13:37.580
So it suggests that
there are some features

00:13:37.580 --> 00:13:41.000
into the visual information
that are encoded

00:13:41.000 --> 00:13:44.492
in less details than others or
with more details than others.

00:13:44.492 --> 00:13:45.950
And what's very
interesting is then

00:13:45.950 --> 00:13:49.280
you can go to
neuroscience and basically

00:13:49.280 --> 00:13:51.230
start studying the
level of details

00:13:51.230 --> 00:13:54.270
or the quality of
encoding of an image

00:13:54.270 --> 00:13:57.150
and see where in
the visual pathway

00:13:57.150 --> 00:14:00.980
an image basically is gone
after 20 seconds or something

00:14:00.980 --> 00:14:02.600
like this.

00:14:02.600 --> 00:14:07.880
So important point--
the rank is conserved.

00:14:07.880 --> 00:14:12.530
So we also look at those
principle of memorability

00:14:12.530 --> 00:14:14.700
that we found for
images in faces.

00:14:14.700 --> 00:14:21.710
So faces is a very
interesting material

00:14:21.710 --> 00:14:24.320
to work with, because
basically it's

00:14:24.320 --> 00:14:28.580
all images that look alike.

00:14:28.580 --> 00:14:30.620
So you have basically
one object and look

00:14:30.620 --> 00:14:32.200
at many, many exemplars.

00:14:32.200 --> 00:14:35.660
And there's no reason to believe
that this high consistency we

00:14:35.660 --> 00:14:38.280
will find for
images as different

00:14:38.280 --> 00:14:43.710
as amphitheater and the parking
and so on, and landscape,

00:14:43.710 --> 00:14:47.250
this high consistency
will be found with faces.

00:14:47.250 --> 00:14:51.240
So we gather a data
set of 10,000 faces.

00:14:51.240 --> 00:14:53.610
The paper is published,
as well as the entire data

00:14:53.610 --> 00:14:55.380
set is available on the web.

00:14:55.380 --> 00:14:58.650
So you can go and download
those 10,000 faces as well as

00:14:58.650 --> 00:15:01.380
all the attribute
that we found where

00:15:01.380 --> 00:15:04.470
we study with the data set.

00:15:04.470 --> 00:15:07.080
And we also found
the same phenomenon,

00:15:07.080 --> 00:15:11.850
very high consistency, both in
the correct positive responses,

00:15:11.850 --> 00:15:14.400
when people remember
seeing a face,

00:15:14.400 --> 00:15:18.990
as well as in the false alarm,
when people did not see a face

00:15:18.990 --> 00:15:22.500
but falsely thought
that they saw a face

00:15:22.500 --> 00:15:24.600
and basically pressed the key.

00:15:24.600 --> 00:15:28.230
So this very high consistency
for both measurement

00:15:28.230 --> 00:15:33.065
suggests that, again, in the
facial features of people,

00:15:33.065 --> 00:15:35.520
or at least the way
a photo is taken,

00:15:35.520 --> 00:15:39.060
there is something at the
level of the image that

00:15:39.060 --> 00:15:43.050
will make a face highly
memorable or highly forgettable

00:15:43.050 --> 00:15:45.790
for most people.

00:15:45.790 --> 00:15:50.710
So all the details of this
study are actually on the web.

00:15:50.710 --> 00:15:59.040
It was a pretty complex study
to run, because while we have

00:15:59.040 --> 00:16:01.710
very different
sensitivity to faces,

00:16:01.710 --> 00:16:06.900
so the race effect on
basically where we grew up,

00:16:06.900 --> 00:16:10.210
so we know there's a lot
of individual differences.

00:16:10.210 --> 00:16:12.660
So the way this study
was run is the collection

00:16:12.660 --> 00:16:17.250
of faces we have followed
the US census in terms

00:16:17.250 --> 00:16:19.860
of male, female, race, and age.

00:16:19.860 --> 00:16:22.990
We started at 18
years old and older.

00:16:22.990 --> 00:16:26.290
So did our population as well.

00:16:26.290 --> 00:16:28.230
So on the group, we
show a collection

00:16:28.230 --> 00:16:32.430
of faces that did match the
population, the people who

00:16:32.430 --> 00:16:35.450
were running the study.

00:16:35.450 --> 00:16:38.110
And, as I say, all the data
are available on the web

00:16:38.110 --> 00:16:43.170
if some of you want to go back
and do additional analysis.

00:16:43.170 --> 00:16:44.320
So we kept going with this.

00:16:44.320 --> 00:16:47.840
So we have the consistency
in the visual material.

00:16:47.840 --> 00:16:50.070
So now what about words?

00:16:50.070 --> 00:16:51.930
So words is a very
interesting case,

00:16:51.930 --> 00:16:54.090
because now you
do know the words.

00:16:54.090 --> 00:16:56.960
But are there some kind of
words that we can predict

00:16:56.960 --> 00:17:00.030
are more forgettable
or memorable?

00:17:00.030 --> 00:17:03.600
Again, there's no reason to
believe a high consistency will

00:17:03.600 --> 00:17:05.180
be found.

00:17:05.180 --> 00:17:07.710
But we've run the study
twice with two different data

00:17:07.710 --> 00:17:15.119
set, again, ten thousand item,
and two different set of words.

00:17:15.119 --> 00:17:18.900
And we found, again, very
high consistency, which

00:17:18.900 --> 00:17:21.599
is that a collection of
words were systematically

00:17:21.599 --> 00:17:25.440
remembered by people and
others were forgettable.

00:17:25.440 --> 00:17:28.620
So this work that is
done in collaboration

00:17:28.620 --> 00:17:32.890
with Mahowald, Isola,
Gibson, and Fedorenko

00:17:32.890 --> 00:17:35.310
is under submission.

00:17:35.310 --> 00:17:38.280
But let me give you a taste
of the words that we found.

00:17:38.280 --> 00:17:42.610
So what make words more
memorable or forgettable?

00:17:42.610 --> 00:17:47.130
So here is a cartoon that give
you the basic idea is, well,

00:17:47.130 --> 00:17:49.710
if there is one word
for one meaning,

00:17:49.710 --> 00:17:52.280
so basically a word
has a single meaning,

00:17:52.280 --> 00:17:53.940
it will have a tendency
to be much more

00:17:53.940 --> 00:17:57.210
memorable than if a
words has many meanings.

00:17:57.210 --> 00:18:00.240
So in the paper, we also
look at the correlation

00:18:00.240 --> 00:18:06.280
between memorability and image
ability or frequency and so on.

00:18:06.280 --> 00:18:09.330
And all this is describe,
but really the main factor

00:18:09.330 --> 00:18:15.470
is this one-to-one referent
between a word and its meaning

00:18:15.470 --> 00:18:16.950
or concept.

00:18:16.950 --> 00:18:20.940
So let's look at
some of the example.

00:18:20.940 --> 00:18:23.810
The paper will come with the
two data set and thousand

00:18:23.810 --> 00:18:28.080
of words that were found
memorable and forgettable.

00:18:28.080 --> 00:18:30.780
So I think if you
write a letter,

00:18:30.780 --> 00:18:34.000
you should not say that
your student is excellent

00:18:34.000 --> 00:18:36.580
but that she is fabulous.

00:18:36.580 --> 00:18:39.450
And our research is not a blast.

00:18:39.450 --> 00:18:40.800
It's in vogue.

00:18:40.800 --> 00:18:44.880
And the idea of a team is not
irrational, they are grotesque.

00:18:44.880 --> 00:18:48.420
So those are just a few
example of the words

00:18:48.420 --> 00:18:50.760
that on average,
of the three first,

00:18:50.760 --> 00:18:54.420
had more referent
and the tendency

00:18:54.420 --> 00:18:55.950
to be more forgettable
because they

00:18:55.950 --> 00:18:59.550
might be used for many
more things than the one.

00:18:59.550 --> 00:19:03.180
I also notice a lot
of the French words

00:19:03.180 --> 00:19:05.220
tend to be memorable.

00:19:05.220 --> 00:19:09.870
So we do find this
stable principle

00:19:09.870 --> 00:19:13.890
that you can
predict the content,

00:19:13.890 --> 00:19:16.800
can predict what type
of images, faces,

00:19:16.800 --> 00:19:18.150
words that are memorable.

00:19:18.150 --> 00:19:19.830
And we also did it
for visualization,

00:19:19.830 --> 00:19:22.560
starting working on
the topic of education.

00:19:22.560 --> 00:19:24.930
So this is very useful,
because at least

00:19:24.930 --> 00:19:28.470
at the level of a group, you can
start making that prediction.

00:19:28.470 --> 00:19:30.780
Oh, I also have
massive and avalanche.

00:19:30.780 --> 00:19:32.170
Forgot about it.

00:19:32.170 --> 00:19:38.530
So now that we were able
to have all those data

00:19:38.530 --> 00:19:41.260
and see that there
is this consistency,

00:19:41.260 --> 00:19:44.440
then one of the next question
you can look at is, OK, well,

00:19:44.440 --> 00:19:47.080
if it seems that we
all have a tendency

00:19:47.080 --> 00:19:53.150
to remember the same image,
can we find a neural signature

00:19:53.150 --> 00:19:55.400
into the human brain?

00:19:55.400 --> 00:19:59.800
So the question of memorability,
is it a perceptual or memory

00:19:59.800 --> 00:20:00.340
question?

00:20:00.340 --> 00:20:02.680
Because in all our
experiment, the images

00:20:02.680 --> 00:20:04.906
are shown for a short time.

00:20:04.906 --> 00:20:06.280
And then, when
they are repeated,

00:20:06.280 --> 00:20:08.020
you see them a second time.

00:20:08.020 --> 00:20:11.410
But basically, all the action
is at the perception level.

00:20:11.410 --> 00:20:14.980
Whenever you perceive this
image for half a second or one

00:20:14.980 --> 00:20:17.390
second, there is
something going on here

00:20:17.390 --> 00:20:20.560
at the perception
level that is going

00:20:20.560 --> 00:20:24.190
to bias if this image is going
to basically go into memory

00:20:24.190 --> 00:20:25.360
or not.

00:20:25.360 --> 00:20:28.720
So, knowing this,
if we want to look

00:20:28.720 --> 00:20:31.540
at the potential neural
framework of memorability,

00:20:31.540 --> 00:20:33.910
we have to look at
the entire brain.

00:20:33.910 --> 00:20:36.190
We have to look at
all the region that

00:20:36.190 --> 00:20:41.710
have been found to be
related to perception,

00:20:41.710 --> 00:20:46.000
faces perception, picture
perception, object, space,

00:20:46.000 --> 00:20:50.260
and so on, as well as the
medial temporal lobe region,

00:20:50.260 --> 00:20:52.750
more in the middle
of the brain, that

00:20:52.750 --> 00:20:54.980
have been related to memory.

00:20:54.980 --> 00:20:57.790
So this is what we did
with Wilma Bainbridge.

00:20:57.790 --> 00:21:01.330
This is her PhD,
basically having

00:21:01.330 --> 00:21:03.370
a look at all those region.

00:21:03.370 --> 00:21:07.180
And here is the very
simple experiment we run.

00:21:07.180 --> 00:21:11.080
So we took a collection
of faces and scene

00:21:11.080 --> 00:21:15.250
from the thousands we
have, and where every one,

00:21:15.250 --> 00:21:18.010
we'll basically separate
those two between looking

00:21:18.010 --> 00:21:20.500
at the region that
are more activated

00:21:20.500 --> 00:21:26.350
for seeing versus the region
more activated for faces.

00:21:26.350 --> 00:21:29.400
We split it that way,
between the memorable

00:21:29.400 --> 00:21:31.570
and the forgettable set.

00:21:31.570 --> 00:21:37.990
So in those set,
every images is novel.

00:21:37.990 --> 00:21:40.030
So exactly like in
the memory experiment,

00:21:40.030 --> 00:21:44.680
we could show them one
time for half a second.

00:21:44.680 --> 00:21:46.100
So you had the perception level.

00:21:46.100 --> 00:21:47.890
You're in a
perception experiment.

00:21:47.890 --> 00:21:52.030
You saw those image one after
the other only one time.

00:21:52.030 --> 00:21:54.340
All those images are novel.

00:21:54.340 --> 00:21:56.020
So we're going to
look at the contrast

00:21:56.020 --> 00:22:01.840
from novel image minus novel
image, from scene minus scene,

00:22:01.840 --> 00:22:07.240
from faces minus faces,
except that some images are

00:22:07.240 --> 00:22:12.430
highly memorable and some
are highly forgettable.

00:22:12.430 --> 00:22:15.040
So other factor that
you have to look at

00:22:15.040 --> 00:22:21.880
is, it's still possible that
within those group of images

00:22:21.880 --> 00:22:26.510
and faces that are highly
memorable or forgettable,

00:22:26.510 --> 00:22:28.120
that there's a lot
of image features

00:22:28.120 --> 00:22:30.260
that basically
correlate with those.

00:22:30.260 --> 00:22:33.400
So if you take a collection of
images like I shown you before,

00:22:33.400 --> 00:22:37.150
and did the environment or
the photo that have people

00:22:37.150 --> 00:22:40.460
or action tend to be
memorable versus a landscape

00:22:40.460 --> 00:22:42.880
tend to be forgettable?

00:22:42.880 --> 00:22:46.270
So here you have a
lot of visual features

00:22:46.270 --> 00:22:51.610
that will co-vary with the
dimension of memorability.

00:22:51.610 --> 00:22:55.780
So in that study for the
brain, we equalize for that,

00:22:55.780 --> 00:22:57.650
because we had enough images.

00:22:57.650 --> 00:23:00.810
So here you have a
sample of the two groups

00:23:00.810 --> 00:23:04.400
and sample of images and the
type of statistic we look at.

00:23:04.400 --> 00:23:06.310
And the two, for
instance, for this scene

00:23:06.310 --> 00:23:10.240
were equalized in term of the
type of category you had--

00:23:10.240 --> 00:23:14.440
outdoor, indoor, beach,
landscape, house, kitchen,

00:23:14.440 --> 00:23:15.680
and so on--

00:23:15.680 --> 00:23:18.790
as well as a collection
of low-level features.

00:23:18.790 --> 00:23:21.850
And you can see some of
the average signature that

00:23:21.850 --> 00:23:28.540
are actually identical on a lot
of low to mid to higher level

00:23:28.540 --> 00:23:31.590
image features that were
equalized between the two

00:23:31.590 --> 00:23:32.390
group.

00:23:32.390 --> 00:23:35.620
So whatever we find
is not going to be

00:23:35.620 --> 00:23:41.330
due to simple statistic due to
the image or the type of object

00:23:41.330 --> 00:23:43.000
those have.

00:23:43.000 --> 00:23:46.960
We could play the same game
for the faces, so we did.

00:23:46.960 --> 00:23:50.320
Here are the numbers again,
memorable and forgettable

00:23:50.320 --> 00:23:54.220
faces that were also equalized
for various attribute

00:23:54.220 --> 00:23:57.550
like attractiveness,
emotion, kindness, happiness,

00:23:57.550 --> 00:24:02.260
and so on, as well
as male, female,

00:24:02.260 --> 00:24:05.620
race, as well as
expression, and so on.

00:24:05.620 --> 00:24:08.560
And you can see that the
statistic, the average faces

00:24:08.560 --> 00:24:11.380
for both the memorable
and the forgettable group,

00:24:11.380 --> 00:24:13.520
are actually also identical.

00:24:13.520 --> 00:24:19.150
So with those group, what's left
is hopefully only the factor

00:24:19.150 --> 00:24:23.700
of something else in the image
at the level of a higher image

00:24:23.700 --> 00:24:27.520
statistic, because only image
statistic will explain the fact

00:24:27.520 --> 00:24:30.520
that very different people
will remember the same faces

00:24:30.520 --> 00:24:31.990
and forget the same faces.

00:24:31.990 --> 00:24:36.040
But certainly not some of
obvious low-level image

00:24:36.040 --> 00:24:36.700
statistic.

00:24:36.700 --> 00:24:40.310
Those cannot explain any result.

00:24:40.310 --> 00:24:44.710
So two years and
four study later,

00:24:44.710 --> 00:24:47.840
we replicated basically
this study four times

00:24:47.840 --> 00:24:50.500
with many different matter.

00:24:50.500 --> 00:24:52.480
Then, I'm just going
to show you here one

00:24:52.480 --> 00:24:55.390
snapshot of the result.

00:24:55.390 --> 00:24:58.090
So this is a Multi-variate
Pattern Analysis

00:24:58.090 --> 00:25:03.270
looking at the memorable
versus the forgettable groups,

00:25:03.270 --> 00:25:06.190
searchlight analysis,
MVPA looking

00:25:06.190 --> 00:25:10.620
for the region of the
brain that are more active.

00:25:10.620 --> 00:25:12.580
Well, that have a
different pattern.

00:25:12.580 --> 00:25:15.310
They are also more active,
but at a different pattern

00:25:15.310 --> 00:25:18.070
for memorable faces.

00:25:18.070 --> 00:25:20.310
And we find signature
in the hippocampus,

00:25:20.310 --> 00:25:22.810
the parahippocampal
area, as well as

00:25:22.810 --> 00:25:30.620
the perirhinal that are typical
for memorable faces and scene,

00:25:30.620 --> 00:25:35.590
and a new signature in the
visual area or even the higher

00:25:35.590 --> 00:25:38.890
visual area, because we
did equalize for those.

00:25:38.890 --> 00:25:43.510
So it seems to show that
those MTL region play

00:25:43.510 --> 00:25:47.520
a role in a kind of higher-order
statistical perception,

00:25:47.520 --> 00:25:52.000
a notion of distinctiveness
that is within those image.

00:25:52.000 --> 00:25:56.110
So this suggest that at the
perception of a new image,

00:25:56.110 --> 00:26:00.180
could be a face or a scene
or a collection of object

00:26:00.180 --> 00:26:04.120
and so on, well, there's already
a signature of this that's

00:26:04.120 --> 00:26:08.650
going to guide or bias if this
is going to be put into memory

00:26:08.650 --> 00:26:09.560
or not.

00:26:09.560 --> 00:26:11.860
And we have those at
the level of the group,

00:26:11.860 --> 00:26:17.840
looking now at the
level of individuals.

00:26:17.840 --> 00:26:19.730
All right.

00:26:19.730 --> 00:26:22.780
So, well, it looks
like we're going,

00:26:22.780 --> 00:26:25.780
given your question
is now trying to model

00:26:25.780 --> 00:26:27.640
this notion of memorability.

00:26:27.640 --> 00:26:31.570
So we have a good case that
there is some information

00:26:31.570 --> 00:26:34.210
into the image of a
higher-level status--

00:26:34.210 --> 00:26:36.180
we don't know which one--

00:26:36.180 --> 00:26:41.410
that cannot be explained by
simple features that make all

00:26:41.410 --> 00:26:43.720
of us reacting the same way.

00:26:43.720 --> 00:26:45.430
Even our brain do
react the same way.

00:26:45.430 --> 00:26:50.530
We also have images, signature
of memorability coming up.

00:26:50.530 --> 00:26:53.860
So we have this intrinsic
information that make all of us

00:26:53.860 --> 00:26:57.110
remember or forget the same
kind of visual information.

00:26:57.110 --> 00:27:02.440
So can we now in a way
imitate or model those result

00:27:02.440 --> 00:27:06.470
into an artificial system?

00:27:06.470 --> 00:27:06.970
All right.

00:27:06.970 --> 00:27:15.220
So you have all heard about the
revolution in computer vision

00:27:15.220 --> 00:27:18.430
a couple of years
ago and deep learning

00:27:18.430 --> 00:27:21.370
and those neural
networks that are now

00:27:21.370 --> 00:27:26.900
able to outperform a lot of--

00:27:26.900 --> 00:27:30.040
that were able to
recognize and perform

00:27:30.040 --> 00:27:33.260
a lot of task, some of them
at the level of humans,

00:27:33.260 --> 00:27:36.130
so recognizing various
object and so on.

00:27:36.130 --> 00:27:39.760
And one of the aspect of
those neural networks,

00:27:39.760 --> 00:27:41.170
and I'm going to
talk about them,

00:27:41.170 --> 00:27:44.150
is that they require
a lot of information.

00:27:44.150 --> 00:27:47.680
So you need to teach
them the classes

00:27:47.680 --> 00:27:49.710
you want to distinguish.

00:27:49.710 --> 00:27:54.160
And they can and need
a lot, a lot of data.

00:27:54.160 --> 00:27:57.540
So everything we did so
far, we are getting that

00:27:57.540 --> 00:28:00.350
on a couple of thousand images.

00:28:00.350 --> 00:28:03.720
And that's really not enough
to even start scratching

00:28:03.720 --> 00:28:05.920
computational modeling.

00:28:05.920 --> 00:28:10.390
So with Aditya Khosla,
we run recently

00:28:10.390 --> 00:28:12.460
a new large-scale
visual memorability

00:28:12.460 --> 00:28:16.360
study on Amazon Mechanical
Turk, but this time getting

00:28:16.360 --> 00:28:19.750
score for 60,000 photograph.

00:28:19.750 --> 00:28:24.310
And you have a set of
a sample over here.

00:28:24.310 --> 00:28:25.022
60,000.

00:28:25.022 --> 00:28:28.420
They are all going to be
available in a few weeks.

00:28:28.420 --> 00:28:29.800
The paper is under revision.

00:28:29.800 --> 00:28:30.970
It's looking good.

00:28:30.970 --> 00:28:32.900
So as soon as we
have a citation,

00:28:32.900 --> 00:28:36.970
we are going to give away all
the data as well as the score

00:28:36.970 --> 00:28:38.800
and the images and so on.

00:28:38.800 --> 00:28:41.080
And in this
experiment, images were

00:28:41.080 --> 00:28:44.210
presented for 600
milliseconds or shorter time,

00:28:44.210 --> 00:28:47.200
but they really did
not change much.

00:28:47.200 --> 00:28:52.420
So as a snapshot, because
the 60,000 images really

00:28:52.420 --> 00:28:55.560
cover a lot of type of photo--

00:28:55.560 --> 00:28:59.860
faces, event, action, and
even a graph and so on--

00:28:59.860 --> 00:29:02.320
well, you either hear
some that were highly

00:29:02.320 --> 00:29:05.050
memorable or forgettable.

00:29:05.050 --> 00:29:06.730
There's also the website.

00:29:06.730 --> 00:29:09.880
It's not populated
yet with many things,

00:29:09.880 --> 00:29:12.370
but it will be very shortly.

00:29:12.370 --> 00:29:17.980
And the correlation we
got on that data set

00:29:17.980 --> 00:29:20.710
was pretty high again, 0.68.

00:29:20.710 --> 00:29:22.560
That was expected.

00:29:22.560 --> 00:29:27.730
And the paper explain
the various split we did.

00:29:27.730 --> 00:29:30.740
But, again, we find this
very high correlation.

00:29:30.740 --> 00:29:34.500
So we do know that there's
something to model there.

00:29:34.500 --> 00:29:38.700
And other-- again, a
very quick summary.

00:29:38.700 --> 00:29:42.080
The type of images that seems
to be the most memorable

00:29:42.080 --> 00:29:45.650
are the one which have a
focus and close settings, that

00:29:45.650 --> 00:29:48.740
show some dynamics and
something distinctive, unusual,

00:29:48.740 --> 00:29:52.910
a little different, whereas
the less memorable one seems

00:29:52.910 --> 00:29:56.120
to have no single focus,
distant view, static,

00:29:56.120 --> 00:29:58.880
and more commonalities.

00:29:58.880 --> 00:30:02.120
All photos, you can still
find two images that

00:30:02.120 --> 00:30:05.390
will be focus and
dynamics, and one of them

00:30:05.390 --> 00:30:07.190
will be more memorable
than the other

00:30:07.190 --> 00:30:12.110
because it will have something
unusual that now our system can

00:30:12.110 --> 00:30:14.120
capture.

00:30:14.120 --> 00:30:16.400
So we have this new data set.

00:30:16.400 --> 00:30:18.950
We have all the memory score.

00:30:18.950 --> 00:30:20.780
We have the high consistency.

00:30:20.780 --> 00:30:26.450
How do we even start thinking
about the computational model

00:30:26.450 --> 00:30:29.730
of visual memory
or memorability?

00:30:29.730 --> 00:30:32.750
Well, in order to give
you a sense of one

00:30:32.750 --> 00:30:36.110
of the basic we needs,
in order to even start

00:30:36.110 --> 00:30:42.060
thinking of a model, I'm going
to show and run another demo.

00:30:42.060 --> 00:30:45.380
So in this demo, you're also
going to see some images.

00:30:45.380 --> 00:30:48.790
And clap your hand whenever
you see an image that repeat.

00:30:48.790 --> 00:30:49.400
OK?

00:30:49.400 --> 00:30:51.550
Exactly the same
game than before.

00:30:51.550 --> 00:30:53.191
Ready?

00:30:53.191 --> 00:30:53.690
All right.

00:30:53.690 --> 00:30:57.070
If everyone play the
game, it will be fun.

00:30:57.070 --> 00:30:59.810
OK.

00:30:59.810 --> 00:31:01.794
[CLAPPING]

00:31:04.770 --> 00:31:05.770
[CLAPPING]

00:31:05.770 --> 00:31:08.382
A little false alarm.

00:31:08.382 --> 00:31:09.298
[CLAPPING]

00:31:09.298 --> 00:31:10.181
False alarm.

00:31:10.181 --> 00:31:10.680
[CLAPPING]

00:31:10.680 --> 00:31:11.930
Good.

00:31:11.930 --> 00:31:12.500
More energy.

00:31:15.161 --> 00:31:15.660
[CLAPPING]

00:31:15.660 --> 00:31:18.330
Good.

00:31:18.330 --> 00:31:19.302
[CLAPPING]

00:31:19.302 --> 00:31:21.732
Sorry.

00:31:21.732 --> 00:31:26.120
[CLAPPING]

00:31:26.120 --> 00:31:26.946
No

00:31:26.946 --> 00:31:28.749
[LAUGHTER]

00:31:30.138 --> 00:31:31.070
[CLAPPING]

00:31:31.070 --> 00:31:33.300
No.

00:31:33.300 --> 00:31:34.250
[CLAPPING]

00:31:34.250 --> 00:31:35.680
Yes.

00:31:35.680 --> 00:31:37.150
[CLAPPING]

00:31:37.150 --> 00:31:37.787
No.

00:31:37.787 --> 00:31:38.620
AUDIENCE: Too close.

00:31:38.620 --> 00:31:39.110
[CLAPPING]

00:31:39.110 --> 00:31:39.735
AUDE OLIVA: No.

00:31:39.735 --> 00:31:42.029
Yeah, that was-- yes.

00:31:42.029 --> 00:31:44.444
[CLAPPING]

00:31:44.444 --> 00:31:46.380
[LAUGHTER]

00:31:46.380 --> 00:31:47.070
All right.

00:31:47.070 --> 00:31:51.710
So for the sake of
the demo, I put here

00:31:51.710 --> 00:31:53.690
really images that
are very different,

00:31:53.690 --> 00:31:55.140
some you are familiar with.

00:31:55.140 --> 00:31:56.910
You have a concept.

00:31:56.910 --> 00:31:58.230
You know what it is.

00:31:58.230 --> 00:31:59.220
This is a restaurant.

00:31:59.220 --> 00:32:00.660
This is an alley.

00:32:00.660 --> 00:32:02.760
This is a stadium, and so on.

00:32:02.760 --> 00:32:08.730
And some for which either you
don't have a specific concept,

00:32:08.730 --> 00:32:10.290
or you have the same concept--

00:32:10.290 --> 00:32:13.950
texture, paintings,
texture, texture, texture.

00:32:13.950 --> 00:32:17.070
And the basic idea
of memory is you

00:32:17.070 --> 00:32:21.720
need to recognize, to put a
unique tag or collection of tag

00:32:21.720 --> 00:32:25.080
in order to remember
that individual image.

00:32:25.080 --> 00:32:27.710
So the fact that you saw
a collection of texture

00:32:27.710 --> 00:32:30.120
or paintings or whatever
you want to call them,

00:32:30.120 --> 00:32:32.240
you'll remember that as a group.

00:32:32.240 --> 00:32:34.950
But to go to the
individual memory of one,

00:32:34.950 --> 00:32:37.740
you're going to need to
have a specific concept.

00:32:37.740 --> 00:32:39.750
And in order to
remember it, this

00:32:39.750 --> 00:32:45.420
is going to be an abstraction,
a format, a collection of words,

00:32:45.420 --> 00:32:48.420
or a coding that make it unique.

00:32:48.420 --> 00:32:50.670
So you need to
recognize to remember,

00:32:50.670 --> 00:32:54.050
which means that if you
want a model of memory which

00:32:54.050 --> 00:32:55.840
start from the raw image--

00:32:55.840 --> 00:32:58.200
I'm not in toy world here.

00:32:58.200 --> 00:33:01.510
I really start from the
raw image, like the retina.

00:33:01.510 --> 00:33:07.050
Well, you're going to need
to build a visual recognition

00:33:07.050 --> 00:33:08.400
system first.

00:33:08.400 --> 00:33:12.840
So first you need a model
that recognize object

00:33:12.840 --> 00:33:15.360
and scene and event and so on.

00:33:15.360 --> 00:33:19.020
And then, from this,
there can be a base

00:33:19.020 --> 00:33:21.570
to start modeling memory.

00:33:21.570 --> 00:33:26.190
So, fortunately, the
field of computer vision

00:33:26.190 --> 00:33:29.120
made a lot of progress in
the past couple of years.

00:33:29.120 --> 00:33:32.732
So now we do have visual
recognition system

00:33:32.732 --> 00:33:33.690
that works pretty well.

00:33:33.690 --> 00:33:35.190
I'm going to describe them.

00:33:35.190 --> 00:33:40.060
We need to first a visual
recognition system.

00:33:40.060 --> 00:33:40.560
All right.

00:33:40.560 --> 00:33:44.210
So, what does a visual
recognition system needs to do?

00:33:44.210 --> 00:33:47.499
So, well, it's your
Sunday morning.

00:33:47.499 --> 00:33:49.290
You're going to the
picnic area, and you're

00:33:49.290 --> 00:33:50.610
faced with that view.

00:33:50.610 --> 00:33:52.440
You take a photo.

00:33:52.440 --> 00:33:55.780
This photo actually
became viral on the web.

00:33:55.780 --> 00:33:59.520
And here is the state of the
art of computer vision system.

00:33:59.520 --> 00:34:01.620
When it comes to
recognize the object--

00:34:01.620 --> 00:34:03.120
I know it's a
different view, but it

00:34:03.120 --> 00:34:06.240
works very well for any view--

00:34:06.240 --> 00:34:09.510
object recognition
for about 1,000

00:34:09.510 --> 00:34:15.256
object category is reaching
human performances so far.

00:34:15.256 --> 00:34:18.460
And so this will tell
you this is a black bear,

00:34:18.460 --> 00:34:21.960
there's a bench, a table,
and so on, and trees

00:34:21.960 --> 00:34:23.070
in the background.

00:34:23.070 --> 00:34:28.320
But it's missing the point,
that this is a picnic area.

00:34:28.320 --> 00:34:33.840
So you do need at least
two kind of information

00:34:33.840 --> 00:34:38.010
in order to reach visual
scene understanding.

00:34:38.010 --> 00:34:39.900
You need the scene
and the context,

00:34:39.900 --> 00:34:44.380
you need to know the place, and
you need to know the object.

00:34:44.380 --> 00:34:47.580
So, as I said, so
far on the challenge

00:34:47.580 --> 00:34:50.820
that's called the ImageNet
Challenge in computer vision,

00:34:50.820 --> 00:34:54.360
computer vision model average
human performances, which

00:34:54.360 --> 00:35:00.540
is 95% correct on exemplars
of objects that have never

00:35:00.540 --> 00:35:04.440
been trained on, a new one,
for 1,000 object category.

00:35:04.440 --> 00:35:09.030
And recently, we basically
published a few papers

00:35:09.030 --> 00:35:12.030
for the other part of visual
scene understanding, the place

00:35:12.030 --> 00:35:13.120
and the context.

00:35:13.120 --> 00:35:16.420
And this is an
output of our system.

00:35:16.420 --> 00:35:17.670
And you can go on the web--

00:35:17.670 --> 00:35:19.836
I'm going to give you the
address-- and play with it

00:35:19.836 --> 00:35:22.410
and see the performances
of recognizing

00:35:22.410 --> 00:35:24.990
the context and the place.

00:35:24.990 --> 00:35:31.890
So just to put into context what
the field of computer vision

00:35:31.890 --> 00:35:36.030
have been doing for
the past 15 years

00:35:36.030 --> 00:35:40.740
is, well, the
number of data set,

00:35:40.740 --> 00:35:44.280
the number of images by data
set has been increasing,

00:35:44.280 --> 00:35:48.160
so that there's more
exemplar to learn from.

00:35:48.160 --> 00:35:51.900
And, in perspective, you can
see that two-years-old kids.

00:35:51.900 --> 00:35:55.380
Of course, it will depend on
the sampling you use in order

00:35:55.380 --> 00:35:58.260
to have an estimate of the
number of visual information

00:35:58.260 --> 00:35:59.750
that the retina sees.

00:35:59.750 --> 00:36:03.450
But it sees much,
much more variety

00:36:03.450 --> 00:36:06.600
and numbers of visual input.

00:36:06.600 --> 00:36:10.650
But right now, both ImageNet,
that is, a data set of objects,

00:36:10.650 --> 00:36:13.800
and Places, which is
a data set of scene

00:36:13.800 --> 00:36:18.580
I'm going to present to you
now, have about 10 million label

00:36:18.580 --> 00:36:20.990
images of many categories.

00:36:20.990 --> 00:36:24.270
So label means that for the
places, it will tell you,

00:36:24.270 --> 00:36:26.880
this is a kitchen or this
is a conference room, and so

00:36:26.880 --> 00:36:28.720
on for hundred of categories.

00:36:28.720 --> 00:36:32.220
So 10 million is
largely enough to start

00:36:32.220 --> 00:36:36.360
building very serious
visual recognition system,

00:36:36.360 --> 00:36:38.610
but it's no near
the human brain.

00:36:38.610 --> 00:36:42.010
However, we might
be getting there.

00:36:42.010 --> 00:36:47.220
So how do we even start
to build a visual scene

00:36:47.220 --> 00:36:49.110
recognition data set?

00:36:49.110 --> 00:36:51.750
This is a work we did
and published in 2010,

00:36:51.750 --> 00:36:53.390
the Scene Understanding,
or SUN data

00:36:53.390 --> 00:36:58.800
set, where we collected the
words from the dictionary that

00:36:58.800 --> 00:37:02.440
correspond to places at
the subordinate level--

00:37:02.440 --> 00:37:04.020
I'm going to give you example--

00:37:04.020 --> 00:37:07.020
and then retrieve a lot
of images from the web.

00:37:07.020 --> 00:37:12.480
And there was a total of
900 different categories,

00:37:12.480 --> 00:37:15.390
and about 400 of them
have enough exemplars

00:37:15.390 --> 00:37:19.590
to build a artificial system.

00:37:19.590 --> 00:37:24.090
So instead of going and only
looking to build images,

00:37:24.090 --> 00:37:28.020
like to build a data set of
a bedroom and kitchen and so

00:37:28.020 --> 00:37:32.340
on, what's happening for
the human brain is that you

00:37:32.340 --> 00:37:35.340
have a different environment.

00:37:35.340 --> 00:37:39.560
You see there's many
attribute you can put in.

00:37:39.560 --> 00:37:43.150
And you are forthcoming
and storytelling and so on.

00:37:43.150 --> 00:37:45.990
So most of the places,
it's not only that this is,

00:37:45.990 --> 00:37:48.600
for instance, a bedroom,
is that you will go more

00:37:48.600 --> 00:37:51.010
to a student bedroom or--

00:37:51.010 --> 00:37:53.130
Well, I think those
are student bedroom two

00:37:53.130 --> 00:37:55.710
doors from my colleague.

00:37:55.710 --> 00:37:58.620
So there are many
type of adjective

00:37:58.620 --> 00:38:01.380
that we can use in order
to retrieve images that

00:38:01.380 --> 00:38:08.130
will give us a larger panorama
of the type of concept

00:38:08.130 --> 00:38:10.020
that are used by the
human brain in order

00:38:10.020 --> 00:38:12.300
to recognize environment.

00:38:12.300 --> 00:38:15.040
So a simple bedroom.

00:38:15.040 --> 00:38:18.720
The tag were put automatically.

00:38:18.720 --> 00:38:20.020
Superior bedroom.

00:38:20.020 --> 00:38:21.750
Senior bedroom.

00:38:21.750 --> 00:38:23.520
Colorful bedroom.

00:38:23.520 --> 00:38:24.510
Hotel bedroom.

00:38:24.510 --> 00:38:25.350
And so on and so on.

00:38:25.350 --> 00:38:27.810
So that was the
retrieval we did,

00:38:27.810 --> 00:38:29.970
which means that now,
for every category there

00:38:29.970 --> 00:38:37.830
is also a tag in term of
the subtype of environment

00:38:37.830 --> 00:38:39.360
this can be.

00:38:39.360 --> 00:38:40.680
Messy bedroom.

00:38:40.680 --> 00:38:45.760
And a couple of years later,
80 million images later,

00:38:45.760 --> 00:38:49.320
and a lot of Amazon
Mechanical Turk experiment,

00:38:49.320 --> 00:38:54.270
we are launching this
week the Places2 data set,

00:38:54.270 --> 00:38:58.830
with 460 categories, different
categories of environment,

00:38:58.830 --> 00:39:01.530
and 10 million images label.

00:39:01.530 --> 00:39:04.530
So this is a larger data
set of label images,

00:39:04.530 --> 00:39:09.850
with a label to be used right
away for artificial system

00:39:09.850 --> 00:39:13.120
learning, deep
learning, and so on.

00:39:13.120 --> 00:39:17.460
So here is just a snapshot
of the differences

00:39:17.460 --> 00:39:21.570
of the places in term of the
number of exemplars with other

00:39:21.570 --> 00:39:25.150
a large data set.

00:39:25.150 --> 00:39:30.990
So the Places data
set is actually

00:39:30.990 --> 00:39:35.260
part of the ImageNet
challenge this year,

00:39:35.260 --> 00:39:38.175
which means that you
can go to ImageNet

00:39:38.175 --> 00:39:42.700
and register for the challenge
and download right now,

00:39:42.700 --> 00:39:46.560
tonight, eight million
images of places

00:39:46.560 --> 00:39:50.370
to use for the learning
of your neural networks,

00:39:50.370 --> 00:39:55.160
as well as of a set that
will be used for the testing,

00:39:55.160 --> 00:39:58.090
and participate
to the challenge.

00:39:58.090 --> 00:40:02.950
So this was launch last
week, and the website

00:40:02.950 --> 00:40:06.710
associated with Places
will be launched this week.

00:40:06.710 --> 00:40:11.460
And we decided to just give this
away to everyone right away.

00:40:11.460 --> 00:40:13.110
We are finishing
up the paper now.

00:40:13.110 --> 00:40:15.510
It will be an archive paper.

00:40:15.510 --> 00:40:19.260
No time to wait for
month and month.

00:40:19.260 --> 00:40:22.320
This is a data set that can
be used by a lot of people

00:40:22.320 --> 00:40:24.313
to make progress fast.

00:40:24.313 --> 00:40:28.360
And so that's what we are doing.

00:40:28.360 --> 00:40:32.190
So, as I said, the
computer vision model

00:40:32.190 --> 00:40:36.460
now require, if you use deep
learning, a lot of data.

00:40:36.460 --> 00:40:40.110
And we hope that
with this data set,

00:40:40.110 --> 00:40:42.760
fast progress are
going to be made.

00:40:42.760 --> 00:40:48.750
So what we specifically did is
using the AlexNet deep learning

00:40:48.750 --> 00:40:49.830
architecture--

00:40:49.830 --> 00:40:51.440
If you don't know
what this is, I

00:40:51.440 --> 00:40:53.570
can tell you later how
to basically access

00:40:53.570 --> 00:40:55.440
to it with the paper on so on.

00:40:55.440 --> 00:40:56.910
This is not my model.

00:40:56.910 --> 00:40:59.710
This is a model put
together by Geoffrey Hinton

00:40:59.710 --> 00:41:01.930
and collaborator
a few years ago.

00:41:01.930 --> 00:41:04.320
And you can download the
model or download the code

00:41:04.320 --> 00:41:04.861
and re-train.

00:41:07.350 --> 00:41:10.020
So neural net now
basically are based

00:41:10.020 --> 00:41:13.610
on the collection
of operation that

00:41:13.610 --> 00:41:17.670
are call layers, convolution,
normalization, simple image

00:41:17.670 --> 00:41:20.760
processing operation that
you do in a sequence.

00:41:20.760 --> 00:41:23.540
You do it one time, then a
second and third and so on.

00:41:23.540 --> 00:41:26.910
And then you have those
multi-layers models.

00:41:26.910 --> 00:41:30.960
And the number of layers is
still a question of research.

00:41:30.960 --> 00:41:32.730
How does layer
correspond to the brain?

00:41:32.730 --> 00:41:36.300
I'm going to say a
little bit about that.

00:41:36.300 --> 00:41:40.060
And using this simple--

00:41:40.060 --> 00:41:45.985
well, this AlexNet model,
we built a scene recognition

00:41:45.985 --> 00:41:46.485
system.

00:41:46.485 --> 00:41:50.920
And now you can go to
places.csail.mit.edu

00:41:50.920 --> 00:41:54.430
with a smartphone will
work, will take a photo.

00:41:54.430 --> 00:41:57.560
And it should tell you
the type of environment

00:41:57.560 --> 00:41:59.620
the photo represent.

00:41:59.620 --> 00:42:02.890
It will give you
several possibilities,

00:42:02.890 --> 00:42:06.070
because basically
environment are ambiguous.

00:42:06.070 --> 00:42:07.320
They can be of different type.

00:42:07.320 --> 00:42:08.653
So I don't know if you can read.

00:42:08.653 --> 00:42:09.950
Let me read a few.

00:42:09.950 --> 00:42:13.300
The first one, it says,
restaurant, coffee shop,

00:42:13.300 --> 00:42:16.990
cafeteria, food court,
restaurant patio.

00:42:16.990 --> 00:42:18.430
I guess they all fit.

00:42:18.430 --> 00:42:21.880
The second one, parking
lot and driveway.

00:42:21.880 --> 00:42:25.000
The third one, conference room,
dining room, banquet hall,

00:42:25.000 --> 00:42:26.100
classroom.

00:42:26.100 --> 00:42:28.220
That was a difficult one.

00:42:28.220 --> 00:42:31.240
And the fourth environment
is patio, restaurant patio,

00:42:31.240 --> 00:42:33.970
or restaurant.

00:42:33.970 --> 00:42:37.720
So if you go there, you
can also give feedback

00:42:37.720 --> 00:42:41.830
if one of the label
match the environment

00:42:41.830 --> 00:42:43.000
that you're looking at.

00:42:43.000 --> 00:42:45.960
And it should be
above 80% correct.

00:42:45.960 --> 00:42:50.890
And this model use 1.5 million
images and 200 categories.

00:42:50.890 --> 00:42:53.110
So soon with a great--

00:42:53.110 --> 00:42:59.310
we hope that things will be even
more interesting and accurate.

00:42:59.310 --> 00:43:02.680
And I took this morning a
couple of photo at breakfast.

00:43:02.680 --> 00:43:06.920
So you may all recognize
the scenery here.

00:43:06.920 --> 00:43:10.870
So from the breakfast
area looking outside,

00:43:10.870 --> 00:43:14.680
it's an outdoor harbor,
dock, boat deck.

00:43:14.680 --> 00:43:16.900
Yeah, it could be
actually on a boat

00:43:16.900 --> 00:43:19.680
deck looking at the harbor.

00:43:19.680 --> 00:43:22.360
And otherwise,
the breakfast area

00:43:22.360 --> 00:43:25.240
was restaurant, cafeteria,
coffee shop, food court,

00:43:25.240 --> 00:43:26.190
or bar.

00:43:26.190 --> 00:43:28.930
All those, again, fits.

00:43:28.930 --> 00:43:31.480
So those model now
works very, very well.

00:43:31.480 --> 00:43:32.410
So why?

00:43:32.410 --> 00:43:34.790
Well, let me tell you why.

00:43:34.790 --> 00:43:40.990
So those neural networks,
you can go to any layers

00:43:40.990 --> 00:43:49.450
and open them and look at what
every single artificial neuron

00:43:49.450 --> 00:43:50.110
do.

00:43:50.110 --> 00:43:52.330
So what we call
the receptive field

00:43:52.330 --> 00:43:54.820
of every single
unit in the layer

00:43:54.820 --> 00:43:58.180
one, the layer two, the
layer three, and so on.

00:43:58.180 --> 00:44:00.634
So the first the layer--

00:44:00.634 --> 00:44:03.880
here, four layers are shown--

00:44:03.880 --> 00:44:10.690
basically learn simple features,
contours and simple texture.

00:44:10.690 --> 00:44:12.700
That's called pool1.

00:44:12.700 --> 00:44:20.080
And those looks like the type of
responses of the visual cells,

00:44:20.080 --> 00:44:21.810
and possibly V1, V2.

00:44:21.810 --> 00:44:23.540
I'm going to tell
more about this.

00:44:23.540 --> 00:44:28.060
And as you go higher
up in the layers,

00:44:28.060 --> 00:44:31.480
then you start having some
texture, some patches,

00:44:31.480 --> 00:44:32.880
that make more sense.

00:44:32.880 --> 00:44:36.600
And higher up in the layers,
like layer number five,

00:44:36.600 --> 00:44:42.090
then you start having
artificial receptive field that

00:44:42.090 --> 00:44:45.640
are specific to part of
object, part of scene,

00:44:45.640 --> 00:44:47.770
or an entire object by itself.

00:44:47.770 --> 00:44:52.090
Like we can see here some
kind of [INAUDIBLE] coffee,

00:44:52.090 --> 00:44:55.330
as well as the tower and so on.

00:44:55.330 --> 00:44:59.950
So it seems that the system are
able to recognize environment

00:44:59.950 --> 00:45:01.520
of object.

00:45:01.520 --> 00:45:04.540
But what they learn are
the part and the object

00:45:04.540 --> 00:45:05.980
that the environment contain.

00:45:05.980 --> 00:45:08.130
And I'm going to show
example of those.

00:45:08.130 --> 00:45:15.050
So jumping, just giving you
a result in neuroscience.

00:45:15.050 --> 00:45:17.800
And so there's a lot of debate
out there about, OK, well,

00:45:17.800 --> 00:45:20.930
there's those model
with different layers.

00:45:20.930 --> 00:45:22.840
And you have different
models out there.

00:45:22.840 --> 00:45:27.520
To which extent they correspond
to the visual hierarchy

00:45:27.520 --> 00:45:28.810
of the human brain?

00:45:28.810 --> 00:45:32.170
Well, first, the
computational model

00:45:32.170 --> 00:45:38.320
were inspire by the visual
hierarchy, the V1, V2, V4,

00:45:38.320 --> 00:45:40.750
and [INAUDIBLE]
and so on, knowing

00:45:40.750 --> 00:45:47.920
that more complex features
are built over time and space.

00:45:47.920 --> 00:45:52.660
So what you can also do
is run through a network

00:45:52.660 --> 00:45:57.550
and run in an fMRI
experiment the same image,

00:45:57.550 --> 00:46:02.500
and then look at the correlation
existing between the responses

00:46:02.500 --> 00:46:05.290
of the cells, let's
say in layer one,

00:46:05.290 --> 00:46:08.380
and the responses you may
have on the human brain

00:46:08.380 --> 00:46:10.340
in different part of the brain.

00:46:10.340 --> 00:46:14.890
And what we find is that the
layer one will correspond more,

00:46:14.890 --> 00:46:18.070
will have a higher
correlation, with responses

00:46:18.070 --> 00:46:21.700
in the visual area,
literally V1, V2.

00:46:21.700 --> 00:46:25.900
And as you move up
through the layers,

00:46:25.900 --> 00:46:30.520
then there is correlation,
higher [INAUDIBLE] correlation,

00:46:30.520 --> 00:46:34.340
with part of the ventral and
the parietal part of the brain.

00:46:34.340 --> 00:46:39.150
And I know you had the lecture
by Jim DiCarlo that actually

00:46:39.150 --> 00:46:41.740
must have explained this.

00:46:41.740 --> 00:46:46.500
So Jim DiCarlo team did it, Jack
Gallant as well in Berkeley.

00:46:46.500 --> 00:46:49.140
And we also did
it with other type

00:46:49.140 --> 00:46:53.100
of images and other network,
and all the result really

00:46:53.100 --> 00:46:57.360
corroborate each other
with this nice correlation

00:46:57.360 --> 00:47:02.970
between low to high visual
areas between the brain,

00:47:02.970 --> 00:47:06.910
the human brain, and
those multi-layers model.

00:47:06.910 --> 00:47:07.410
All right.

00:47:07.410 --> 00:47:09.810
Let me show you some
of the receptive

00:47:09.810 --> 00:47:11.700
field, the artificial
receptive field

00:47:11.700 --> 00:47:14.670
that we find in
the higher layers.

00:47:14.670 --> 00:47:19.990
So this network was trained
for scene categorization.

00:47:19.990 --> 00:47:22.080
So the only thing
that this network,

00:47:22.080 --> 00:47:24.480
the one we are using
here, learn was

00:47:24.480 --> 00:47:29.940
to discriminate between, in
this case, 200 categories--

00:47:29.940 --> 00:47:33.090
the kitchen, the bedroom,
the bathroom, the alley,

00:47:33.090 --> 00:47:35.890
the living room, the
forest, and so on and so on.

00:47:35.890 --> 00:47:37.860
200 of those.

00:47:37.860 --> 00:47:40.510
So that's the task.

00:47:40.510 --> 00:47:42.960
What the network
learn and what we

00:47:42.960 --> 00:47:46.650
observe is that object
discriminant and diagnostical

00:47:46.650 --> 00:47:49.640
information between
those category

00:47:49.640 --> 00:47:52.620
form the emerging representation
that are automatically,

00:47:52.620 --> 00:47:55.950
naturally learn by the networks.

00:47:55.950 --> 00:48:00.900
So the network has
never learned a wheel,

00:48:00.900 --> 00:48:04.840
but the representation
emerge, as you can see here.

00:48:04.840 --> 00:48:09.920
So this is one
artificial neurone.

00:48:09.920 --> 00:48:14.190
And it is receptive
field and its responses,

00:48:14.190 --> 00:48:17.880
the highest response it got
for a collection of images.

00:48:17.880 --> 00:48:22.650
And as you can see, those
higher pool5 receptive field

00:48:22.650 --> 00:48:24.780
are more independent
to the location.

00:48:24.780 --> 00:48:26.610
They are built that way.

00:48:26.610 --> 00:48:29.850
But the network never
learn the parts.

00:48:29.850 --> 00:48:32.250
This is something
that emerged naturally

00:48:32.250 --> 00:48:34.262
by learning different
environment.

00:48:34.262 --> 00:48:36.720
The other thing that's very
interesting in this model, when

00:48:36.720 --> 00:48:39.060
you can open up and look at
the receptive field using

00:48:39.060 --> 00:48:44.460
various method, here is one,
is, well, this model has never

00:48:44.460 --> 00:48:47.800
learned the notion
of shape or object.

00:48:47.800 --> 00:48:51.420
So it's going to
basically become

00:48:51.420 --> 00:48:55.840
sensitive to discriminant
and diagnostic information.

00:48:55.840 --> 00:48:59.820
So you have this unit that is
discriminant to the bottom part

00:48:59.820 --> 00:49:03.000
of either the legs of
animate, or even you

00:49:03.000 --> 00:49:04.690
can see the trees over there.

00:49:04.690 --> 00:49:05.790
So this is a unit.

00:49:05.790 --> 00:49:10.650
It seems that it was needed in
order to classify environment

00:49:10.650 --> 00:49:14.520
to have units that we might
not have a word for it.

00:49:14.520 --> 00:49:17.040
But the human
brain might as well

00:49:17.040 --> 00:49:19.690
have many of those unit
that do not correspond

00:49:19.690 --> 00:49:21.750
to necessarily a word.

00:49:21.750 --> 00:49:24.150
So, basically,
with this model you

00:49:24.150 --> 00:49:27.450
can have a lot of new object
emerging that you might not

00:49:27.450 --> 00:49:31.260
have thought of, but they
become parts of the code

00:49:31.260 --> 00:49:35.590
needed in order to identify
an environment or an object.

00:49:35.590 --> 00:49:38.760
So we also have another
unit for this bottom parts

00:49:38.760 --> 00:49:40.560
of a collection of chairs.

00:49:40.560 --> 00:49:43.110
We also have chair,
of course, showing up.

00:49:43.110 --> 00:49:43.860
Faces.

00:49:43.860 --> 00:49:46.500
The model never learned faces.

00:49:46.500 --> 00:49:49.290
Only learn kitchen,
bathroom, street.

00:49:49.290 --> 00:49:53.110
We have several unit
emerging for faces.

00:49:53.110 --> 00:49:53.610
Why?

00:49:53.610 --> 00:49:56.190
Because those are
correlated with a collection

00:49:56.190 --> 00:49:57.650
of environment.

00:49:57.650 --> 00:50:01.350
Then, entire object
shapes, like bed,

00:50:01.350 --> 00:50:04.110
very diagnostic of a
bedroom in this case.

00:50:04.110 --> 00:50:09.130
So that will be a unit that
is very, very specific.

00:50:09.130 --> 00:50:13.650
That would be really
only for beds, that one.

00:50:13.650 --> 00:50:17.970
And then, others
like lamps and so on.

00:50:17.970 --> 00:50:20.810
There's thousand
and thousand here.

00:50:20.810 --> 00:50:24.720
Another unit never
learned, screen monitors.

00:50:24.720 --> 00:50:29.005
And here is specific unit
for that that emerge.

00:50:29.005 --> 00:50:32.160
Also, collection
of object or space,

00:50:32.160 --> 00:50:35.010
collection of chairs over here.

00:50:35.010 --> 00:50:39.660
The network found that this
is a discriminant information

00:50:39.660 --> 00:50:42.300
to classify environment, crowds.

00:50:42.300 --> 00:50:45.030
It's a very interesting
unit because it's

00:50:45.030 --> 00:50:50.250
really independent of the
location, as well as basically

00:50:50.250 --> 00:50:53.710
the number of people and if
they are closer or further away.

00:50:53.710 --> 00:50:56.490
But it does capture
this notion of crowd.

00:50:56.490 --> 00:50:58.440
So the model doesn't
have the word crowd.

00:50:58.440 --> 00:51:01.390
So it has never known the crowd.

00:51:01.390 --> 00:51:03.300
It's one of the
unit emerging that

00:51:03.300 --> 00:51:05.850
now can be used as
an object detector

00:51:05.850 --> 00:51:08.970
to enhance the recognition of
what's going on in the scene.

00:51:08.970 --> 00:51:13.630
So this is an ice skating
area, and there is a crowd.

00:51:13.630 --> 00:51:18.380
And also unit that are
more specific to space

00:51:18.380 --> 00:51:20.610
and useful for
navigation, for instance.

00:51:20.610 --> 00:51:22.300
We have many of those.

00:51:22.300 --> 00:51:30.090
So in that case, just the
fact that there is lamps up

00:51:30.090 --> 00:51:33.810
or perspective,
so we have a unit

00:51:33.810 --> 00:51:36.030
like this specific to this.

00:51:36.030 --> 00:51:39.030
So it's not the only
object as physical object.

00:51:39.030 --> 00:51:40.890
There's also a
collection of unit

00:51:40.890 --> 00:51:45.130
that are related to spatial
layout and geometry that

00:51:45.130 --> 00:51:49.120
are also discriminant
for environment.

00:51:49.120 --> 00:51:52.080
And many, many other
that are showing up.

00:51:52.080 --> 00:51:53.760
So those object
detector naturally

00:51:53.760 --> 00:51:56.390
emerge inside this
kind of network trained

00:51:56.390 --> 00:51:58.770
for scene understanding.

00:51:58.770 --> 00:52:02.740
So now I only have a couple of
minutes to wrap up, 20 minutes,

00:52:02.740 --> 00:52:09.670
so I'm going to just give you
the hint of the next part.

00:52:09.670 --> 00:52:15.480
So with the Places challenge
that is starting this week,

00:52:15.480 --> 00:52:20.500
certainly in less than a year
the computational vision model

00:52:20.500 --> 00:52:22.440
of scene recognition
are going to be very

00:52:22.440 --> 00:52:25.170
close to human performances.

00:52:25.170 --> 00:52:30.360
And then there's a long way
to go and many more things

00:52:30.360 --> 00:52:32.100
to match, like the error.

00:52:32.100 --> 00:52:33.660
So know that the
error look alike

00:52:33.660 --> 00:52:36.450
or when a category can
have many type of object

00:52:36.450 --> 00:52:37.570
or what can happen next.

00:52:37.570 --> 00:52:39.240
So you can really expand.

00:52:39.240 --> 00:52:41.130
But let's say we
can consider now

00:52:41.130 --> 00:52:44.250
that we have a base
of visual recognition

00:52:44.250 --> 00:52:49.210
into a model that works pretty
well at the level of human,

00:52:49.210 --> 00:52:52.140
or close enough, or
will be close enough.

00:52:52.140 --> 00:52:56.640
So, now that we have that,
we can add the memory module.

00:52:56.640 --> 00:52:59.190
How to add the
memorability module

00:52:59.190 --> 00:53:01.260
is really open in the air.

00:53:01.260 --> 00:53:03.000
There's many ways to do.

00:53:03.000 --> 00:53:08.270
So we just did it one way, to
have a ground very first model

00:53:08.270 --> 00:53:12.010
of visual memorability
at the level of human.

00:53:12.010 --> 00:53:15.870
And this is going to be out--
the paper is in revision--

00:53:15.870 --> 00:53:16.495
in a few weeks.

00:53:16.495 --> 00:53:19.078
And you're going to be able to
download the images, the model,

00:53:19.078 --> 00:53:19.800
and so on.

00:53:19.800 --> 00:53:23.130
Again, this is model number one,
and we hope that then a better

00:53:23.130 --> 00:53:24.480
model can be done.

00:53:24.480 --> 00:53:28.920
So we went for the
Occam razor approach.

00:53:28.920 --> 00:53:34.620
The most simple one given
the model is we took AlexNet,

00:53:34.620 --> 00:53:38.550
we feed AlexNet with
both ImageNet and Places.

00:53:38.550 --> 00:53:41.122
Because all the images that
are memorable or unforgettable,

00:53:41.122 --> 00:53:43.080
they might have object,
they might have places.

00:53:43.080 --> 00:53:46.290
OK, so let's put the two
together so we have more power.

00:53:46.290 --> 00:53:50.190
We train the model, so we have
the visual recognition model.

00:53:50.190 --> 00:53:53.820
We do know that those
units make sense.

00:53:53.820 --> 00:53:55.680
They recognize that
this is a kitchen.

00:53:55.680 --> 00:53:58.760
And we do know why,
because we have the parts.

00:53:58.760 --> 00:54:02.160
So that's a classical
standard AlexNet.

00:54:02.160 --> 00:54:05.295
The output is scene and
object categorization,

00:54:05.295 --> 00:54:07.785
so we are still in
categorization land.

00:54:07.785 --> 00:54:10.710
And we can remove
the last layer.

00:54:10.710 --> 00:54:14.610
And then this is a procedure
that has been well published

00:54:14.610 --> 00:54:17.790
in computer vision
and computer science,

00:54:17.790 --> 00:54:20.610
this notion of fine tuning
and back propagation.

00:54:20.610 --> 00:54:23.070
So you use the
network on learning

00:54:23.070 --> 00:54:24.990
that are learn to
recognize places,

00:54:24.990 --> 00:54:27.840
and you finely tune
and adopt the feature

00:54:27.840 --> 00:54:29.760
so that the task has change.

00:54:29.760 --> 00:54:31.980
So the task was recognition.

00:54:31.980 --> 00:54:33.720
And now, for the
network, at the end

00:54:33.720 --> 00:54:37.190
there is a new task for
that network that has learn

00:54:37.190 --> 00:54:40.260
all those object and scene.

00:54:40.260 --> 00:54:44.310
And the new task is now to
learn that those element are

00:54:44.310 --> 00:54:47.700
of high, medium, or
low memorability,

00:54:47.700 --> 00:54:50.800
which is a continuous value.

00:54:50.800 --> 00:54:54.300
So by doing this,
we have now a model

00:54:54.300 --> 00:54:56.610
where we give it a
new image, and it's

00:54:56.610 --> 00:55:02.390
going to output a
score between 0 and 1.

00:55:02.390 --> 00:55:06.820
And the human to human I'll
show you is 0.68 correlation.

00:55:06.820 --> 00:55:10.620
The human to computer
is 0.65, which mean now

00:55:10.620 --> 00:55:16.950
that there is a first
model that can basically

00:55:16.950 --> 00:55:22.500
replicate human memorability
of a group nearly at

00:55:22.500 --> 00:55:24.330
the level of a human.

00:55:24.330 --> 00:55:28.590
And we use the data
set of 60,000 image

00:55:28.590 --> 00:55:32.010
to finely tune the
recognition model,

00:55:32.010 --> 00:55:35.770
as well as test with images
that this has not shown.

00:55:35.770 --> 00:55:42.360
And because we can open up this
new network of memorability

00:55:42.360 --> 00:55:47.250
and look also now at the
receptive field of the neuron

00:55:47.250 --> 00:55:51.600
that are related to high
or low memorability image--

00:55:51.600 --> 00:55:56.340
and we will publish every single
receptive field on the web

00:55:56.340 --> 00:55:58.420
as well with this paper--

00:55:58.420 --> 00:56:02.900
and thus now we can see
the unit that are related,

00:56:02.900 --> 00:56:09.270
this higher level information
of object or space or parts

00:56:09.270 --> 00:56:11.760
and so on that is
related with images

00:56:11.760 --> 00:56:14.940
that are highly memorable
in green, strong positive,

00:56:14.940 --> 00:56:16.590
or highly forgettable.

00:56:16.590 --> 00:56:21.330
And we find, again, that
if you have animate object

00:56:21.330 --> 00:56:23.280
or kind of object,
roundish object,

00:56:23.280 --> 00:56:26.100
and so on, you can go
[INAUDIBLE] those will make

00:56:26.100 --> 00:56:29.340
your images more memorable.

00:56:29.340 --> 00:56:33.230
And so our last part
is, so now that we

00:56:33.230 --> 00:56:36.910
do have this model that
spill out responses

00:56:36.910 --> 00:56:42.470
at the level of human and
indicate the parts that

00:56:42.470 --> 00:56:47.210
are related to higher memory,
lower memory, at least

00:56:47.210 --> 00:56:51.560
as a guideline, then we can
go back to a given image

00:56:51.560 --> 00:56:54.680
and emphasize the
part that correspond

00:56:54.680 --> 00:56:58.210
to the high memorability
receptive field

00:56:58.210 --> 00:57:01.280
and de-emphasize the
part corresponding

00:57:01.280 --> 00:57:06.680
to the forgettable part
of the receptive field.

00:57:06.680 --> 00:57:08.630
And this give you
images on the right

00:57:08.630 --> 00:57:11.270
like that that
have been weighted

00:57:11.270 --> 00:57:12.830
by the element
that are memorable

00:57:12.830 --> 00:57:14.690
and the element that
are forgettable.

00:57:14.690 --> 00:57:18.360
So maybe here it doesn't matter
that the ground is forgettable.

00:57:18.360 --> 00:57:21.430
Here, the part I
like to emphasize

00:57:21.430 --> 00:57:23.390
are the memorable one.

00:57:23.390 --> 00:57:26.940
But, for instance, in this
imagine we have two person.

00:57:26.940 --> 00:57:31.550
And, well, she
just happened to be

00:57:31.550 --> 00:57:34.700
more forgettable in this case,
which will make a perfect CIA

00:57:34.700 --> 00:57:38.270
agent, if we think about it.

00:57:38.270 --> 00:57:40.530
And we did tested those.

00:57:40.530 --> 00:57:45.320
So then, for a values
part of navigation,

00:57:45.320 --> 00:57:51.260
I mean values scene, we do
find that the element of exit

00:57:51.260 --> 00:57:53.820
or entrance, so where
there's basically

00:57:53.820 --> 00:57:57.340
path and a 3D structure
for navigation,

00:57:57.340 --> 00:57:58.970
tend also to be more memorable.

00:57:58.970 --> 00:58:02.510
So those are highlighted
in those images.

00:58:02.510 --> 00:58:04.640
Another example,
where the kids will

00:58:04.640 --> 00:58:08.890
be more memorable than the
features of the person.

00:58:08.890 --> 00:58:09.810
And I don't--

00:58:09.810 --> 00:58:11.510
Well, I'm not an expert in game.

00:58:11.510 --> 00:58:14.480
You can explain that one to me.

00:58:14.480 --> 00:58:18.470
So I have to stop
because we need a break,

00:58:18.470 --> 00:58:21.420
and I might need to talk.

00:58:21.420 --> 00:58:27.410
However, here is a vision
of where we are going.

00:58:27.410 --> 00:58:30.980
And maybe other people
will be interested to go

00:58:30.980 --> 00:58:33.350
on this adventure.

00:58:33.350 --> 00:58:36.180
Because it's really,
really just the beginning.

00:58:36.180 --> 00:58:39.410
But if you now can
have a model that

00:58:39.410 --> 00:58:42.110
at the level of a group of
human predict which image

00:58:42.110 --> 00:58:47.000
are memorable, and
as well, also match

00:58:47.000 --> 00:58:51.530
part of the visual region and
even the higher level object

00:58:51.530 --> 00:58:56.720
recognition, then you can start
having this similarity going on

00:58:56.720 --> 00:59:01.070
in this study between the
human brain and all the part,

00:59:01.070 --> 00:59:05.180
from perception to memory
and computational model,

00:59:05.180 --> 00:59:09.770
to characterize what the
computation underlying

00:59:09.770 --> 00:59:11.780
those particular region.

00:59:11.780 --> 00:59:15.500
Because now you have model zero.

00:59:15.500 --> 00:59:17.540
I'm not saying that the
model I'm showing to you

00:59:17.540 --> 00:59:20.520
is a model that
imitate the brain.

00:59:20.520 --> 00:59:23.090
But it's a model
zero, and now it

00:59:23.090 --> 00:59:27.110
can be tuned to better
learn the calculation that

00:59:27.110 --> 00:59:30.110
are done at different
level along the visual

00:59:30.110 --> 00:59:36.340
to the memory hierarchy in order
to characterize visual memory.

