WEBVTT
Kind: captions
Language: en

00:00:00.060 --> 00:00:02.500
The following content is
provided under a Creative

00:00:02.500 --> 00:00:04.019
Commons license.

00:00:04.019 --> 00:00:06.360
Your support will help
MIT OpenCourseWare

00:00:06.360 --> 00:00:10.730
continue to offer high quality
educational resources for free.

00:00:10.730 --> 00:00:13.330
To make a donation or
view additional materials

00:00:13.330 --> 00:00:17.217
from hundreds of MIT courses,
visit MIT OpenCourseWare

00:00:17.217 --> 00:00:17.842
at ocw.mit.edu.

00:00:21.546 --> 00:00:22.530
PROFESSOR: All right.

00:00:22.530 --> 00:00:24.649
I want to complete
the discussion

00:00:24.649 --> 00:00:26.940
on volatility modeling in
the first part of the lecture

00:00:26.940 --> 00:00:27.870
today.

00:00:27.870 --> 00:00:32.950
And last time we
addressed the definition

00:00:32.950 --> 00:00:39.420
of ARCH models, which allow
for time-varying volatility

00:00:39.420 --> 00:00:42.940
in modeling the returns of
a financial time series.

00:00:42.940 --> 00:00:45.040
And we were looking
last time at modeling

00:00:45.040 --> 00:00:48.770
the euro-dollar
exchange rate returns.

00:00:48.770 --> 00:00:54.970
And we went through fitting
ARCH models to those returns,

00:00:54.970 --> 00:00:59.880
and also looked at fitting the
GARCH model to those returns.

00:00:59.880 --> 00:01:07.980
And to recap, the GARCH model
extends upon the ARCH model

00:01:07.980 --> 00:01:10.110
by adding some extra terms.

00:01:10.110 --> 00:01:12.890
So if you look at this
expression for the GARCH model,

00:01:12.890 --> 00:01:15.950
the first two terms for the
time-varying volatility sigma

00:01:15.950 --> 00:01:20.815
squared t is a linear
combination of the past sort

00:01:20.815 --> 00:01:23.750
of residual returns squared.

00:01:23.750 --> 00:01:25.690
That's the ARCH
model, p of those.

00:01:25.690 --> 00:01:27.750
So the current
volatility depends

00:01:27.750 --> 00:01:30.520
upon what's happened in
excess returns over the last p

00:01:30.520 --> 00:01:31.460
periods.

00:01:31.460 --> 00:01:33.960
But then we add an
extra term, which

00:01:33.960 --> 00:01:39.380
is corresponds to q levels
of the previous volatility.

00:01:39.380 --> 00:01:42.670
And so what we're
doing with GARCH

00:01:42.670 --> 00:01:46.150
models is adding extra
parameters to the ARCH,

00:01:46.150 --> 00:01:49.840
but an advantage of considering
these extra parameters which

00:01:49.840 --> 00:01:53.440
relate basically the current
volatility sigma squared t

00:01:53.440 --> 00:01:56.280
with the previous or lagged
value sigma squared t

00:01:56.280 --> 00:01:58.940
minus j for lags
j is that we may

00:01:58.940 --> 00:02:02.910
be able to have a model
with many fewer parameters.

00:02:02.910 --> 00:02:09.930
So indeed, if we fit these
models to the exchange rate

00:02:09.930 --> 00:02:15.670
returns, what we found last
time-- let me go through

00:02:15.670 --> 00:02:19.970
and show that--
was-- basically here

00:02:19.970 --> 00:02:25.890
are various fits of the
three cases of ARCH models.

00:02:25.890 --> 00:02:29.210
ARCH orders 1, 2, and
10, thinking we maybe

00:02:29.210 --> 00:02:31.760
need many lags to
fit volatility.

00:02:31.760 --> 00:02:35.650
And then the GARCH model 1,1,
where we only have one ARCH

00:02:35.650 --> 00:02:38.890
term and one GARCH term.

00:02:38.890 --> 00:02:45.110
And so basically the green line,
or rather the blue line in this

00:02:45.110 --> 00:02:48.900
graph, shows the plot of
the fitted GARCH(1,1) model

00:02:48.900 --> 00:02:51.540
as compared with
the ARCH models.

00:02:51.540 --> 00:02:55.960
Now, in looking
at this graph, one

00:02:55.960 --> 00:02:58.980
can actually see
some features of how

00:02:58.980 --> 00:03:01.240
these models are fitting
volatility, which

00:03:01.240 --> 00:03:04.090
is important to understand.

00:03:04.090 --> 00:03:07.820
One is that the ARCH
models have a hard lower

00:03:07.820 --> 00:03:10.950
bound on the volatility.

00:03:10.950 --> 00:03:14.030
Basically there's
a constant term

00:03:14.030 --> 00:03:16.070
in the volatility equation.

00:03:16.070 --> 00:03:22.410
And because the additional terms
are squared excess returns,

00:03:22.410 --> 00:03:24.970
it-- basically, the volatility
does have the lower bound

00:03:24.970 --> 00:03:25.800
of that intercept.

00:03:25.800 --> 00:03:29.230
So depending on what range
you fit the data over,

00:03:29.230 --> 00:03:33.190
that lower bound is
going to be defined by--

00:03:33.190 --> 00:03:37.080
or it will be determined by
the data you're fitting to.

00:03:37.080 --> 00:03:41.700
As you increase the ARCH
order, you basically

00:03:41.700 --> 00:03:46.940
allow for a greater range of--
or a lower lower bound of that.

00:03:46.940 --> 00:03:48.590
And with the GARCH
model you can see

00:03:48.590 --> 00:03:52.150
that this blue line
is actually predicting

00:03:52.150 --> 00:03:55.030
very different
levels of volatility

00:03:55.030 --> 00:03:57.300
over the entire
range of the series.

00:03:57.300 --> 00:03:59.310
So it really is
much more flexible.

00:04:02.500 --> 00:04:07.580
Now-- and in these fits, we are
assuming Gaussian distributions

00:04:07.580 --> 00:04:11.140
for the innovations
in the return series.

00:04:11.140 --> 00:04:15.000
We'll soon pursue looking
at alternatives to that,

00:04:15.000 --> 00:04:18.100
but let me talk just
a little bit more

00:04:18.100 --> 00:04:23.980
about the GARCH model going
back to lecture notes here.

00:04:23.980 --> 00:04:26.295
So let me expand this.

00:04:31.150 --> 00:04:31.650
OK.

00:04:31.650 --> 00:04:32.858
So there's the specification.

00:04:32.858 --> 00:04:35.000
The GARCH(1,1) model.

00:04:35.000 --> 00:04:43.000
One thing to note is that this
GARCH(1,1) model does relate

00:04:43.000 --> 00:04:47.060
to an ARMA, an autoregressive
moving average process

00:04:47.060 --> 00:04:49.410
in the squared residuals.

00:04:49.410 --> 00:04:54.010
So if we look at the top
line, which is the equation

00:04:54.010 --> 00:05:00.200
for the GARCH(1,1) model,
consider eliminating sigma

00:05:00.200 --> 00:05:09.090
squared t by using a new
innovation term, little u_t,

00:05:09.090 --> 00:05:11.690
which is the difference
between the squared residual

00:05:11.690 --> 00:05:15.060
and the true volatility
given by the model.

00:05:15.060 --> 00:05:19.910
So if you plug in the difference
between our squared excess

00:05:19.910 --> 00:05:22.680
return and the
current volatility,

00:05:22.680 --> 00:05:27.710
that should have mean
0 because sigma squared

00:05:27.710 --> 00:05:33.360
t, the t-th volatility squared,
is equal to the square--

00:05:33.360 --> 00:05:38.070
or is equal to the expectation
of the squared excess residual

00:05:38.070 --> 00:05:40.440
return, epsilon_t squared.

00:05:40.440 --> 00:05:42.650
So if we plug that
in, we basically

00:05:42.650 --> 00:05:47.330
get an ARMA model for
the squared residuals.

00:05:47.330 --> 00:05:50.750
And so epsilon_t squared
is alpha_0 plus alpha_1

00:05:50.750 --> 00:05:56.400
plus beta_1 the squared t minus
1 lag plus u_t minus beta_1

00:05:56.400 --> 00:05:57.300
u_t.

00:05:57.300 --> 00:06:04.800
And so what this implies is an
ARMA(1,1 model with white noise

00:06:04.800 --> 00:06:08.740
that has mean 0 and variance
2 sigma to the fourth.

00:06:08.740 --> 00:06:11.150
Just plugging things in.

00:06:11.150 --> 00:06:15.380
And through our
knowledge, understanding,

00:06:15.380 --> 00:06:19.120
of univariate time series
models, ARMA models,

00:06:19.120 --> 00:06:23.370
we can express this ARMA model
for the squared residuals

00:06:23.370 --> 00:06:27.660
as basically a polynomial
lag of the squared residuals

00:06:27.660 --> 00:06:32.690
is equal to a polynomial
lag of the innovations.

00:06:32.690 --> 00:06:38.330
And so we have this expression
for what the innovations are.

00:06:38.330 --> 00:06:47.020
And it's required that the
roots of this a of L operator,

00:06:47.020 --> 00:06:49.820
when it thought of
on the complex plane,

00:06:49.820 --> 00:06:53.100
have roots outside
the unit circle, which

00:06:53.100 --> 00:06:56.370
corresponds to alpha_1 plus
beta_1 being less than 1

00:06:56.370 --> 00:06:57.690
in magnitude.

00:06:57.690 --> 00:07:01.020
So in order for these
volatility models

00:07:01.020 --> 00:07:04.780
not to blow up and be
stationary, covariance

00:07:04.780 --> 00:07:08.480
stationary, we have these
bounds on the parameters.

00:07:14.790 --> 00:07:19.330
OK, let's look at the
unconditional volatility

00:07:19.330 --> 00:07:23.260
or long-run variance
of the GARCH model.

00:07:23.260 --> 00:07:29.190
If you take expectations on
both sides of the GARCH model

00:07:29.190 --> 00:07:34.270
equation, you basically have
the expectation of sigma

00:07:34.270 --> 00:07:36.830
squared sub t--
in the long run is

00:07:36.830 --> 00:07:40.750
sigma star squared-- is alpha_0
plus alpha_1 plus beta_1 sigma

00:07:40.750 --> 00:07:43.630
star squared.

00:07:43.630 --> 00:07:45.550
So that sigma star
squared there is

00:07:45.550 --> 00:07:57.740
the expectation of the t
minus 1 volatility squared

00:07:57.740 --> 00:07:58.521
in the limit.

00:07:58.521 --> 00:08:00.020
And then you can
just solve for this

00:08:00.020 --> 00:08:02.490
and see that sigma star
squared is equal alpha_0 over 1

00:08:02.490 --> 00:08:05.050
minus alpha_1 minus beta_1.

00:08:05.050 --> 00:08:10.180
And in terms of the stationarity
conditions for the process,

00:08:10.180 --> 00:08:15.050
if the long-run variance, in
order for that to be finite,

00:08:15.050 --> 00:08:20.500
you need alpha_1 plus beta_1
to be less than 1 in magnitude.

00:08:20.500 --> 00:08:26.710
And if you consider the
general GARCH(p,1) model,

00:08:26.710 --> 00:08:31.460
then the same argument leads to
a long-run variance being equal

00:08:31.460 --> 00:08:35.490
to alpha_0, the sort of
intercept term in the GARCH

00:08:35.490 --> 00:08:40.030
model, divided by 1 minus the
sum of all the parameters.

00:08:40.030 --> 00:08:43.740
So these GARCH models
lead to constraints

00:08:43.740 --> 00:08:50.060
on the parameters that are
important to incorporate

00:08:50.060 --> 00:08:54.870
when we're doing any estimation
of these underlying parameters.

00:08:54.870 --> 00:08:56.650
And it does complicate
things, actually.

00:08:59.460 --> 00:09:04.820
So with maximum
likelihood estimation,

00:09:04.820 --> 00:09:07.270
the routine for maximum
likelihood estimation

00:09:07.270 --> 00:09:08.840
is the same for all models.

00:09:08.840 --> 00:09:11.090
We basically want to
determine the likelihood

00:09:11.090 --> 00:09:14.430
function of our data given
the unknown parameters.

00:09:14.430 --> 00:09:17.090
And the likelihood function
is the probability density

00:09:17.090 --> 00:09:21.380
function of the data
conditional on the parameters.

00:09:21.380 --> 00:09:23.280
So our likelihood
function as a function

00:09:23.280 --> 00:09:26.470
of the unknown parameters
c, alpha, and beta

00:09:26.470 --> 00:09:30.390
is the value of the probability
density, the joint density

00:09:30.390 --> 00:09:34.530
of all the data conditional
on those parameters.

00:09:34.530 --> 00:09:37.820
And that joint
density function can

00:09:37.820 --> 00:09:41.030
be expressed as the product
of successive conditional

00:09:41.030 --> 00:09:42.600
expectations of the time series.

00:09:45.420 --> 00:09:51.600
And those conditional densities
are normal random variables.

00:09:51.600 --> 00:09:53.650
So we can just plug
in what we know

00:09:53.650 --> 00:09:56.290
to be the probability
densities of normals

00:09:56.290 --> 00:10:01.270
for the t-th
innovation epsilon_t.

00:10:01.270 --> 00:10:04.340
And we just optimize
that function.

00:10:04.340 --> 00:10:09.050
Now, the challenge with
estimating these GARCH models

00:10:09.050 --> 00:10:13.990
in part is the constraints
on the underlying parameters.

00:10:13.990 --> 00:10:16.000
Those need to be enforced.

00:10:16.000 --> 00:10:19.250
So we have to have that the
alpha_i are greater than 0.

00:10:19.250 --> 00:10:21.860
Also, the beta_j
are greater than 0.

00:10:21.860 --> 00:10:25.800
And the sum of all of
them is between 0 and 1.

00:10:29.406 --> 00:10:33.860
Who in this class has had
courses in numerical analysis

00:10:33.860 --> 00:10:39.680
and done some
optimization of functions?

00:10:39.680 --> 00:10:40.840
Non-linear functions?

00:10:40.840 --> 00:10:43.220
Anybody?

00:10:43.220 --> 00:10:43.990
OK.

00:10:43.990 --> 00:10:48.490
Well, in addressing this kind
of problem, which will come up

00:10:48.490 --> 00:10:51.300
with any complex model
that you need to estimate,

00:10:51.300 --> 00:10:56.640
say via maximum likelihood,
the optimization methods

00:10:56.640 --> 00:11:02.610
do really well if you're
optimizing a convex function,

00:11:02.610 --> 00:11:05.020
finding the minimum
of a convex function.

00:11:05.020 --> 00:11:09.520
And it's always nice
to do minimization

00:11:09.520 --> 00:11:14.460
over sort of an unconstrained
range of underlying parameters.

00:11:14.460 --> 00:11:18.630
And one of the tricks in
solving these problems

00:11:18.630 --> 00:11:23.070
is to transform the
parameters to a scale

00:11:23.070 --> 00:11:27.480
where they're unlimited
in range, basically.

00:11:27.480 --> 00:11:29.420
So if you have a
positive random variable,

00:11:29.420 --> 00:11:32.200
you might use to log of
that variable as the thing

00:11:32.200 --> 00:11:33.860
to be optimizing over.

00:11:33.860 --> 00:11:36.180
If the variable's
between 0 and 1,

00:11:36.180 --> 00:11:39.050
then you might use
that variable divided

00:11:39.050 --> 00:11:43.699
by 1 minus that variable and
then take the log of that.

00:11:43.699 --> 00:11:44.740
And that's unconstrained.

00:11:44.740 --> 00:11:48.300
So there are tricks for how
you do this optimization, which

00:11:48.300 --> 00:11:49.580
come into play.

00:11:49.580 --> 00:11:53.290
Anyway, that's the likelihood
with the normal distribution.

00:11:53.290 --> 00:11:57.657
And we have computer
programs that

00:11:57.657 --> 00:11:59.740
will solve that directly
so we don't have to worry

00:11:59.740 --> 00:12:01.270
about this particular case.

00:12:05.430 --> 00:12:08.320
Once we fit this model,
we want to evaluate

00:12:08.320 --> 00:12:13.260
how good it is and
the evaluation is

00:12:13.260 --> 00:12:16.860
based upon looking at the
residuals from the model.

00:12:16.860 --> 00:12:19.450
So what we have are
these innovations,

00:12:19.450 --> 00:12:25.250
epsilon hat t, which should
be distributed with variance

00:12:25.250 --> 00:12:28.880
or volatility sigma hat t.

00:12:28.880 --> 00:12:34.600
Those should be uncorrelated
with themselves or at least

00:12:34.600 --> 00:12:37.620
to the extent that they can be.

00:12:37.620 --> 00:12:39.520
And the squared
standardized residuals

00:12:39.520 --> 00:12:40.732
should also be uncorrelated.

00:12:40.732 --> 00:12:42.440
What we're trying to
do with these models

00:12:42.440 --> 00:12:45.990
is to capture the
dependence, actually,

00:12:45.990 --> 00:12:51.560
in the squared residuals, which
is measuring the magnitude

00:12:51.560 --> 00:12:52.627
of the excess returns.

00:12:52.627 --> 00:12:53.960
So those should be uncorrelated.

00:12:56.690 --> 00:12:58.340
There are various
test for normality.

00:12:58.340 --> 00:13:01.630
I've listed some of those that
are the most popular here.

00:13:01.630 --> 00:13:06.810
And then there's issues of model
selection for deciding sort

00:13:06.810 --> 00:13:10.470
of which GARCH model to apply.

00:13:10.470 --> 00:13:15.400
I wanted to go
through an example

00:13:15.400 --> 00:13:20.630
of this analysis with the
euro-dollar exchange rate.

00:13:20.630 --> 00:13:27.480
So let me go to this
case study note.

00:13:27.480 --> 00:13:31.700
So let's see.

00:13:31.700 --> 00:13:37.480
There's a package in R called
rugarch for univariate GARCH

00:13:37.480 --> 00:13:44.420
models, which fits various
GARCH models with different--

00:13:44.420 --> 00:13:47.780
and fits them by
maximum likelihood.

00:13:47.780 --> 00:13:52.280
So with this packet-- with
this particular library in R,

00:13:52.280 --> 00:14:08.320
I fit the GARCH
model after actually

00:14:08.320 --> 00:14:12.470
fitting the mean process for
the exchange rate returns.

00:14:12.470 --> 00:14:14.470
Now, when we looked
at things last time,

00:14:14.470 --> 00:14:16.950
we basically looked at
modeling the squared returns.

00:14:16.950 --> 00:14:19.560
In fact, there may be an
underlying mean process that

00:14:19.560 --> 00:14:21.320
needs to be specified as well.

00:14:21.320 --> 00:14:24.010
So in this section
of the case note,

00:14:24.010 --> 00:14:29.970
I initially fit an
autoregressive process

00:14:29.970 --> 00:14:32.020
using the Akaike
information criterion

00:14:32.020 --> 00:14:35.590
to choose the order of
the autoregressive process

00:14:35.590 --> 00:14:42.200
and then fit a GARCH model
with normal GARCH terms.

00:14:42.200 --> 00:14:47.730
And this is a plot of
the normal q-q plot

00:14:47.730 --> 00:14:51.800
of the autoregressive residuals.

00:14:51.800 --> 00:14:56.220
And what you can see is
that the points lie along

00:14:56.220 --> 00:14:59.350
a straight line sort of in
the middle of the range.

00:14:59.350 --> 00:15:03.260
But on the extremes, they
depart from that straight line.

00:15:03.260 --> 00:15:09.510
This basically is a measure
of standardized quantiles.

00:15:09.510 --> 00:15:12.120
So in terms of
standard units away

00:15:12.120 --> 00:15:16.120
from the mean for
the residuals, we

00:15:16.120 --> 00:15:19.499
tend to get many more high
values and many more low values

00:15:19.499 --> 00:15:20.790
with the Gaussian distribution.

00:15:20.790 --> 00:15:24.620
So that really isn't
fitting very well.

00:15:24.620 --> 00:15:33.300
If we proceed and fit--
OK, actually that plot

00:15:33.300 --> 00:15:37.590
was just the simple ARCH
model with no GARCH terms.

00:15:37.590 --> 00:15:46.240
And then this is the graph of
the q-q plot with the Gaussian

00:15:46.240 --> 00:15:47.350
assumption.

00:15:47.350 --> 00:15:50.840
So here we can see that the
residuals from this model

00:15:50.840 --> 00:15:54.310
are suggesting that it may
do a pretty good job when

00:15:54.310 --> 00:15:57.600
things are only a few standard
deviations away from the mean.

00:15:57.600 --> 00:15:59.310
Less than 2, 2.5.

00:15:59.310 --> 00:16:02.480
But when we get to
more extreme values,

00:16:02.480 --> 00:16:04.690
this isn't modeling things well.

00:16:04.690 --> 00:16:08.980
So one alternative
is to consider

00:16:08.980 --> 00:16:12.740
a heavier-tailed distribution
than the normal, namely

00:16:12.740 --> 00:16:14.920
the t distribution.

00:16:14.920 --> 00:16:19.230
And consider identifying
what t distribution best

00:16:19.230 --> 00:16:20.000
fits the data.

00:16:24.490 --> 00:16:31.620
So let's just look at what ends
up being the maximum likelihood

00:16:31.620 --> 00:16:34.340
estimate for the degrees
of freedom parameter, which

00:16:34.340 --> 00:16:37.150
is 10 degrees of freedom.

00:16:37.150 --> 00:16:40.110
This shows the q-q
plot when you have

00:16:40.110 --> 00:16:41.880
a non-Gaussian
distribution that's

00:16:41.880 --> 00:16:45.380
t with 10 degrees of freedom.

00:16:45.380 --> 00:16:47.880
It basically is explaining
these residuals quite well,

00:16:47.880 --> 00:16:53.690
so that's accommodating the
heavier-tailed distribution

00:16:53.690 --> 00:16:55.245
of these values.

00:16:59.630 --> 00:17:06.550
With this GARCH
model, let's see--

00:17:06.550 --> 00:17:11.400
if you compare sort of estimates
of volatility under the GARCH

00:17:11.400 --> 00:17:17.470
and ARCH models--
the GARCH models

00:17:17.470 --> 00:17:24.040
with the t distribution-- sorry
t distribution versus Gaussian.

00:17:24.040 --> 00:17:26.869
Here's just a graph
showing time series plots

00:17:26.869 --> 00:17:28.777
of the estimated
volatility over time, which

00:17:28.777 --> 00:17:29.860
actually look quite close.

00:17:29.860 --> 00:17:31.401
But when you look
at the differences,

00:17:31.401 --> 00:17:33.640
there really are differences.

00:17:33.640 --> 00:17:43.220
And so it turns out that
the volatility function

00:17:43.220 --> 00:17:46.380
or the volatility estimate
GARCH models with Gaussian

00:17:46.380 --> 00:17:48.460
versus GARCH with
t distributions

00:17:48.460 --> 00:17:51.560
are really very, very similar.

00:17:51.560 --> 00:17:53.250
And the heavier
tailed distribution

00:17:53.250 --> 00:17:58.740
of the t distribution
means that the distribution

00:17:58.740 --> 00:18:03.200
of actual volatility is greater.

00:18:03.200 --> 00:18:05.240
But in terms of
estimating the volatility,

00:18:05.240 --> 00:18:09.380
you have quite similar estimates
of the volatility coming out.

00:18:09.380 --> 00:18:13.110
And this display--
which you'll be

00:18:13.110 --> 00:18:16.600
able to see more clearly in the
case notes that I'll post up--

00:18:16.600 --> 00:18:18.875
show that these are really
quite similar in magnitude.

00:18:22.260 --> 00:18:31.350
And the value at risk concept
that was just-- by Ken couple

00:18:31.350 --> 00:18:35.950
weeks ago in his lecture
from Morgan Stanley--

00:18:35.950 --> 00:18:38.500
concerns the issue
of estimating what

00:18:38.500 --> 00:18:44.480
is the likelihood of returns
exceeding some threshold.

00:18:44.480 --> 00:18:52.310
And if we use the t distribution
for measuring variability

00:18:52.310 --> 00:18:57.420
of the excess returns, then
the computations in the notes

00:18:57.420 --> 00:19:03.000
indicate how you would compute
these value at risk limits.

00:19:03.000 --> 00:19:05.360
If you compare
the t distribution

00:19:05.360 --> 00:19:08.920
with a Gaussian distribution at
these nominal levels for value

00:19:08.920 --> 00:19:13.140
at risk of like 2.5%
or 5%, surprisingly you

00:19:13.140 --> 00:19:16.070
won't get too much difference.

00:19:16.070 --> 00:19:19.690
It's really in looking at
sort of the extreme tails

00:19:19.690 --> 00:19:24.400
of the distribution that
things come into play.

00:19:24.400 --> 00:19:33.530
And so I wanted to show you how
that plays out by showing you

00:19:33.530 --> 00:19:36.030
another graph here.

00:19:36.030 --> 00:19:40.080
Those of you who have had
a statistics course before

00:19:40.080 --> 00:19:43.350
have heard that sort
of a t distribution

00:19:43.350 --> 00:19:46.801
can be a good
approximation to a normal--

00:19:46.801 --> 00:19:48.550
or it can be approximated
well by a normal

00:19:48.550 --> 00:19:53.060
if the degrees of freedom
for the t are at some level.

00:19:53.060 --> 00:19:56.670
And who wants to suggest
a degrees of freedom

00:19:56.670 --> 00:20:00.140
that you might
have before you're

00:20:00.140 --> 00:20:02.330
comfortable approximating
a t with a normal?

00:20:05.480 --> 00:20:05.980
Danny?

00:20:05.980 --> 00:20:06.771
AUDIENCE: 30 or 40.

00:20:06.771 --> 00:20:08.600
PROFESSOR: 30 or 40.

00:20:08.600 --> 00:20:10.230
Sometimes people say even 25.

00:20:10.230 --> 00:20:13.210
Above 25, you can almost
expect the t distribution

00:20:13.210 --> 00:20:15.260
to be a good approximation
to the normal.

00:20:15.260 --> 00:20:19.340
Well, this is a graph the
PDF for a standard normal

00:20:19.340 --> 00:20:22.350
versus a standard t with
30 degrees of freedom.

00:20:22.350 --> 00:20:26.080
And you can see that the density
functions are very, very close.

00:20:26.080 --> 00:20:29.050
The standard-- the CDFs,
the cumulative distribution

00:20:29.050 --> 00:20:31.190
functions, which is
the likelihood of being

00:20:31.190 --> 00:20:35.390
less than or equal to the
horizontal value, ranges

00:20:35.390 --> 00:20:37.779
between 0 and 1, is
almost indistinguishable.

00:20:37.779 --> 00:20:39.820
But if you look at the
tails of the distribution,

00:20:39.820 --> 00:20:43.470
here I've computed the
log of the CDF function.

00:20:43.470 --> 00:20:46.290
You basically have
to move much more

00:20:46.290 --> 00:20:48.290
than two standard deviations
away from the mean

00:20:48.290 --> 00:20:50.380
before there's really
a difference in the t

00:20:50.380 --> 00:20:53.742
distribution with 30
degrees of freedom.

00:20:53.742 --> 00:20:56.010
Now I'm going to
page up by reducing

00:20:56.010 --> 00:20:58.390
the degrees of freedom.

00:20:58.390 --> 00:21:00.628
Let's see.

00:21:00.628 --> 00:21:04.480
If we could do a page down here.

00:21:04.480 --> 00:21:07.420
Page down.

00:21:07.420 --> 00:21:08.850
Oh, page up.

00:21:08.850 --> 00:21:10.130
OK.

00:21:10.130 --> 00:21:15.520
So here is 20
degrees of freedom.

00:21:15.520 --> 00:21:20.470
Here's 10 degrees of
freedom, in our case,

00:21:20.470 --> 00:21:23.010
which turns out to be sort
of the best fit of the t

00:21:23.010 --> 00:21:24.180
distribution.

00:21:24.180 --> 00:21:29.240
And what you can see is that,
in terms of standard deviation

00:21:29.240 --> 00:21:31.870
units, up to about two standard
deviations below the mean,

00:21:31.870 --> 00:21:34.120
we're basically getting
virtually the same probability

00:21:34.120 --> 00:21:37.040
mass at the extreme below.

00:21:37.040 --> 00:21:40.860
But as we go to four or
six standard deviations,

00:21:40.860 --> 00:21:45.210
then we get heavier mass
with the t distribution.

00:21:45.210 --> 00:21:49.340
In discussion of
results in finance

00:21:49.340 --> 00:21:51.730
when you sort of fit models,
people talk about, oh, there

00:21:51.730 --> 00:21:55.670
was six standard deviation
move or-- which is just

00:21:55.670 --> 00:21:57.210
virtually impossible to occur.

00:21:57.210 --> 00:22:02.100
Well, with t distributions a
six standard deviation move

00:22:02.100 --> 00:22:07.700
occurs about 1 in 10,000
times according to this fit.

00:22:07.700 --> 00:22:11.240
And so it actually is a
common [? idiomatic. ?]

00:22:11.240 --> 00:22:16.460
And so it's important to know
that these t distributions are

00:22:16.460 --> 00:22:19.920
benefiting us by giving us
a much better gauge of what

00:22:19.920 --> 00:22:22.250
the tail distribution is like.

00:22:22.250 --> 00:22:27.220
And we call these
distributions leptokurtic,

00:22:27.220 --> 00:22:30.260
meaning they're heavier tailed
than a normal distribution.

00:22:30.260 --> 00:22:33.950
Actually, lepto means
slender, I believe,

00:22:33.950 --> 00:22:40.000
if you're Greek or have the
Greek origin of the word.

00:22:40.000 --> 00:22:44.100
And you can see that
the blue curve, which

00:22:44.100 --> 00:22:47.070
is the t distribution, is
sort of a bit more slender

00:22:47.070 --> 00:22:49.379
in the center of the
distribution, which allows

00:22:49.379 --> 00:22:50.420
it to have heavier tails.

00:22:56.270 --> 00:22:56.770
All right.

00:22:56.770 --> 00:22:59.710
So t distributions
are very useful.

00:22:59.710 --> 00:23:10.230
Let's go back to
this case note here

00:23:10.230 --> 00:23:14.940
which discusses-- this
case note goes through,

00:23:14.940 --> 00:23:17.950
actually, fitting
the t distribution--

00:23:17.950 --> 00:23:21.160
identifying the degrees of
freedom for this t model.

00:23:21.160 --> 00:23:25.060
And so with the
rugarch package, we

00:23:25.060 --> 00:23:30.350
can get the log-likelihood
of the data fit

00:23:30.350 --> 00:23:33.150
under the t
distribution assumption.

00:23:33.150 --> 00:23:36.030
And here's a graph of the
negative log-likelihood

00:23:36.030 --> 00:23:40.900
versus the degrees of
freedom in the t model.

00:23:40.900 --> 00:23:44.240
So with maximum
likelihood we identify

00:23:44.240 --> 00:23:48.040
the value which minimizes
the negative log likelihood.

00:23:48.040 --> 00:23:51.012
And that comes out
as that 10 value.

00:23:54.260 --> 00:23:56.280
All right.

00:23:56.280 --> 00:23:58.120
Let's go back to these
notes and see what

00:23:58.120 --> 00:23:59.293
else we want to talk about.

00:24:05.250 --> 00:24:05.750
All right.

00:24:13.380 --> 00:24:16.610
OK, with these GARCH
models we actually

00:24:16.610 --> 00:24:19.860
are able to model
volatility clustering.

00:24:19.860 --> 00:24:26.700
And volatility clustering
is where, over time, you

00:24:26.700 --> 00:24:30.630
expect volatility to be
high during some periods

00:24:30.630 --> 00:24:33.060
and to be low during
other periods.

00:24:33.060 --> 00:24:35.900
And the GARCH model
can accommodate that.

00:24:35.900 --> 00:24:38.080
So large volatilities
tend to be followed

00:24:38.080 --> 00:24:39.830
by large, small
volatilities tend

00:24:39.830 --> 00:24:42.711
to be followed by small ones.

00:24:42.711 --> 00:24:43.210
OK.

00:24:43.210 --> 00:24:49.240
The returns have heavier tails
than Gaussian distributions.

00:24:49.240 --> 00:24:53.020
Actually, even if we have
Gaussian errors in the GARCH

00:24:53.020 --> 00:24:56.870
model, it's still heavier
tailed than a Gaussian.

00:24:56.870 --> 00:24:59.990
The homework goes into
that a little bit.

00:24:59.990 --> 00:25:06.590
And the-- well, actually
one of the original papers

00:25:06.590 --> 00:25:10.690
by Engle with Bollerslev, who
introduced the GARCH model,

00:25:10.690 --> 00:25:13.490
discusses these
features and how useful

00:25:13.490 --> 00:25:17.000
they are for modeling
financial time series.

00:25:17.000 --> 00:25:25.860
Now, a property of these models
that may be obvious, perhaps,

00:25:25.860 --> 00:25:29.090
but it is-- OK,
these are models that

00:25:29.090 --> 00:25:33.310
are appropriate for modeling
covariance stationary time

00:25:33.310 --> 00:25:34.230
series.

00:25:34.230 --> 00:25:37.970
So the volatility
measure, which is

00:25:37.970 --> 00:25:42.540
a measure of the
squared excess return,

00:25:42.540 --> 00:25:46.950
is basically a covariance
stationary process.

00:25:46.950 --> 00:25:48.320
So what does that mean?

00:25:48.320 --> 00:25:51.870
That means that's going
to have a long-term mean.

00:25:51.870 --> 00:25:55.060
So with these GARCH models
that are covariance stationary,

00:25:55.060 --> 00:25:58.320
there's going to be a long-term
mean of the GARCH process.

00:25:58.320 --> 00:26:06.820
And this discussion here
details how this GARCH process

00:26:06.820 --> 00:26:14.390
is essentially a mean
reversion of the volatility

00:26:14.390 --> 00:26:16.420
to that value.

00:26:16.420 --> 00:26:21.110
So basically, the sort
of excess volatility

00:26:21.110 --> 00:26:23.930
of the squared
residuals relative

00:26:23.930 --> 00:26:27.790
to their long-term
average is some multiple

00:26:27.790 --> 00:26:32.300
of the previous period's
excess volatility.

00:26:32.300 --> 00:26:36.770
So if we build forecasting
models of volatility

00:26:36.770 --> 00:26:42.390
with GARCH models,
what's going to happen?

00:26:42.390 --> 00:26:45.840
Basically, in the
long run we predict

00:26:45.840 --> 00:26:48.610
that any volatility
value is going to revert

00:26:48.610 --> 00:26:51.470
to this long-run average.

00:26:51.470 --> 00:26:54.600
And in the short run, it's
going to move incrementally

00:26:54.600 --> 00:26:56.740
to that value.

00:26:56.740 --> 00:27:02.080
So these GARCH models are very
good for describing volatility

00:27:02.080 --> 00:27:03.650
relative to the
long-term average.

00:27:03.650 --> 00:27:06.620
In terms of their
usefulness for prediction,

00:27:06.620 --> 00:27:09.580
well, they really
predict that volatility

00:27:09.580 --> 00:27:13.250
is going to revert back
to the mean at some rate.

00:27:13.250 --> 00:27:18.840
And the rate at which the
volatility reverts back

00:27:18.840 --> 00:27:22.150
is given by alpha_1 plus beta_1.

00:27:22.150 --> 00:27:25.870
So that number,
which is less than 1

00:27:25.870 --> 00:27:28.020
for covariance
stationarity, is sort

00:27:28.020 --> 00:27:33.150
of measuring, basically, how
quickly you are reverting back

00:27:33.150 --> 00:27:33.900
to the mean.

00:27:33.900 --> 00:27:36.860
And that sum is actually
called a persistence parameter

00:27:36.860 --> 00:27:38.820
in GARCH models as well.

00:27:38.820 --> 00:27:40.970
So is volatility
persisting or not?

00:27:40.970 --> 00:27:42.660
Well, the larger
alpha_1 plus beta_1

00:27:42.660 --> 00:27:47.290
is, the more persistent
volatility is, meaning it's

00:27:47.290 --> 00:27:51.330
reverting back to that long-run
average very, very slowly.

00:27:51.330 --> 00:27:54.180
In the implementation
of volatility estimates

00:27:54.180 --> 00:27:58.410
with the risk
metrics methodology,

00:27:58.410 --> 00:28:04.090
they actually don't assume that
there is a long-run volatility.

00:28:04.090 --> 00:28:08.540
And so that basically you'll
have alpha_1 be equal to 0

00:28:08.540 --> 00:28:14.130
and beta_1 equal to, say, 0.95.

00:28:14.130 --> 00:28:22.270
So or rather the alpha_0 is
0 and the alpha_1 and beta_1

00:28:22.270 --> 00:28:24.110
will actually sum to 1.

00:28:24.110 --> 00:28:28.220
And so you actually are tracking
a potentially non-stationary

00:28:28.220 --> 00:28:35.550
volatility, which allows you
to be estimating the volatility

00:28:35.550 --> 00:28:39.239
without presuming a
long-run average is

00:28:39.239 --> 00:28:40.280
consistent with the past.

00:28:45.290 --> 00:28:48.410
There are many extensions
of the GARCH models.

00:28:48.410 --> 00:28:53.162
And there's wide
literature on that.

00:28:53.162 --> 00:28:55.370
For this course, I think
it's important to understand

00:28:55.370 --> 00:28:58.200
the fundamentals of
these models in terms

00:28:58.200 --> 00:29:02.890
of how they're specified under
Gaussian and t assumptions.

00:29:02.890 --> 00:29:07.050
Extending them can
be very interesting.

00:29:07.050 --> 00:29:12.650
And there are many papers
to look at for that.

00:29:12.650 --> 00:29:13.150
OK.

00:29:16.260 --> 00:29:21.210
let's pause for a minute
and get the next topic.

00:29:34.921 --> 00:29:35.420
All right.

00:29:35.420 --> 00:29:42.630
The next topic is time series,
multivariate time series.

00:29:42.630 --> 00:29:45.290
In two lectures ago
of mine, we talked

00:29:45.290 --> 00:29:49.270
about univariate time series
and basic methodologies there.

00:29:49.270 --> 00:29:52.240
We're now going to be
extending that to multivariate

00:29:52.240 --> 00:29:55.040
time series.

00:29:55.040 --> 00:30:00.110
Turns out there's a multivariate
Wold representation theorem,

00:30:00.110 --> 00:30:02.860
extension of the univariate one.

00:30:02.860 --> 00:30:04.750
There are
autoregressive processes

00:30:04.750 --> 00:30:07.050
for multivariate
cases, which are vector

00:30:07.050 --> 00:30:09.150
autoregressive processes.

00:30:09.150 --> 00:30:12.040
Least squares estimation
comes into play.

00:30:12.040 --> 00:30:17.460
And then we'll see where
our regression analysis

00:30:17.460 --> 00:30:20.090
understanding
allows us to specify

00:30:20.090 --> 00:30:25.670
these vector autoregressive
processes nicely.

00:30:25.670 --> 00:30:30.120
There's an optimality properties
of ordinary least squares

00:30:30.120 --> 00:30:35.020
estimates component wise, which
we'll highlight in about a half

00:30:35.020 --> 00:30:35.530
an hour.

00:30:35.530 --> 00:30:39.870
And go through the maximum
likelihood estimation model

00:30:39.870 --> 00:30:44.680
selection methods,
which are just

00:30:44.680 --> 00:30:47.210
very straightforward
extensions of the same concepts

00:30:47.210 --> 00:30:54.270
for univariate time series
and univariate regressions.

00:30:54.270 --> 00:30:56.940
So let's talk-- let's
introduce the notation

00:30:56.940 --> 00:30:59.010
for multivariate time series.

00:30:59.010 --> 00:31:04.990
We have a stochastic process,
which now is multivariate.

00:31:04.990 --> 00:31:11.320
So we have bold X of t is
some m-dimensional valued

00:31:11.320 --> 00:31:13.390
random variable.

00:31:13.390 --> 00:31:21.170
And it's a stochastic process
that varies over time t.

00:31:21.170 --> 00:31:28.910
And we can think of this
as m different time series

00:31:28.910 --> 00:31:31.940
corresponding to the m
components of the given

00:31:31.940 --> 00:31:32.440
process.

00:31:32.440 --> 00:31:34.900
So, say, with exchange
rates we could

00:31:34.900 --> 00:31:40.130
be modeling m different exchange
rate values and want to model

00:31:40.130 --> 00:31:42.090
those jointly as a time series.

00:31:42.090 --> 00:31:47.450
Or we could have collections
of stocks that we're modeling.

00:31:47.450 --> 00:31:49.160
So each of the
components individually

00:31:49.160 --> 00:31:53.680
can be treated as univariate
series with univariate methods.

00:31:58.070 --> 00:32:00.740
With the multivariate
case, we extend

00:32:00.740 --> 00:32:04.000
the definition of
covariance stationarity

00:32:04.000 --> 00:32:11.390
to correspond to finite, bounded
first and second order moments.

00:32:11.390 --> 00:32:14.060
So we need to talk
about the first order

00:32:14.060 --> 00:32:17.820
moment of the
multivariate time series.

00:32:17.820 --> 00:32:22.970
Mu now is an m vector, which is
the vector of expected values

00:32:22.970 --> 00:32:25.020
of the individual
components, which we can

00:32:25.020 --> 00:32:27.470
denote by mu_1 through mu_m.

00:32:27.470 --> 00:32:31.535
So we basically have m
vectors for our mean.

00:32:35.830 --> 00:32:40.370
Then for the
variance/covariance matrix,

00:32:40.370 --> 00:32:46.490
let's define gamma_0 to be
the variance/covariance matrix

00:32:46.490 --> 00:32:51.440
of the t-th observation of
our multivariate process.

00:32:51.440 --> 00:32:57.740
So that's equal to the
expected value of X_t minus mu

00:32:57.740 --> 00:32:59.770
X_t minus mu prime.

00:32:59.770 --> 00:33:10.900
So when we write that
down, we have X_t minus mu.

00:33:10.900 --> 00:33:16.160
This is basically
an m by 1 vector

00:33:16.160 --> 00:33:23.530
and then X_t minus mu
prime is a 1 by m vector.

00:33:23.530 --> 00:33:28.010
And so the product of that
is an m by m quantity.

00:33:28.010 --> 00:33:34.750
So the 1, 1 element of that
product is the variance

00:33:34.750 --> 00:33:36.266
of X_(1,t).

00:33:36.266 --> 00:33:38.015
And the diagonal entries
are the variances

00:33:38.015 --> 00:33:40.800
of the components series.

00:33:40.800 --> 00:33:43.790
And the off-diagonal
values are the covariance

00:33:43.790 --> 00:33:50.220
between the i-th row series
and the j-th column series,

00:33:50.220 --> 00:33:55.560
as given by the i-th row of
X and the j-th column of X

00:33:55.560 --> 00:33:58.100
transpose.

00:33:58.100 --> 00:33:59.680
So we're just
collecting together

00:33:59.680 --> 00:34:02.770
all the variances/covariances
together.

00:34:02.770 --> 00:34:07.630
And the notation is very
straightforward and simple

00:34:07.630 --> 00:34:10.800
with the matrix
notation given here.

00:34:10.800 --> 00:34:21.030
Now, the correlation matrix,
r_0, is obtained by pre-

00:34:21.030 --> 00:34:25.500
and post-multiplying this
covariance matrix gamma_0

00:34:25.500 --> 00:34:31.110
by a diagonal matrix
with the square roots

00:34:31.110 --> 00:34:32.969
of the diagonal of this matrix.

00:34:32.969 --> 00:34:34.010
Now what's a correlation?

00:34:34.010 --> 00:34:38.800
Correlation is the correlation
between two random variables

00:34:38.800 --> 00:34:42.389
where we've standardized
the variables

00:34:42.389 --> 00:34:45.580
to have mean 0 and variance 1.

00:34:49.020 --> 00:34:53.440
So what we want to do is
basically divide through all

00:34:53.440 --> 00:34:57.710
of these variables by
their standard deviation

00:34:57.710 --> 00:35:02.440
and compute the covariance
matrix on that new scaling.

00:35:02.440 --> 00:35:04.980
That's equivalent to just
pre- and post-multiplying

00:35:04.980 --> 00:35:08.390
by that diagonal of the inverse
of the standard deviations.

00:35:08.390 --> 00:35:11.630
So with matrix
algebra, that formula

00:35:11.630 --> 00:35:18.510
is-- I think it's very clear.

00:35:18.510 --> 00:35:26.320
And this is-- now with-- the
previous discussion was just

00:35:26.320 --> 00:35:29.230
looking at the sort of
contemporaneous covariance

00:35:29.230 --> 00:35:32.570
matrix of the time series
values at the given time

00:35:32.570 --> 00:35:34.820
t with itself.

00:35:34.820 --> 00:35:38.970
We want to look at, also, the
cross-covariance matrices.

00:35:38.970 --> 00:35:44.890
So how are the current values
of the multivariate time series,

00:35:44.890 --> 00:35:52.000
X_t-- how do they covary with
the k-th lag of those values?

00:35:52.000 --> 00:35:58.340
So gamma_k is looking at how
the current period vector

00:35:58.340 --> 00:36:04.750
values is covaried with the
k-th lag of those values.

00:36:04.750 --> 00:36:09.670
So this covariance matrix
has covariance elements

00:36:09.670 --> 00:36:14.090
given in this display.

00:36:14.090 --> 00:36:18.890
And we can define the
cross-correlation matrix

00:36:18.890 --> 00:36:21.450
by similarly pre-
and post-multiplying

00:36:21.450 --> 00:36:24.960
by the inverse of the
standard deviations.

00:36:24.960 --> 00:36:28.565
The diagonal of gamma_0
is the covariance--

00:36:28.565 --> 00:36:33.680
or is the matrix of diagonal
entries of variances.

00:36:33.680 --> 00:36:35.980
Now, properties of
these matrices is-- OK,

00:36:35.980 --> 00:36:40.150
gamma_0 is a symmetric
matrix that we had before.

00:36:40.150 --> 00:36:44.142
But gamma k where k is
greater than 1 or less than--

00:36:44.142 --> 00:36:46.600
or greater or equal to 1 or
less than-- basically different

00:36:46.600 --> 00:36:47.700
from 0.

00:36:47.700 --> 00:36:51.020
This is not symmetric.

00:36:51.020 --> 00:36:56.680
Basically, you may have
lags of some variables that

00:36:56.680 --> 00:37:01.010
are positively correlated with
others and not vice versa.

00:37:01.010 --> 00:37:08.510
So the off-diagonal entries
here aren't necessarily even

00:37:08.510 --> 00:37:11.930
of the same sign, let
alone equal and symmetric.

00:37:11.930 --> 00:37:17.800
So with these
covariance matrices,

00:37:17.800 --> 00:37:20.820
one can look at
how things covary

00:37:20.820 --> 00:37:25.290
and whether they are--
whether there is, basically,

00:37:25.290 --> 00:37:28.300
a dependence between them.

00:37:28.300 --> 00:37:30.860
And you can define--
it's basically the j star

00:37:30.860 --> 00:37:36.890
series-- the j star component
of the multivariate time series

00:37:36.890 --> 00:37:43.790
may lead the j-th one if the
covariance of the k-th lag of j

00:37:43.790 --> 00:37:50.650
star is different from 0--
or the covariance of j star k

00:37:50.650 --> 00:37:57.510
lags ago is non-zero,
covaries with the j-th lag.

00:37:57.510 --> 00:37:58.770
Sorry.

00:37:58.770 --> 00:37:59.780
The current lag.

00:37:59.780 --> 00:38:02.260
So X_(t, j star)
will lead X_(t, j).

00:38:02.260 --> 00:38:07.290
Basically, there's information
in the lagged values

00:38:07.290 --> 00:38:13.110
of j star for the component j.

00:38:13.110 --> 00:38:19.100
So if we're trying to build
models-- linear regression

00:38:19.100 --> 00:38:22.810
models, even, where we're
trying to look at how-- trying

00:38:22.810 --> 00:38:26.520
to predict values, then if
there's a non-zero covariance,

00:38:26.520 --> 00:38:29.110
then we can use those
variables' information

00:38:29.110 --> 00:38:34.760
to actually project what the
one variable is given the other.

00:38:34.760 --> 00:38:41.900
Now, it can be the case that
you have non-zero covariance

00:38:41.900 --> 00:38:44.000
in both directions.

00:38:44.000 --> 00:38:46.280
And so that suggests
that there can

00:38:46.280 --> 00:38:48.400
be sort of feedback
between these variables.

00:38:48.400 --> 00:38:51.420
It's not just that one
variable causes another,

00:38:51.420 --> 00:38:54.170
but there can
actually be feedback.

00:38:54.170 --> 00:38:55.970
In economics and
finance, there's

00:38:55.970 --> 00:39:00.790
a notion of Granger causality.

00:39:00.790 --> 00:39:04.200
And basically that--
well, Granger and Engle

00:39:04.200 --> 00:39:08.890
got the Nobel Prize number of
years ago based on their work.

00:39:08.890 --> 00:39:13.190
And that work deals
with identifying,

00:39:13.190 --> 00:39:18.440
in part, judgments of
causality between--

00:39:18.440 --> 00:39:20.547
or Granger causality
between variables

00:39:20.547 --> 00:39:22.470
in economic time series.

00:39:22.470 --> 00:39:24.780
And so Granger
causality basically

00:39:24.780 --> 00:39:31.900
is sort of positive or non-zero
correlation between variables

00:39:31.900 --> 00:39:35.720
where lags of one variable will
cause another or cause changes

00:39:35.720 --> 00:39:36.650
in another.

00:39:41.310 --> 00:39:42.040
All right.

00:39:42.040 --> 00:39:45.560
I want to just alert you to
the existence of this Wold

00:39:45.560 --> 00:39:47.750
decomposition theorem.

00:39:47.750 --> 00:39:53.060
This is an advanced theorem,
but it's a useful theorem

00:39:53.060 --> 00:39:55.600
to know exists.

00:39:55.600 --> 00:40:01.000
And this extends the univariate
Wold decomposition theorem,

00:40:01.000 --> 00:40:03.930
which concerns the--
whenever we have

00:40:03.930 --> 00:40:06.750
a covariant stationary
process, there

00:40:06.750 --> 00:40:11.740
exists a representation
of that process, which

00:40:11.740 --> 00:40:19.050
is the sum of a deterministic
process and a moving

00:40:19.050 --> 00:40:23.160
average process
of a white noise.

00:40:23.160 --> 00:40:27.860
So if you're modeling
a time series

00:40:27.860 --> 00:40:30.160
and you're going
to be specifying

00:40:30.160 --> 00:40:33.890
a covariance stationary
process for that,

00:40:33.890 --> 00:40:38.120
there does exist a Wold
decomposition representation

00:40:38.120 --> 00:40:39.190
of that.

00:40:39.190 --> 00:40:44.660
You can basically
determine-- identify

00:40:44.660 --> 00:40:47.740
the deterministic process
that the process might follow.

00:40:47.740 --> 00:40:52.650
It might be a linear trend over
time or an exponential trend.

00:40:52.650 --> 00:40:58.490
And if you remove that sort
of deterministic process V_t,

00:40:58.490 --> 00:41:02.520
then what remains
is a process that

00:41:02.520 --> 00:41:09.390
can be modeled with a moving
average of white noise, these.

00:41:09.390 --> 00:41:12.610
Now here, everything is
changed from univariate case

00:41:12.610 --> 00:41:16.670
to multivariate case, so we have
matrices in place of constants

00:41:16.670 --> 00:41:18.280
from before.

00:41:18.280 --> 00:41:24.330
So these-- new
concepts here are we

00:41:24.330 --> 00:41:26.555
have a multivariate
white noise process.

00:41:29.180 --> 00:41:33.820
That's going to be a process
eta_t which is m-dimensional

00:41:33.820 --> 00:41:36.660
which has mean 0.

00:41:36.660 --> 00:41:42.020
And the variance
matrix of this m-vector

00:41:42.020 --> 00:41:46.470
is going to be sigma, which is
now m by m variance/covariance

00:41:46.470 --> 00:41:49.810
matrix of the components.

00:41:49.810 --> 00:41:52.930
And that must be a
positive semi-definite.

00:41:52.930 --> 00:41:58.880
And for white noise, we have
covariances between, say,

00:41:58.880 --> 00:42:03.320
the current t innovation and
a lag of its value are 0.

00:42:03.320 --> 00:42:07.440
So these are uncorrelated
multivariate white noise

00:42:07.440 --> 00:42:09.250
processes.

00:42:09.250 --> 00:42:14.130
And so they're uncorrelated
with each other at various lags.

00:42:14.130 --> 00:42:19.470
And the innovation eta_t
has a covariance of 0

00:42:19.470 --> 00:42:22.070
with the deterministic process.

00:42:22.070 --> 00:42:25.720
Actually, that's
pretty much a given

00:42:25.720 --> 00:42:28.470
if we have a
deterministic process.

00:42:28.470 --> 00:42:32.000
Now, the term
psi_k-- basically we

00:42:32.000 --> 00:42:36.020
have this m-vector X_t is equal
to some m-vectored process

00:42:36.020 --> 00:42:38.565
V_t plus this weighted
average of innovations.

00:42:41.970 --> 00:42:48.300
What's required is that the sum
of this-- basically each term

00:42:48.300 --> 00:42:51.810
psi_k and its
transpose converges.

00:42:51.810 --> 00:42:54.990
Now, if you were to
take that X_t process

00:42:54.990 --> 00:42:57.920
and say let me compute the
variance/covariance matrix

00:42:57.920 --> 00:43:02.310
of that representation,
then you would basically

00:43:02.310 --> 00:43:05.300
get terms in the
covariance matrix which

00:43:05.300 --> 00:43:08.710
includes this sum of terms.

00:43:08.710 --> 00:43:12.260
So that sum has to be
finite in order for this

00:43:12.260 --> 00:43:15.509
to be covariance stationary.

00:43:15.509 --> 00:43:16.425
AUDIENCE: [INAUDIBLE].

00:43:16.425 --> 00:43:17.230
PROFESSOR: Yes?

00:43:17.230 --> 00:43:20.270
AUDIENCE: Could you define
what you mean by innovation?

00:43:20.270 --> 00:43:22.960
PROFESSOR: Oh, OK.

00:43:22.960 --> 00:43:27.050
Well, the innovation
is-- let's see.

00:43:27.050 --> 00:43:32.230
With-- let me go back up here.

00:43:32.230 --> 00:43:32.730
OK.

00:43:39.240 --> 00:43:43.700
The innovation process--
innovation process.

00:43:46.330 --> 00:43:50.220
OK, if we have, as
in this case, we

00:43:50.220 --> 00:43:58.760
have sort of our X_t
stochastic process.

00:43:58.760 --> 00:44:04.710
And we have sort of, say,
f sub t minus 1 equal

00:44:04.710 --> 00:44:15.250
to the information on
X_(t-1), X_(t-2)...

00:44:15.250 --> 00:44:19.730
Basically consisting of the
information set available

00:44:19.730 --> 00:44:21.520
before time t.

00:44:21.520 --> 00:44:28.810
Then we can model X_t to be
the expected value of X_t given

00:44:28.810 --> 00:44:38.530
F_(t-1) plus an innovation.

00:44:38.530 --> 00:44:40.680
And so our objective
in these models

00:44:40.680 --> 00:44:50.000
is to be thinking of how is that
process evolving where we can

00:44:50.000 --> 00:44:53.190
model the process as well as
possible using information up

00:44:53.190 --> 00:44:54.200
to time before t.

00:44:54.200 --> 00:44:59.600
And then there's some
disturbance about that model.

00:44:59.600 --> 00:45:02.280
There's something new that's
happened at time t that

00:45:02.280 --> 00:45:04.460
wasn't available before.

00:45:04.460 --> 00:45:07.660
And that's this
innovation process.

00:45:07.660 --> 00:45:11.120
So this representation
with the Wold decomposition

00:45:11.120 --> 00:45:16.600
is converting the-- or
representing, basically,

00:45:16.600 --> 00:45:20.410
the bits of information that
are affecting the process that

00:45:20.410 --> 00:45:24.294
are occurring at time t and
wasn't available prior to that.

00:45:27.590 --> 00:45:29.060
All right.

00:45:29.060 --> 00:45:33.690
Well, let's move on to vector
autoregressive processes.

00:45:39.840 --> 00:45:43.600
OK, this representation for a
vector autoregressive process

00:45:43.600 --> 00:45:47.580
is an extension of the
univariate autoregressive

00:45:47.580 --> 00:45:49.783
process to p dimensions.

00:45:49.783 --> 00:45:52.380
Sorry, to m dimensions.

00:45:52.380 --> 00:45:56.630
And so our X_t is an m-vector.

00:45:56.630 --> 00:46:05.120
That's going to be equal to some
constant vector C plus a matrix

00:46:05.120 --> 00:46:10.850
phi_1 times lag of X_t
first order, X_(t-1).

00:46:10.850 --> 00:46:19.790
Plus another matrix, phi_2 times
the second lag of X_t, X_(t-2).

00:46:19.790 --> 00:46:25.040
Up to the p-th term, which is
a phi_p, m by m matrix times,

00:46:25.040 --> 00:46:28.800
X_(t-p) plus this
innovation term.

00:46:28.800 --> 00:46:31.390
So this is essentially--
this is basically

00:46:31.390 --> 00:46:34.450
how a univariate
autoregressive process extends

00:46:34.450 --> 00:46:37.320
to an m-variate case.

00:46:37.320 --> 00:46:43.550
And what this
allows one to do is

00:46:43.550 --> 00:46:47.500
model how a given component
of the multivariate series--

00:46:47.500 --> 00:46:51.030
like how one
exchange rate varies

00:46:51.030 --> 00:46:54.500
depending on how other
exchange rates might vary.

00:46:54.500 --> 00:47:00.490
Exchange rates tend to co-move
together in that example.

00:47:00.490 --> 00:47:04.070
So if we look at
what this represents

00:47:04.070 --> 00:47:08.130
in terms of basically
a component series,

00:47:08.130 --> 00:47:10.770
we can consider
fixing j, a component

00:47:10.770 --> 00:47:13.830
of the multivariate process.

00:47:13.830 --> 00:47:16.080
It could be the first,
the last, or the j-th,

00:47:16.080 --> 00:47:17.300
somewhere in the middle.

00:47:17.300 --> 00:47:20.590
And that component
time series-- like

00:47:20.590 --> 00:47:23.645
a fixed exchange
rate series or time

00:47:23.645 --> 00:47:26.920
series, whatever we're
focused on in our modeling--

00:47:26.920 --> 00:47:31.730
is a generalization of the
autoregressive model where

00:47:31.730 --> 00:47:37.020
we have the autoregressive
terms of the j-th series on lags

00:47:37.020 --> 00:47:39.890
of the j-th series
up to order p.

00:47:39.890 --> 00:47:43.410
So we have the univariate
autoregressive model,

00:47:43.410 --> 00:47:47.140
but we also add to that
terms corresponding

00:47:47.140 --> 00:47:52.130
to the relationship
between X_j and X_(j star).

00:47:52.130 --> 00:47:54.340
So how does X_j,
the j-th component,

00:47:54.340 --> 00:47:57.360
depend on other variables,
other components

00:47:57.360 --> 00:47:58.790
of the multivariate series.

00:47:58.790 --> 00:48:01.840
And those are given here.

00:48:01.840 --> 00:48:07.890
So it's a convenient way to
allow for interdependence

00:48:07.890 --> 00:48:10.219
among the components
and model that.

00:48:15.210 --> 00:48:16.850
OK.

00:48:16.850 --> 00:48:25.490
This slide deals with
representing a p-th order

00:48:25.490 --> 00:48:32.720
process as a first order process
with vector autoregressions.

00:48:32.720 --> 00:48:37.970
Now the concept here is really
a very powerful concept that's

00:48:37.970 --> 00:48:41.750
applied in time
series methods, which

00:48:41.750 --> 00:48:51.110
is when you are modeling
dependence that goes back, say,

00:48:51.110 --> 00:48:57.060
a number of lags like
p lags, the structure

00:48:57.060 --> 00:49:02.450
can actually be re-expressed
as simply a first order

00:49:02.450 --> 00:49:04.660
dependence only.

00:49:04.660 --> 00:49:09.070
And so it's much easier sort
of to deal with just a lag one

00:49:09.070 --> 00:49:13.670
dependence than to
consider p lag dependence

00:49:13.670 --> 00:49:17.560
and the complications
involved with that.

00:49:17.560 --> 00:49:22.500
So-- and this
technique is one where,

00:49:22.500 --> 00:49:26.700
in the early days of fitting,
like autoregressive moving

00:49:26.700 --> 00:49:34.520
average processes and
various smoothing methods,

00:49:34.520 --> 00:49:40.520
the model-- basically
accommodating

00:49:40.520 --> 00:49:44.960
p lags complicated the
analysis enormously.

00:49:44.960 --> 00:49:46.740
But one can actually
re-express it just

00:49:46.740 --> 00:49:48.930
as a first order lag problem.

00:49:48.930 --> 00:49:53.860
So in this case, what
one does is one considers

00:49:53.860 --> 00:49:57.580
for a vector autoregressive
process of order of p,

00:49:57.580 --> 00:50:08.090
simply stacking the
values of the process.

00:50:08.090 --> 00:50:11.640
So let me just highlight
what's going on there.

00:50:17.620 --> 00:50:29.500
So if we have basically--
OK, so if we have X_1,

00:50:29.500 --> 00:50:38.680
X_2, X_n, which are
all m by 1 values,

00:50:38.680 --> 00:50:42.640
m-vectors of the
stochastic process.

00:50:42.640 --> 00:50:56.770
Then consider defining Z_t
to be equal to X_t transpose,

00:50:56.770 --> 00:51:01.889
X_(t-1) transpose up
to X_(t-p-1) transpose.

00:51:07.378 --> 00:51:09.930
Or this is t minus (p-1).

00:51:09.930 --> 00:51:10.915
So there are p terms.

00:51:13.630 --> 00:51:20.286
And then if we consider
the lagged value of that,

00:51:20.286 --> 00:51:30.470
that's X_(t-1), X_(t-2),
X_(t-p) transpose.

00:51:30.470 --> 00:51:35.380
So what we've done is
we're considering Z_t.

00:51:35.380 --> 00:51:40.380
This is going to be m times p.

00:51:40.380 --> 00:51:46.892
It's actually 1 by m
times p in this notation.

00:51:46.892 --> 00:51:50.850
Well, actually I guess I
should put transpose here.

00:51:50.850 --> 00:51:54.740
So m times p by 1.

00:51:54.740 --> 00:51:57.040
OK, in the lecture
notes it actually

00:51:57.040 --> 00:52:00.640
is primed there to
indicate the transpose.

00:52:00.640 --> 00:52:03.660
Well, if you define Z_t
and Z_(t-1) this way,

00:52:03.660 --> 00:52:10.610
then Z_t is equal to D
plus A of Z_(t-1) plus F.

00:52:10.610 --> 00:52:14.420
Where this is d, basically
the constant term has the C

00:52:14.420 --> 00:52:16.660
entering and then
0's everywhere else.

00:52:16.660 --> 00:52:23.820
And the A matrix is
phi_1, phi_2, up to phi_p.

00:52:23.820 --> 00:52:36.410
And so basically the Z_t
vector transforms the Z_t--

00:52:36.410 --> 00:52:40.590
or is the transform--
this linear transformation

00:52:40.590 --> 00:52:43.290
of the Z_(t-1).

00:52:43.290 --> 00:52:46.460
And we have sort of
a very simple form

00:52:46.460 --> 00:52:52.640
for the constant term and a very
simple form for the F vector.

00:52:52.640 --> 00:52:59.700
And this is-- renders the model
into a sort of a first order

00:52:59.700 --> 00:53:06.270
time series model with a
larger multivariate series,

00:53:06.270 --> 00:53:09.590
basically mp by 1.

00:53:09.590 --> 00:53:21.380
Now, with this representation
we basically have-- we

00:53:21.380 --> 00:53:31.040
can demonstrate that the process
is going to be stationary

00:53:31.040 --> 00:53:34.750
if all eigenvalues of
the companion matrix A

00:53:34.750 --> 00:53:38.060
have modulus less than 1.

00:53:38.060 --> 00:53:43.320
And let's see-- if we go
back to the expression.

00:53:43.320 --> 00:53:50.750
OK, if the eigenvalues of
this matrix A are less than 1,

00:53:50.750 --> 00:53:55.440
then we won't get sort
of an explosive behavior

00:53:55.440 --> 00:54:00.320
of the process when this
basically increments over time

00:54:00.320 --> 00:54:03.000
with every previous
value getting

00:54:03.000 --> 00:54:08.420
multiplied by the A matrix and
scaling the process over time

00:54:08.420 --> 00:54:10.880
by the A-th power.

00:54:10.880 --> 00:54:12.450
So that is required.

00:54:12.450 --> 00:54:14.620
All eigenvalues of A
have to be less than 1.

00:54:14.620 --> 00:54:17.090
And equivalently, all
roots of this equation

00:54:17.090 --> 00:54:21.300
need to be outside
the unit circle.

00:54:21.300 --> 00:54:25.560
You remember there was a
constraint of-- or a condition

00:54:25.560 --> 00:54:30.030
for univariate
autoregressive models

00:54:30.030 --> 00:54:35.100
to be stationary, that the roots
of the characteristic equation

00:54:35.100 --> 00:54:38.100
are all outside the unit circle.

00:54:38.100 --> 00:54:40.170
And the class notes
go through and went

00:54:40.170 --> 00:54:41.760
through the derivation of that.

00:54:41.760 --> 00:54:46.820
This is the extension of that
to the multivariate case.

00:54:46.820 --> 00:54:50.290
And so basically
one needs to solve

00:54:50.290 --> 00:54:54.100
for roots of a polynomial in
z and determine whether those

00:54:54.100 --> 00:54:59.120
are outside the unit circle.

00:54:59.120 --> 00:55:01.460
Who can tell me what the
order of the polynomial

00:55:01.460 --> 00:55:07.880
is here for this sort
of determinant equation?

00:55:07.880 --> 00:55:09.720
AUDIENCE: [INAUDIBLE] mp.

00:55:09.720 --> 00:55:11.100
PROFESSOR: mp.

00:55:11.100 --> 00:55:11.600
Yes.

00:55:11.600 --> 00:55:13.670
It's basically of power mp.

00:55:13.670 --> 00:55:15.870
So in a determinant
you basically

00:55:15.870 --> 00:55:19.910
are taking products
of the m components

00:55:19.910 --> 00:55:24.050
in the matrix, various
linear combinations of those.

00:55:24.050 --> 00:55:28.610
So that's going to be an
mp-dimensional polynomial.

00:55:28.610 --> 00:55:29.110
All right.

00:55:29.110 --> 00:55:32.680
Well, the mean of the
stationary VAR process

00:55:32.680 --> 00:55:37.220
can be computed rather
easily by taking expectations

00:55:37.220 --> 00:55:41.660
of this on both sides.

00:55:41.660 --> 00:55:44.720
So if we take the
expectation of X_t

00:55:44.720 --> 00:55:48.710
and take expectations
across both sides,

00:55:48.710 --> 00:55:57.860
we get that mu is the C vector
plus the product of the phi_k's

00:55:57.860 --> 00:55:59.670
times mu plus 0.

00:55:59.670 --> 00:56:05.620
So mu, the unconditional
mean of the process,

00:56:05.620 --> 00:56:10.640
actually has this
formula, just solving

00:56:10.640 --> 00:56:18.810
for mu in the top-- in the
second line to the third line.

00:56:18.810 --> 00:56:27.380
So here we can see that
basically this expression 1

00:56:27.380 --> 00:56:33.040
minus phi_1 through phi_p,
that inverse has to exist.

00:56:33.040 --> 00:56:36.890
And actually, if we then
plug in the value of C

00:56:36.890 --> 00:56:39.050
in terms of the
unconditional mean,

00:56:39.050 --> 00:56:43.860
we get this expression
for the original process.

00:56:43.860 --> 00:56:49.920
So the unconditional mean C,
if we demeaned the process,

00:56:49.920 --> 00:56:52.290
there's basically no mean term.

00:56:52.290 --> 00:56:53.730
There's 0.

00:56:53.730 --> 00:56:57.500
And so basically the
mean-adjusted process

00:56:57.500 --> 00:57:02.460
X follows this multivariate
vector autoregression

00:57:02.460 --> 00:57:08.430
with no mean, which is actually
used when this is specified.

00:57:11.450 --> 00:57:18.850
Now, this vector
autoregression model

00:57:18.850 --> 00:57:25.760
can be expressed as a system
of regression equations.

00:57:25.760 --> 00:57:33.820
And so what we have with the
multivariate series, if we have

00:57:33.820 --> 00:57:38.870
multivariate data, we'll have
n sample observations x_t,

00:57:38.870 --> 00:57:42.180
which is basically the m-vector
of the multivariate process

00:57:42.180 --> 00:57:45.710
observed for n time points.

00:57:45.710 --> 00:57:48.000
And for the
computations here, we're

00:57:48.000 --> 00:57:52.050
going to assume that
we have p sort of-- we

00:57:52.050 --> 00:57:56.100
have pre-sample observations
available to us.

00:57:56.100 --> 00:57:58.980
So we're essentially going
to be considering models

00:57:58.980 --> 00:58:01.610
where we condition
on the first p time

00:58:01.610 --> 00:58:07.660
points in order to facilitate
the estimation methodology.

00:58:07.660 --> 00:58:12.040
Then we can set up m
regression models corresponding

00:58:12.040 --> 00:58:16.080
to each component of
the m-variate series.

00:58:16.080 --> 00:58:32.190
And so what we have
is our original--

00:58:32.190 --> 00:58:39.100
we have our collection of data
values, which is x_1 transpose,

00:58:39.100 --> 00:58:45.750
x_2 transpose, down
to x_n transpose,

00:58:45.750 --> 00:58:52.290
which is an n by m matrix.

00:58:52.290 --> 00:58:54.540
OK, this is our
multivariate time series

00:58:54.540 --> 00:58:56.230
where we were
just-- the first row

00:58:56.230 --> 00:58:59.015
corresponds to the first time
values, nth row to the nth time

00:58:59.015 --> 00:58:59.515
values.

00:59:02.180 --> 00:59:05.580
And we can set up
m regression models

00:59:05.580 --> 00:59:11.410
where we're going
to consider modeling

00:59:11.410 --> 00:59:15.920
the j-th column of this matrix.

00:59:15.920 --> 00:59:19.180
So we're just picking out
the univariate time series

00:59:19.180 --> 00:59:21.320
corresponding to
the j-th component.

00:59:21.320 --> 00:59:23.990
That's y j.

00:59:23.990 --> 00:59:30.260
And we're going to
model that as Z beta j

00:59:30.260 --> 00:59:41.380
plus epsilon j where Z is given
by the vector of lagged values

00:59:41.380 --> 00:59:46.320
of the multivariate
process where there's,

00:59:46.320 --> 00:59:49.040
for the t-th-- t
minus first value

00:59:49.040 --> 00:59:52.510
we have that current
value-- or the t

00:59:52.510 --> 00:59:54.890
minus first, t minus
second, up to t minus p.

00:59:54.890 --> 01:00:01.210
So we have basically
p m-vectors here.

01:00:01.210 --> 01:00:09.100
And so this j-th time
series has elements

01:00:09.100 --> 01:00:14.190
that follow a linear
regression model

01:00:14.190 --> 01:00:18.100
on the lags of the entire
multivariate series up to p

01:00:18.100 --> 01:00:23.380
lags with their regression
parameter given by beta j.

01:00:23.380 --> 01:00:28.250
And basically the beta
j regression parameters

01:00:28.250 --> 01:00:36.049
corresponds to the various
elements of the phi matrices.

01:00:36.049 --> 01:00:38.590
So now there's a one-to-one one
correspondence between those.

01:00:50.800 --> 01:00:51.300
All right.

01:00:51.300 --> 01:00:59.280
So I'm using now a notation
where superscript j corresponds

01:00:59.280 --> 01:01:02.930
to the j-th component
of the series,

01:01:02.930 --> 01:01:07.760
of the multivariate
stochastic process.

01:01:07.760 --> 01:01:12.550
So we have an mp plus 1 vector
of regression parameters

01:01:12.550 --> 01:01:16.160
for each series j, and
we have an epsilon j

01:01:16.160 --> 01:01:22.100
for-- an n-vector of innovation
errors for each series.

01:01:22.100 --> 01:01:31.970
And so basically if this,
the j-th column, is y j,

01:01:31.970 --> 01:01:35.740
we're modeling that to be
equal to the simple matrix

01:01:35.740 --> 01:01:44.540
Z times beta j plus epsilon
j, where this is n by 1.

01:01:44.540 --> 01:01:47.790
This is n by np plus 1.

01:01:51.520 --> 01:01:55.920
And this beta j is the mp
plus 1 regression parameter.

01:02:04.845 --> 01:02:05.345
OK.

01:02:10.140 --> 01:02:12.030
One might think,
OK, one can consider

01:02:12.030 --> 01:02:17.320
each of these regressions for
each of the component series,

01:02:17.320 --> 01:02:19.630
you could consider
them separately.

01:02:19.630 --> 01:02:23.940
But to consider
them all together,

01:02:23.940 --> 01:02:29.270
we can define the
multivariate regression model,

01:02:29.270 --> 01:02:33.420
which has the following form.

01:02:33.420 --> 01:02:40.277
We basically have the n-vectors
for the first component,

01:02:40.277 --> 01:02:42.360
and then the second component
up to nth component.

01:02:42.360 --> 01:02:46.730
So an n by p matrix of
dependent variables,

01:02:46.730 --> 01:02:53.540
where each column corresponds
to a different component series,

01:02:53.540 --> 01:02:55.820
follows a linear
regression model

01:02:55.820 --> 01:02:59.990
with the same Z matrix
with different regression

01:02:59.990 --> 01:03:03.460
coefficient parameters, beta
1 through beta m corresponding

01:03:03.460 --> 01:03:08.040
to the different components
of the multivariate series.

01:03:08.040 --> 01:03:14.330
And we have epsilon 1,
epsilon 2, up to epsilon m.

01:03:14.330 --> 01:03:20.670
So we're thinking of taking--
so basically the y 1, y 2,

01:03:20.670 --> 01:03:26.160
up to y m is essentially
this original matrix

01:03:26.160 --> 01:03:29.720
of our multivariate
time series because it's

01:03:29.720 --> 01:03:35.820
the first component
in the first column

01:03:35.820 --> 01:03:37.670
and the nth component
in the nth column.

01:03:37.670 --> 01:03:42.230
And the-- this
regression parameter

01:03:42.230 --> 01:03:47.090
or this explanatory variables
matrix X, Z in this case

01:03:47.090 --> 01:03:53.210
corresponds to lags of the
whole process up to p lags.

01:03:53.210 --> 01:03:56.080
So we're having
lags of all the--

01:03:56.080 --> 01:03:58.230
the m-variate
process up to p lags.

01:03:58.230 --> 01:04:02.700
So that's mp and then
plus 1 for our constant.

01:04:02.700 --> 01:04:05.790
So this is the set up for a
multivariate regression model.

01:04:12.880 --> 01:04:14.910
In terms of how
one specifies this,

01:04:14.910 --> 01:04:17.630
well, actually,
in economic theory

01:04:17.630 --> 01:04:20.930
this is also related
to seemingly unrelated

01:04:20.930 --> 01:04:23.750
regressions, which you'll
find in econometrics.

01:04:26.730 --> 01:04:32.850
If we want to specify this
multivariate model, well,

01:04:32.850 --> 01:04:35.520
what we could do is
we could actually

01:04:35.520 --> 01:04:37.550
specify each of the
component models

01:04:37.550 --> 01:04:41.974
separately because we
basically have sort of-- can

01:04:41.974 --> 01:04:43.390
think of the
univariate regression

01:04:43.390 --> 01:04:47.010
model for each component series.

01:04:47.010 --> 01:04:52.580
And this slide
indicates basically what

01:04:52.580 --> 01:04:53.810
the formulas are for that.

01:04:53.810 --> 01:04:58.189
So if we don't know anything
about multivariate regression

01:04:58.189 --> 01:04:59.730
we can say, well,
let's start by just

01:04:59.730 --> 01:05:03.620
doing the univariate regression
of each component series

01:05:03.620 --> 01:05:04.820
on the lags.

01:05:04.820 --> 01:05:07.540
And so we get our beta
hat j's least squares

01:05:07.540 --> 01:05:10.330
estimates given by the
usual formula where

01:05:10.330 --> 01:05:14.950
the independent variables matrix
Z goes Z transpose Z inverse Z

01:05:14.950 --> 01:05:17.280
transpose Y are the residuals.

01:05:17.280 --> 01:05:20.090
So these are familiar formulas.

01:05:20.090 --> 01:05:28.680
And if we did this for each
of the component series j,

01:05:28.680 --> 01:05:33.750
then we would actually
get sample estimates

01:05:33.750 --> 01:05:37.750
of the innovation process,
the eta_1, basically

01:05:37.750 --> 01:05:40.970
the whole eta series.

01:05:40.970 --> 01:05:45.980
And we could actually
define from these estimates

01:05:45.980 --> 01:05:49.110
of the innovations
our covariance matrix

01:05:49.110 --> 01:05:52.500
for the innovations as
the sample covariance

01:05:52.500 --> 01:05:54.640
matrix of these etas.

01:05:54.640 --> 01:05:58.170
So all of these formulas
are-- you're basically

01:05:58.170 --> 01:06:00.830
applying very
straightforward estimation

01:06:00.830 --> 01:06:05.440
methods for the parameters
of a linear regression

01:06:05.440 --> 01:06:08.855
and then estimating
variances/covariances

01:06:08.855 --> 01:06:11.440
of these innovation terms.

01:06:11.440 --> 01:06:14.470
So from this, we
actually have estimates

01:06:14.470 --> 01:06:20.420
of this process in terms of
the sigma and the beta hats.

01:06:20.420 --> 01:06:24.220
But it's made
assuming that we can

01:06:24.220 --> 01:06:26.755
treat each of these component
regressions separately.

01:06:33.410 --> 01:06:35.310
A rather remarkable
result is that

01:06:35.310 --> 01:06:40.300
these component-wise
regressions are actually

01:06:40.300 --> 01:06:44.470
the optimal estimates for
the multivariate regression

01:06:44.470 --> 01:06:46.030
as well.

01:06:46.030 --> 01:06:51.840
And as mathematicians,
this kind of result

01:06:51.840 --> 01:06:54.610
is, I think, rather
neat and elegant.

01:06:54.610 --> 01:06:58.900
And maybe some of you will
think this is very obvious,

01:06:58.900 --> 01:07:05.720
but it actually-- it
isn't quite obvious.

01:07:05.720 --> 01:07:08.010
That said, this
component-wise estimation

01:07:08.010 --> 01:07:10.430
should be optimal as well.

01:07:10.430 --> 01:07:13.100
And the next section
of the lecture notes

01:07:13.100 --> 01:07:16.965
goes through this argument.

01:07:20.140 --> 01:07:22.480
And I'm going to, in
the interest of time,

01:07:22.480 --> 01:07:26.590
go through this-- just sort of
highlight what the results are.

01:07:26.590 --> 01:07:29.580
The details are in these
notes that you can go through.

01:07:29.580 --> 01:07:34.750
And I will be happy to go
into more detail about them

01:07:34.750 --> 01:07:37.260
during office hours.

01:07:37.260 --> 01:07:41.520
But if we're fitting a vector
autoregression model where

01:07:41.520 --> 01:07:43.980
there are no constraints
on the coefficient

01:07:43.980 --> 01:07:47.620
matrices phi_1
through phi_p, then

01:07:47.620 --> 01:07:52.640
these component-wise
estimates, accounting

01:07:52.640 --> 01:08:00.707
for arbitrary covariance matrix
sigma for the innovations,

01:08:00.707 --> 01:08:02.790
those basically are equal
to the generalized least

01:08:02.790 --> 01:08:06.030
squares estimates of these
underlying parameters.

01:08:06.030 --> 01:08:09.280
You'll recall we talked about
the Gauss-Markov theorem

01:08:09.280 --> 01:08:15.370
where we were able to extend the
assumption of equal variances

01:08:15.370 --> 01:08:20.189
across observations to unequal
variances and covariances.

01:08:20.189 --> 01:08:23.832
Well, it turns out to
these component-wise OLS

01:08:23.832 --> 01:08:26.040
estimates are, in fact, the
generalized least squared

01:08:26.040 --> 01:08:27.180
estimates.

01:08:27.180 --> 01:08:30.160
And under the assumption
of Gaussian distributions

01:08:30.160 --> 01:08:32.410
for the innovations,
they, in fact,

01:08:32.410 --> 01:08:34.569
are maximum
likelihood estimates.

01:08:34.569 --> 01:08:41.210
And this theory applies
Kronecker products.

01:08:41.210 --> 01:08:43.609
We're not going to have
any homework with Kronecker

01:08:43.609 --> 01:08:44.580
products.

01:08:44.580 --> 01:08:47.160
These notes really
are for those who

01:08:47.160 --> 01:08:51.120
have some more extensive
background in linear algebra.

01:08:51.120 --> 01:08:55.130
But it's a very nice use
of these Kronecker product

01:08:55.130 --> 01:08:56.180
operators.

01:08:56.180 --> 01:09:00.130
Basically, this
notation-- I don't

01:09:00.130 --> 01:09:04.540
know, x circle, I'll
call it Kronecker--

01:09:04.540 --> 01:09:08.560
is one where you take a
matrix A and a matrix B

01:09:08.560 --> 01:09:11.170
and you consider
the matrix which

01:09:11.170 --> 01:09:16.250
takes each element of A times
the whole matrix B. So we start

01:09:16.250 --> 01:09:22.220
with an m by n matrix A and
end up with an mp by qn matrix

01:09:22.220 --> 01:09:25.550
by taking each element of
A times the whole matrix B.

01:09:25.550 --> 01:09:29.010
So it's, they say, has
this block structure.

01:09:29.010 --> 01:09:32.510
So this is very
simple definition.

01:09:32.510 --> 01:09:37.080
If you look at properties of
transposition of matrices,

01:09:37.080 --> 01:09:38.540
you can prove these results.

01:09:38.540 --> 01:09:42.850
These are properties of
the Kronecker product.

01:09:42.850 --> 01:09:54.320
And there's a vec operator
which takes a matrix

01:09:54.320 --> 01:09:58.470
and simply stacks
the columns together.

01:09:58.470 --> 01:10:04.700
And in the talk last Tuesday of
Ivan's, talking about modeling

01:10:04.700 --> 01:10:07.845
the volatility surface,
he basically, he

01:10:07.845 --> 01:10:11.410
was modeling a two dimensional
surface-- or a surface

01:10:11.410 --> 01:10:13.910
in three dimensions,
but there was

01:10:13.910 --> 01:10:16.830
two dimensions explaining it.

01:10:16.830 --> 01:10:22.140
You basically can stack
columns of the matrix

01:10:22.140 --> 01:10:27.030
and be modeling a vector
instead of a matrix of values.

01:10:27.030 --> 01:10:32.130
So the vectorizing operator
allows us to manipulate terms

01:10:32.130 --> 01:10:35.400
into a more convenient form.

01:10:35.400 --> 01:10:39.040
And this multivariate
regression model

01:10:39.040 --> 01:10:50.950
is one where it's set up as
sort of a n by m matrix Y,

01:10:50.950 --> 01:10:53.490
having that structure.

01:10:53.490 --> 01:10:57.110
It can be expressed in terms
of the linear regression form

01:10:57.110 --> 01:11:06.380
as y star equaling the
vector, the vec of y.

01:11:06.380 --> 01:11:15.340
So we basically have y 1, y
2, down to y m all lined up.

01:11:15.340 --> 01:11:18.480
So this is pm by 1.

01:11:21.600 --> 01:11:30.055
That's going to be equal to
some matrix plus the epsilon 1,

01:11:30.055 --> 01:11:33.920
epsilon 2, down to epsilon n.

01:11:33.920 --> 01:11:38.850
And then there's
going to be a matrix

01:11:38.850 --> 01:11:43.320
and a regression
coefficient matrix beta

01:11:43.320 --> 01:11:47.360
1, beta 2, down to beta p.

01:11:47.360 --> 01:11:51.320
So we consider vectorizing
the beta matrix,

01:11:51.320 --> 01:11:55.340
vectorizing epsilon,
and vectorizing y.

01:11:55.340 --> 01:11:59.465
And then in order
to define this sort

01:11:59.465 --> 01:12:03.370
of simple linear regression
model, univariate regression

01:12:03.370 --> 01:12:08.750
model, well, we need to
have a Z in the first column

01:12:08.750 --> 01:12:14.800
here corresponding to beta 1 for
y 1, and 0's everywhere else.

01:12:14.800 --> 01:12:20.160
In the second block
we want to have

01:12:20.160 --> 01:12:25.920
a Z in the second off diagonal
with 0's everywhere else and so

01:12:25.920 --> 01:12:27.100
forth.

01:12:27.100 --> 01:12:30.960
So this is just re-expressing
everything in this notation.

01:12:30.960 --> 01:12:34.290
But the notation is very nice
because, at the end of the day

01:12:34.290 --> 01:12:37.536
we basically have a regression
model like we had when we were

01:12:37.536 --> 01:12:39.030
doing our regression analysis.

01:12:39.030 --> 01:12:42.930
So all the theory we have
for specifying these models

01:12:42.930 --> 01:12:46.270
plays through with
univariate regression.

01:12:46.270 --> 01:12:50.441
And one can go through
this technical argument

01:12:50.441 --> 01:12:52.190
to show that the
generalized least squares

01:12:52.190 --> 01:12:56.900
estimate is, in
fact, the equivalent

01:12:56.900 --> 01:12:59.430
to the component-wise values.

01:12:59.430 --> 01:13:03.980
And that's very, very good.

01:13:03.980 --> 01:13:07.000
Maximum likelihood
estimation with these models.

01:13:07.000 --> 01:13:12.130
Well, we actually use
this vectorized notation

01:13:12.130 --> 01:13:15.050
to define the
likelihood function.

01:13:15.050 --> 01:13:20.560
And if these
assumptions are made

01:13:20.560 --> 01:13:24.610
about the linear
regression model,

01:13:24.610 --> 01:13:28.740
we basically have
an n times m vector

01:13:28.740 --> 01:13:34.780
of dependent variable values,
whereas your multivariate

01:13:34.780 --> 01:13:38.700
normal with mean given
by x star beta star

01:13:38.700 --> 01:13:41.870
and then a covariance
matrix epsilon.

01:13:41.870 --> 01:13:47.380
The covariance matrix of
epsilon star is sigma star.

01:13:47.380 --> 01:13:50.900
Well, sigma star is I_n
Kronecker product sigma.

01:13:50.900 --> 01:13:54.130
So if you go through
the math of this,

01:13:54.130 --> 01:13:59.250
everything matches up in terms
of what the assumptions are.

01:13:59.250 --> 01:14:05.340
And the conditional probability
density function of this data

01:14:05.340 --> 01:14:12.260
is the usual functions
of log-normal

01:14:12.260 --> 01:14:14.930
or of a normal sample.

01:14:14.930 --> 01:14:20.850
So we have unknown
parameters beta star sigma,

01:14:20.850 --> 01:14:26.960
which are equal to
the joint density

01:14:26.960 --> 01:14:29.740
of this normal linear
regression model.

01:14:29.740 --> 01:14:34.350
So this corresponds to what we
had before in our regression

01:14:34.350 --> 01:14:34.980
analysis.

01:14:34.980 --> 01:14:37.050
We just had this more
complicated definition

01:14:37.050 --> 01:14:40.900
of the independent
variables matrix X star.

01:14:40.900 --> 01:14:42.470
And a more
complicated definition

01:14:42.470 --> 01:14:47.270
of our variance/covariance
matrix sigma star.

01:14:47.270 --> 01:14:50.250
But the log-likelihood
function ends up

01:14:50.250 --> 01:14:54.390
being equal to a
term proportional

01:14:54.390 --> 01:14:59.090
to the log of the determinant
of our sigma matrix

01:14:59.090 --> 01:15:03.790
and minus one half Q of beta
sigma, where Q of beta sigma

01:15:03.790 --> 01:15:08.860
is the least squares criterion
for each of the component

01:15:08.860 --> 01:15:12.960
models summed up.

01:15:12.960 --> 01:15:16.660
So the component-wise
maximum likelihood estimation

01:15:16.660 --> 01:15:19.115
is-- for the
underlying parameters,

01:15:19.115 --> 01:15:23.260
is the same as the large one.

01:15:23.260 --> 01:15:31.880
And in terms of estimating
the covariance matrix,

01:15:31.880 --> 01:15:37.420
there's a notion called the
concentrated log-likelihood,

01:15:37.420 --> 01:15:45.200
which comes into play in
models with many parameters.

01:15:45.200 --> 01:15:48.390
In this model, we have
unknown parameters--

01:15:48.390 --> 01:15:52.230
our regression parameters
beta and our covariance matrix

01:15:52.230 --> 01:15:55.190
for the innovations sigma.

01:15:55.190 --> 01:15:59.850
It turns out that our estimate
of the regression parameter

01:15:59.850 --> 01:16:05.190
beta is independent, doesn't
depend-- not statistically

01:16:05.190 --> 01:16:06.570
independent-- but
does not depend

01:16:06.570 --> 01:16:10.700
on the value of the
covariance matrix sigma.

01:16:10.700 --> 01:16:14.110
So whatever sigma is, we have
the same maximum likelihood

01:16:14.110 --> 01:16:15.620
estimate for the betas.

01:16:15.620 --> 01:16:19.760
So we can consider
the log-likelihood

01:16:19.760 --> 01:16:24.540
setting the beta parameter
equal to its maximum likelihood

01:16:24.540 --> 01:16:25.410
estimate.

01:16:25.410 --> 01:16:27.270
And then we have a
function that just

01:16:27.270 --> 01:16:31.350
depends on the data and the
unknown parameter sigma.

01:16:31.350 --> 01:16:34.230
So that's a concentrated
likelihood function

01:16:34.230 --> 01:16:36.210
that needs to be maximized.

01:16:36.210 --> 01:16:40.570
And the maximization of the log
of a determinant of a matrix

01:16:40.570 --> 01:16:45.690
minus n over 2 the trace of that
matrix times an estimate of it,

01:16:45.690 --> 01:16:47.210
that has been solved.

01:16:47.210 --> 01:16:50.240
It's a bit involved.

01:16:50.240 --> 01:16:52.760
But if you're interested in
the mathematics for how that's

01:16:52.760 --> 01:16:55.470
actually solved and how you
take derivatives of determinants

01:16:55.470 --> 01:16:58.152
and so forth, there's a
paper by Anderson and Olkin

01:16:58.152 --> 01:16:59.860
that goes through all
the details of that

01:16:59.860 --> 01:17:01.622
that you can Google on the web.

01:17:05.910 --> 01:17:07.550
Finally, let's see.

01:17:07.550 --> 01:17:09.270
There's-- well, not finally.

01:17:09.270 --> 01:17:12.240
There's model selection
criteria that can be applied.

01:17:12.240 --> 01:17:14.720
These have been applied
before for regression models

01:17:14.720 --> 01:17:18.780
for univariate time series
model, the Akaike Information

01:17:18.780 --> 01:17:22.770
Criterion, the Bayes Information
Criterion, Hannan-Quinn

01:17:22.770 --> 01:17:24.640
Criterion.

01:17:24.640 --> 01:17:27.060
These definitions
are all consistent

01:17:27.060 --> 01:17:29.680
with the other definitions.

01:17:29.680 --> 01:17:33.330
They basically take
the likelihood function

01:17:33.330 --> 01:17:37.570
and you try to maximize that
plus a penalty for the number

01:17:37.570 --> 01:17:39.330
of unknown parameters.

01:17:39.330 --> 01:17:43.380
And that's given here.

01:17:45.920 --> 01:17:47.780
OK, then the last
section goes through

01:17:47.780 --> 01:17:53.500
an asymptotic distribution
of least squares estimates.

01:17:53.500 --> 01:17:56.950
And I'll let you read
that on your own.

01:17:56.950 --> 01:17:57.450
Let's see.

01:17:57.450 --> 01:18:02.470
For this lecture I put
together an example

01:18:02.470 --> 01:18:05.180
of fitting vector
autoregressions

01:18:05.180 --> 01:18:09.330
with some macroeconomic
variables.

01:18:09.330 --> 01:18:15.360
And I just wanted to
point that out to you.

01:18:15.360 --> 01:18:23.738
So let me go to
this document here.

01:18:23.738 --> 01:18:25.690
What have we got here?

01:18:29.594 --> 01:18:30.580
All right.

01:18:30.580 --> 01:18:31.310
Well, OK.

01:18:31.310 --> 01:18:37.410
Modeling macroeconomic time
series is an important topic.

01:18:37.410 --> 01:18:39.940
It's what sort of
central bankers do.

01:18:39.940 --> 01:18:42.120
They want to
understand what factors

01:18:42.120 --> 01:18:44.670
are affecting the economy in
terms of growth, inflation,

01:18:44.670 --> 01:18:45.880
unemployment.

01:18:45.880 --> 01:18:50.600
And what's the impact of
interest rate policies.

01:18:50.600 --> 01:18:52.940
There are some really
important papers

01:18:52.940 --> 01:18:56.750
by Robert Litterman and
Christopher Sims dealing

01:18:56.750 --> 01:18:59.150
with fitting vector
autoregression models

01:18:59.150 --> 01:19:01.590
to a macroeconomic time series.

01:19:01.590 --> 01:19:03.420
And actually, the
framework within which

01:19:03.420 --> 01:19:07.680
they specified these models
was a Bayesian framework,

01:19:07.680 --> 01:19:11.320
which is an extension of the
maximum likelihood method where

01:19:11.320 --> 01:19:15.680
you'll incorporate reasonable
sort of prior assumptions

01:19:15.680 --> 01:19:18.190
about what the
parameters ought to be.

01:19:18.190 --> 01:19:26.130
But in this note,
I sort of basically

01:19:26.130 --> 01:19:29.870
go through collecting various
macroeconomic variables

01:19:29.870 --> 01:19:33.240
directly off the web
using the package R.

01:19:33.240 --> 01:19:36.550
All this stuff
is-- these are data

01:19:36.550 --> 01:19:39.040
that you can get your hands on.

01:19:39.040 --> 01:19:43.900
Here's the unemployment
rate from January 1946

01:19:43.900 --> 01:19:47.030
up through this past month.

01:19:47.030 --> 01:19:52.670
Anyone can see how that's
varied between much less than 4%

01:19:52.670 --> 01:19:56.470
to over 10%, as it was recently.

01:19:56.470 --> 01:19:59.550
And there's also
the Fed funds rate,

01:19:59.550 --> 01:20:02.100
which is one of
the key variables

01:20:02.100 --> 01:20:06.610
that the Federal Reserve Open
Market Committee controls,

01:20:06.610 --> 01:20:08.880
or I should say
controlled in the past,

01:20:08.880 --> 01:20:10.840
to try and affect the economy.

01:20:10.840 --> 01:20:14.720
Now that value of that
rate is set almost at zero

01:20:14.720 --> 01:20:18.830
and other means
are applied to have

01:20:18.830 --> 01:20:24.940
an impact on economic growth
and the economic situation

01:20:24.940 --> 01:20:31.340
of the market-- of
the economy, rather.

01:20:31.340 --> 01:20:32.120
Let's see.

01:20:32.120 --> 01:20:34.330
There's also-- anyway, a
bunch of other variables.

01:20:34.330 --> 01:20:38.470
CPI, which is a
measure of inflation.

01:20:38.470 --> 01:20:45.502
What this note goes through
is the specification

01:20:45.502 --> 01:20:52.070
of vector autoregression
models for these series.

01:20:52.070 --> 01:20:54.490
And I use just a
small set of cases.

01:20:54.490 --> 01:20:58.640
I look at unemployment
rate, federal funds,

01:20:58.640 --> 01:21:02.470
and the CPI, which is
a measure of inflation.

01:21:02.470 --> 01:21:06.580
And there's-- if
one goes through,

01:21:06.580 --> 01:21:10.780
there are multivariate
versions of the autocorrelation

01:21:10.780 --> 01:21:14.670
function, as given on
the top right panel here,

01:21:14.670 --> 01:21:17.110
between these variables.

01:21:17.110 --> 01:21:20.350
And one can also do the partial
autocorrelation function.

01:21:20.350 --> 01:21:23.289
You'll recall that
autocorrelation functions

01:21:23.289 --> 01:21:24.830
and partial
autocorrelation functions

01:21:24.830 --> 01:21:29.040
are related to what kind of--
or help us understand what kind

01:21:29.040 --> 01:21:31.390
of order ARMA processes
might be appropriate

01:21:31.390 --> 01:21:32.670
for univariate series.

01:21:32.670 --> 01:21:36.750
For multivariate series,
then there are basically

01:21:36.750 --> 01:21:39.760
cross lags between variables
that are important,

01:21:39.760 --> 01:21:42.750
and these can call be captured
with vector autoregression

01:21:42.750 --> 01:21:43.520
models.

01:21:43.520 --> 01:21:47.820
So this goes through and
shows how these things

01:21:47.820 --> 01:21:50.610
are correlated with themselves.

01:21:50.610 --> 01:21:51.970
And let's see.

01:21:51.970 --> 01:21:59.550
At the end of this note,
there are some impulse

01:21:59.550 --> 01:22:02.660
response functions
graphed, which

01:22:02.660 --> 01:22:07.370
are looking at what is the
impact of an innovation in one

01:22:07.370 --> 01:22:11.090
of the components of the
multivariate time series.

01:22:11.090 --> 01:22:16.570
So like if Fed funds were to be
increased by a certain value,

01:22:16.570 --> 01:22:20.140
what would the likely impact
be on the unemployment rate?

01:22:20.140 --> 01:22:22.240
Or on GNP?

01:22:22.240 --> 01:22:25.540
Basically, the production
level of the economy.

01:22:25.540 --> 01:22:30.790
And this looks at-- let's see.

01:22:30.790 --> 01:22:32.260
Well, actually
here we're looking

01:22:32.260 --> 01:22:34.541
at the impulse function.

01:22:34.541 --> 01:22:36.040
You can look at the
impulse function

01:22:36.040 --> 01:22:39.250
of innovations on any of
the component variables

01:22:39.250 --> 01:22:40.000
on all the others.

01:22:40.000 --> 01:22:42.150
And in this case,
on the left panel

01:22:42.150 --> 01:22:47.790
here is-- it shows what
happens when unemployment

01:22:47.790 --> 01:22:50.360
has a spike up, or unit spike.

01:22:50.360 --> 01:22:51.760
A unit impulse up.

01:22:51.760 --> 01:22:55.460
Well, this second
panel shows what's

01:22:55.460 --> 01:22:57.190
likely to happen to
the Fed funds rate.

01:22:57.190 --> 01:22:59.730
It turns out that's
likely to go down.

01:22:59.730 --> 01:23:01.670
And that sort of is
indicating-- it's sort

01:23:01.670 --> 01:23:03.370
of reflecting
what, historically,

01:23:03.370 --> 01:23:09.180
was the policy of the Fed to
basically reduce interest rates

01:23:09.180 --> 01:23:11.550
if unemployment was rising.

01:23:11.550 --> 01:23:16.400
And then-- so anyway, these
impulse response functions

01:23:16.400 --> 01:23:19.530
correspond to essentially those
innovation terms on the Wold

01:23:19.530 --> 01:23:20.450
decomposition.

01:23:20.450 --> 01:23:22.360
And why are these important?

01:23:22.360 --> 01:23:26.720
Well, this indicates a
connection, basically,

01:23:26.720 --> 01:23:30.260
between that sort of moving
average representation

01:23:30.260 --> 01:23:31.870
and these time series models.

01:23:31.870 --> 01:23:35.480
And the way these
graphs are generated

01:23:35.480 --> 01:23:39.090
is by essentially finding
the Wold decomposition

01:23:39.090 --> 01:23:43.880
and then incorporating
that into these values.

01:23:43.880 --> 01:23:47.540
So-- OK, we'll finish
there for today.

