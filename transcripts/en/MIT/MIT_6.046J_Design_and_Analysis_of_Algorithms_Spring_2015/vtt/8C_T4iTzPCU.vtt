WEBVTT
Kind: captions
Language: en

00:00:00.070 --> 00:00:02.500
The following content is
provided under a Creative

00:00:02.500 --> 00:00:04.010
Commons license.

00:00:04.010 --> 00:00:06.360
Your support will help
MIT OpenCourseWare

00:00:06.360 --> 00:00:10.730
continue to offer high quality
educational resources for free.

00:00:10.730 --> 00:00:13.330
To make a donation or
view additional materials

00:00:13.330 --> 00:00:17.215
from hundreds of MIT courses,
visit MIT OpenCourseWare

00:00:17.215 --> 00:00:17.840
at ocw.mit.edu.

00:00:21.460 --> 00:00:24.280
PROFESSOR: Good morning
everyone, let's get started.

00:00:24.280 --> 00:00:29.650
So part two of our two lecture
sequence on network flow.

00:00:29.650 --> 00:00:33.910
So all the pain
involved in the concepts

00:00:33.910 --> 00:00:41.390
and notation from Tuesday turns
into algorithmic fun today.

00:00:41.390 --> 00:00:44.530
But we will do a little
bit of a review just

00:00:44.530 --> 00:00:46.620
to make sure we're
all on the same page

00:00:46.620 --> 00:00:48.640
with respect to all
of these notions

00:00:48.640 --> 00:00:54.020
that we talked about
on flow networks.

00:00:54.020 --> 00:00:58.920
So we call that a
flow network is simply

00:00:58.920 --> 00:01:03.490
a directed graph,
GVE, and each edge

00:01:03.490 --> 00:01:09.570
is going to have two
numbers associated with it.

00:01:09.570 --> 00:01:12.580
If you see something
like 1:3, this

00:01:12.580 --> 00:01:15.755
means that this is the flow,
and that's the capacity.

00:01:19.600 --> 00:01:22.100
We have a basic
constraint dissociated

00:01:22.100 --> 00:01:26.140
with the flow not exceeding
the capacity on any given edge.

00:01:26.140 --> 00:01:28.850
And we also have the
laws of conservation

00:01:28.850 --> 00:01:32.060
that say that other than the
sink in the source, which

00:01:32.060 --> 00:01:35.970
are the two distinguished
vertices in G V E,

00:01:35.970 --> 00:01:38.260
you're going to have
a situation where

00:01:38.260 --> 00:01:43.070
for any intermediate vertex, all
of the flow entering the vertex

00:01:43.070 --> 00:01:46.020
has to leave the vertex.

00:01:46.020 --> 00:01:48.910
And with those two
constraints you

00:01:48.910 --> 00:01:53.960
need to find the max flow
in your flow network.

00:01:53.960 --> 00:02:01.970
The flow value corresponding
to this potential max

00:02:01.970 --> 00:02:10.470
flow, or some flow, is
F equaling f of s of V.

00:02:10.470 --> 00:02:14.100
And so s is the source.

00:02:14.100 --> 00:02:17.390
V corresponds to
all of the vertices.

00:02:17.390 --> 00:02:21.120
And you're looking at pushing
flow from the source vertex

00:02:21.120 --> 00:02:30.280
to any of the vertices, and
thus the V includes t, as well.

00:02:30.280 --> 00:02:33.370
And that this is
essentially a flow value,

00:02:33.370 --> 00:02:37.780
and we showed-- this is
implicit summation notation,

00:02:37.780 --> 00:02:41.980
we showed, using a
little bit of algebra,

00:02:41.980 --> 00:02:48.240
that this equals f of V t.

00:02:51.350 --> 00:02:56.665
And that's going to hold because
of flow conservation primarily.

00:02:59.190 --> 00:03:04.860
So that was not
particularly surprising.

00:03:04.860 --> 00:03:07.470
Something that was a
little more surprising,

00:03:07.470 --> 00:03:09.560
or is a little
more surprising, is

00:03:09.560 --> 00:03:15.070
that you can have an arbitrary
cut in the flow network,

00:03:15.070 --> 00:03:17.940
and a cut corresponds
to any partition.

00:03:22.860 --> 00:03:32.130
S, T, such that s belongs to
S, the source belongs to S,

00:03:32.130 --> 00:03:38.920
and the sink belongs to T,
and that's essentially a cut.

00:03:38.920 --> 00:03:42.460
And so obviously there could
be exponentially many cuts

00:03:42.460 --> 00:03:44.950
in a given flow network.

00:03:44.950 --> 00:03:47.200
But the amazing
thing is that you

00:03:47.200 --> 00:03:50.870
can show a lemma that
says that, regardless

00:03:50.870 --> 00:03:57.270
of what the actual flow values
are in any of these edges--

00:03:57.270 --> 00:04:01.250
across any cut you're
going to see this flow.

00:04:01.250 --> 00:04:17.510
And this is true for any cut S,
T. The corollary to this lemma

00:04:17.510 --> 00:04:22.890
is that the flow is
going to be less than

00:04:22.890 --> 00:04:29.120
or equal to the
capacity of any cut.

00:04:34.720 --> 00:04:37.495
And that simply comes
from the edge constraints.

00:04:37.495 --> 00:04:39.870
You know that the flow
value corresponding

00:04:39.870 --> 00:04:42.050
to any particular
edge has to be less

00:04:42.050 --> 00:04:43.610
than the capacity of that edge.

00:04:43.610 --> 00:04:47.090
So you obey that
constraint and you just

00:04:47.090 --> 00:04:52.070
look at the edges that go from
S to T, sum up those capacities,

00:04:52.070 --> 00:05:00.670
and your maximum flow or any
flow is bounded, [INAUDIBLE],

00:05:00.670 --> 00:05:01.890
by that quantity.

00:05:01.890 --> 00:05:05.840
And that's our c S T.

00:05:05.840 --> 00:05:12.280
So you now have a relationship
between the minimum capacity

00:05:12.280 --> 00:05:15.750
cut and the max flow.

00:05:15.750 --> 00:05:20.570
We didn't actually
exploit that in the sense

00:05:20.570 --> 00:05:27.630
of the algorithm itself, the
operation of the algorithm.

00:05:27.630 --> 00:05:29.520
But we're going to
actually exploit

00:05:29.520 --> 00:05:33.570
the notion of a cut when we
prove this algorithm that I got

00:05:33.570 --> 00:05:38.900
to late on Tuesday's
lecture that corresponds

00:05:38.900 --> 00:05:40.570
to the Ford-Fulkerson algorithm.

00:05:40.570 --> 00:05:42.052
I showed you the
execution of it.

00:05:42.052 --> 00:05:43.510
In order to prove
that it's correct

00:05:43.510 --> 00:05:47.450
we're going to need
this notion of that

00:05:47.450 --> 00:05:52.340
bound on the flow value
given any cut in the network.

00:05:52.340 --> 00:05:55.070
So the max flow
related to min-cut.

00:05:55.070 --> 00:05:57.440
But before we get to that.

00:05:57.440 --> 00:06:00.780
Just to catch you
up on the algorithm

00:06:00.780 --> 00:06:04.010
that we actually got to, the
Ford-Fulkerson algorithm, that

00:06:04.010 --> 00:06:06.870
was based on the notion
of a residual graph.

00:06:06.870 --> 00:06:11.740
This is now a new graph,
G f, which is V, E f,

00:06:11.740 --> 00:06:13.520
so it depends on the flow.

00:06:13.520 --> 00:06:18.460
You computed based on G,
based on the given flow f.

00:06:18.460 --> 00:06:28.850
And this graph has edges that
have strictly positive residual

00:06:28.850 --> 00:06:37.050
capacities which are
defined as the C f u,

00:06:37.050 --> 00:06:44.810
v equals C u,v minus f u, v.
And this needs to be strictly

00:06:44.810 --> 00:06:50.400
greater than 0 in order for
the edge to exist in G of f.

00:06:50.400 --> 00:06:53.250
So if this is zero then you
don't have an edge between u

00:06:53.250 --> 00:06:55.680
and v in G of f.

00:06:55.680 --> 00:07:01.300
If it's one or greater assuming
integral flow values, you will.

00:07:01.300 --> 00:07:06.985
And in the residual graph--
two more quick definitions.

00:07:09.530 --> 00:07:13.370
One of which you saw
last time, and another

00:07:13.370 --> 00:07:19.230
that I just verbalized, is the
notion of an augmenting path.

00:07:19.230 --> 00:07:24.530
And this is any path
from s to t in G f.

00:07:24.530 --> 00:07:27.880
So if there's a path
from S to T in G f

00:07:27.880 --> 00:07:30.340
you do not have a maximum flow.

00:07:30.340 --> 00:07:31.774
There is an augmenting path.

00:07:31.774 --> 00:07:34.190
You're going to be able to
increase the flow corresponding

00:07:34.190 --> 00:07:37.740
to the f value, that gave
you your residual network, G

00:07:37.740 --> 00:07:39.060
S of f.

00:07:39.060 --> 00:07:42.000
And how much can you
increase this flow by?

00:07:42.000 --> 00:07:43.950
How much can you increase f by?

00:07:43.950 --> 00:07:52.590
That is termed the
residual capacity.

00:07:52.590 --> 00:07:59.040
And the residual capacity
of an augmenting path

00:07:59.040 --> 00:08:03.700
is simply C f of P,
that's what we're

00:08:03.700 --> 00:08:07.310
going to call it,
C f of P, equals

00:08:07.310 --> 00:08:14.380
the minimum value of
the residual capacities

00:08:14.380 --> 00:08:16.620
along the path.

00:08:16.620 --> 00:08:25.750
So this would be C f u,v,
and that's pretty much it.

00:08:25.750 --> 00:08:30.040
And some examples should
make this clearer.

00:08:30.040 --> 00:08:35.280
If we look at what the
actual algorithm is, and go

00:08:35.280 --> 00:08:37.640
through a simpler example
than we did last time,

00:08:37.640 --> 00:08:42.480
but it'll give you a
sense of how augmentations

00:08:42.480 --> 00:08:43.990
are going to change the flow.

00:08:43.990 --> 00:08:48.230
How augmenting paths appear
and eventually disappear

00:08:48.230 --> 00:08:53.590
as the flow increases and
reaches the maximum flow.

00:08:53.590 --> 00:08:56.270
So this simply
corresponds to, as I

00:08:56.270 --> 00:08:59.380
said, walking through
the augmenting path,

00:08:59.380 --> 00:09:01.610
looking at each of the
residual capacities,

00:09:01.610 --> 00:09:04.260
and picking the Min value.

00:09:04.260 --> 00:09:10.330
So the Ford-Fulkerson
algorithm, which

00:09:10.330 --> 00:09:17.390
is the first algorithm for
max flow continues to be used,

00:09:17.390 --> 00:09:33.100
sets the flow on each
of the edges to be zero.

00:09:33.100 --> 00:09:35.550
So you just set
everything to zero.

00:09:35.550 --> 00:09:38.810
Clearly satisfies
flow conservation,

00:09:38.810 --> 00:09:41.360
satisfies all the
edge capacities.

00:09:41.360 --> 00:09:45.090
And you have literally
three lines of code

00:09:45.090 --> 00:09:59.200
here, while an augmenting
path in G f exists.

00:09:59.200 --> 00:10:02.922
So there's a lot going on here,
you have to compute G of f,

00:10:02.922 --> 00:10:06.960
I mean it's three lines
of code, but the sub

00:10:06.960 --> 00:10:09.880
routines calls, which
are fairly complicated,

00:10:09.880 --> 00:10:12.690
you have to compute
g of f, given f,

00:10:12.690 --> 00:10:14.760
and initially it's all zeros.

00:10:14.760 --> 00:10:16.700
And then eventually
it's going to change,

00:10:16.700 --> 00:10:20.610
or immediately after you
discover an augmenting path.

00:10:20.610 --> 00:10:23.040
And now you have discovered
an augmenting path.

00:10:23.040 --> 00:10:26.940
So you have to now do
either breadth-first search

00:10:26.940 --> 00:10:29.740
or depth-first search, in
order to discover whether this

00:10:29.740 --> 00:10:31.610
is a path from s
to t because that's

00:10:31.610 --> 00:10:33.500
the definition of
an augmenting path.

00:10:33.500 --> 00:10:45.510
And then the last line of code
says, augment f by C f of p.

00:10:45.510 --> 00:10:47.900
And again this is
fairly involved.

00:10:47.900 --> 00:10:52.210
You have to take
the path in G of f.

00:10:52.210 --> 00:11:00.680
You're going to increase
the values of the edges that

00:11:00.680 --> 00:11:08.100
correspond to G of f translating
those edges back to G.

00:11:08.100 --> 00:11:11.920
And there's possibly an
inversion in direction

00:11:11.920 --> 00:11:15.070
that is associated
with that translation.

00:11:15.070 --> 00:11:18.970
And so if you take a really
straightforward example.

00:11:18.970 --> 00:11:25.511
Let's say if you have G is s,
and this one vertex here, a,

00:11:25.511 --> 00:11:27.310
and then you have t.

00:11:27.310 --> 00:11:31.210
And right now I
have 1:2 and 1:4.

00:11:34.250 --> 00:11:36.940
I now am going to
compute G of f.

00:11:36.940 --> 00:11:40.395
So G of f is going to be-- I'm
just going to write down s,

00:11:40.395 --> 00:11:44.599
a, and t because the
vertices are all the same.

00:11:44.599 --> 00:11:46.015
The edges are
obviously different.

00:11:48.710 --> 00:11:52.110
Am I going to have
an edge from s to a?

00:11:52.110 --> 00:11:53.700
Yes.

00:11:53.700 --> 00:11:56.850
And what number should
I put on that edge?

00:11:56.850 --> 00:12:00.400
1, and that's because
it's 2 minus 1,

00:12:00.400 --> 00:12:01.845
not because it's just plain 1.

00:12:04.450 --> 00:12:10.750
And this edge here is going to
have a residual capacity of 1,

00:12:10.750 --> 00:12:14.830
and that just comes
directly from that 1.

00:12:14.830 --> 00:12:17.530
And what that means is,
and this is important,

00:12:17.530 --> 00:12:22.340
what that means is that, in
effect, this residual capacity

00:12:22.340 --> 00:12:26.900
says in this direction you
still have a capacity of 1.

00:12:26.900 --> 00:12:28.360
Which means that
in this direction

00:12:28.360 --> 00:12:30.110
you could reduce by one.

00:12:30.110 --> 00:12:32.790
OK, that's the important
thing to remember.

00:12:32.790 --> 00:12:36.710
And over here I'm
going to have--

00:12:36.710 --> 00:12:40.100
this way I'm going to
have 3 and coming back

00:12:40.100 --> 00:12:42.410
I'm going to have 1.

00:12:42.410 --> 00:12:46.260
All right, so
straightforward example but I

00:12:46.260 --> 00:12:50.840
think evocative in the sense
that it applies the concepts

00:12:50.840 --> 00:12:55.110
that we've seen so
far other than cuts.

00:12:55.110 --> 00:12:56.370
And we'll get to that.

00:12:56.370 --> 00:13:00.230
And what happens here, is
there a path from s to t?

00:13:00.230 --> 00:13:01.380
Absolutely.

00:13:01.380 --> 00:13:05.210
And what is the
residual capacity

00:13:05.210 --> 00:13:06.755
of the path from s to t?

00:13:09.460 --> 00:13:12.130
It's one because you've got a
1 here, and you got a 3 here,

00:13:12.130 --> 00:13:14.340
you got to take the
Min, and it's 1.

00:13:14.340 --> 00:13:17.860
Right, I guess that's the
number of choice today, one.

00:13:17.860 --> 00:13:21.110
But once you do that and
you find that, you say, OK,

00:13:21.110 --> 00:13:22.915
what I'm going to
do is-- and this

00:13:22.915 --> 00:13:24.540
is going to be pretty
straightforward--

00:13:24.540 --> 00:13:27.280
I'm going to look
at these edges,

00:13:27.280 --> 00:13:31.110
and I know that I have
c of t being a one

00:13:31.110 --> 00:13:32.680
corresponding to this value.

00:13:32.680 --> 00:13:35.800
And I'm going to go add one
to those corresponding edges

00:13:35.800 --> 00:13:40.642
because these are the edges
that actually shows in terms

00:13:40.642 --> 00:13:41.600
of the augmenting path.

00:13:41.600 --> 00:13:44.960
So the augmenting path is that.

00:13:44.960 --> 00:13:55.660
And so this turns into s to
a to t, and that becomes 2:2.

00:13:55.660 --> 00:13:59.420
This becomes 2:4 and that's it.

00:14:02.850 --> 00:14:05.540
So I now have G with the new f.

00:14:05.540 --> 00:14:10.880
So I can now think about what
is G of f1 correspond to.

00:14:10.880 --> 00:14:15.810
The G is separate from
f, so in some sense

00:14:15.810 --> 00:14:20.320
the G stayed the same, but the
f changed, as you saw here.

00:14:20.320 --> 00:14:25.078
And now G f1 looks like s, a, t.

00:14:27.980 --> 00:14:29.380
What do I do between s and a?

00:14:32.160 --> 00:14:34.312
Well, I only have
something from a to s.

00:14:34.312 --> 00:14:36.500
I don't have
anything from s to a

00:14:36.500 --> 00:14:39.250
because the residual capacity
in this direction, thanks

00:14:39.250 --> 00:14:42.880
to the fact that the flow has
saturated the edge capacity,

00:14:42.880 --> 00:14:44.340
is actually zero.

00:14:44.340 --> 00:14:46.440
But I do have an edge this way.

00:14:46.440 --> 00:14:49.220
And what is the
value for that edge?

00:14:49.220 --> 00:14:50.750
It's 2.

00:14:50.750 --> 00:14:53.480
And over here, what
are the numbers

00:14:53.480 --> 00:14:55.530
for the top and the bottom?

00:14:55.530 --> 00:14:57.610
2 and 2.

00:14:57.610 --> 00:15:00.770
And now I look at
G f1, and I've gone

00:15:00.770 --> 00:15:03.710
through one iteration
of this pseudo code,

00:15:03.710 --> 00:15:07.290
and I now try to find an
augmenting path in G f1.

00:15:07.290 --> 00:15:10.010
And is there a path from s to t?

00:15:10.010 --> 00:15:12.590
No, which means I'm done.

00:15:12.590 --> 00:15:14.740
This is the max flow.

00:15:14.740 --> 00:15:15.440
You believe me?

00:15:19.270 --> 00:15:21.310
For this example,
you believe me.

00:15:21.310 --> 00:15:26.230
You believe me, like,
for every example?

00:15:26.230 --> 00:15:30.260
No, absolutely not because
you haven't done a proof!

00:15:30.260 --> 00:15:33.191
We haven't done a proof!

00:15:33.191 --> 00:15:34.690
That's why you
shouldn't believe me.

00:15:37.310 --> 00:15:39.410
So we got to do a proof.

00:15:39.410 --> 00:15:43.740
So how is it that we
know, given all of this,

00:15:43.740 --> 00:15:49.320
it's not because I told you
so, that when we've converged.

00:15:49.320 --> 00:15:55.380
When we get out of this while
loop, that we have a max flow.

00:15:55.380 --> 00:15:56.130
Looks pretty good.

00:15:56.130 --> 00:15:59.430
Worked for that
example and it's going

00:15:59.430 --> 00:16:03.010
to work for every example,
but that's not a proof.

00:16:03.010 --> 00:16:06.330
So now we have to sort of
take in all of the notions

00:16:06.330 --> 00:16:09.650
that we've talked about here,
including primarily the notion

00:16:09.650 --> 00:16:10.870
of cuts.

00:16:10.870 --> 00:16:13.510
And the proof, the max
flow, min-cut theorem,

00:16:13.510 --> 00:16:18.790
which is going to show this
key result that we require,

00:16:18.790 --> 00:16:22.240
which is that when we
terminate in the Ford-Fulkerson

00:16:22.240 --> 00:16:26.420
algorithm, we're going
to have a max flow.

00:16:26.420 --> 00:16:29.245
And that's the reason why
it's a maximum flow algorithm.

00:16:29.245 --> 00:16:30.620
If you don't have
that proof, you

00:16:30.620 --> 00:16:31.953
don't have a max flow algorithm.

00:16:34.320 --> 00:16:37.560
So hopefully all
of that is clear.

00:16:37.560 --> 00:16:40.020
Pipe up if you have questions.

00:16:40.020 --> 00:16:44.360
And let's write out the
max flow min-cut theorem,

00:16:44.360 --> 00:16:51.550
which I mentioned
of it last time

00:16:51.550 --> 00:16:55.810
but never really
got to even stating,

00:16:55.810 --> 00:16:59.840
but today we're going to
state it and prove it.

00:17:09.540 --> 00:17:11.960
So this is an
interesting theorem.

00:17:11.960 --> 00:17:14.569
I mean it's a legendary theorem.

00:17:14.569 --> 00:17:21.750
And it's not your usual
theorem in the sense

00:17:21.750 --> 00:17:26.670
that it makes a
simple statement.

00:17:26.670 --> 00:17:29.430
It actually makes
three statements.

00:17:29.430 --> 00:17:32.100
It says the following
are equivalent,

00:17:32.100 --> 00:17:36.470
and it's got three things
which are the following.

00:17:39.140 --> 00:17:52.080
F equals c S T
for some cut, S,T.

00:17:52.080 --> 00:17:57.810
This says that some
cut is saturated.

00:17:57.810 --> 00:18:00.130
Right, so that's
just the statement.

00:18:00.130 --> 00:18:07.650
The second statement, which is
equivalent to the first one,

00:18:07.650 --> 00:18:10.410
says that f is a maximum flow.

00:18:14.990 --> 00:18:17.960
So f being a maximum
flow means there's

00:18:17.960 --> 00:18:19.710
some cut that's saturated.

00:18:19.710 --> 00:18:22.520
If there's some cut that's
saturated for a given flow,

00:18:22.520 --> 00:18:24.360
you have a max flow.

00:18:24.360 --> 00:18:26.900
And then the last one,
which is important

00:18:26.900 --> 00:18:31.020
for our Ford-Fulkerson
algorithm,

00:18:31.020 --> 00:18:40.525
is that f admits no
augmenting paths.

00:18:43.860 --> 00:18:50.780
And so if I want to show that
the Ford-Fulkerson algorithm

00:18:50.780 --> 00:18:54.670
terminates with a
max flow, tell me,

00:18:54.670 --> 00:18:59.990
based on implications--
like i implies j,

00:18:59.990 --> 00:19:04.000
for numbers i and j, what
it is that I need to prove.

00:19:04.000 --> 00:19:06.960
We're going to prove a bunch
of other things along the way

00:19:06.960 --> 00:19:12.020
but crucially what is the
i implies j that I want

00:19:12.020 --> 00:19:15.920
if I want to claim that the
Ford-Fulkerson algorithm

00:19:15.920 --> 00:19:17.650
terminates with the max flow.

00:19:22.040 --> 00:19:23.315
Yeah, you over there.

00:19:23.315 --> 00:19:25.690
AUDIENCE: 3 implies 2.

00:19:25.690 --> 00:19:29.010
PROFESSOR: 3 implies 2.

00:19:29.010 --> 00:19:31.670
That's right, 3 implies
2, exactly right.

00:19:31.670 --> 00:19:32.450
3 implies 2.

00:19:35.990 --> 00:19:41.110
So we need to do 3 implies 2.

00:19:41.110 --> 00:19:45.760
Now of course, the theorem
says that, 1 implies 2,

00:19:45.760 --> 00:19:49.950
2 implies 3, 3
implies 1, et cetera.

00:19:49.950 --> 00:19:52.350
There's a lot of
implications here.

00:19:52.350 --> 00:19:59.710
What we are going to do is we're
going to show that 1 implies 2,

00:19:59.710 --> 00:20:03.590
2 implies 3, and 3 implies 1.

00:20:03.590 --> 00:20:06.746
And that pretty much
takes care of everything.

00:20:06.746 --> 00:20:08.120
And it turns out
the reason we do

00:20:08.120 --> 00:20:13.400
this is simply because it
makes for the simplest proofs.

00:20:13.400 --> 00:20:18.040
1 implies 2 and 2 implies 3 are
one- liners, and that 3 implies

00:20:18.040 --> 00:20:20.410
1 is a little more
interesting and involved,

00:20:20.410 --> 00:20:24.060
but it's a little bit
easier than directly

00:20:24.060 --> 00:20:26.532
doing 3 implies 2.

00:20:26.532 --> 00:20:28.240
So that's the way
we're going to do this.

00:20:28.240 --> 00:20:31.290
You could certainly play
around and do other things.

00:20:31.290 --> 00:20:32.690
So any questions so far?

00:20:35.750 --> 00:20:38.040
OK, so let's go
ahead and do this.

00:20:56.780 --> 00:20:59.260
All right, we should be
able to knock these two,

00:20:59.260 --> 00:21:03.930
1 implies 2 and 2 implies 3,
in just about a minute each.

00:21:06.940 --> 00:21:11.370
So I want to show 1
implies 2, and essentially

00:21:11.370 --> 00:21:16.510
what I want to say
here is, if I've

00:21:16.510 --> 00:21:21.150
saturated a particular
cuts capacity then

00:21:21.150 --> 00:21:22.460
I have a max flow.

00:21:22.460 --> 00:21:26.350
And really, I mean this
comes from the definitions,

00:21:26.350 --> 00:21:31.860
since I'm going to have f less
than or equal to the c S, T,

00:21:31.860 --> 00:21:36.300
and this is simply because
of edge capacity constraints.

00:21:36.300 --> 00:21:54.510
This is just edge capacities
for any cut S T, the assumption

00:21:54.510 --> 00:22:07.187
that f equals c of S T implies
f is a maximum flow because f

00:22:07.187 --> 00:22:07.895
can be increased.

00:22:14.460 --> 00:22:16.700
And that's basically it.

00:22:16.700 --> 00:22:19.770
Right, so this is pretty easy.

00:22:19.770 --> 00:22:23.950
Next one is easy as well.

00:22:23.950 --> 00:22:26.660
2 implies 3.

00:22:26.660 --> 00:22:33.080
If the rare and
augmenting path--

00:22:33.080 --> 00:22:37.650
so I'm going to do
this by contradiction--

00:22:37.650 --> 00:22:47.890
if the rare and augmenting
path, the flow value

00:22:47.890 --> 00:22:48.860
can be increased.

00:22:48.860 --> 00:22:50.790
Because remember
the augmenting path

00:22:50.790 --> 00:22:55.070
corresponds to a path
with strictly positive

00:22:55.070 --> 00:22:57.560
residual capacities.

00:22:57.560 --> 00:22:59.560
The only reason there's
an edge in there

00:22:59.560 --> 00:23:04.560
is because you have a greater
than zero residual capacity.

00:23:04.560 --> 00:23:08.260
So that means that
the flow value

00:23:08.260 --> 00:23:12.580
could be increased by some
small amount corresponding

00:23:12.580 --> 00:23:17.650
to C f u v, and each of
these quantities that

00:23:17.650 --> 00:23:21.130
are in here-- I'm sorry,
corresponding to min C f u

00:23:21.130 --> 00:23:24.510
v, that's the residual capacity.

00:23:24.510 --> 00:23:27.910
But you know that each of
these capacities, C F u v,

00:23:27.910 --> 00:23:30.140
are strictly greater than zero.

00:23:30.140 --> 00:23:32.860
So the min clearly is
strictly greater than zero.

00:23:32.860 --> 00:23:36.310
It might be some tiny quantity
but it's greater than zero.

00:23:36.310 --> 00:23:38.540
Which means that
the flow value could

00:23:38.540 --> 00:23:46.440
be increased contradicting
the maximality of f.

00:23:53.510 --> 00:23:55.880
So a little proof
by contradiction

00:23:55.880 --> 00:23:58.520
gives you 2 implies 3.

00:23:58.520 --> 00:24:00.560
So, so far, so good.

00:24:00.560 --> 00:24:03.350
This is primarily based
on the definitions

00:24:03.350 --> 00:24:06.819
of augmenting paths, residual
capacities, et cetera.

00:24:06.819 --> 00:24:07.610
Now the fun starts.

00:24:10.310 --> 00:24:12.850
Now we want to
show really, I mean

00:24:12.850 --> 00:24:16.950
this is truly what the max flow
min cut is because I really

00:24:16.950 --> 00:24:18.420
didn't have cuts up there.

00:24:18.420 --> 00:24:20.450
So what's min-cut
about that, right?

00:24:24.540 --> 00:24:28.600
So I had a question at the
end of Tuesday's lecture

00:24:28.600 --> 00:24:33.510
on how come the algorithm
didn't use cuts.

00:24:33.510 --> 00:24:37.740
And, well, the algorithm
doesn't use cuts,

00:24:37.740 --> 00:24:41.770
but showing that the algorithm
converges to the max flow

00:24:41.770 --> 00:24:42.850
uses the notion of cuts.

00:24:45.990 --> 00:24:50.220
So what we want to
do is do 3 implies 1,

00:24:50.220 --> 00:24:53.060
and that's going to take
care of what we want.

00:24:53.060 --> 00:24:59.330
And we're going to
say, well, suppose

00:24:59.330 --> 00:25:04.345
f admits no augmenting paths.

00:25:08.450 --> 00:25:10.220
So what does that mean?

00:25:10.220 --> 00:25:15.140
Well, that means that
you can't reach t from s.

00:25:15.140 --> 00:25:18.720
If you reach t from s,
there's an augmenting path,

00:25:18.720 --> 00:25:24.100
so you can't make that
move or make that path.

00:25:24.100 --> 00:25:27.730
So let's go ahead and, sort
of, look at this a little more

00:25:27.730 --> 00:25:32.020
carefully and figure
out what's causing

00:25:32.020 --> 00:25:36.150
this lack of connectivity.

00:25:36.150 --> 00:25:38.610
There's a lack of
connectivity between s and t.

00:25:38.610 --> 00:25:40.780
You can't actually get there.

00:25:40.780 --> 00:25:43.570
So there's like a
chasm, there's a gap.

00:25:43.570 --> 00:25:45.320
And so what's causing that?

00:25:45.320 --> 00:25:50.680
And what we're going
to do is figure

00:25:50.680 --> 00:25:55.230
out what the cut
of the partition

00:25:55.230 --> 00:26:00.740
is that could be
the reason why you

00:26:00.740 --> 00:26:03.900
have this lack of connectivity.

00:26:03.900 --> 00:26:07.010
So we're going to define
S which is exactly

00:26:07.010 --> 00:26:13.610
going to be our S from our
favorite s t cut definition,

00:26:13.610 --> 00:26:16.110
but bear with me
for just a second.

00:26:16.110 --> 00:26:19.590
Because I will
tell you what S is.

00:26:19.590 --> 00:26:35.020
There exists a path
in G f from s to u.

00:26:35.020 --> 00:26:37.120
So what I'm doing
is I'm collecting

00:26:37.120 --> 00:26:46.080
the reachable vertices from
S. And I know that I'm not

00:26:46.080 --> 00:26:50.940
going to have t,
small t, in this set

00:26:50.940 --> 00:26:55.400
because f admits no augmenting
paths so I can't possibly

00:26:55.400 --> 00:26:59.440
reach t from s.

00:26:59.440 --> 00:27:02.510
But I can collect
all of the vertices

00:27:02.510 --> 00:27:08.530
that that can be reached
from s in this given G of f

00:27:08.530 --> 00:27:14.050
and put them all into S.

00:27:14.050 --> 00:27:17.590
And then my T
definition is simply

00:27:17.590 --> 00:27:23.780
V minus S. And the key
observation which I already

00:27:23.780 --> 00:27:26.890
have made but
important to emphasize

00:27:26.890 --> 00:27:36.680
is that small t belongs to T,
and obviously s belongs to S,

00:27:36.680 --> 00:27:43.030
therefore S T is a cut.

00:27:43.030 --> 00:27:47.230
It satisfies the
definition of a cut.

00:27:47.230 --> 00:27:49.350
So we've set ourselves up.

00:27:49.350 --> 00:27:55.430
And then we have
a key observation

00:27:55.430 --> 00:27:59.399
to make that I'm going to
try and extract out or you.

00:27:59.399 --> 00:28:00.315
Let me move over here.

00:28:17.370 --> 00:28:28.160
All right, so what I want to
do is I want to draw this out,

00:28:28.160 --> 00:28:31.620
and I want to look
at this as this

00:28:31.620 --> 00:28:36.700
is S, that is T. I know
that S is over here,

00:28:36.700 --> 00:28:39.660
are s is over here,
t is over here.

00:28:43.660 --> 00:28:51.520
I'm going to pick some
vertex v that's over here,

00:28:51.520 --> 00:28:56.170
some vertex u that's over here.

00:28:56.170 --> 00:29:00.850
I know that there's
a path, and I'm

00:29:00.850 --> 00:29:03.480
writing this is a
dotted path because this

00:29:03.480 --> 00:29:06.470
is a path in G of f.

00:29:06.470 --> 00:29:10.340
We're going to actually
move between a G of f and G

00:29:10.340 --> 00:29:11.670
in this proof.

00:29:11.670 --> 00:29:14.260
So that's the only hard
part of this proof.

00:29:14.260 --> 00:29:16.780
The movement between
g and G of f.

00:29:16.780 --> 00:29:20.420
The edges are different
between g and G of f.

00:29:20.420 --> 00:29:21.699
Got to keep that in your head.

00:29:21.699 --> 00:29:23.740
Because we're going to
make arguments about that.

00:29:23.740 --> 00:29:27.730
I know that all of the vertices
that are on the left hand side

00:29:27.730 --> 00:29:31.930
are reachable from s, so I just
picked an arbitrary vertex u,

00:29:31.930 --> 00:29:33.390
and I know that
there's a path in G

00:29:33.390 --> 00:29:38.360
of f because that's the way
I defined it from s to u.

00:29:38.360 --> 00:29:46.485
I also know that there is
no path in G of f from u

00:29:46.485 --> 00:29:55.610
to v because if there was a path
v would have been on this side,

00:29:55.610 --> 00:29:57.860
correct?

00:29:57.860 --> 00:30:01.590
So there's clearly no
path in g of f from u

00:30:01.590 --> 00:30:04.170
to v. That's why v is over here.

00:30:04.170 --> 00:30:11.510
But let's say that
I'm picking something

00:30:11.510 --> 00:30:20.000
where there's an edge in G, the
original flow network from u

00:30:20.000 --> 00:30:24.280
to v. So the edge that I've
drawn here, the dark edge,

00:30:24.280 --> 00:30:33.062
is an edge between u and
v in G. And that edge

00:30:33.062 --> 00:30:34.560
had a certain capacity.

00:30:34.560 --> 00:30:36.230
It existed.

00:30:36.230 --> 00:30:37.880
The reason it existed
is because it had

00:30:37.880 --> 00:30:40.670
a nonzero capacity, correct?

00:30:40.670 --> 00:30:43.340
Because we originally
defined our flow network

00:30:43.340 --> 00:30:46.120
to be something where--
if you only put edges

00:30:46.120 --> 00:30:48.400
in there, in the
original flow network

00:30:48.400 --> 00:30:51.380
if the capacities were
greater than zero.

00:30:51.380 --> 00:30:56.577
So that edge in there, in
G had some capacity c u

00:30:56.577 --> 00:30:57.990
v which was greater than zero.

00:31:00.720 --> 00:31:09.460
But this edge does not
exist in G of f, correct?

00:31:09.460 --> 00:31:11.347
So what can I say?

00:31:11.347 --> 00:31:11.847
Go ahead.

00:31:11.847 --> 00:31:15.130
AUDIENCE: [INAUDIBLE]

00:31:17.480 --> 00:31:18.630
PROFESSOR: Exactly.

00:31:18.630 --> 00:31:26.090
In the original graph, we have
saturated this edge-- that

00:31:26.090 --> 00:31:28.950
was my best throw of the term.

00:31:28.950 --> 00:31:31.030
I like that.

00:31:31.030 --> 00:31:36.090
And you guys didn't notice,
but maybe it'll be on video.

00:31:36.090 --> 00:31:38.430
Actually, it won't be
on video unfortunately.

00:31:38.430 --> 00:31:44.580
But so I got this edge
here in the original graph

00:31:44.580 --> 00:31:47.710
and I didn't get
this dotted edge here

00:31:47.710 --> 00:31:51.540
because the residual
capacity was zero.

00:31:51.540 --> 00:31:53.260
The original
capacity was nonzero.

00:31:53.260 --> 00:31:54.910
The residual capacity is zero.

00:31:54.910 --> 00:32:08.600
The only reason that happens is
because C f u v is zero since

00:32:08.600 --> 00:32:19.900
if C f u v greater than zero
then v would belong to S,

00:32:19.900 --> 00:32:23.590
not v belonging to T as assumed.

00:32:26.720 --> 00:32:30.430
So that's essentially what
we've determined here.

00:32:30.430 --> 00:32:44.100
So this means that f
of u v equals c of u v

00:32:44.100 --> 00:32:58.720
simply because c f u v equals
c u v minus f u v which is 0.

00:32:58.720 --> 00:33:03.820
All right, so that's the
statement I can make.

00:33:03.820 --> 00:33:09.620
Now, I did not have
any constraint on u

00:33:09.620 --> 00:33:15.090
and v. I just said u belongs
to s and v belongs to t.

00:33:15.090 --> 00:33:17.290
And for each of
those things, I know

00:33:17.290 --> 00:33:23.050
that I can't have edges in
the residual network between u

00:33:23.050 --> 00:33:26.506
and v which means that any edge
that exists-- maybe there's

00:33:26.506 --> 00:33:28.380
no edge in the original
network, but if there

00:33:28.380 --> 00:33:31.920
was an edge in that original
network it's saturated

00:33:31.920 --> 00:33:33.730
according to this argument.

00:33:33.730 --> 00:33:42.640
So every edge from S to T in the
original network is saturated,

00:33:42.640 --> 00:33:45.800
and that essentially
means that I've gotten

00:33:45.800 --> 00:33:47.902
to the capacity of my cut.

00:33:47.902 --> 00:33:49.360
That's essentially
what that means.

00:33:49.360 --> 00:33:53.230
If I've saturated every edge,
I've reached that capacity.

00:33:53.230 --> 00:33:56.300
OK, so that's it.

00:33:56.300 --> 00:33:59.480
All you do once you recognize
that you had an arbitrary

00:33:59.480 --> 00:34:09.260
choice of u and v, you just say
summing over all u belonging

00:34:09.260 --> 00:34:31.280
to s and v belong to t yields
f of S T equals c of S T.

00:34:31.280 --> 00:34:34.110
All right, I'm on a roll
not just with Frisbees.

00:34:34.110 --> 00:34:38.050
I finished the proof
with a finger to spare.

00:34:38.050 --> 00:34:41.679
So f of S T equals
c of S T. All right,

00:34:41.679 --> 00:34:44.070
so that's exactly what we want.

00:34:44.070 --> 00:34:49.810
We are saying f of S T is
obviously a cardinality of f

00:34:49.810 --> 00:34:52.080
so I've shown this
thing over here.

00:34:54.639 --> 00:34:58.660
So that's why the
Ford-Fulkerson algorithm works.

00:34:58.660 --> 00:35:00.830
It's because of
this analysis that

00:35:00.830 --> 00:35:03.210
the Ford-Fulkerson
algorithm works.

00:35:03.210 --> 00:35:05.490
So are we done?

00:35:05.490 --> 00:35:13.020
What are we missing in algorithm
design, our algorithm analysis?

00:35:13.020 --> 00:35:13.770
Not you, yet.

00:35:13.770 --> 00:35:14.872
AUDIENCE: [INAUDIBLE]

00:35:14.872 --> 00:35:15.580
PROFESSOR: Sorry?

00:35:15.580 --> 00:35:16.413
AUDIENCE: A runtime?

00:35:16.413 --> 00:35:19.400
PROFESSOR: Runtime,
good runtime.

00:35:19.400 --> 00:35:22.450
I know you have
too many Frisbees.

00:35:22.450 --> 00:35:23.810
You probably do, too.

00:35:23.810 --> 00:35:28.690
But so we haven't done--
we've done correctness,

00:35:28.690 --> 00:35:30.080
we've done conversions.

00:35:30.080 --> 00:35:33.690
We haven't done a
complexity analysis.

00:35:33.690 --> 00:35:37.730
And so there's
some bad news here.

00:35:37.730 --> 00:35:51.070
And erasing a pivotal
moment in 6 over 6.

00:36:27.920 --> 00:36:32.370
Why is this a pivotal
moment in 6 over 6?

00:36:32.370 --> 00:36:34.680
It's on the Frisbee, exactly.

00:36:34.680 --> 00:36:37.970
This picture-- maybe not
exactly that picture.

00:36:37.970 --> 00:36:40.275
It's not quite that picture
but it's the same graph.

00:36:42.980 --> 00:36:45.750
It's a pivotal moment because
this picture was famous

00:36:45.750 --> 00:36:47.500
because it was in
a textbook but now

00:36:47.500 --> 00:36:52.206
it's iconic because
it's on a Frisbee.

00:36:52.206 --> 00:36:56.230
We immortalized this picture.

00:36:56.230 --> 00:37:01.920
So this is an example
of really a failure

00:37:01.920 --> 00:37:04.780
of the Ford-Fulkerson algorithm.

00:37:04.780 --> 00:37:06.750
So it's kind of a bad
example on this Frisbee

00:37:06.750 --> 00:37:10.100
but it's going to lead
us to better algorithms.

00:37:10.100 --> 00:37:15.060
So what I have here is a
weird flow network that

00:37:15.060 --> 00:37:17.040
has these strange capacities.

00:37:17.040 --> 00:37:19.390
I mean it's a fairly
straightforward flow network.

00:37:19.390 --> 00:37:22.975
But at the as of the moment
it obviously satisfies

00:37:22.975 --> 00:37:25.830
a flow conservation
because all of the flows

00:37:25.830 --> 00:37:28.780
are 0 and therefore
edge capacities.

00:37:28.780 --> 00:37:34.860
But we're going to now take
a pathological execution

00:37:34.860 --> 00:37:37.060
of the Ford-Fulkerson algorithm.

00:37:37.060 --> 00:37:40.390
Which, by the way,
did not specify

00:37:40.390 --> 00:37:44.760
how you are going to
select the augmenting path.

00:37:44.760 --> 00:37:47.470
It just said, find
an augmenting path.

00:37:47.470 --> 00:37:51.690
And in passing, I said,
overuse depth-first search,

00:37:51.690 --> 00:37:54.230
we'll use breadth-first search,
and kind of hand wave my way

00:37:54.230 --> 00:37:55.280
through that.

00:37:55.280 --> 00:37:58.200
Now it turns out you
can't hand wave when you

00:37:58.200 --> 00:37:59.860
want to run algorithms, right.

00:37:59.860 --> 00:38:02.680
You eventually have to code them
up, you got to pick something.

00:38:02.680 --> 00:38:03.770
So people did.

00:38:03.770 --> 00:38:07.980
People picked different
ways of selecting

00:38:07.980 --> 00:38:11.840
augmenting paths in the
Ford-Fulkerson framework.

00:38:11.840 --> 00:38:15.790
And they discovered that some
of them would work beautifully

00:38:15.790 --> 00:38:18.860
and some of them
would fail miserably

00:38:18.860 --> 00:38:21.110
on networks that were
as small as this,

00:38:21.110 --> 00:38:23.250
and they'd just take forever.

00:38:23.250 --> 00:38:26.680
And this is before complexity
analysis, kind of maybe

00:38:26.680 --> 00:38:30.940
before asymptotic complexity
really came into vogue.

00:38:30.940 --> 00:38:33.640
So they just had
empirical analysis

00:38:33.640 --> 00:38:36.580
to tell them that some
techniques for discovering

00:38:36.580 --> 00:38:40.150
augmenting paths work
better than other techniques

00:38:40.150 --> 00:38:44.220
on an empirical basis.

00:38:44.220 --> 00:38:50.980
So what could go wrong with
Ford-Fulkerson on this example?

00:38:50.980 --> 00:38:54.360
What selection
strategy corresponding

00:38:54.360 --> 00:39:00.220
to the augmenting path could
cause Ford-Fulkerson to have

00:39:00.220 --> 00:39:02.890
a huge number of iterations?

00:39:02.890 --> 00:39:03.392
Go ahead.

00:39:03.392 --> 00:39:05.475
AUDIENCE: So each time you
pick an augmenting path

00:39:05.475 --> 00:39:07.300
you pick one that
goes to the center.

00:39:07.300 --> 00:39:11.990
PROFESSOR: Exactly, and so let's
walk through a couple here.

00:39:11.990 --> 00:39:15.604
So the first one, let's
just say this was a and b,

00:39:15.604 --> 00:39:16.270
to make it easy.

00:39:16.270 --> 00:39:19.830
So what is the first
path that we picked?

00:39:19.830 --> 00:39:20.500
What is it--?

00:39:20.500 --> 00:39:21.380
Go for it.

00:39:21.380 --> 00:39:22.658
You're not off the hook yet.

00:39:22.658 --> 00:39:24.450
AUDIENCE: You pick
s to a to b to t.

00:39:24.450 --> 00:39:27.230
PROFESSOR: Yeah, and then when
you pick s to a to b to t,

00:39:27.230 --> 00:39:29.670
I'm not going to
draw G of f for this,

00:39:29.670 --> 00:39:36.680
it's pretty clear that G of f
is going to have-- I'm sorry,

00:39:36.680 --> 00:39:40.530
you end up picking s to
a to b to t corresponding

00:39:40.530 --> 00:39:42.680
to the G of f that
you've created.

00:39:42.680 --> 00:39:47.860
Right, so the G of f might
have something like s

00:39:47.860 --> 00:39:50.600
to a-- there's only going
to be an edge this way

00:39:50.600 --> 00:39:52.450
that's going to
have 10 raised to 9.

00:39:52.450 --> 00:39:55.740
And then this way it's going to
have 1, and that's going to b,

00:39:55.740 --> 00:40:04.330
and over here to t it's
going to have 10 raised to 9.

00:40:04.330 --> 00:40:09.170
It's also going to have
10 raised to 9 this way,

00:40:09.170 --> 00:40:12.700
and it's going to have
10 raised to 9 this way.

00:40:12.700 --> 00:40:14.530
And the reason the
gentleman was laughing

00:40:14.530 --> 00:40:19.730
is because we ended
up picking this path.

00:40:19.730 --> 00:40:24.010
S in G of f-- this is G of f
just to be clear-- you pick

00:40:24.010 --> 00:40:27.930
s a b t.

00:40:27.930 --> 00:40:33.980
And so what you end up doing
is you end up making this 1,

00:40:33.980 --> 00:40:41.720
making this 1, and coming
over here making this 1.

00:40:41.720 --> 00:40:42.220
Wow!

00:40:42.220 --> 00:40:43.720
OK, you did increase the flow.

00:40:43.720 --> 00:40:46.590
Very good, made progress.

00:40:46.590 --> 00:40:48.400
Now what happens?

00:40:48.400 --> 00:40:54.330
When you do this, certainly
you'll get a different G of f.

00:40:54.330 --> 00:40:56.732
And the G of f is going
to turn into-- well,

00:40:56.732 --> 00:40:58.440
this gets a little
more interesting here.

00:40:58.440 --> 00:41:01.050
This is 10 raised to 9 minus 1.

00:41:01.050 --> 00:41:04.440
This is 10 raised to 9--
no, that doesn't change.

00:41:04.440 --> 00:41:06.890
This is still 10 raised to 9.

00:41:06.890 --> 00:41:09.720
This is 10 raised to 9 minus 1.

00:41:09.720 --> 00:41:11.860
And that this edge
here also changes

00:41:11.860 --> 00:41:16.580
because basically
what ends up happening

00:41:16.580 --> 00:41:22.580
is you end up going
this way with 1.

00:41:22.580 --> 00:41:24.430
Did I get that right?

00:41:24.430 --> 00:41:25.850
Yeah?

00:41:25.850 --> 00:41:26.800
OK, good.

00:41:26.800 --> 00:41:30.990
So now what's the bad path?

00:41:30.990 --> 00:41:35.260
s b a t.

00:41:35.260 --> 00:41:41.790
So s b a t would now say,
oh, what I'm going to do is,

00:41:41.790 --> 00:41:44.300
I'm going to go ahead
and make this 1.

00:41:44.300 --> 00:41:45.883
What do I do with that?

00:41:45.883 --> 00:41:46.830
AUDIENCE: Make it 0.

00:41:46.830 --> 00:41:49.650
PROFESSOR: Make it 0, exactly.

00:41:49.650 --> 00:41:54.440
Make it 0, and then
this 1 becomes a 1.

00:41:54.440 --> 00:41:56.380
Shall we keep going?

00:41:56.380 --> 00:41:58.260
No.

00:41:58.260 --> 00:42:03.190
So 2 billion iterations for
a graph with four vertices.

00:42:03.190 --> 00:42:06.060
Now that's performance for you.

00:42:06.060 --> 00:42:12.100
So this particular, potentially
a depth-first Ford-Fulkerson

00:42:12.100 --> 00:42:14.830
on this example, depending
on the ordering of vertices

00:42:14.830 --> 00:42:19.127
it's certainly quite
possible that you

00:42:19.127 --> 00:42:24.640
would get to this particular
order in the depth-first search

00:42:24.640 --> 00:42:26.760
on G of f.

00:42:26.760 --> 00:42:31.190
The computers is dumb, it
does what you tell it to do,

00:42:31.190 --> 00:42:32.750
it's doing depth-first search.

00:42:32.750 --> 00:42:35.510
It's producing these
paths, and you end up

00:42:35.510 --> 00:42:39.850
with 2 billion
iterations for this.

00:42:39.850 --> 00:42:44.840
And how many iterations
do you really

00:42:44.840 --> 00:42:48.480
need if you did it right?

00:42:48.480 --> 00:42:50.560
Two.

00:42:50.560 --> 00:42:52.860
So that's a billion
factor slowdown.

00:42:52.860 --> 00:42:54.340
So this is a
pathological example,

00:42:54.340 --> 00:42:56.520
a simple pathological
example, to just show you

00:42:56.520 --> 00:42:57.920
what the problem is.

00:42:57.920 --> 00:43:01.320
But you can imagine that if
you use depth-first search

00:43:01.320 --> 00:43:04.817
you might be a factor of
five slower on average

00:43:04.817 --> 00:43:06.400
than if you use some
of the technique.

00:43:06.400 --> 00:43:08.850
And a factor of 5 is
nothing to be scoffed at,

00:43:08.850 --> 00:43:10.990
especially if you're
running for minutes.

00:43:10.990 --> 00:43:15.270
And, you know, back in the day
computers were horribly slow.

00:43:15.270 --> 00:43:19.780
So how is this problem fixed?

00:43:19.780 --> 00:43:23.310
Well, any number of ways.

00:43:23.310 --> 00:43:28.160
But the first real way
that took into account

00:43:28.160 --> 00:43:32.650
asymptotitc complexity, did
analysis, and did all of that

00:43:32.650 --> 00:43:34.785
was due to Edmonds
and Karp, which

00:43:34.785 --> 00:43:36.650
is a few years after
Ford-Fulkerson.

00:43:36.650 --> 00:43:39.080
In fact, several years
after Ford-Fulkerson.

00:43:39.080 --> 00:43:45.380
And their contribution was
not as much a new algorithm,

00:43:45.380 --> 00:43:47.980
though it is called
Edmonds-Karp algorithm.

00:43:47.980 --> 00:43:51.484
It's really a small
change to Ford-Fulkerson.

00:43:51.484 --> 00:43:53.650
You might want to think
about it as the Edmonds-Karp

00:43:53.650 --> 00:43:56.550
analysis, which was
kind of important,

00:43:56.550 --> 00:43:58.550
and we're not going
to do that here.

00:43:58.550 --> 00:44:01.490
But you'll see it in
section on Friday.

00:44:01.490 --> 00:44:03.670
But the Edmonds-Karp
algorithm slash

00:44:03.670 --> 00:44:21.150
analysis is that, if you use a
breadth-first augmenting path,

00:44:21.150 --> 00:44:26.220
it's obviously just as easy
to discover complexity-wise

00:44:26.220 --> 00:44:28.190
as the depth-first path.

00:44:28.190 --> 00:44:32.020
Then you could get a
polynomial bound that did not

00:44:32.020 --> 00:44:36.170
depend on the capacities
for the complexity

00:44:36.170 --> 00:44:40.690
and therefore iteration
count of Ford-Fulkerson.

00:44:40.690 --> 00:44:43.480
So that was their contribution.

00:44:43.480 --> 00:44:54.590
And so the breadth-first path is
the shortest path in G f from s

00:44:54.590 --> 00:45:07.550
to t if you count where
each edge has weight 1.

00:45:07.550 --> 00:45:09.840
So in that case you would
get your two iterations.

00:45:09.840 --> 00:45:12.579
You'd get the shortest
path of weight 2.

00:45:12.579 --> 00:45:14.370
Whenever I say weight,
I mean just counting

00:45:14.370 --> 00:45:16.210
the number of edges.

00:45:16.210 --> 00:45:18.510
And these algorithms
they actually

00:45:18.510 --> 00:45:22.000
recognized that
breadth-first implementations

00:45:22.000 --> 00:45:26.160
of Ford-Fulkerson were
the ones that ran faster

00:45:26.160 --> 00:45:28.352
than all the other ones.

00:45:28.352 --> 00:45:30.310
And then they said, wait,
what's going on here?

00:45:30.310 --> 00:45:31.590
This looks pretty good.

00:45:31.590 --> 00:45:35.620
And so they went off and did a
fairly sophisticated analysis

00:45:35.620 --> 00:45:42.700
that showed that
you had, and as I

00:45:42.700 --> 00:45:47.810
said you'll see that in
section, that you only

00:45:47.810 --> 00:45:53.230
need order VE augmentations
in the worst case.

00:45:56.015 --> 00:45:57.890
So this is what we know
and love with respect

00:45:57.890 --> 00:46:00.450
to algorithmic complexity.

00:46:00.450 --> 00:46:03.160
Order VE augmentations,
in the worst case,

00:46:03.160 --> 00:46:06.310
provided you used
breadth-first augmentation.

00:46:06.310 --> 00:46:09.700
And so what is the overall
complexity of the algorithm,

00:46:09.700 --> 00:46:11.720
let's call it the
Edmonds-Karp algorithm,

00:46:11.720 --> 00:46:23.850
with this breadth-first
search it would be-- order?

00:46:23.850 --> 00:46:28.250
VE square, you could
think about it as,

00:46:28.250 --> 00:46:30.980
let's assume that e
is greater than V,

00:46:30.980 --> 00:46:34.760
and so we just say
order V squared.

00:46:37.520 --> 00:46:39.520
So that was the Edmonds
and Karp's contribution.

00:46:42.070 --> 00:46:45.660
This mean that an amazing amount
of work over the past 30, 40

00:46:45.660 --> 00:46:48.960
years on improving that bound.

00:46:48.960 --> 00:46:52.470
[? Denic ?]
independently arrived

00:46:52.470 --> 00:46:58.700
at a similar analysis in bound,
along with Emonds and Karp.

00:46:58.700 --> 00:47:03.160
They were the first to give
polynomial bounds on max flow.

00:47:03.160 --> 00:47:12.650
And there was an algorithm
that came out of MIT partly,

00:47:12.650 --> 00:47:19.220
or at least MIT alums,
where you improve that.

00:47:19.220 --> 00:47:22.236
And so until about
2011-- and there's

00:47:22.236 --> 00:47:23.860
a lot of different
algorithms depending

00:47:23.860 --> 00:47:27.290
on whether you're taking
into account the value

00:47:27.290 --> 00:47:31.420
of the capacity, and you want to
think about this is as some max

00:47:31.420 --> 00:47:34.520
value, and if you want to
include that in the complexity

00:47:34.520 --> 00:47:36.720
then things change a bit.

00:47:36.720 --> 00:47:37.790
If you can limit that.

00:47:37.790 --> 00:47:41.010
But assuming that those
capacities are essentially

00:47:41.010 --> 00:47:45.500
unbounded, then the
fastest algorithm in 2011

00:47:45.500 --> 00:47:50.285
was King, Raul, and Tarjan.

00:47:53.070 --> 00:48:05.364
And that runs in order VE log
of E divided by V log V V.

00:48:05.364 --> 00:48:07.280
Obviously, we need
something for the log here.

00:48:10.500 --> 00:48:12.340
So that's kind of,
pretty good here, right,

00:48:12.340 --> 00:48:14.030
because this is pretty small.

00:48:14.030 --> 00:48:17.880
If you really think about it,
if E's large for dense graphs

00:48:17.880 --> 00:48:19.960
this is going to be
a small constant.

00:48:19.960 --> 00:48:21.040
So that's pretty good.

00:48:21.040 --> 00:48:23.620
So that was the
fastest until 2011.

00:48:23.620 --> 00:48:27.320
Very recently,
Orlin, who's at MIT,

00:48:27.320 --> 00:48:30.010
came up with an
order v e algorithm

00:48:30.010 --> 00:48:36.200
and there are many variants here
that use fast matrix multiply.

00:48:36.200 --> 00:48:40.220
And then just this
past September,

00:48:40.220 --> 00:48:42.350
Aleksander Madry
joined our department,

00:48:42.350 --> 00:48:50.680
and he has a variant algorithm
that is better in some sense,

00:48:50.680 --> 00:48:56.254
not in the pure sense
of V and E but there's

00:48:56.254 --> 00:48:57.670
a couple of
constraints associated

00:48:57.670 --> 00:49:01.410
with it that are very mild.

00:49:01.410 --> 00:49:04.520
But it's in that sense
better than order VE

00:49:04.520 --> 00:49:06.110
and I won't get to that.

00:49:06.110 --> 00:49:10.590
In the context of 6
over 6 we are obviously

00:49:10.590 --> 00:49:13.930
going to stick with
Edmonds-Karp and Ford-Fulkerson

00:49:13.930 --> 00:49:16.040
in terms of what
you're responsible for,

00:49:16.040 --> 00:49:20.364
but this as been a rich area
of research for decades.

00:49:20.364 --> 00:49:21.030
All right, yeah.

00:49:21.030 --> 00:49:21.530
[INAUDIBLE]

00:49:26.354 --> 00:49:32.810
Oh, simply because the
breadth-first augmentation

00:49:32.810 --> 00:49:33.670
takes order e.

00:49:33.670 --> 00:49:36.470
I'm just saying it
takes order E time.

00:49:36.470 --> 00:49:40.730
So this was the order VE
augmentations, in the worst

00:49:40.730 --> 00:49:45.090
case, each augmentation
takes order E time,

00:49:45.090 --> 00:49:47.490
because breadth-first search
takes order E time assuming

00:49:47.490 --> 00:49:51.290
that E is greater than V and I
just added the E square factor.

00:49:51.290 --> 00:49:52.040
Thanks for asking.

00:49:52.040 --> 00:49:55.130
I'm sure a lot of other
people had the same question.

00:49:55.130 --> 00:49:58.510
Yes, I just sort of skipped
over that a little too quickly.

00:49:58.510 --> 00:50:01.990
That's why you get
the order VE square.

00:50:01.990 --> 00:50:04.500
And so that's a
significant reduction

00:50:04.500 --> 00:50:07.610
if you assume that constant
factors don't explode

00:50:07.610 --> 00:50:12.300
going from Edmonds-Karp
to King, Raul, and Tarjan,

00:50:12.300 --> 00:50:16.610
and certainly Orlin, So
now we got algorithms,

00:50:16.610 --> 00:50:19.990
we've proven them correct,
we've done some analysis.

00:50:19.990 --> 00:50:24.610
At tomorrow, in section,
you're going to see,

00:50:24.610 --> 00:50:28.390
at the analysis that
gives you the order VE.

00:50:28.390 --> 00:50:32.720
Which is a half hours worth
of pain, should I say.

00:50:32.720 --> 00:50:35.960
Uh, no, a half hours
worth of excitement!

00:50:35.960 --> 00:50:40.410
That corresponds to the order
VE, that bound that we have.

00:50:40.410 --> 00:50:42.340
All right, so it's not
that difficult. It's

00:50:42.340 --> 00:50:45.600
about the level of
this thing over here,

00:50:45.600 --> 00:50:47.990
and maybe a little
bit more involved.

00:50:47.990 --> 00:50:50.198
So, yeah?

00:50:50.198 --> 00:50:53.460
AUDIENCE: [INAUDIBLE]

00:50:56.210 --> 00:51:00.580
PROFESSOR: Breadth-first search
takes order v plus e time.

00:51:00.580 --> 00:51:04.220
On a graph, breadth-first
search is order v plus e.

00:51:04.220 --> 00:51:06.430
And I'm getting a
little lazy here,

00:51:06.430 --> 00:51:10.470
and I'm paying for it, by
putting in the v e square.

00:51:10.470 --> 00:51:12.560
But I should really
have been-- if I

00:51:12.560 --> 00:51:15.081
wanted to be a stickler, I
would have put in v plus e,

00:51:15.081 --> 00:51:15.580
thank you.

00:51:18.250 --> 00:51:21.570
So where was I?

00:51:21.570 --> 00:51:22.720
So we got an algorithm now.

00:51:22.720 --> 00:51:24.800
You know, we've got a
bunch of algorithms.

00:51:24.800 --> 00:51:27.220
Network flow is
this amazing hammer

00:51:27.220 --> 00:51:29.500
that is being used in a
wide variety of things.

00:51:29.500 --> 00:51:31.880
And it's been used for
matching, bipartite matching,

00:51:31.880 --> 00:51:33.920
all sorts of things.

00:51:33.920 --> 00:51:39.600
My favorite example
is something that

00:51:39.600 --> 00:51:43.290
is related to
sports, I should say,

00:51:43.290 --> 00:51:45.900
which is a baseball elimination.

00:51:45.900 --> 00:51:49.450
So what is this
application about?

00:51:49.450 --> 00:51:54.260
You see charts that correspond
to how your team is doing.

00:51:54.260 --> 00:51:55.190
Standings, right?

00:51:55.190 --> 00:52:00.530
Go to ESPN and click on
standings for baseball,

00:52:00.530 --> 00:52:02.440
for example.

00:52:02.440 --> 00:52:04.860
It doesn't-- now you
see pre season standings

00:52:04.860 --> 00:52:07.690
but let's assume it's
August or something--

00:52:07.690 --> 00:52:11.660
and you click on standings.

00:52:11.660 --> 00:52:17.510
And you see a chart that
corresponds to games played,

00:52:17.510 --> 00:52:21.780
games won, games
lost, games remaining,

00:52:21.780 --> 00:52:23.540
and so on and so forth.

00:52:23.540 --> 00:52:37.640
And back in the day
when they were just

00:52:37.640 --> 00:52:45.050
winners corresponding
to each division

00:52:45.050 --> 00:52:49.270
you had to win your division
to make the playoffs.

00:52:49.270 --> 00:52:51.520
So you had to be the best.

00:52:51.520 --> 00:52:54.540
Now if you ended
up being tied, you

00:52:54.540 --> 00:52:58.440
ended up having to play
an elimination game.

00:52:58.440 --> 00:53:02.290
But the whole purpose
of this analysis

00:53:02.290 --> 00:53:07.360
is deciding whether
you still have a chance

00:53:07.360 --> 00:53:09.420
to make the playoffs or not.

00:53:09.420 --> 00:53:12.950
So the goal here is you
want an algorithm that

00:53:12.950 --> 00:53:15.480
is going to look
at the standings

00:53:15.480 --> 00:53:19.700
and decide if your
team is alive or not.

00:53:19.700 --> 00:53:22.830
Is there a chance
of God is great,

00:53:22.830 --> 00:53:26.970
whatever, everything goes your
way, angels in the outfield,

00:53:26.970 --> 00:53:28.100
right?

00:53:28.100 --> 00:53:30.410
Are you going to
make the playoffs?

00:53:30.410 --> 00:53:33.610
And so you can be
very optimistic.

00:53:33.610 --> 00:53:36.160
And you might think that
this is a straightforward,

00:53:36.160 --> 00:53:38.970
I'm going to add up a couple
numbers and figure this out.

00:53:38.970 --> 00:53:42.200
It turns out it's not quite
that simple though sportswriters

00:53:42.200 --> 00:53:44.710
would love to think that.

00:53:44.710 --> 00:53:47.280
That it's as simple as
adding up a bunch of numbers.

00:53:47.280 --> 00:53:52.840
And obviously, there's network
flow lurking in here somewhere.

00:53:52.840 --> 00:53:55.250
Otherwise, I wouldn't be
talking about it here.

00:53:55.250 --> 00:54:02.434
So this is not quite historical
in terms of the standings.

00:54:02.434 --> 00:54:03.850
I tweaked the
numbers a little bit

00:54:03.850 --> 00:54:06.340
in order to make this example
a little more interesting.

00:54:06.340 --> 00:54:10.950
But roughly speaking,
August 30th, 1996, this

00:54:10.950 --> 00:54:14.490
was the standings corresponding
to the American League East.

00:54:14.490 --> 00:54:16.890
And now this is
1996 so the teams

00:54:16.890 --> 00:54:21.060
are a little bit different
from what you have.

00:54:21.060 --> 00:54:24.500
So New York, unfortunately, was
leading the division even back

00:54:24.500 --> 00:54:25.550
then.

00:54:25.550 --> 00:54:27.910
Still had Baltimore.

00:54:27.910 --> 00:54:30.980
Then there was Boston.

00:54:30.980 --> 00:54:33.440
And then was Toronto.

00:54:33.440 --> 00:54:37.007
And then this is back in
1996, and Detroit was part

00:54:37.007 --> 00:54:38.215
of the [INAUDIBLE] back then.

00:54:38.215 --> 00:54:41.180
This was before realignment.

00:54:41.180 --> 00:54:46.720
So I just called these one,
two, three, four, and five.

00:54:46.720 --> 00:54:48.050
Those are the positions.

00:54:48.050 --> 00:54:51.064
All right, so Boston
was in third place.

00:54:51.064 --> 00:54:52.480
And so, I'm going
to say these are

00:54:52.480 --> 00:54:56.940
w i, which are the wins, so
far. l i, which are the losses.

00:54:59.510 --> 00:55:04.300
And r i, which are
remaining games to play.

00:55:07.710 --> 00:55:10.390
And This is usually
what you see.

00:55:10.390 --> 00:55:19.010
I mean this is the simple
version of the standings

00:55:19.010 --> 00:55:25.090
and I'm just going to give you
most of these, I should say.

00:55:25.090 --> 00:55:28.677
And I'm not going to write
down what Detroit is, simply

00:55:28.677 --> 00:55:30.260
because I'm going
to vary that number.

00:55:30.260 --> 00:55:32.690
Because it gets interesting
as that number varies.

00:55:32.690 --> 00:55:34.990
And each of these teams
had 28 games to play.

00:55:39.380 --> 00:55:42.970
OK so that's usually what you
see when you see a snapshot.

00:55:42.970 --> 00:55:54.150
OK now I'm going
to also add rij,

00:55:54.150 --> 00:56:01.920
which are the games that these
teams play against each other.

00:56:01.920 --> 00:56:04.260
And the reason is simple.

00:56:04.260 --> 00:56:07.350
This linear correlation
between win loss column,

00:56:07.350 --> 00:56:09.520
specifically
corresponding to games

00:56:09.520 --> 00:56:11.710
that Boston plays with New York.

00:56:11.710 --> 00:56:14.820
No ties in baseball,
so Boston wins,

00:56:14.820 --> 00:56:17.500
New York loses and vice versa.

00:56:17.500 --> 00:56:20.490
So I want those numbers
as well and these

00:56:20.490 --> 00:56:24.100
are things that you
typically don't see

00:56:24.100 --> 00:56:25.620
but they're going
to be important,

00:56:25.620 --> 00:56:29.540
because they do determine
whether your team is alive

00:56:29.540 --> 00:56:31.620
or eliminated.

00:56:31.620 --> 00:56:40.670
So 5 The reason I
have dashes here

00:56:40.670 --> 00:56:45.140
is because the team doesn't
play itself in the diagonal.

00:56:49.727 --> 00:56:51.310
AUDIENCE: Is that
the number of games?

00:56:51.310 --> 00:56:53.518
PROFESSOR: Yes that is
absolutely the number of games

00:56:53.518 --> 00:56:56.070
that they're going to
play against each other.

00:56:56.070 --> 00:57:00.460
All right so I've
got small number.

00:57:00.460 --> 00:57:03.120
I mean 5 plus 7, 12.

00:57:03.120 --> 00:57:04.809
Plus 4.

00:57:04.809 --> 00:57:05.350
What is that?

00:57:05.350 --> 00:57:06.130
16.

00:57:06.130 --> 00:57:08.050
16 plus 3 is 19.

00:57:08.050 --> 00:57:10.416
There's 28 games
left to play, there's

00:57:10.416 --> 00:57:12.790
obviously games that are being
played against other teams

00:57:12.790 --> 00:57:14.160
outside the division.

00:57:14.160 --> 00:57:17.110
And so this is just rij, OK?

00:57:17.110 --> 00:57:20.020
So that's the story here.

00:57:20.020 --> 00:57:29.640
And I should say that when
I talk about this, what

00:57:29.640 --> 00:57:30.800
are these corresponding to?

00:57:30.800 --> 00:57:34.430
Well this corresponds to NY.

00:57:34.430 --> 00:57:37.880
This column corresponds to
Baltimore this corresponds

00:57:37.880 --> 00:57:42.330
to Boston, same order, this
corresponds to Toronto.

00:57:42.330 --> 00:57:45.830
This column
corresponds to Detroit.

00:57:45.830 --> 00:57:50.010
So otherwise it's
under specified.

00:57:50.010 --> 00:57:51.880
Any questions about
the table here?

00:57:59.556 --> 00:58:01.180
If you want to know,
the Bo Sox did not

00:58:01.180 --> 00:58:02.796
make the playoffs that year.

00:58:07.150 --> 00:58:12.550
So all that most sportswriters
do, most people do,

00:58:12.550 --> 00:58:18.685
is say Team i is eliminated.

00:58:22.220 --> 00:58:37.304
If wi plus ri is strictly
less than wj for sum j.

00:58:37.304 --> 00:58:39.220
So you're just going to
say, OK you know what?

00:58:39.220 --> 00:58:41.240
If I've won all my
games, I'm still

00:58:41.240 --> 00:58:43.440
not going to make it
to this team that's

00:58:43.440 --> 00:58:47.030
already won 75 games, right?

00:58:47.030 --> 00:58:50.360
And so clearly I'm done.

00:58:50.360 --> 00:58:51.860
That team has already
won more games

00:58:51.860 --> 00:58:53.234
than I could possibly win.

00:58:53.234 --> 00:58:54.150
So that's an easy one.

00:58:56.970 --> 00:58:59.330
And so this is strictly
less, because we're just

00:58:59.330 --> 00:59:03.650
talking about elimination,
total elimination.

00:59:03.650 --> 00:59:08.070
You get your summer vacation,
or your fall vacation.

00:59:08.070 --> 00:59:13.240
So let's just say
that you had Detroit,

00:59:13.240 --> 00:59:14.980
and you're talking
about w 5 years.

00:59:14.980 --> 00:59:20.380
You're talking about
this number here, is 46.

00:59:20.380 --> 00:59:25.020
l5 is 88.

00:59:25.020 --> 00:59:28.180
So they're having
a bad season here.

00:59:28.180 --> 00:59:34.700
And you're going to
say 46 plus 28 equals

00:59:34.700 --> 00:59:38.340
74, which is less than 75.

00:59:38.340 --> 00:59:42.734
Therefore in this scenario,
Detroit is eliminated.

00:59:42.734 --> 00:59:43.400
All makes sense?

00:59:46.240 --> 00:59:53.610
So this is sufficient, but
I claim it's not necessary.

00:59:53.610 --> 00:59:56.280
So you might actually
have false hope

00:59:56.280 --> 01:00:02.130
if you are a Mirror
sportswriter, who

01:00:02.130 --> 01:00:04.160
only knows to add
up these numbers

01:00:04.160 --> 01:00:06.240
and compare it with others.

01:00:06.240 --> 01:00:09.370
But if you were a
6046 student, and you

01:00:09.370 --> 01:00:12.590
went through these wonderful
lectures on network flow,

01:00:12.590 --> 01:00:15.080
you know much more.

01:00:15.080 --> 01:00:21.530
And we're not quite there
yet in terms of requiring

01:00:21.530 --> 01:00:23.420
the power of network flow.

01:00:23.420 --> 01:00:32.360
But I ask, what if w5 were 47?

01:00:37.080 --> 01:00:42.120
Can you make an argument which
clearly doesn't work here,

01:00:42.120 --> 01:00:46.260
with this naive argument,
you have 75 equals 75.

01:00:46.260 --> 01:00:47.595
So you still have hope.

01:00:47.595 --> 01:00:51.900
But can you make an
argument that if w5 were 47,

01:00:51.900 --> 01:00:55.750
given the whole table-- that's
the hint, the whole table,

01:00:55.750 --> 01:00:59.170
that Detroit was
actually eliminated.

01:00:59.170 --> 01:01:01.630
Worth the Frisbee
for sure, right?

01:01:01.630 --> 01:01:02.130
Over there.

01:01:02.130 --> 01:01:03.436
I saw you first.

01:01:03.436 --> 01:01:05.792
AUDIENCE: It looks
like the only way

01:01:05.792 --> 01:01:09.400
that Detroit can get more
wins than New York is

01:01:09.400 --> 01:01:12.050
if they win all their games and
New York loses all their games?

01:01:12.050 --> 01:01:12.740
PROFESSOR: Yes.

01:01:12.740 --> 01:01:13.726
AUDIENCE: and if New York
loses all their games,

01:01:13.726 --> 01:01:15.698
they're going to lose
7 games to Boston

01:01:15.698 --> 01:01:19.650
And Boston will have 76 wins.

01:01:19.650 --> 01:01:21.190
PROFESSOR: Exactly right.

01:01:21.190 --> 01:01:22.315
So that's a great argument.

01:01:22.315 --> 01:01:25.850
All right, could you stand up?

01:01:25.850 --> 01:01:26.350
All right.

01:01:29.910 --> 01:01:32.980
You catch well too.

01:01:32.980 --> 01:01:37.490
So that's basically the
essence of the issue.

01:01:37.490 --> 01:01:40.210
There's stuff going
on which corresponds

01:01:40.210 --> 01:01:45.610
to teams playing each other
that can affect the outcome.

01:01:45.610 --> 01:01:48.480
OK so let me write out what
the gentleman just said.

01:01:52.320 --> 01:02:05.220
Still eliminated, boohoo,
because w5 plus r5

01:02:05.220 --> 01:02:06.940
equals 75 now.

01:02:06.940 --> 01:02:10.710
47 plus 28 is 75.

01:02:10.710 --> 01:02:19.420
But either New York
or Baltimore-- I

01:02:19.420 --> 01:02:23.650
mean there's many ways
you could show this.

01:02:23.650 --> 01:02:25.760
But we'll just
take-- this is not

01:02:25.760 --> 01:02:27.390
exactly what the gentleman said.

01:02:27.390 --> 01:02:31.130
But this is what I had here,
so let me just write that out.

01:02:31.130 --> 01:02:33.475
Either New York or
Baltimore will win 76 games.

01:02:36.700 --> 01:02:38.740
Since they play
each other 5 times.

01:02:46.780 --> 01:02:47.280
OK?

01:02:51.840 --> 01:02:58.630
So now I'm going to do some
more sophisticated analysis

01:02:58.630 --> 01:03:02.430
and I got what I wanted.

01:03:02.430 --> 01:03:10.780
But let's say that w5 was 48.

01:03:10.780 --> 01:03:14.330
So this was 76 and
it kind of worked out

01:03:14.330 --> 01:03:16.060
for both of the examples.

01:03:16.060 --> 01:03:22.000
The one with Boston and the 7
games, that I got as an answer.

01:03:22.000 --> 01:03:25.250
And this one that
I just wrote down.

01:03:25.250 --> 01:03:33.265
But what if w5 equals 48?

01:03:37.910 --> 01:03:41.340
Is Detroit eliminated?

01:03:41.340 --> 01:03:44.797
How many people think
Detroit is eliminated?

01:03:44.797 --> 01:03:45.880
All right, a bunch of you.

01:03:49.400 --> 01:03:51.800
You want to explain why?

01:03:51.800 --> 01:03:54.240
AUDIENCE: So New York,
Baltimore, and Boston

01:03:54.240 --> 01:03:57.690
are guaranteed to get a total
of at least 14 more wins.

01:03:57.690 --> 01:03:59.865
Because of their
total number of wins

01:03:59.865 --> 01:04:02.274
that they're allowed to have
without any of them getting

01:04:02.274 --> 01:04:04.202
to 77 is only going to be 13.

01:04:04.202 --> 01:04:06.612
So it will be 1 plus
5 so that's too many.

01:04:06.612 --> 01:04:08.540
But they're not
actually eliminated,

01:04:08.540 --> 01:04:10.903
because this is from the
Wild Card round anyway.

01:04:13.669 --> 01:04:16.840
PROFESSOR: I didn't like the
last part of your answer.

01:04:16.840 --> 01:04:19.610
Did, but the other question
is, did people understand

01:04:19.610 --> 01:04:22.760
the first part of his answer?

01:04:22.760 --> 01:04:23.820
That was complicated.

01:04:23.820 --> 01:04:25.780
I mean it was the
right analysis.

01:04:25.780 --> 01:04:27.430
Great job.

01:04:27.430 --> 01:04:30.300
So, we want a
systematic way so we

01:04:30.300 --> 01:04:33.940
can run an algorithm
so we need to bother

01:04:33.940 --> 01:04:37.040
with this type of analysis.

01:04:37.040 --> 01:04:39.450
And you're exactly right.

01:04:39.450 --> 01:04:42.940
It turns out that I
skipped this little detail,

01:04:42.940 --> 01:04:45.450
because it was
inconvenient, which

01:04:45.450 --> 01:04:48.870
is that in '95, a wild
card was introduced.

01:04:48.870 --> 01:04:51.930
So we could think about
this as elimination

01:04:51.930 --> 01:04:54.550
from being the division winner.

01:04:54.550 --> 01:04:56.300
So that's an aside.

01:04:56.300 --> 01:04:59.380
Thanks for pointing out,
it's important to know

01:04:59.380 --> 01:05:01.590
sports history.

01:05:01.590 --> 01:05:02.320
Really important.

01:05:02.320 --> 01:05:04.810
Almost as important as 046.

01:05:04.810 --> 01:05:11.060
But basically it gets
more and more complicated

01:05:11.060 --> 01:05:16.230
as you get closer and
closer to these edge cases.

01:05:16.230 --> 01:05:19.860
And so you can do an analysis,
and I'll actually describe

01:05:19.860 --> 01:05:22.620
roughly what your analysis was.

01:05:22.620 --> 01:05:23.340
What's your name?

01:05:23.340 --> 01:05:24.173
AUDIENCE: Alexander.

01:05:24.173 --> 01:05:26.930
PROFESSOR: Alexander's
analysis was.

01:05:26.930 --> 01:05:28.310
But I don't want to do that yet.

01:05:28.310 --> 01:05:30.030
I'll do that at the
end if you have time.

01:05:30.030 --> 01:05:32.460
I want to give you a general
purpose technique that

01:05:32.460 --> 01:05:36.140
obviously is going to use max
flow to solve this problem.

01:05:36.140 --> 01:05:36.901
All right?

01:05:36.901 --> 01:05:38.150
So we're going to set this up.

01:05:38.150 --> 01:05:40.680
And so that's actually
one of the nice things

01:05:40.680 --> 01:05:42.430
about maximum flow.

01:05:42.430 --> 01:05:43.380
How you can translate.

01:05:43.380 --> 01:05:47.260
You can sort of create
these networks from tables

01:05:47.260 --> 01:05:51.320
or what have you, and
add capacities to edges,

01:05:51.320 --> 01:05:54.280
find max flows, and
solve these problems.

01:05:54.280 --> 01:05:57.570
So this is a good
example of that.

01:05:57.570 --> 01:06:01.980
So let's go ahead and do that
I am going to draw a flow

01:06:01.980 --> 01:06:06.970
network based on this table.

01:06:06.970 --> 01:06:11.310
It's going to have a source
and a sync that are basically

01:06:11.310 --> 01:06:13.880
these dummy nodes that
are essentially going

01:06:13.880 --> 01:06:17.012
to source about infinite flow.

01:06:17.012 --> 01:06:18.470
But the capacities
of the edges are

01:06:18.470 --> 01:06:19.850
going to come from that table.

01:06:19.850 --> 01:06:23.690
And the edges themselves are
straightforward in the sense

01:06:23.690 --> 01:06:28.410
that the graph is going
to look the same based

01:06:28.410 --> 01:06:30.860
on the number of teams,
and the particular team

01:06:30.860 --> 01:06:32.300
that you're analyzing.

01:06:32.300 --> 01:06:34.880
So this graph is
trying to determine--

01:06:34.880 --> 01:06:46.178
this is a flow network
to determine if team

01:06:46.178 --> 01:06:50.455
5, Detroit, is eliminated.

01:06:53.800 --> 01:06:56.590
This is just the flow network.

01:06:56.590 --> 01:07:00.490
I'm going to have
a bunch of edges.

01:07:00.490 --> 01:07:06.870
I'm going to just draw this
once and we'll be moving things

01:07:06.870 --> 01:07:09.220
around on this.

01:07:09.220 --> 01:07:18.680
2 to 3, 1 to 4, 2 to
4, and lastly 3 to 4.

01:07:18.680 --> 01:07:21.230
And the reason you
don't see 5 here

01:07:21.230 --> 01:07:24.915
is because this is an
analysis for team 5.

01:07:33.950 --> 01:07:36.250
So the other 4
teams show up here.

01:07:36.250 --> 01:07:38.700
They have edges going to t.

01:07:38.700 --> 01:07:42.710
s is over here, and it's
got a bunch of edges

01:07:42.710 --> 01:07:44.600
going to all of these nodes.

01:07:44.600 --> 01:07:46.220
What are these pairs?

01:07:46.220 --> 01:07:50.940
As you can imagine, these
pairs correspond to the games

01:07:50.940 --> 01:07:55.220
that each of these teams
that are inside the circle

01:07:55.220 --> 01:07:56.710
play against each other.

01:07:56.710 --> 01:07:59.870
So 3 plays 4 a certain
number of times.

01:07:59.870 --> 01:08:01.930
According to that
table it's 4 times.

01:08:01.930 --> 01:08:04.140
So I'm going to put a 4 in here.

01:08:04.140 --> 01:08:05.420
This is 4 as well.

01:08:05.420 --> 01:08:11.940
4, 5, 7, 2.

01:08:11.940 --> 01:08:13.250
OK?

01:08:13.250 --> 01:08:16.254
And the edges in
between here are

01:08:16.254 --> 01:08:18.279
going to have
capacities of infinity

01:08:18.279 --> 01:08:22.240
and how are these
edges structured?

01:08:22.240 --> 01:08:25.310
So far I've just explained
how the left-hand side works.

01:08:25.310 --> 01:08:27.310
These edges have a
capacity of infinity.

01:08:27.310 --> 01:08:30.590
1 2 goes to 1, 1 2 goes to 2.

01:08:30.590 --> 01:08:34.149
1 3 goes to 1, 1 3 goes to 3.

01:08:34.149 --> 01:08:36.450
And that's pretty much it.

01:08:36.450 --> 01:08:39.700
So that's where these edges are.

01:08:39.700 --> 01:08:43.540
That's how these
edges are introduced.

01:08:43.540 --> 01:08:50.439
And all of these edges have
a capacity of infinity.

01:08:50.439 --> 01:09:00.529
2 to 4, 3 to 4, and that.

01:09:00.529 --> 01:09:03.229
So far, it's pretty
straightforward.

01:09:03.229 --> 01:09:05.970
There's one last thing
that we need to do,

01:09:05.970 --> 01:09:09.939
which is add capacities
to these edges.

01:09:09.939 --> 01:09:11.670
This is actually crucial.

01:09:11.670 --> 01:09:14.432
It turns out that we have
to add capacities such

01:09:14.432 --> 01:09:17.490
that this max flow is going
to represent elimination.

01:09:17.490 --> 01:09:19.490
So these capacities
have to be chosen.

01:09:19.490 --> 01:09:22.910
And in particular
the equation for this

01:09:22.910 --> 01:09:28.700
is simply w5 plus r5 minus w1.

01:09:31.640 --> 01:09:35.310
In this case if it's 48, I'm
asking the question for 48

01:09:35.310 --> 01:09:40.939
for w5 plus 28 minus 75.

01:09:40.939 --> 01:09:43.979
So that's 1.

01:09:43.979 --> 01:09:48.920
And then this thing
here would be w5.

01:09:48.920 --> 01:09:54.090
I'll just write that out. w5
plus r5 minus w2, hopefully

01:09:54.090 --> 01:09:57.440
you can read that, and that's
a 5 and then similarly this

01:09:57.440 --> 01:10:00.520
is a 7, and that's 13.

01:10:00.520 --> 01:10:03.280
And you might ask, what
does this represent?

01:10:03.280 --> 01:10:04.940
Well it kind of
represents what you

01:10:04.940 --> 01:10:07.510
think it represents if you
just look at that equation.

01:10:07.510 --> 01:10:14.430
It says how much can
I push through here

01:10:14.430 --> 01:10:18.780
before I get greater than w1.

01:10:18.780 --> 01:10:19.780
So what is the capacity?

01:10:19.780 --> 01:10:20.905
What's the difference here?

01:10:23.680 --> 01:10:30.980
if I have a 1 here, that means
that I still see w5 plus r5,

01:10:30.980 --> 01:10:32.930
I'm 1 greater than w1.

01:10:32.930 --> 01:10:36.190
So in particular, if I
want to write this out,

01:10:36.190 --> 01:10:47.430
the capacities are the
number of games team

01:10:47.430 --> 01:11:05.730
i can win, and not have
more wins than team 5.

01:11:10.970 --> 01:11:15.770
So this is a
statement about team 1

01:11:15.770 --> 01:11:18.090
when I see that over there.

01:11:18.090 --> 01:11:19.960
Our team 2 for the other edge.

01:11:19.960 --> 01:11:23.320
And I'm just trying
to figure out

01:11:23.320 --> 01:11:29.540
how much can I allow, with
respect to this other team,

01:11:29.540 --> 01:11:32.080
to determine elimination
for the particular team

01:11:32.080 --> 01:11:34.430
that I'm looking
at, team 5 or not.

01:11:34.430 --> 01:11:41.740
So in particular,
let's say that team i

01:11:41.740 --> 01:11:44.120
wins one particular game.

01:11:44.120 --> 01:11:46.850
So it goes up to
it goes up to 76.

01:11:46.850 --> 01:11:53.850
And if it goes up to 76,
it does not necessarily

01:11:53.850 --> 01:11:56.910
have more wins then
team 5, because team 5

01:11:56.910 --> 01:11:59.000
can go up to 76 as well.

01:11:59.000 --> 01:12:00.940
That's pretty much
it, and the same thing

01:12:00.940 --> 01:12:03.950
for team 2, team 3, and team 4.

01:12:07.410 --> 01:12:15.040
So the intuition here
is the following.

01:12:15.040 --> 01:12:19.670
So what we're going to do
is compute the max flow

01:12:19.670 --> 01:12:22.280
and look for a certain
property on that network.

01:12:22.280 --> 01:12:25.210
And that's going to tell
us if team 5 is eliminated

01:12:25.210 --> 01:12:29.180
or not for the particular
choice of wi of 48,

01:12:29.180 --> 01:12:32.560
because that's how I constructed
this particular example.

01:12:32.560 --> 01:12:45.530
So the intuition is, assume
team 5 wins all remaining games.

01:12:45.530 --> 01:12:47.890
That makes perfect sense.

01:12:47.890 --> 01:12:49.200
This is the best case scenario.

01:12:49.200 --> 01:12:51.830
You're trying to figure out,
in the best case scenario,

01:12:51.830 --> 01:12:53.340
are you eliminated or not?

01:12:53.340 --> 01:12:55.520
Is team 5 eliminated or not?

01:12:55.520 --> 01:13:00.620
And what you want to do is
divvy up the remaining games.

01:13:03.440 --> 01:13:07.830
And that corresponds to sending
flow through these edges.

01:13:07.830 --> 01:13:12.780
So flow through these edges
corresponds to assigning wins

01:13:12.780 --> 01:13:16.440
to teams 1, 2, 3, and 4.

01:13:16.440 --> 01:13:18.190
That's the key.

01:13:18.190 --> 01:13:28.330
Divvy up the remaining games,
so all teams have less than

01:13:28.330 --> 01:13:32.440
or equal to w5 plus r5 wins.

01:13:32.440 --> 01:13:35.030
So what does it mean if
all teams have less than

01:13:35.030 --> 01:13:37.620
or equal to w5 plus r5 wins?

01:13:37.620 --> 01:13:39.620
If you can divvied up
such that that's the case,

01:13:39.620 --> 01:13:44.080
what does that mean for team 5?

01:13:44.080 --> 01:13:45.650
Team five is still in the game.

01:13:45.650 --> 01:13:48.250
Team 5 is not eliminated.

01:13:48.250 --> 01:14:01.850
So if you can do this, then
team 5 is not eliminated

01:14:01.850 --> 01:14:03.910
If you can't team
5 is eliminated.

01:14:03.910 --> 01:14:11.760
Because some team is
going to get 77 wins.

01:14:11.760 --> 01:14:14.400
And so that analysis that
Alexander carried out

01:14:14.400 --> 01:14:16.240
wasn't a flow analysis.

01:14:16.240 --> 01:14:18.732
But somehow, that's
got to be in here.

01:14:18.732 --> 01:14:20.190
That type of analysis
has got to be

01:14:20.190 --> 01:14:24.210
in here to show that some team
is going to get to 77 wins.

01:14:24.210 --> 01:14:28.440
Because you know, well
thanks to Alexander,

01:14:28.440 --> 01:14:31.870
that Detroit is going
to be eliminated here.

01:14:31.870 --> 01:14:35.470
But we want to show that
in terms of the max flow

01:14:35.470 --> 01:14:37.010
on this network.

01:14:39.730 --> 01:14:50.586
So there is an associated
theorem that I'll write.

01:14:50.586 --> 01:14:52.400
We've got enough time here.

01:14:52.400 --> 01:14:55.710
We just have to find
the max flow for this.

01:14:55.710 --> 01:14:58.340
And that should
take just a minute.

01:14:58.340 --> 01:15:05.040
But I want to set it up so
that's pretty much the end.

01:15:05.040 --> 01:15:08.400
Finding the max flow,
what that means.

01:15:08.400 --> 01:15:10.490
But the theorem is,
which is essentially

01:15:10.490 --> 01:15:15.210
a more precise restatement
of this observation,

01:15:15.210 --> 01:15:21.170
is that team 5-- obviously
you can do this for any team--

01:15:21.170 --> 01:15:22.020
is eliminated.

01:15:24.590 --> 01:15:37.760
If and only if the max
flow does not saturate

01:15:37.760 --> 01:15:42.705
all edges leaving the source.

01:15:46.394 --> 01:15:56.090
I.e. in this case, the max
flow is strictly less than 26,

01:15:56.090 --> 01:15:58.470
which corresponds to
the sum of those things.

01:15:58.470 --> 01:16:01.310
5 plus 7 plus,
yadda yadda yadda.

01:16:01.310 --> 01:16:05.490
And so what does it mean to
saturate all of the edges?

01:16:05.490 --> 01:16:07.970
It means that all the
games have been played.

01:16:07.970 --> 01:16:12.330
All the relevant games here that
affect team 5 have been played.

01:16:12.330 --> 01:16:14.350
So you want to
play all the games,

01:16:14.350 --> 01:16:18.600
because that's the
end game, if you will.

01:16:18.600 --> 01:16:21.980
But basically what
that intuition here

01:16:21.980 --> 01:16:26.060
is that saturation
of those edges

01:16:26.060 --> 01:16:34.005
corresponds to playing
all remaining games.

01:16:39.930 --> 01:16:48.380
And the argument is
not a proof if you

01:16:48.380 --> 01:17:01.690
can't play all the
remaining games,

01:17:01.690 --> 01:17:15.660
without exceeding the
capacity of i to t edges,

01:17:15.660 --> 01:17:16.535
team 5 is eliminated.

01:17:25.040 --> 01:17:29.400
So you have to saturate, because
you have to play all the games.

01:17:29.400 --> 01:17:33.140
And when you do that saturation,
obviously your flow network

01:17:33.140 --> 01:17:37.350
has to satisfy its laws
corresponding to the capacity

01:17:37.350 --> 01:17:38.750
that we have in there.

01:17:38.750 --> 01:17:42.040
And it's quite
possible that this

01:17:42.040 --> 01:17:46.420
is going to cause a restriction
over here that corresponds

01:17:46.420 --> 01:17:49.240
to requiring that
not all the games

01:17:49.240 --> 01:17:54.850
we play in order for
Detroit to survive.

01:17:54.850 --> 01:17:56.660
and that's basically the game.

01:17:56.660 --> 01:17:59.880
So saturating the network
means playing all the games.

01:17:59.880 --> 01:18:05.000
And if you get a max
flow, to be more concrete,

01:18:05.000 --> 01:18:12.380
if you can get a max flow
here that's less than 26,

01:18:12.380 --> 01:18:16.260
then you've saturated
all the edges

01:18:16.260 --> 01:18:21.260
and this means that you
played all the games.

01:18:21.260 --> 01:18:26.550
And you found this
team that beats you.

01:18:26.550 --> 01:18:28.960
That beats team 5.

01:18:28.960 --> 01:18:33.950
So what happens over here?

01:18:33.950 --> 01:18:38.510
Let's take a look at this
and all you have to do here,

01:18:38.510 --> 01:18:41.180
is to find the max
flow and then we

01:18:41.180 --> 01:18:43.590
can go back to the
statement of the theorem.

01:18:43.590 --> 01:18:48.680
So I want to find the
min cut corresponding

01:18:48.680 --> 01:18:51.470
to this, which is going to
tell me what the max flow is.

01:18:51.470 --> 01:18:54.450
I could go around for focus
in on this, and start with 0

01:18:54.450 --> 01:18:56.530
and augment, and
so on and so forth.

01:18:56.530 --> 01:18:59.850
But there's another way which
is not necessarily algorithmic,

01:18:59.850 --> 01:19:01.330
but eyeballs.

01:19:01.330 --> 01:19:08.140
Use your eyeballs and
find me the min cut here.

01:19:08.140 --> 01:19:11.430
Which has the minimum capacity.

01:19:11.430 --> 01:19:13.700
The min cut, the
minimum capacity

01:19:13.700 --> 01:19:16.720
is going to tell me
what my max flow is.

01:19:16.720 --> 01:19:19.880
We know that from the
max-flow min-cut theorem.

01:19:19.880 --> 01:19:25.680
So I want to cut this network
into capital S and capital T

01:19:25.680 --> 01:19:30.680
and find the max flow.

01:19:30.680 --> 01:19:33.210
So any ideas?

01:19:33.210 --> 01:19:35.110
It's a little hard.

01:19:35.110 --> 01:19:38.910
So I see some people
waving their hands.

01:19:38.910 --> 01:19:40.670
All right, let me just
do that since we're

01:19:40.670 --> 01:19:41.630
running out of time.

01:19:41.630 --> 01:19:43.270
It's kind of cool.

01:19:43.270 --> 01:19:45.150
So I'm going to use
a different color.

01:19:45.150 --> 01:19:48.980
We're going to go
like that, yeah.

01:19:48.980 --> 01:19:57.060
and then veer over this way,
and then jump up like that,

01:19:57.060 --> 01:20:00.510
and then come over like that.

01:20:00.510 --> 01:20:09.450
And this is S. And
on this side is T.

01:20:09.450 --> 01:20:14.330
So I got small s and capital
S, small t and cap T.

01:20:14.330 --> 01:20:18.250
But I got this 4 in cap T.
Notice that I got 3 to 4,

01:20:18.250 --> 01:20:24.500
2 to 4, 1 to 4 in cap T and I
got these other ones in cap S.

01:20:24.500 --> 01:20:27.250
So all I've done here,
forget baseball elimination,

01:20:27.250 --> 01:20:29.160
is find a min-cut.

01:20:29.160 --> 01:20:33.740
And if you look at what the
value of this min-cut is,

01:20:33.740 --> 01:20:37.075
the value of the
min-cut is simply

01:20:37.075 --> 01:20:42.330
the capacities of the
edges that go from S to T.

01:20:42.330 --> 01:20:47.650
So it's simply the S
to T edges and summing

01:20:47.650 --> 01:20:49.030
over the capacities.

01:20:49.030 --> 01:20:50.995
And if you take a
look, obviously S to T,

01:20:50.995 --> 01:20:52.810
you've got 4 plus 4 plus 4.

01:20:52.810 --> 01:20:56.020
So you've got 4 plus 4
plus 4, corresponding

01:20:56.020 --> 01:20:59.860
to this one, this
one, and that one.

01:20:59.860 --> 01:21:02.630
Do I need to add
this edge in here?

01:21:02.630 --> 01:21:03.130
Over here?

01:21:07.050 --> 01:21:08.290
Where does this edge go?

01:21:08.290 --> 01:21:10.540
From this to over there?

01:21:10.540 --> 01:21:12.824
That goes from T to
S. So that's good

01:21:12.824 --> 01:21:14.490
because that has a
capacity of infinity.

01:21:14.490 --> 01:21:16.680
That would cause trouble.

01:21:16.680 --> 01:21:21.580
And so the other edges
are I got 1, 5, and 7.

01:21:21.580 --> 01:21:25.680
So I need this one,
that one, and that one.

01:21:25.680 --> 01:21:33.400
So 4 plus 4 plus 4, plus
1 plus 5 plus 7 equals 25.

01:21:33.400 --> 01:21:41.350
And so this implies elimination,
because what I've done

01:21:41.350 --> 01:21:46.050
is found the max flow, which
is strictly less than 26.

01:21:46.050 --> 01:21:49.280
Which means that I have not
saturated all of the edges that

01:21:49.280 --> 01:21:50.540
come out of s.

01:21:50.540 --> 01:21:53.730
I have not been able
to play all the games.

01:21:53.730 --> 01:21:55.860
And so if I'd played
all the games,

01:21:55.860 --> 01:21:59.100
I'd be exceeding some
capacity constraint.

01:21:59.100 --> 01:22:00.800
And if I push more
flow in there,

01:22:00.800 --> 01:22:03.830
I'd be exceeding some capacity
constraint on this side.

01:22:03.830 --> 01:22:07.880
And that would
imply that Detroit

01:22:07.880 --> 01:22:11.190
is eliminated, because some
team would have 77 wins.

01:22:11.190 --> 01:22:16.140
All right, so hopefully
you've got the gist of it.

01:22:16.140 --> 01:22:17.510
I'll put this in the notes.

01:22:17.510 --> 01:22:19.510
Take a long, hard look at it.

01:22:19.510 --> 01:22:22.340
That particular values
aren't important.

01:22:22.340 --> 01:22:25.100
The framework of
translation is important.

01:22:25.100 --> 01:22:28.780
And you can certainly
ask questions of your TAs

01:22:28.780 --> 01:22:29.330
on Friday.

01:22:29.330 --> 01:22:32.220
And I'll stick around
here for questions.

