Speaker 1:          00:00          In the sixth century BC, Pythagoras, Pythagoras, propose a concept called the music of the spheres to describe the proportional movements of celestial bodies like the sun, moon, and planets. This music is not thought of as being literally audible, but instead a mathematical concept. Math and music are intrinsically connected. The field of algorithmic composition dates back to the early days of computer science. Translation models take an existing nonmusical medium like a picture and translated into a sound. These are usually rule based, so a rule may be that if it sees a horizontal line and an image, it will interpret that as a constant pitch. Whereas a vertical line represents an ascending scale. Evolutionary models are based off of genetic algorithms through mutation and natural selection. Different solutions evolve towards a suitable composition. Then there's the learning models. By providing it musical data instead of rules, we can let it learn for itself how to create music.

Speaker 1:          01:01          We're fast approaching the point where we'll no longer have to wonder how Mozart or Jimi Hendrix would have composed a piece on a certain topic. We'll just be able to ask there, algorithmic counterparts ourselves. Hello world. It's Saroj and today we're going to use deep learning to generate some jazz music. The first attempt by anyone to use a computer to compose music was by two American professors at the University of Illinois, Urbana, champagne, hiller and Isaacson. They program the university's ILIAC computer to compose music and it generated pitches using random numbers before testing them according to the rules of classical counterpoint. So if a pitch didn't it apiece, another note was generated. It also relied on probabilities via a Markov chain. It use the past to determine the probabilities of the future. The first piece was completed in 1957 and was called the ILIAC suite for string quartet.

Speaker 2:          02:02          Hmm.

Speaker 1:          02:03          Although it made headlines in scientific American, the musical establishment was pretty hostile to them. They thought it undermined human creativity and didn't include them in their journals until after Hitler's death. Nowadays, there are a ton of amazing generative programs for composers to aid them when they compose music like iTunes. Let understand this process by building a model to generate jazz pieces using care Os. Well, first examine the music. We're going to train our model on our input data is going to be a collection of piano pieces by American jazz musicians. Pat Metheny in mid I format mid thigh or musical instrument. Digital interface is like the digital alphabet for music. It contains a list of messages that tell an electronic device like a sound card, how to generate a certain sound so it doesn't store actual sound itself, which lens to a much smaller file. Since these messages are a sequence, we'll use a recurrent network as our sequence learning model.

Speaker 1:          02:59          For each [inaudible] file. We'll extract the stream of notes for both the melody and the harmony. The harmony's cords accompany the melodies, single notes. Then we'll group them all together by measure number. So each measure has its own grouping of chords and this measure cord pair is what we'll call our abstract grammars. We'll vectorize these inputs by converting them into binary matrices so we can feed them into our model. Now we can build our model. This is going to be a double stack to LSTM network, so our computation graph will look like this. The vectorized sequence of notes will be input into the first LSTM cell. Then we'll apply dropout to help ensure that the model generalized well and we'll do that process one more time. Then we'll feed the data to our last fully connected layer, labeled dense. Since every neuron in the previous layer is connecting to every neuron in this layer, it will mix all the learned signals together.

Speaker 1:          03:52          So our prediction is truly based on the whole input sequence will lastly transform the result with a softmax activation function into an output probability for what is likely to next note in the sequence when we build our first LSTM layer, by default, it will only return the last spectrum rather than the entire sequence. So we set return sequences to true so that it returns the entire sequence, which is necessary to be able to stack another LSTM later on using to LSTM layers instead of one allows for a more complex feature representation of the input, which means more generalization ability and thus a better prediction. Recall that recurrent networks are essentially like a series of feedforward networks that are connected to each other. The output of each and it's hidden layer is fed into the next one and when we back propagate with each layer, the magnitude of the gradients gets exponentially smaller, which makes the steps also very small, which results in a very slow learning rate of the weights in the lower layers of a deep network. This is the vanishing gradient problem and Lstm recurrent notes help solve that by preserving the error that can be backed propagated through time and layers. Let's look a little closer at this process, but first I got to say

Speaker 3:          05:04          again. S remembers the good stuff that outputs the breast.

Speaker 1:          05:15          An LSTM cell consists of three gates. The input, forget and output as well as a cell state. The cell state is like a conveyor belt. It just lets memory flow across unchanged except for a few minor linear interactions. These interactions are the gates we can add or remove memory from the cell state regulated by them. They optionally let memory through each is a sigmoid neural net player and a multiplication operation. The sigmoid outputs a value between zero and one which describes how much of each component should be let through, will represent each of the gates with the following equations where w is the set of weights at each gate. The way it's internal memory changes is similar to piping water through a pipe, so think of memory as water. It flows into a pipe. If we want to change the memory flow, this change is controlled by two valves, the forget now first if we shut it, no old memory will be kept if we keep it open, all old memory will pass through.

Speaker 1:          06:11          The other is the new memory valve. New memory comes in through a t shaped joint and merges with the old memory and the amount of new memory that comes in is controlled by this valve. The input is an old memory and it passes through the forget now, which is actually a multiplication operation. The old memory hits the t shape joint pipe, which represents a summation operation, new and old memory merge through this operation. In total, this updates the old memory to the new memory. We'll define our loss function as categorical cross entropy. The Cross entropy between two probability distributions measure the average number of bits needed to identify an event from a set of possibilities. Since our data is fed in sequences, this measures the difference between the real next note and our predicted next note. Well, minimize this loss function using rms prop, which is an implementation of stochastic gradient descent. So we'll predict the next note in the sequence over and over again until we have a sequence of generated notes will translate this into mid thigh format and write it to a file so we can listen to it. Let's hear what this sounds like. It

Speaker 1:          07:25          at least it's better than Kenny G so we're all good. So to break it down, we can generate music using l s t m networks to predict sequences of notes. LSTM consists of three gates. The input for get an output, Kate, and we can think of these gates as valves controlling how memory is stored in our network to eliminate banishing gradients. The winner of the coding challenge from the last video is Michelle Batu. Not only did he performed multiple style transfer, but he took it a step further by applying it to video as well. Wizard of the week, and the runner up is Michael Palka. He successfully performed multicell transferred through a clever matrix operation. The coding challenge for this video is to generate a music clip for a genre that you choose. Remember, it's just a sequence of mid I messages posts. You're gambling in the comments and I'll announce the winner. Next video, please subscribe for more programming videos. Check out this related video. And for now I've got to memorize memory cells, so thanks for watching.