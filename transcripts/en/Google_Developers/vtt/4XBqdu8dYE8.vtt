WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:06.000
&gt;&gt; BLAIN: Hello, everybody. I'm Matthew Blain
and I'm a member of the Google App Engine

00:00:06.000 --> 00:00:10.220
team, I'm one of the engineers on it and I'm
here to talk to you today about the Bulkloader.

00:00:10.220 --> 00:00:16.420
The title of this talk is Data Migration with
App Engine. So, you can follow along with

00:00:16.420 --> 00:00:22.400
the presentation, we'll be taking notes on
Google Wave and you can also find sample code

00:00:22.400 --> 00:00:27.289
which I'd be referencing throughout the presentation
on bulkloadersample.appspot.com where you

00:00:27.289 --> 00:00:33.120
can look at the wave at bit.ly/appengine4.
So, I'm going to give you a bit of background

00:00:33.120 --> 00:00:37.820
about the Bulkloader and what you need to
know to understand the Bulkloader. Tell you

00:00:37.820 --> 00:00:41.950
a bit about how the Bulkloader works. To show
you how to create your own configurations

00:00:41.950 --> 00:00:45.700
for the Bulkloader, talk a bit about some
of the advance features you'd use to create

00:00:45.700 --> 00:00:49.820
sort of more complicated configurations and
tell you a bit about troubleshooting and the

00:00:49.820 --> 00:00:57.510
feature directions for the Bulkloader. All
right, so what is the Bulkloader? How many

00:00:57.510 --> 00:01:02.219
of you actually use the Bulkloader in App
Engine? All right, a good number of you, that's

00:01:02.219 --> 00:01:08.361
good. So, the Bulkloader is a tool for datastore
import and export in App Engine. So, you can

00:01:08.361 --> 00:01:13.040
use it to import data set in to App Engine
from like CSV file and external database or

00:01:13.040 --> 00:01:17.960
something. You can export out of App Engine
similarly to a CSV file or external database.

00:01:17.960 --> 00:01:22.340
You can also use it to migrate data between
application and since it--so, for example,

00:01:22.340 --> 00:01:26.610
people have demonstrated use like a QA server
and a production server, if you'd setup to

00:01:26.610 --> 00:01:30.530
QA, you know, move it for your production
server. You can also move between dev app

00:01:30.530 --> 00:01:34.689
server and production which is also particular
use from QA. Even though the title of this

00:01:34.689 --> 00:01:39.240
talk is Data Migration in App Engine, it's
not really about schema migration. I suppose

00:01:39.240 --> 00:01:43.970
you could use the Bulkloader for by downloading
all of your data. If one schema then uploading

00:01:43.970 --> 00:01:47.299
it all to a slightly different schema to a
different kind, it's not meant for that, it

00:01:47.299 --> 00:01:52.420
could work for it. So, again I'm going to
show you how to configure and use the Bulkloader.

00:01:52.420 --> 00:01:57.640
So, just a bit of background of, I'm guessing
everyone haven't used the Bulkloader has used

00:01:57.640 --> 00:02:01.810
the App Engine datastore but how many of you
are familiar with App Engine datastore? Well,

00:02:01.810 --> 00:02:09.289
I guess, that's good. But just a bit of review,
basically you have schemeless tables of entities.

00:02:09.289 --> 00:02:16.769
Every sort of table in relational terms is
one kind of entity called an App Engine. Every

00:02:16.769 --> 00:02:20.559
entity is a single key which is the identity
of the entities. You can fetch an entity by

00:02:20.559 --> 00:02:24.889
keys which is also indexes which you can use
to sort of--well, if you want to do other

00:02:24.889 --> 00:02:29.889
queries, more interesting than just get me
the key X. And keys are not hierarchical but

00:02:29.889 --> 00:02:33.420
they're actually they're--they're kind of
for entity groups, this will become important

00:02:33.420 --> 00:02:37.290
later, you can have apparent keys on your
keys and properties are types. So there's

00:02:37.290 --> 00:02:42.419
no schema on a kind but typically you have
a bunch of different property--a bunch of

00:02:42.419 --> 00:02:47.980
different properties on it and they have particular
types but not all strings. In Python, you

00:02:47.980 --> 00:02:54.689
wrap this model class, in Java you can wrap
this with JDO or JPA or whatever annotations

00:02:54.689 --> 00:03:01.040
and there's other wrappers for other languages
which people have running on the JVM and App

00:03:01.040 --> 00:03:06.760
Engine. So, the fundamentals of what the Bulkloader
does is it takes your data from your external

00:03:06.760 --> 00:03:10.659
source, converts them all to the datastore
entities, the type to use an App Engine and

00:03:10.659 --> 00:03:15.579
it just shifts it over to the datastore and
HTTP. One thing is sort of important to note

00:03:15.579 --> 00:03:18.790
is that the Bulkloader is actually running
on your client work station. This is how we

00:03:18.790 --> 00:03:22.819
get around things like 30-second limits and
small amounts of upload is that we do the

00:03:22.819 --> 00:03:27.920
primary work on your local works station then
to shift the data of to App Engine. And just

00:03:27.920 --> 00:03:32.620
a bit of background, you all have seen an
App Engine after run it, you can run App Server

00:03:32.620 --> 00:03:37.879
or on production. One of the frequently ask
questions people have is, does it only work

00:03:37.879 --> 00:03:44.169
on Python or do you have a Java version available?
And the answer is that the server runs either

00:03:44.169 --> 00:03:47.950
on Python or on Java. The client, however,
we only have one client right now, it's implemented

00:03:47.950 --> 00:03:53.579
in Python. [INDISTINCT] Python 134SDK particularly
for the new configuration format I'm talking

00:03:53.579 --> 00:03:58.129
about today. You'll need the newest version
of SDK and you need Python to run it, don't

00:03:58.129 --> 00:04:04.049
need anything else. You don't need any particular
IDs or anything like that. All right. So,

00:04:04.049 --> 00:04:08.780
how does the Bulkloader work? And just a quick
demo. Actually, before we even get to that,

00:04:08.780 --> 00:04:14.669
I sort of remind people about zero configuration,
its features been out for a little bit but

00:04:14.669 --> 00:04:18.550
as I mentioned particular if you're migrating
data from one instance to another or you just

00:04:18.550 --> 00:04:23.220
take a daily back up or something, for example,
if you're--it's a good thing to do before

00:04:23.220 --> 00:04:27.250
you do schema migration, you backup all of
your data incase something goes horribly wrong.

00:04:27.250 --> 00:04:32.009
You can just dump your data from App Engine
to your local work station. One thing which

00:04:32.009 --> 00:04:36.870
is it somewhat new is that this is a fully
integrated App CFG right now, you can also

00:04:36.870 --> 00:04:41.289
use the bulkloader but basically you just
run this command App CFG download data pointer

00:04:41.289 --> 00:04:48.560
application, point for local file name, runs
and the inverse upload data. All right. So,

00:04:48.560 --> 00:04:55.050
I'm just going to do quick demo using the
new configuration format which came out in

00:04:55.050 --> 00:05:15.740
134. All right, so, my wonderful demo system--all
right. For a second--so, I'm going to be demoing

00:05:15.740 --> 00:05:21.509
with the guestbook application. This is the
same application that you see with either

00:05:21.509 --> 00:05:29.569
of getting starter guide for Java or for Python.
So, I have just some very simple sample data,

00:05:29.569 --> 00:05:33.550
the guestbook has just comments, data comments
are written, the author that--who wrote the

00:05:33.550 --> 00:05:37.700
comments. So, I'm going to upload the data,
all right. Upload the data command, it's got

00:05:37.700 --> 00:05:42.409
lots of command and arguments but, basically,
it says here is the file, here is the configuration,

00:05:42.409 --> 00:05:47.909
here is the destination. And if we actually
look at our guestbook now, it has the same

00:05:47.909 --> 00:05:58.389
exact--same comments we just uploaded, I could
say something else. Hello, IO, good afternoon.

00:05:58.389 --> 00:06:06.930
And we can also download them, pretty much
the same way. All right, so, here's the downloaded

00:06:06.930 --> 00:06:21.280
data. You should see, hello, IO, good afternoon,
right their. So how do that work? So, a simple

00:06:21.280 --> 00:06:25.139
data we've got the model, this is the model
many of you familiar with. We've gone through

00:06:25.139 --> 00:06:28.240
the sample and this is the same thing in Java.
Right there is actually equivalent applications.

00:06:28.240 --> 00:06:34.759
I happen to be running the Python one here
today. So how did it work? So, this is described,

00:06:34.759 --> 00:06:37.210
this is the new configuration format, this
is probably the first time that many of you

00:06:37.210 --> 00:06:42.699
have seen the configuration format. It's designed
to be declarative so that if your Java developer

00:06:42.699 --> 00:06:46.419
or some other language developer not necessarily
Python developer, you don't actually need

00:06:46.419 --> 00:06:50.569
to understand Python. It's still somewhat
based on Python but the fundamentals are designed

00:06:50.569 --> 00:06:55.910
to be just basically declarative. So it described
that we have a kind greeting that it came

00:06:55.910 --> 00:07:02.039
from a CSV file and it has three properties.
Basically, the first property is a string,

00:07:02.039 --> 00:07:06.860
the second property is a user property, third
property is a date. Dates are a little bit

00:07:06.860 --> 00:07:10.340
more complicated because we have to define
the formats the date is in because CSV files

00:07:10.340 --> 00:07:17.210
are strings and dates are internally are all
set for an app or something like that. So

00:07:17.210 --> 00:07:21.680
the way it works is that the Bulkloader is
a three phase process. There's sort of there's

00:07:21.680 --> 00:07:25.470
three transitions. This is going to--this
is the first two of the transitions. So on

00:07:25.470 --> 00:07:32.270
import, we have our data CSV file, you see
it's cool stuff, comma date, comma some author.

00:07:32.270 --> 00:07:37.340
What we have is called the connector for the
first phase. So the connector takes the data

00:07:37.340 --> 00:07:41.090
from the external format and in this case,
CSV and converts it until they call neutral

00:07:41.090 --> 00:07:47.330
dictionary. The dictionary is just dictionary
of key value pairs representing each row basically

00:07:47.330 --> 00:07:51.919
in your input data. The reason for this intermediate
format is that we can have different connectors

00:07:51.919 --> 00:07:56.469
not necessarily CSV but any format can happen
into internal dictionary to pass onto the

00:07:56.469 --> 00:08:02.229
next stage which is basically what the property
map describe us to do or retransform this

00:08:02.229 --> 00:08:06.880
dictionary in to a datastore entity. So as
you can see that the property--the amo file

00:08:06.880 --> 00:08:12.930
who describe that content had no transform,
the date had this transformed with this strings

00:08:12.930 --> 00:08:17.830
specifying the date format and the user just
specified the user property. So it passed

00:08:17.830 --> 00:08:23.470
into these three arguments and then we basically
got entity which deserve Python representation

00:08:23.470 --> 00:08:30.000
of the type data, again, the string, a date
and a user property. The next phase as I mentioned

00:08:30.000 --> 00:08:33.870
earlier, we found this on a client work station
and now we're going to upload it to the server.

00:08:33.870 --> 00:08:38.951
So, this uses a remote_API as many group are
familiar with this but basically every single--every

00:08:38.951 --> 00:08:45.990
single API called an App Engine effect is
an RPC. So, what remote_API does is we're

00:08:45.990 --> 00:08:50.120
actually running basically the full App Engines
stacks similar to a dev App server on your

00:08:50.120 --> 00:08:55.329
local client workstation and the data start
calls remoted using remote_API to the server.

00:08:55.329 --> 00:08:59.339
The other interesting thing to know about
the upload part and this whole process is

00:08:59.339 --> 00:09:03.029
that it's actually done in parallel. We run
mutltiple threads, the defaults four threads

00:09:03.029 --> 00:09:09.420
like a four clouds here. An upload because
it has all the data locally and is pushing

00:09:09.420 --> 00:09:15.320
to the datastore with identified keys, well
let that in a minute. It does the full conversion

00:09:15.320 --> 00:09:19.279
process and parallel on download because you
want to order of your output file to be the

00:09:19.279 --> 00:09:23.920
order of your original data. They actually
downloads all the data locally and then runs

00:09:23.920 --> 00:09:30.790
over this transform step after it's done downloading
all the data. The other piece you need to

00:09:30.790 --> 00:09:34.790
enable remote_API is sort of a quick reminder.
You can see this in documentation. I'm trying

00:09:34.790 --> 00:09:39.910
to hear it partly because the most common
error that people make when they turn on remote_API

00:09:39.910 --> 00:09:43.930
is that they have some sort of catch all handler
in our application and they promote remote_API

00:09:43.930 --> 00:09:49.380
after it. So, the simplest advice just put
it first thing in your app.amo or first thing

00:09:49.380 --> 00:09:51.440
in your web.xml, this is the equivalent--the
exact the same equivalent in Java for handling

00:09:51.440 --> 00:10:00.340
the exact same case. All right, so how do
you create these configurations on your own?

00:10:00.340 --> 00:10:05.730
The most important thing you'll get out of
this talk today. So, it's a big wall of text

00:10:05.730 --> 00:10:09.210
I just showed you this sort of the key pieces
of the configuration for the guestbook but

00:10:09.210 --> 00:10:14.640
if you provided yourself, we've actually provided
a wizard to create it. So, anyone, have ideas

00:10:14.640 --> 00:10:19.439
on how it gets--how it gets the data that
actually create those test?

00:10:19.439 --> 00:10:23.480
&gt;&gt; Stats.
&gt;&gt; BLAIN: Stats, all right, you got it. So,

00:10:23.480 --> 00:10:26.670
the data source are just---are features that's
have been at App Engine for a while. If you

00:10:26.670 --> 00:10:31.110
seen them in the--in your admin console, it
says, "Here's how much data you have of this

00:10:31.110 --> 00:10:35.851
kind. How many, how much data you have of
this property, of this type of property?"

00:10:35.851 --> 00:10:41.529
The datastore statistics are actually themselves
stored in your datastore. So, if you're interested

00:10:41.529 --> 00:10:45.360
in seeing sort of slightly more complexed
Bulkloader configuration, you can actually

00:10:45.360 --> 00:10:50.280
look at how the Bulkloader creates the Bulkloder
configuration. If you don't care, you can

00:10:50.280 --> 00:10:53.980
just use it. Don't worry about it. But basically
trade Bulkloader config itself actually uses

00:10:53.980 --> 00:11:02.519
the Bulkloader to download your data. So,
again, just a quick demo, this is the command

00:11:02.519 --> 00:11:08.660
line app CFG create Bulkloader config. One
of the disadvantages of using datastore stats

00:11:08.660 --> 00:11:12.269
is that they're only available on production
and they take up to a day to generate. So,

00:11:12.269 --> 00:11:16.790
if you modified your schema very recently,
you basically have to wait a day to use a

00:11:16.790 --> 00:11:20.089
Bulkloader; you just have to fall back to
editing, follow yourself I guess. So, here

00:11:20.089 --> 00:11:25.630
we go and here's the auto generated file.
This is in fact just what was created right

00:11:25.630 --> 00:11:33.230
now. You can see I have--this is a bunch of
boilerplate at the top, some instructions

00:11:33.230 --> 00:11:37.269
and then there is several different properties
which I have them in my demo app permits several

00:11:37.269 --> 00:11:50.240
different entity kinds I have in my demo app
which you can modify. All right. So, what

00:11:50.240 --> 00:11:55.339
do you need to do after you've created the
auto generated file? It's not completely automatic,

00:11:55.339 --> 00:11:58.940
right, you're not totally done but actually
in many cases you are already done. If you

00:11:58.940 --> 00:12:03.290
have a simple schema like the guestbook one,
quiet possibly all you need to do is set the

00:12:03.290 --> 00:12:09.671
connector. So, in this case I'm supposed to
saying connector CVS said to do and the default

00:12:09.671 --> 00:12:13.819
options for some of the connectors are pretty
simple. It's pretty common that default encoding

00:12:13.819 --> 00:12:18.660
for them is UTF-8 all the ones that we're
shipping right now. CSV files are pretty frequently

00:12:18.660 --> 00:12:25.769
in Windows-1252, so you might want to add
including Windows-1252. And you also want

00:12:25.769 --> 00:12:30.360
to review the property map. It's most like
the thing you're going to need to do next.

00:12:30.360 --> 00:12:34.070
So, keys are actually really interesting and
spend a bunch of time talking about that in

00:12:34.070 --> 00:12:38.269
a few minutes. You also may need to check
for duplicates, pretty rare, but if you have--you

00:12:38.269 --> 00:12:42.660
know, datastore statistics you've had a schema
change. You've changed specifically from the

00:12:42.660 --> 00:12:47.949
phone number type to a string type or vice
versa. You may have some entities which are--which

00:12:47.949 --> 00:12:51.180
the datastore statistics have noticed which
are the old types; you just seemed to go and

00:12:51.180 --> 00:12:57.860
delete that. And there's a bunch of to-do's
you have to deal with. So, let me talk about

00:12:57.860 --> 00:13:01.610
the keys for a minute. So if you're used to
the relational model, you have primary keys

00:13:01.610 --> 00:13:05.779
and foreign keys not quiet the same thing
in App Engine. This is the schema for the,

00:13:05.779 --> 00:13:10.329
let's say SQL schema somewhat similar to their
app, some Bulkloader sample, which I just

00:13:10.329 --> 00:13:16.279
downloaded to create the wizard for it. And
so, instead of a primary key App Engine again,

00:13:16.279 --> 00:13:21.670
every entity has a key called as a primary
key, there's also references may have seen

00:13:21.670 --> 00:13:26.310
that the talk about the corporate applications
we got in Google and talked about reference

00:13:26.310 --> 00:13:31.290
keys and some of them. And you can basically
use a key to identify another particular entity

00:13:31.290 --> 00:13:35.220
of another kind. There's also entity groups
which I mentioned earlier. These are typically

00:13:35.220 --> 00:13:38.769
use for transactional reasons but you may
in fact use them for actually structuring

00:13:38.769 --> 00:13:44.839
your data, in particular, in Java, their own
relationships, and also just to reminder for

00:13:44.839 --> 00:13:48.190
those of you who familiar with it or something
new for those who don't. When you have keys

00:13:48.190 --> 00:13:52.750
in your entities, there's basically two types;
one is the auto generated key. If you don't

00:13:52.750 --> 00:13:56.420
really care about the key, what it means,
App Engine will auto generate one for you

00:13:56.420 --> 00:14:02.480
which is a number associated with it. The
numbers increment overtime approximately no

00:14:02.480 --> 00:14:08.000
particular guarantees about them being unique.
You can also have a developer named key; this

00:14:08.000 --> 00:14:11.170
particular useful if you care about your key
structure. There's something really easy to

00:14:11.170 --> 00:14:16.350
calcu1ate to identify a key. Say you have
a blog with post on it or Wikipedia articles

00:14:16.350 --> 00:14:21.879
or you have a user object, it represents a
particular user. It's often really useful

00:14:21.879 --> 00:14:25.649
to use, for example, if you use the Google
account API, we'd give you a unique user ID

00:14:25.649 --> 00:14:29.850
for every user. You can just use that as the
key and so the useful because if you have

00:14:29.850 --> 00:14:34.250
any instances where you have this thing which
you identify, you can then look up whatever

00:14:34.250 --> 00:14:40.379
you want just by key. And these are also particularly
useful for creating relations. So, again,

00:14:40.379 --> 00:14:45.639
if you have bunch of tables in here, and say,
you've exported from a database in different

00:14:45.639 --> 00:14:50.339
CSV modules, you can then calculate the keys
so you can create your create your references

00:14:50.339 --> 00:14:56.029
automatically. So, common--most common case,
the key with no parent, where you set the

00:14:56.029 --> 00:15:02.519
entity key is pseudo property__key__. This
shouldn't conflict of any of your properties

00:15:02.519 --> 00:15:07.451
because we claim everything underscore is
reserved. So, if you just specify a string

00:15:07.451 --> 00:15:13.740
on import that will just create a named key.
You do need to actually use a transform. We've

00:15:13.740 --> 00:15:17.290
provided a bunch of helper functions. I haven't
mentioned it yet, but there's a whole library

00:15:17.290 --> 00:15:20.759
called "transform" which is imported by default
in the auto-generated Bulkloader.amo. It's

00:15:20.759 --> 00:15:26.100
got a bunch of helpers. One of them is key_id_or_name_as_string,
this one is basically safe. If you happen

00:15:26.100 --> 00:15:29.910
to have a mix of IDs and names, it will just
printout the ID or the name as a string. There

00:15:29.910 --> 00:15:34.379
are some other helpers if you want to distinguish
between them in your output. And for reference

00:15:34.379 --> 00:15:38.689
properties, we also created another method
which is called "create foreign keys." So

00:15:38.689 --> 00:15:43.910
if the schema identifies anything that's having
a key in the datastore statistics, the auto-generated

00:15:43.910 --> 00:15:49.079
amo will actually just fill in the transform
fill with to do. We have to specify the kind

00:15:49.079 --> 00:15:56.949
and that's all they need if it's just a single
level key. All right, so this is an example

00:15:56.949 --> 00:16:03.420
from the schema. We've got a customer and
a visit. Every visit has one customer. And

00:16:03.420 --> 00:16:07.680
we have an external visit ID. The export--I
know it's always names, so I can actually

00:16:07.680 --> 00:16:11.519
use a simpler key name. I should have used
the example which I had before. And then it

00:16:11.519 --> 00:16:15.329
has a customer property, which in this case
is an extra hand of customer. This is just

00:16:15.329 --> 00:16:21.269
supposed to be the--this is the name part
of the customer key. So again, transform.create_foreign_key_('customer')

00:16:21.269 --> 00:16:27.860
will just generate that. On the parent keys,
I mentioned are used for creating entity groups

00:16:27.860 --> 00:16:33.880
or on relationships. So, you'd probably actually
shouldn't do this. I'll explain that you should

00:16:33.880 --> 00:16:38.660
probably be using--those properties for this
but--let's say you want to actually have the

00:16:38.660 --> 00:16:43.490
visit and the visit activity has represent
exactly the schema, where we have Visit and

00:16:43.490 --> 00:16:48.149
the visit activities to handle the one too
many relationship. You can do this by creating

00:16:48.149 --> 00:16:53.490
entity group for each visit, and we've provided
a bunch of helper functions. Again, this transform

00:16:53.490 --> 00:16:58.810
method, which call--I called it "create deep
key." So, every key is identified. Every level

00:16:58.810 --> 00:17:03.740
of the key is identified by the kind and then
the identity. So in this case, the--this helper

00:17:03.740 --> 00:17:07.840
function knows how to pullout the visit column,
associated with the visit kind, pulls out

00:17:07.840 --> 00:17:12.020
the activity column associates it with the
visit activity kind which is in fact not listed

00:17:12.020 --> 00:17:17.190
here, but it's the kind that we're importing.
And on export, it's actually also the first

00:17:17.190 --> 00:17:21.420
time I've shown this to you guys, which is
that since this is a multicolumn thing, we

00:17:21.420 --> 00:17:25.950
have one key, which is representing the CSV
as two different columns. On import there's

00:17:25.950 --> 00:17:30.200
one function which knows how to actually look
at the whole rows of--holistically and actually

00:17:30.200 --> 00:17:33.820
pulls out the different columns that needs
to create the key. On export we actually can

00:17:33.820 --> 00:17:38.730
specify two different exports. So we have
the export to the external column visit. Export

00:17:38.730 --> 00:17:42.680
to the external column activity. And they
have two different transforms which run the

00:17:42.680 --> 00:17:49.890
function, just pulls the key apart. So as
I've mentioned, you'd probably don't actually

00:17:49.890 --> 00:17:54.781
want to use an owned relationship for this
very simple visit activity. You actually want

00:17:54.781 --> 00:17:59.100
to use the much more efficient list property
which is present in App Engine to generate--just

00:17:59.100 --> 00:18:04.480
put them all in the same entity. So, when
you fetch the entity, you get all the data.

00:18:04.480 --> 00:18:08.690
So, if you know your schema you'll know this
when your editing the file, but you may, you

00:18:08.690 --> 00:18:12.660
may have forgotten about it. You may just
be reviewing the auto-generated amo file.

00:18:12.660 --> 00:18:16.520
The statistics unfortunately don't actually
know about list properties. What they do know

00:18:16.520 --> 00:18:21.280
is, sort of, the count of properties you've
of each kind. So in this case, we have about

00:18:21.280 --> 00:18:25.570
twice as many activities as we have customers,
so it's a good chance that list property you

00:18:25.570 --> 00:18:31.351
should obviously review your own model, your
own job and notations to verify this. But

00:18:31.351 --> 00:18:35.030
it's a sort of a glaring note that, by the
way, this one is special, and you should probably

00:18:35.030 --> 00:18:40.103
do something about with it. So what do you
actually do with it? Again, this is sort of

00:18:40.103 --> 00:18:44.040
the simplest way to deal with it. If you have
something like CSV file, and you've already

00:18:44.040 --> 00:18:49.070
auto-generated it in some system. If you have
one of the columns is itself some sort of

00:18:49.070 --> 00:18:53.890
delimited colon. In this is I used a semicolon,
it's different from a comma. Try to the helpers

00:18:53.890 --> 00:18:57.940
called "split string," "join list" because
it's a type properties. These helpers will

00:18:57.940 --> 00:19:03.470
return a list of strings or take a list of
strings and split it out. These are actually

00:19:03.470 --> 00:19:09.600
types, so if you do something else, listed
integers, you actually want to do something

00:19:09.600 --> 00:19:15.710
slightly different. All right, so that's sort
of just a basic overview of how you auto-generate

00:19:15.710 --> 00:19:19.780
amo, some of the thing you want to do when
reviewing it. It'll auto-generate some of

00:19:19.780 --> 00:19:22.810
the most basic types, these are just some
of the transforms. You'll want to look into

00:19:22.810 --> 00:19:26.720
the transform class to actually see a list.
And the other transforms, I will be providing

00:19:26.720 --> 00:19:31.760
more later. So let's talk about some sort
of more advanced configuration. Multiple tables,

00:19:31.760 --> 00:19:35.770
again, let's imagine we have this visit and
visit activity table, and it's actually represented

00:19:35.770 --> 00:19:41.940
as two different tables and say in a relational
database. And you don't necessarily want to

00:19:41.940 --> 00:19:48.350
actually to write the preprocessing to smash
them together. How do you handle that? Right

00:19:48.350 --> 00:19:51.620
now, actually, the example that I'm giving
actually sort of--this [INDISTINCT] this,

00:19:51.620 --> 00:19:55.630
it actually does actually use some sort of
preprocessing. If you're a database or you

00:19:55.630 --> 00:19:59.940
have a tool, which exists--can output in a
hierarchy format. So, instead of just taking

00:19:59.940 --> 00:20:04.000
like a CSV file and smashing the hierarchy
into using semicolons, and hoping you get

00:20:04.000 --> 00:20:07.910
the escaping right. If you have something
like an XML file, we actually provide also

00:20:07.910 --> 00:20:12.880
an XML connect. So, since this is natively
hierarchal we can do something much more interesting

00:20:12.880 --> 00:20:21.110
with it. So this is just a quick intro to
the XML connector. It's called "simplexml"

00:20:21.110 --> 00:20:24.860
because it only handles really, really simple
XML right now. Someday, we can imagine someone

00:20:24.860 --> 00:20:29.530
writing a more complicated one. But basically,
you provide the XPath, which represents each

00:20:29.530 --> 00:20:34.750
of the primary entity. In this case visit
is the XPath. There's multiple visit nodes

00:20:34.750 --> 00:20:39.150
you can imagine. And it handles both element
centric and attribute centric, but for this

00:20:39.150 --> 00:20:44.120
case we're going to use the element centric
schema. Then we have the list properties,

00:20:44.120 --> 00:20:49.271
so all the other properties alighted here
they worked just like before and to say, you

00:20:49.271 --> 00:20:53.770
know, whatever the primary key and et cetera.
But what we can do is we say this cctivity

00:20:53.770 --> 00:20:57.150
is propered, it's list property. Again, I'm
just going to provide a helper here, which

00:20:57.150 --> 00:20:59.299
says list list_from_xml_node. And they all
look at the source of the helper. It's kind

00:20:59.299 --> 00:21:02.920
of interesting. You may look at the source
of helpers, kind of interesting. I'll go into

00:21:02.920 --> 00:21:07.040
this later how it actually works. But basically
again, it looks holistically at the thing

00:21:07.040 --> 00:21:11.160
similar to the way they create deep key work.
And it actually looks at the XML node, runs

00:21:11.160 --> 00:21:15.130
a different XPath on it for the activities,
pulls out all the activities and generates

00:21:15.130 --> 00:21:20.490
a list out that. And on export, it does sort
of the inverse. It takes all the activities

00:21:20.490 --> 00:21:27.460
and actually converts it into a list. So,
the neutral dictionary--for a CVS file, all

00:21:27.460 --> 00:21:31.030
in the neutral dictionary is just going to
be strings. But for other file types, it's

00:21:31.030 --> 00:21:36.200
actually not necessary that the dictionary
only have strings in it. In this case, simplexml

00:21:36.200 --> 00:21:40.890
supports on output if you have, basically,
a dictionary as one of the items. In the dictionary

00:21:40.890 --> 00:21:45.850
it'll actually create the child nodes for
you. So that's what the x--the transform helper

00:21:45.850 --> 00:21:52.650
on export does, it actually puts a dictionary
in dictionary, recursively because it's recursive

00:21:52.650 --> 00:21:59.410
format. So, how does this work? How would
you write your own code? And going to that.

00:21:59.410 --> 00:22:03.960
So, this is the first time I've actually shown
a Python code here since the Bulkloader is

00:22:03.960 --> 00:22:09.080
written in Python you won't need to list have
some basic understanding of Python. So, this

00:22:09.080 --> 00:22:14.380
is sort of a composite slide of three different
ways of doing the same thing. So you can actually

00:22:14.380 --> 00:22:18.660
have an inline lambda in your transform. So
similar to how you just--if you just specify

00:22:18.660 --> 00:22:23.570
let's say "int." That's actually the Python
in conversion function. You can specify a

00:22:23.570 --> 00:22:27.870
lambda function. So in this case, I've just
specified a lambda which takes the value,

00:22:27.870 --> 00:22:34.190
just passed in, converts it into an integer,
multiplies it by two. You can also do something

00:22:34.190 --> 00:22:38.730
very similar to the transforms we've help
provided in the transform.pie or the sample

00:22:38.730 --> 00:22:42.620
transform helper.pie. Where if you want to
do something multiple times, you know, and

00:22:42.620 --> 00:22:47.410
it's like copy and paste this lambda 50 times
in your file. You can just define a function

00:22:47.410 --> 00:22:51.280
double--again, it takes one argument. It takes
the value, it converts to and then it returns

00:22:51.280 --> 00:22:55.850
it. And then your import transform, you just
reference it. You've import your module called

00:22:55.850 --> 00:22:59.860
"double." And the third thing you can do,
and this is what many of the transform helpers

00:22:59.860 --> 00:23:04.080
do or sort of going far more complicated here
is if you want to have a function which takes

00:23:04.080 --> 00:23:09.200
an argument. So in this case, this is a sample
multiply function it takes a multiple and

00:23:09.200 --> 00:23:14.030
actually returns a function using sub-closures
in Python. So, you import transform--if you

00:23:14.030 --> 00:23:19.490
say my module multiply three, we're now tripling
instead of doubling the number. And my module

00:23:19.490 --> 00:23:26.510
multiply three is itself a function. So when
we pass in the string, you know, "1 1", it

00:23:26.510 --> 00:23:33.900
all converted into an integer and triple it
and you get back 33. But that doesn't explain

00:23:33.900 --> 00:23:39.430
how the listing--or the deep key functions
work. How do we actually get this information,

00:23:39.430 --> 00:23:43.540
because they only pass in a single value,
a single column? And the answer, how those

00:23:43.540 --> 00:23:48.170
work is if you specify a keyword argument
bulkload state typically it's a second argument,

00:23:48.170 --> 00:23:52.020
but it actually has explicitly that name.
We pass in this bulkload state object which

00:23:52.020 --> 00:23:55.660
what you see the bigger picture of what's
actually happening. So get things like the

00:23:55.660 --> 00:24:00.380
filename, loader options, and explore options
pass in the command line but more interestingly

00:24:00.380 --> 00:24:04.700
you get the current dictionary and the current
instance which is being transported. So when

00:24:04.700 --> 00:24:09.780
you're doing this--the way it worked with
sort of created a deep key or the ones that

00:24:09.780 --> 00:24:15.680
took the XML and convert into a list, is that
the function knew we provided an XPath into

00:24:15.680 --> 00:24:19.870
the XML node. We look at the bulkload state
for the current dictionary and the current

00:24:19.870 --> 00:24:24.460
dictionary in the XML converter contains the
node itself as one of it properties. So we

00:24:24.460 --> 00:24:28.570
looked at that and said, "Okay, we're going
to run this XPath on it. Do a bunch of information

00:24:28.570 --> 00:24:35.780
and return this list property." Similarly,
for the one which create a deep key, it actually

00:24:35.780 --> 00:24:39.500
said, "OK, here are the columns I care about
for my keys so it was visit and activity."

00:24:39.500 --> 00:24:42.500
Actually, look to the dictionary and said,
"Okay, I'm going to look for the visit property

00:24:42.500 --> 00:24:45.370
in the dictionary, the activity property in
the dictionary. Combined them together and

00:24:45.370 --> 00:24:50.750
create a deep key." In fact, here's a simplified
version of create deep key. So again, this

00:24:50.750 --> 00:24:54.640
is one of these things that are just in the
closure so that it can be reused in multiple

00:24:54.640 --> 00:25:00.100
different amo files. And, we have a transfer
function and we'd basically go through the

00:25:00.100 --> 00:25:05.450
list of kinds and column that were provided
in the amo file and pullout the information

00:25:05.450 --> 00:25:10.470
from the dictionary. So the kind column is
something that would specify in the amo file

00:25:10.470 --> 00:25:14.140
and the result is basically we take this,
iterate through it, and calls the key from

00:25:14.140 --> 00:25:21.730
path function which is how you build one of
these hierarchal keys in the Python data story

00:25:21.730 --> 00:25:27.350
behind. The other useful thing, so, sort of,
mention you could sort of introspect in the

00:25:27.350 --> 00:25:31.520
state on your transforms. You can also do
a post processing as post import function,

00:25:31.520 --> 00:25:37.320
post export function. These will take--it
will take, it will pass an entire dictionary

00:25:37.320 --> 00:25:40.710
that the neutral dictionary and the entire
entity, in both directions, we pass both of

00:25:40.710 --> 00:25:45.340
them. And you can do whatever you want with
it. So you could for example have no, nothing

00:25:45.340 --> 00:25:48.590
in your property map and have a post import
or post export functions which did all the

00:25:48.590 --> 00:25:54.890
work yourself. I'm not quite sure why you'd
do that. More useful is that, you can do filtering.

00:25:54.890 --> 00:26:00.800
So on import, you can say, "Okay." For some
reason this is invalid or its nothing that

00:26:00.800 --> 00:26:05.250
I care about, it's going to return none, similarly
on export. In fact, that's how the Bulkloader

00:26:05.250 --> 00:26:08.110
takes it is, sorts it, it provides a useful
Bulkloader.amu because it's actually a bunch

00:26:08.110 --> 00:26:13.720
of noise in the data source, we just don't
care about. So it just says return none on

00:26:13.720 --> 00:26:18.190
all those. The other thing you can do on import,
you can't don't this on export which you can

00:26:18.190 --> 00:26:23.370
actually provide a second kind. That becomes
interesting if you sort of doing data de-normalization

00:26:23.370 --> 00:26:27.820
or normalization of some form on your imported
function. It's not particularly easy to do

00:26:27.820 --> 00:26:32.420
right now, so I'm just going to skip over
that. I'll [INDISTINCT]. The other interesting

00:26:32.420 --> 00:26:36.300
thing is writing your own connector. So the
CSV is not the whole world, right. I mean,

00:26:36.300 --> 00:26:41.110
sure you can export most databases into CSV
rights and processing. Even XML, it's a very

00:26:41.110 --> 00:26:46.010
basic XML kind of, you can do hierarchy. But,
let's say you want to talk with some other

00:26:46.010 --> 00:26:51.630
file format like a database format or even
a connection to a live database or even other

00:26:51.630 --> 00:26:56.820
websites. I think somebody asked some question
about gdata on the wave before the talk. You

00:26:56.820 --> 00:27:00.420
can write your own connector. There's a simple
connector interface. Although, actually as

00:27:00.420 --> 00:27:04.309
we've been sort of extending beyond the simple
model of files. It has some limitations that

00:27:04.309 --> 00:27:09.500
might change, but it'll certainly stay there,
there's a backwards compatible version. So,

00:27:09.500 --> 00:27:17.490
this is actually a complete connector on the
screen right here. The--when you notice, for

00:27:17.490 --> 00:27:21.101
example, I set the encoding option in CSV
connector much earlier in the demo. The connector

00:27:21.101 --> 00:27:26.300
gets this when it gets--there's a method you
define to create your connector, in this case,

00:27:26.300 --> 00:27:31.070
it's just a contracture of the object, you
can just get it from the dictionary. And on

00:27:31.070 --> 00:27:34.460
import, there's just one method, which returns
[INDISTINCT], in this case generate import

00:27:34.460 --> 00:27:39.750
records, so I'm just reading JSON. So, this
format is particularly useful if you have

00:27:39.750 --> 00:27:46.860
something that you can say, export to a hierarchal
format. Each line in the file represents a

00:27:46.860 --> 00:27:51.040
row, and each line actually represents a JSON
document. So it's not actually one JSON files,

00:27:51.040 --> 00:27:55.480
multiple JSON documents in a single file.
Open the file, for every line in the file

00:27:55.480 --> 00:28:03.160
just run simple JSON [INDISTINCT] it and yield
it, it's generates--it a very simple way we

00:28:03.160 --> 00:28:09.450
by implementing this. On export, very similar.
There's three methods instead of one. You've

00:28:09.450 --> 00:28:13.610
got initialized methods which open the file.
For every dictionary it gets passed in, we

00:28:13.610 --> 00:28:16.850
do something interesting with it. In this
case, we just [INDISTINCT] it using simple

00:28:16.850 --> 00:28:23.060
JSON and write it, on its own mind and we're
done, we close the file to clean-up after

00:28:23.060 --> 00:28:27.700
ourselves. All right, so now you've sort of
learned sort of how the Bulkloader works,

00:28:27.700 --> 00:28:31.800
how to write your own connectors, how to write
your own configuration file. It's going to

00:28:31.800 --> 00:28:37.570
go to a couple little of tips about how to
use the Bulkloader. One of the particular

00:28:37.570 --> 00:28:40.520
things which happens a lot when using the
Bulkloader because your running them your

00:28:40.520 --> 00:28:44.650
client and because your application is sitting
on a server is, things happen. It may get

00:28:44.650 --> 00:28:49.490
aborted. You may temporarily run-out of quota.
You know, someone may trip over the wire and

00:28:49.490 --> 00:28:53.640
disconnect you, whatever. You may have an
error. You can resume both uploads and downloads.

00:28:53.640 --> 00:28:58.760
You don't have to just start from scratch.
So, on uploads you need to pass db filename

00:28:58.760 --> 00:29:02.760
that's where the Bulkloader stores its state
about, "Okay, how far long am I into this

00:29:02.760 --> 00:29:07.740
file? What's going on?" On downloads you need
to pass both the state and the result db filename.

00:29:07.740 --> 00:29:12.570
As I mentioned earlier it downloads everything.
And then it runs it through the transform.

00:29:12.570 --> 00:29:17.970
This is also particularly useful on export.
You can use this for a couple of things. You

00:29:17.970 --> 00:29:21.950
can--if you sort of just messed up you export
transform, you don't need to re-download all

00:29:21.950 --> 00:29:27.140
your data. You can just rerun it, point to
the same result db file and run over it. You

00:29:27.140 --> 00:29:30.520
can also do tricks where you use different
formats. So if you downloaded the data and

00:29:30.520 --> 00:29:34.500
want to run some sort of two different reports
or something on the same data, you can specify

00:29:34.500 --> 00:29:43.920
to Bulkloader.amo files on the same result
database. So talk about tuning, so, I mentioned

00:29:43.920 --> 00:29:50.370
much earlier that it runs in parallel, and
the downloads run in parallel, and the upload

00:29:50.370 --> 00:29:53.262
is running in parallel too. They're all running
in parallel. There's a couple of different

00:29:53.262 --> 00:29:57.980
arguments, these are just three of the common
ones, you need to deal with. Because of the

00:29:57.980 --> 00:30:01.590
limitations and code on App Engine, unfortunately,
frequently you need to tune this down to much

00:30:01.590 --> 00:30:05.951
smaller sizes than a default. However, you
can also generally tune and turn up the number

00:30:05.951 --> 00:30:11.870
of threads. So, the Bulkloader is actually
making estimations on the client of what your

00:30:11.870 --> 00:30:16.020
server can handle, it's based on some of the
rules and some of the codes that we have.

00:30:16.020 --> 00:30:20.150
And actually in the last maybe year or so,
since we originally wrote this code, some

00:30:20.150 --> 00:30:24.230
of the rules we've had have actually changed
and relax, so it's destinations off and off.

00:30:24.230 --> 00:30:28.000
So unfortunately, it usually just works but
occasionally de-tune it, so fairly common

00:30:28.000 --> 00:30:32.980
tuning operations turned down the number of
request the number of data that sent at any

00:30:32.980 --> 00:30:37.130
one time, and turn off the number of threads
so you can do more things in parallel. You

00:30:37.130 --> 00:30:42.010
can also as I noted run it against the dev
app server. The Python want a single threaded,

00:30:42.010 --> 00:30:46.640
so if you use the default number threads for
and have lots of data, it will actually dive

00:30:46.640 --> 00:30:52.080
our way through which probably [INDISTINCT]
if you don't do this but, you can specify

00:30:52.080 --> 00:30:58.310
number threads equals one. All right, so those
are some service tips tuning and running it,

00:30:58.310 --> 00:31:03.780
start about troubleshooting for a bit. So,
as I mentioned earlier, when I showed you

00:31:03.780 --> 00:31:11.610
the web XML and the app.amo, the remote API
URL is frequently configured wrong. It should

00:31:11.610 --> 00:31:15.340
say the request do not contain in the necessary
header, it's kind of goofy error message,

00:31:15.340 --> 00:31:19.090
it's just saying that you're browser is not
the remote API client, and you should be using

00:31:19.090 --> 00:31:25.140
remote API client. But it's also not telling--it
also, it ensures that when you're logging

00:31:25.140 --> 00:31:28.840
in, you can log in as an admin and that's
configured correctly, it doesn't say show

00:31:28.840 --> 00:31:33.230
the homepage of your application which you
might do of the of the cache or handler. And

00:31:33.230 --> 00:31:37.740
you also need to specify the user remote API
URL in the Bulkloader configuration if using

00:31:37.740 --> 00:31:42.430
--URL. If you're using a Python application,
you can actually just specify an app directory

00:31:42.430 --> 00:31:47.680
just like you do for the upload and download
and all those other operations, it'll actually

00:31:47.680 --> 00:31:51.910
automatically infer things for you. For a
job developer, at the moment you have to use

00:31:51.910 --> 00:32:01.070
--URL. And so, debugging the config file.
So, since the config file is sort of a fairly

00:32:01.070 --> 00:32:04.970
clean format, the-â€“it'll spit out error
messages telling your line numbers and error

00:32:04.970 --> 00:32:10.120
messages. Hopefully, they're helpful. Occasionally,
if the actual Python file your importing contains

00:32:10.120 --> 00:32:13.920
an error, it will tell you there's an error
not line of the app.amo and it's just telling

00:32:13.920 --> 00:32:17.950
you that in fact there's an error in the Python
file which is referenced on that line. On

00:32:17.950 --> 00:32:22.700
import, you can run a dry run. So the dry
run takes all of your data, runs it through

00:32:22.700 --> 00:32:26.440
all of the import transforms, but doesn't
actually write it through datastore. So this

00:32:26.440 --> 00:32:30.260
is useful particular validation. I didn't
mention this, you can actually use a Python

00:32:30.260 --> 00:32:35.270
model to ensure your validation. And basically,
I'm sure that it can pass you your entire

00:32:35.270 --> 00:32:39.590
input file without actually writing any data.
And of course you can display it through that

00:32:39.590 --> 00:32:46.190
app server both for upload and download because
small sets of data, just always a good practice.

00:32:46.190 --> 00:32:53.000
All right, so, talking a bit about the future
directions, so, dressing the pain. So, the

00:32:53.000 --> 00:32:55.750
whole point of the Bulkloader, of course,
is to make your life easier when you're moving

00:32:55.750 --> 00:33:00.190
data in and out of App Engine. So, just showed
you some of the transforms today, you may

00:33:00.190 --> 00:33:03.960
have notice someone said transform reference
have transformed, that's because I wrote them

00:33:03.960 --> 00:33:08.760
in the [INDISTINCT] instead of in the actual
SDK. We'll be adding more useful ones in the

00:33:08.760 --> 00:33:13.910
SDK. This is also an opportunity sort of for
you, the general audience, or anyone to contribute

00:33:13.910 --> 00:33:19.230
since the SDKs is in fact an open source project
give useful ones, which only opens to eventually

00:33:19.230 --> 00:33:25.320
more connector. So right now, we only have
CSV and XML connectors and this demo simple

00:33:25.320 --> 00:33:30.210
JSON one. Obviously, wants someone that's
actually connects to a real database is perhaps

00:33:30.210 --> 00:33:33.320
using the standard Python database interface
so that you can connect to multi-database,

00:33:33.320 --> 00:33:38.501
start writing your own connector, exporting
data. And the real big one, the server side

00:33:38.501 --> 00:33:43.560
processing. So I've talked about this tuning
and these errors you can get. On a road map,

00:33:43.560 --> 00:33:47.220
are things like map reduce and long running
processes, and we're investigating which of

00:33:47.220 --> 00:33:51.690
those sort of most useful way of actually
handling data entirely on the server. So this

00:33:51.690 --> 00:33:55.491
isn't useful, necessarily if you're running
a local SQL database connection though, actually,

00:33:55.491 --> 00:33:59.080
I have some ideas on how to make that work
too. It's particularly useful if you have

00:33:59.080 --> 00:34:03.160
entire CSV file, and you can export your entire
database. Usually, we upload it using the

00:34:03.160 --> 00:34:07.390
[INDISTINCT] API and app engine. Here's the
giant blob, a single HTTP connection, it's

00:34:07.390 --> 00:34:12.299
very reliable. And then we can run it on the
applications server, iterate through all of

00:34:12.299 --> 00:34:18.690
your data on the server side, and it's basically
gets rid of the most common pain point people

00:34:18.690 --> 00:34:22.179
have which is the Bulkloader today which is
at something happens at some point during

00:34:22.179 --> 00:34:28.611
the transfer process and, you know, you have
to debug that and deal with that. All right,

00:34:28.611 --> 00:34:35.889
so just a summary, Q&amp;A. So, just a reminder,
the Bulkloader runs on your workstation, transforms

00:34:35.889 --> 00:34:39.740
data, loads it at the server. If you don't
want to do any configuration, you're just

00:34:39.740 --> 00:34:44.609
moving data backing it up, moving up from
server to from client from one instance to

00:34:44.609 --> 00:34:50.319
another. We got a zero configuration, even
automatic configuration tool. So, you don't

00:34:50.319 --> 00:34:53.750
actually have to write one of these Bulkloader
configurations from scratch, but, we've got

00:34:53.750 --> 00:34:58.049
lots of helpers and transform for help to
make your life easier, and you can also write

00:34:58.049 --> 00:35:02.880
your own helpers, write your own connectors,
and just--keys are to key to creating relationships

00:35:02.880 --> 00:35:06.530
if you have more than one table and you have
an example probably more complicated than

00:35:06.530 --> 00:35:10.310
the initial Bulkloader, initial guess book
example. All right, so, I guess, I gave you

00:35:10.310 --> 00:35:26.519
lots of time for Q&amp;A. So, any questions--microphone.
I'm going to see if I can open the wave here.

00:35:26.519 --> 00:35:33.339
Any hope for Java version? So, let me ask
you why, I guess, why...

00:35:33.339 --> 00:35:35.930
&gt;&gt; [INDISTINCT] speaks Python.
&gt;&gt; BLAIN: Is that what?

00:35:35.930 --> 00:35:38.289
&gt;&gt; I don't speak Python.
&gt;&gt; BLAIN: You don't speak Python that's one

00:35:38.289 --> 00:35:42.589
of the specific goals I mentioned which is
that you shouldn't have to speak Python to

00:35:42.589 --> 00:35:45.440
use this new configuration format. It's in
the amo, I didn't actually mentioned what

00:35:45.440 --> 00:35:51.230
amo is, yet another--amo is not markup language
I think is its name. Anyway, it's it is yet

00:35:51.230 --> 00:35:56.359
another markup language similar to XML. And
hopefully the create wizard will just create

00:35:56.359 --> 00:36:01.872
a wizard well enough free, you don't have
to actually have to do very much to it. One

00:36:01.872 --> 00:36:06.329
of the things which we've looked at is actually
running this code using Jython on the Java

00:36:06.329 --> 00:36:11.480
SDK so at least you don't have to download
Python, download the Python SDK. We've looked

00:36:11.480 --> 00:36:19.280
at it, I don't have actually anything to report
interesting about that at that time.

00:36:19.280 --> 00:36:33.570
&gt;&gt; Thank you for the presentations. So, I
have same kind of question on top of that.

00:36:33.570 --> 00:36:34.570
So, is there any limitation for the file size
uploader?

00:36:34.570 --> 00:36:37.289
&gt;&gt; BLAIN: Right, so that's why, so that is
the biggest pain point right now. So, we've

00:36:37.289 --> 00:36:41.690
changed some of how the scheduling works in
App Engine, so that you can upload a little

00:36:41.690 --> 00:36:44.470
bit better than that. Unfortunately, right
now, you do have to tune it and it is still

00:36:44.470 --> 00:36:48.349
much slower than we'd like. So, you know,
we're looking a little bit of the performance

00:36:48.349 --> 00:36:53.380
of it in the future but, sort of, my big hope
is actually that really soon we'd be actually

00:36:53.380 --> 00:36:57.089
say instead of uploading the data piece by
piece which is why it takes so long because

00:36:57.089 --> 00:37:01.210
it's this tons of overhead on every request
that we can just upload it through the server

00:37:01.210 --> 00:37:05.400
all at once, this one giant blog and you say
the map functions to run over it and just,

00:37:05.400 --> 00:37:10.759
and they'll run much quicker because instead
of this giant overhead for every single bit

00:37:10.759 --> 00:37:16.980
of data, it's all quite close to each other.
&gt;&gt; So, your answer is currently...

00:37:16.980 --> 00:37:18.190
&gt;&gt; BLAIN: Unfortunately, it's still pretty
slow right now and...

00:37:18.190 --> 00:37:21.520
&gt;&gt; Just slower, you know.
&gt;&gt; BLAIN: It's slow and, yeah, exactly. I

00:37:21.520 --> 00:37:28.369
mean, I've certainly seeing people with that
sort of greater, greater speed. And certainly

00:37:28.369 --> 00:37:32.400
seeing stuff go much fast in that.
&gt;&gt; And there--is there any limitation about,

00:37:32.400 --> 00:37:37.680
you know, App Engine is or, has a 30-second
rule per one request, right? So, is that the

00:37:37.680 --> 00:37:45.940
case upright to the...
&gt;&gt; BLAIN: So, that's part of why the Bulkloader

00:37:45.940 --> 00:37:51.410
runs on the client today is because App Engine
can't currently handle large amounts of data.

00:37:51.410 --> 00:37:55.460
It can now, couldn't--it was originally created.
It can handle dealing of large amounts of

00:37:55.460 --> 00:37:59.750
data, it can't handle request longer than
30 seconds. So right now, that's why the Bulkloader

00:37:59.750 --> 00:38:03.630
runs on the client. They can talk to whatever
you client can handle amount of data and it

00:38:03.630 --> 00:38:08.300
sends it bit by bit to the server using the
remote API which is why sort of the cool new

00:38:08.300 --> 00:38:13.559
features we have dealing with, you know, not
produce, or long running processes will actually

00:38:13.559 --> 00:38:16.640
alleviate that, but they're not there right
now so.

00:38:16.640 --> 00:38:19.619
&gt;&gt; So, your planning you'll improve that kind
of [INDISTINCT]...

00:38:19.619 --> 00:38:22.070
&gt;&gt; BLAIN: Yeah, specifically, something which
I'm actually really excited about those features

00:38:22.070 --> 00:38:29.000
is because it let us solve this problem.
&gt;&gt; Continue the, you know, how quick their--you

00:38:29.000 --> 00:38:33.359
were, release that kind of refraction, time
schedule.

00:38:33.359 --> 00:38:39.460
&gt;&gt; BLAIN: Hopefully soon. I can't really say
anything about time but it is real, it's not,

00:38:39.460 --> 00:38:48.740
you know, [INDISTINCT] but I don't have any
specific time on this.

00:38:48.740 --> 00:38:51.140
&gt;&gt; Okay, thank you.
&gt;&gt; BLAIN: Yeah.

00:38:51.140 --> 00:38:52.500
&gt;&gt; Hey, hi.
&gt;&gt; BLAIN: Hi.

00:38:52.500 --> 00:38:57.570
&gt;&gt; I'm curious if you're planning to get some
graphical user interface. I don't know if

00:38:57.570 --> 00:38:59.340
you're familiar with an application called
GEA BAR, that makes your back up and restore

00:38:59.340 --> 00:39:02.900
a web interface. So, is that something planned
or stick to the compiler?

00:39:02.900 --> 00:39:19.799
&gt;&gt; BLAIN: No plans for that at this time so.
Okay, yes. Okay, so, talk about this question.

00:39:19.799 --> 00:39:25.579
So, the--I don't know the details of how ECG
based database has worked, but the fundamental

00:39:25.579 --> 00:39:29.190
answer to this question is pretty much the
same actually get to any question about how

00:39:29.190 --> 00:39:34.740
do I deal with data inside and outside of
App Engine, which is that everything's an

00:39:34.740 --> 00:39:39.049
HTTP request. So the way Bulkloader works
that uses remote API to transfer information

00:39:39.049 --> 00:39:44.250
from your local client to server, but, as
I said you can imagine the connector which

00:39:44.250 --> 00:39:48.951
runs in your local workstation, connects to
Amazon, and talk to the server. And you can

00:39:48.951 --> 00:39:52.309
also imagine something running on the server
using task queue or something which does an

00:39:52.309 --> 00:39:57.680
HTTP request to your Amazon and sends, and
pulls the data back and forth. That's sort

00:39:57.680 --> 00:40:02.030
of-â€“that's how the Bulkloader, so we can
run it with the client which talks to Amazon

00:40:02.030 --> 00:40:05.220
or was in either direction. Yeah.
&gt;&gt; [INDISTINCT] Bulkloader does it support

00:40:05.220 --> 00:40:14.920
a margin of the data like if I believe some
data in my local, [INDISTINCT] server too?

00:40:14.920 --> 00:40:19.130
&gt;&gt; BLAIN: So, that's an important question.
When I mentioned keys are key, I didn't actually

00:40:19.130 --> 00:40:26.749
go into detail what I really meant by that.
So, the Bulkloader, when it's running either

00:40:26.749 --> 00:40:32.530
import or export is only doing either reads
or writes. So, when it's doing import of data

00:40:32.530 --> 00:40:38.603
from your local workstation to App Engine.
All right, so it's importing to App Engine

00:40:38.603 --> 00:40:40.920
that's the direction we're going here. It's
creating, it's doing a put for every single

00:40:40.920 --> 00:40:46.090
entity that's in your input source to App
Engine. So, if you've not specified any keys

00:40:46.090 --> 00:40:50.460
at all, App Engine will auto generate the
keys and it will be purely in append. If you

00:40:50.460 --> 00:40:54.870
specify the key for every single row in your
data file, if they're all new it will all

00:40:54.870 --> 00:40:59.730
be new entities, if you specify the key with-â€“which
is represents an existing entity it will be

00:40:59.730 --> 00:41:04.309
an overwrite. So there's no specific delete,
that's one of the other features which people

00:41:04.309 --> 00:41:08.829
very commonly ask for an App Enginem, "How
do I delete all my entities for this kind?"

00:41:08.829 --> 00:41:12.279
And the answer right now is you can run on
a task queue, task over and the answer in

00:41:12.279 --> 00:41:21.579
the future will be using some of the new technologies
that I've talked about. All right, similar

00:41:21.579 --> 00:41:26.300
answer to the answer of the previous question,
I guess, I'm not exactly sure how the supplies

00:41:26.300 --> 00:41:33.339
in the Bulkloader-â€“this is more used SDC
with HTTP so SDC is the Secured Data Connector;

00:41:33.339 --> 00:41:37.740
it lets you, if you're applications inside
your firewall, it's a tool which lets you

00:41:37.740 --> 00:41:43.900
communicate over HTTP through your firewall.
Our entity is written in order such that reference

00:41:43.900 --> 00:41:50.270
properties are always valid. So this is, again,
my keys are key, if you're reference propertiesâ€“-if

00:41:50.270 --> 00:41:54.630
you have a way of calculating your keys for
every entity. So, again, if you're the user

00:41:54.630 --> 00:41:59.220
and the email address you need for the user;
Wikipedia articles and you've got a specific

00:41:59.220 --> 00:42:04.069
title for every article. Anytime you have
a reference to this, or even if you actually

00:42:04.069 --> 00:42:07.880
use the gwid which your database has generated,
any time you have reference to this anywhere

00:42:07.880 --> 00:42:12.529
you can use it. So, it doesn't matter what
order of things are written in, because you

00:42:12.529 --> 00:42:18.440
can have a key which points to-â€“you can
have a reference property if it which points

00:42:18.440 --> 00:42:22.849
the key in another entity even though that
entity doesn't exist. Now if you try to do

00:42:22.849 --> 00:42:27.369
reference it, you got a failure because it
try to get the entity, it doesn't exist. But,

00:42:27.369 --> 00:42:30.799
as long as you you've had created both entities,
you've created both the parent and the child

00:42:30.799 --> 00:42:34.569
and the reference, by the time you actually
come around to reading the information is

00:42:34.569 --> 00:42:39.430
there. So, about ordering-â€“the ordering
is not strictly guaranteed on upload because

00:42:39.430 --> 00:42:44.579
it does stuff in parallel. On download, the
ordering is strictly guaranteed. Another feature

00:42:44.579 --> 00:42:48.950
I didn't mention is that you can actually
specify by default when we run through it,

00:42:48.950 --> 00:42:51.930
I said downloads everything just runs through
all of your data. Right now it runs through

00:42:51.930 --> 00:42:57.609
in key order. There's a property can specify
on the amo where you can actually specify

00:42:57.609 --> 00:43:01.759
an arbitrary sort of property. So, if you
have a bunch of other generating keys, you

00:43:01.759 --> 00:43:04.910
can actually want to sort by email address.
You can actually specify the email address

00:43:04.910 --> 00:43:12.490
should be the sort key on the upload. Invalid
characters; someone in this room probably

00:43:12.490 --> 00:43:14.319
asked this.
&gt;&gt; [INDISTINCT].

00:43:14.319 --> 00:43:23.710
&gt;&gt; BLAIN: So, okay. Yeah, so, the encoding,
and again, this is dry runs particularly useful

00:43:23.710 --> 00:43:28.540
for this. The connectors in both the CSV and
XML connector support multiple encoding. So,

00:43:28.540 --> 00:43:31.809
if you use the default [INDISTINCT] encoding
data is actually Windows 1252. You're likely

00:43:31.809 --> 00:43:36.509
to get in there a part way through your file.
So, this is where dry runs particularly useful

00:43:36.509 --> 00:43:40.400
because it will run through all of your input
data, and when you hit the end, no errors,

00:43:40.400 --> 00:43:44.249
you're good to go. If it dies part way through
you, you've known before you even try to write

00:43:44.249 --> 00:43:53.839
to your datastore that you've an error part
way through your file. All right, anybody

00:43:53.839 --> 00:44:00.140
have questions? That's the end of the day,
and I guess you're all are welcome to hang

00:44:00.140 --> 00:44:04.039
out. Talk me, I'll be here afterwards. I'll
probably be around, I'm, I'll be here in office

00:44:04.039 --> 00:44:07.150
hours tomorrow. And if go to bulkloadersample@appspot.com,
you'll see all the samples, I guess. I'm just

00:44:07.150 --> 00:44:17.529
matthew.blane@google.com if you have any questions
for me over email. All right. Thanks.

