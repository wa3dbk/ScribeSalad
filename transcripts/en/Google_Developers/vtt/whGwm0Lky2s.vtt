WEBVTT
Kind: captions
Language: en

00:00:09.720 --> 00:00:10.800
Hey everybody.

00:00:10.800 --> 00:00:12.770
Welcome to another
fantastic episode

00:00:12.770 --> 00:00:16.017
of Google Developers Live,
where we here at Google

00:00:16.017 --> 00:00:18.350
get a chance to talk to you
about all the amazing things

00:00:18.350 --> 00:00:21.270
going on on the web, in the
tech, and all the platforms

00:00:21.270 --> 00:00:23.130
and the fantastic things
you can do with it.

00:00:23.130 --> 00:00:24.610
My name is Colt McAnlis.

00:00:24.610 --> 00:00:27.270
I'm a developer advocate
here on the Chrome team,

00:00:27.270 --> 00:00:30.710
and I mostly work with web
performance issues as well as

00:00:30.710 --> 00:00:31.860
web gaming.

00:00:31.860 --> 00:00:33.647
Now, what I'd like
to talk to you today

00:00:33.647 --> 00:00:35.480
about is something
that's very near and dear

00:00:35.480 --> 00:00:38.580
and close to my heart,
and that's compression.

00:00:38.580 --> 00:00:41.510
Today, we're going to be talking
about GZIP on the web platform

00:00:41.510 --> 00:00:45.360
and how you can modify it
and address it and munge

00:00:45.360 --> 00:00:48.740
it to try to minimize your
website memory footprint

00:00:48.740 --> 00:00:50.360
and actually get
faster loading data.

00:00:50.360 --> 00:00:52.940
Now, before we get into
this talk too much,

00:00:52.940 --> 00:00:55.900
I'd actually like to point
out the #perfmatters hashtag.

00:00:55.900 --> 00:00:57.400
This is a fantastic
hashtag that's

00:00:57.400 --> 00:00:59.440
being used all
around the intertubes

00:00:59.440 --> 00:01:02.370
on various social networks
for people in web performance

00:01:02.370 --> 00:01:04.940
who are trying to
find the same problems

00:01:04.940 --> 00:01:07.380
and address difficult issues
and have conversations

00:01:07.380 --> 00:01:10.230
about performance in
interesting and insightful ways.

00:01:10.230 --> 00:01:11.570
So use this hashtag.

00:01:11.570 --> 00:01:14.864
If anything during this talk
is interesting or inspiring,

00:01:14.864 --> 00:01:17.530
feel free to go to your favorite
social media network of choice.

00:01:17.530 --> 00:01:20.550
It's a fantastic outlet, and
you need to be following it.

00:01:20.550 --> 00:01:23.120
So with that, let's
get into things.

00:01:23.120 --> 00:01:24.690
So let's start at the beginning.

00:01:24.690 --> 00:01:26.940
I can't really talk about
compression on the web

00:01:26.940 --> 00:01:29.675
without actually taking a look
at the state of the web today.

00:01:29.675 --> 00:01:31.640
Now, besides being
filled with cats,

00:01:31.640 --> 00:01:33.710
there's actually some
interesting data out there

00:01:33.710 --> 00:01:35.070
for us to look at.

00:01:35.070 --> 00:01:37.627
There's a great site
called httparchive.org,

00:01:37.627 --> 00:01:39.210
and what this site
will do is actually

00:01:39.210 --> 00:01:42.640
run large amounts of website
through processing filters

00:01:42.640 --> 00:01:43.512
during the day.

00:01:43.512 --> 00:01:44.970
So what will happen
is they'll take

00:01:44.970 --> 00:01:47.989
300,000, 400,000
very common websites,

00:01:47.989 --> 00:01:49.530
run them through a
processing system,

00:01:49.530 --> 00:01:51.470
and then scrape
information about them

00:01:51.470 --> 00:01:52.950
to present on their page.

00:01:52.950 --> 00:01:55.450
Now, what you can see
here in this graph

00:01:55.450 --> 00:01:57.310
is actually one of the
latest studies that

00:01:57.310 --> 00:02:00.910
shows the average bytes of
a page per content type.

00:02:00.910 --> 00:02:03.140
So what this graph
shows us is that images,

00:02:03.140 --> 00:02:06.350
for an average site,
actually take up

00:02:06.350 --> 00:02:10.870
the lion's share of the content
that's being streamed to users.

00:02:10.870 --> 00:02:12.390
In fact, it's more than 50%.

00:02:12.390 --> 00:02:16.222
It's actually a massive amount
of the content being delivered,

00:02:16.222 --> 00:02:18.680
while scripts actually remain
the second largest, ending up

00:02:18.680 --> 00:02:21.119
at around 260k or so.

00:02:21.119 --> 00:02:22.660
Now, the interesting
thing about this

00:02:22.660 --> 00:02:25.990
is when you look at the
individual response sizes,

00:02:25.990 --> 00:02:28.150
you see a similar
trend, of course,

00:02:28.150 --> 00:02:33.243
Flash being one of the largest
sizes per number of responses.

00:02:33.243 --> 00:02:34.993
But of course, that
comes down to the fact

00:02:34.993 --> 00:02:39.020
that you generally only have one
or two flashes per page where

00:02:39.020 --> 00:02:43.090
they tend to be pretty large,
whereas JavaScript or PNGs

00:02:43.090 --> 00:02:45.410
or JPEGs tend to be lots
of requests for a page.

00:02:45.410 --> 00:02:47.490
That actually drives
that number down.

00:02:47.490 --> 00:02:49.390
So what we're looking
at here is basically

00:02:49.390 --> 00:02:53.020
a vision of the web that shows
that we're dominated by images

00:02:53.020 --> 00:02:55.070
and we're dominated
by image data.

00:02:55.070 --> 00:02:56.870
But I'm not
necessarily sure that

00:02:56.870 --> 00:03:00.260
means that we can give up on
trying to minimize and reduce

00:03:00.260 --> 00:03:03.632
the size of the text data
that actually drives our web.

00:03:03.632 --> 00:03:04.840
So let's take a look at this.

00:03:04.840 --> 00:03:07.490
This is another great set
of charts from HTTP Archive.

00:03:07.490 --> 00:03:09.323
And effectively, what
you're looking at here

00:03:09.323 --> 00:03:11.730
is three graphs showing
the transfer size

00:03:11.730 --> 00:03:14.380
against the number of requests.

00:03:14.380 --> 00:03:16.840
And so what you're
seeing is over time,

00:03:16.840 --> 00:03:20.390
our number of requests have
remained pretty constant

00:03:20.390 --> 00:03:23.130
over the past couple
of years, while you

00:03:23.130 --> 00:03:26.840
see a general increasing
trend in the overall size

00:03:26.840 --> 00:03:27.960
per request.

00:03:27.960 --> 00:03:29.750
Now, this is
specifically three graphs

00:03:29.750 --> 00:03:32.760
for HTML, JavaScript, and CSS.

00:03:32.760 --> 00:03:35.170
What this is telling us is
that while images may make up

00:03:35.170 --> 00:03:38.140
the lion's share of
the internet right now,

00:03:38.140 --> 00:03:41.580
text information, the backbone
of these three formats,

00:03:41.580 --> 00:03:44.560
isn't going away
anytime soon, nor is it

00:03:44.560 --> 00:03:46.742
slated to get any smaller.

00:03:46.742 --> 00:03:48.450
So for most people,
they would say, well,

00:03:48.450 --> 00:03:52.370
if the web is mostly images,
why should we care about text?

00:03:52.370 --> 00:03:54.100
Well, here's why.

00:03:54.100 --> 00:03:57.000
You see, images may
be fantastic and they

00:03:57.000 --> 00:04:00.120
may be the dominant
form of content for us,

00:04:00.120 --> 00:04:03.170
but it's apparent and been
shown that they're not

00:04:03.170 --> 00:04:04.581
as small as they could be.

00:04:04.581 --> 00:04:06.580
So for example, let's
take this fantastic image,

00:04:06.580 --> 00:04:08.380
which was captured on a camera.

00:04:08.380 --> 00:04:11.130
This is about one
megapixel of data.

00:04:11.130 --> 00:04:12.840
Now, if we say
this is a PNG file,

00:04:12.840 --> 00:04:16.540
it actually comes out at
about 2.3 megabytes of data.

00:04:16.540 --> 00:04:18.500
Now, for those of you
playing the home game,

00:04:18.500 --> 00:04:21.050
you'll note that
HTTP Archive averages

00:04:21.050 --> 00:04:23.070
that the average
website out there

00:04:23.070 --> 00:04:26.850
is about 1.2 to
1.1 megs of data.

00:04:26.850 --> 00:04:29.350
So this single PNG image,
if hosted on your website,

00:04:29.350 --> 00:04:31.400
is already larger
than the average size

00:04:31.400 --> 00:04:32.890
of a website on the internet.

00:04:32.890 --> 00:04:34.115
Now, PNG's fantastic.

00:04:34.115 --> 00:04:36.170
It allows you to
get transparency,

00:04:36.170 --> 00:04:38.620
and it does support GZIP
compression internally

00:04:38.620 --> 00:04:41.902
as a format, but it really only
supports a lossless encoding,

00:04:41.902 --> 00:04:43.610
which means you're
not really getting rid

00:04:43.610 --> 00:04:46.240
of any visual data
that may be redundant.

00:04:46.240 --> 00:04:48.740
JPEG, on the other
hand, as a format

00:04:48.740 --> 00:04:51.560
is built both a lossy
and lossless encoder.

00:04:51.560 --> 00:04:55.100
This allows the JPEG encoding
system to actually remove

00:04:55.100 --> 00:04:57.620
visually redundant
information from the image

00:04:57.620 --> 00:05:00.135
so that you don't
really notice it's gone.

00:05:00.135 --> 00:05:03.180
The human eye is pretty
complex, but still,

00:05:03.180 --> 00:05:05.570
at 32 bits per pixel,
there's a lot of information

00:05:05.570 --> 00:05:07.250
that we can't discern.

00:05:07.250 --> 00:05:09.065
So if you JPEG this
image, you lose

00:05:09.065 --> 00:05:10.940
the ability to get
transparency, however, you

00:05:10.940 --> 00:05:15.880
do cut the size of the
image down to 297k.

00:05:15.880 --> 00:05:20.150
Now, these have been the
two mostly dominant image

00:05:20.150 --> 00:05:23.100
formats, besides animated
cat GIFs on the internet,

00:05:23.100 --> 00:05:27.710
but earlier this year,
internet technology evangelists

00:05:27.710 --> 00:05:30.930
and other enthusiasts
decided that maybe we

00:05:30.930 --> 00:05:32.160
weren't done here yet.

00:05:32.160 --> 00:05:34.480
Maybe the compression
system of JPEG

00:05:34.480 --> 00:05:36.990
wasn't good enough for
the rest of the web,

00:05:36.990 --> 00:05:39.970
and thus we had the
WebP format being born.

00:05:39.970 --> 00:05:41.854
Now, the WebP format is new.

00:05:41.854 --> 00:05:43.770
It's not necessarily
supported by all browsers

00:05:43.770 --> 00:05:44.870
as of this date.

00:05:44.870 --> 00:05:47.010
But it's actually got
some exciting properties

00:05:47.010 --> 00:05:48.710
that are really making
people on the web

00:05:48.710 --> 00:05:49.950
stand up and take notice.

00:05:49.950 --> 00:05:52.515
So first off, if you compress
this image with WebP,

00:05:52.515 --> 00:05:55.140
it actually drops to about 198k.

00:05:55.140 --> 00:05:57.350
That's 100 kilobytes
of data difference

00:05:57.350 --> 00:05:58.565
for this single image.

00:05:58.565 --> 00:06:00.190
In addition to that,
WebP will actually

00:06:00.190 --> 00:06:02.580
allow you to get
transparency and even

00:06:02.580 --> 00:06:06.220
support some forms of animation,
which means this single image

00:06:06.220 --> 00:06:08.850
format gets you the
compression sizes of JPEG,

00:06:08.850 --> 00:06:11.170
the transparency
properties of PNG,

00:06:11.170 --> 00:06:14.580
as well as the animation
properties of GIFs.

00:06:14.580 --> 00:06:17.050
It's an exciting,
one stop solution

00:06:17.050 --> 00:06:19.140
for a lot of your image needs.

00:06:19.140 --> 00:06:22.320
Now, this is the important
thing to note about this image

00:06:22.320 --> 00:06:24.240
size, and about
this image format,

00:06:24.240 --> 00:06:27.250
is that most mobile phones
right now are actually

00:06:27.250 --> 00:06:29.700
about five megapixels
for their cameras,

00:06:29.700 --> 00:06:32.440
which means people are taking
snapshots of the food they eat

00:06:32.440 --> 00:06:34.120
or signs on the
street or their kids,

00:06:34.120 --> 00:06:36.370
uploading them to their
favorite social media network,

00:06:36.370 --> 00:06:39.580
and that's actually coming in
at five megapixels uncompressed.

00:06:39.580 --> 00:06:42.330
Now, if we look at PNG and
JPEG and the compression ratios

00:06:42.330 --> 00:06:44.030
they got with a
single megabyte image,

00:06:44.030 --> 00:06:47.280
you can see that over time,
as the number of images that

00:06:47.280 --> 00:06:49.290
fill the internet
increases, we're

00:06:49.290 --> 00:06:51.960
going to quickly run into
compression and data size

00:06:51.960 --> 00:06:52.650
issues.

00:06:52.650 --> 00:06:54.770
That's why WebP shows
up, and it actually

00:06:54.770 --> 00:06:56.930
solves a huge problem
for us and allows

00:06:56.930 --> 00:06:58.230
us to address a lot of issues.

00:06:58.230 --> 00:07:01.870
Now, there's a whole separate
series of GDLs done on WebP.

00:07:01.870 --> 00:07:03.620
I encourage you to go
to the GDL website,

00:07:03.620 --> 00:07:04.880
take a look at
some of these talks

00:07:04.880 --> 00:07:06.760
for more information on how
to get started with that.

00:07:06.760 --> 00:07:08.593
Now, that's all I'm
going to say about image

00:07:08.593 --> 00:07:11.090
compression for now.

00:07:11.090 --> 00:07:13.070
But in the meantime,
let's talk about text.

00:07:13.070 --> 00:07:17.680
So I said before that text
data-- CSS, HTML, JavaScript--

00:07:17.680 --> 00:07:20.830
actually drive the rendering
performance and initial page

00:07:20.830 --> 00:07:24.220
load of your page
more than images do.

00:07:24.220 --> 00:07:25.694
This is for a very
specific reason.

00:07:25.694 --> 00:07:27.110
So if you look at
this graph here,

00:07:27.110 --> 00:07:30.380
we actually can't start
rendering anything on your page

00:07:30.380 --> 00:07:34.520
until the HTML has been parsed
and subsequent JavaScript

00:07:34.520 --> 00:07:39.010
and CSS may be loaded to create
both the DOM and the CSS DOM

00:07:39.010 --> 00:07:40.890
so that we can actually
parse the render tree

00:07:40.890 --> 00:07:43.070
and actually start getting
pixels on the screen.

00:07:43.070 --> 00:07:46.395
So if we look at this average
flow of events from HTML

00:07:46.395 --> 00:07:48.710
to CSS to actually
getting pixels there,

00:07:48.710 --> 00:07:50.650
we can actually see an
interesting example.

00:07:50.650 --> 00:07:53.780
So we have a standard HTML,
which actually links to a CSS.

00:07:53.780 --> 00:07:56.180
That CSS is updating
some of the information

00:07:56.180 --> 00:07:57.660
on the page, the text data.

00:07:57.660 --> 00:07:59.340
We also have an image
that's referenced

00:07:59.340 --> 00:08:01.780
and a script that's
at the bottom of it.

00:08:01.780 --> 00:08:04.690
Now you can see on the
mobile image on the side

00:08:04.690 --> 00:08:07.200
there that nothing
has been shown yet,

00:08:07.200 --> 00:08:09.830
even though this
HTML has been parsed.

00:08:09.830 --> 00:08:12.120
So we've got our little HTML
box that's mostly orange.

00:08:12.120 --> 00:08:14.170
You can see that the DOM
has mostly been parsed.

00:08:14.170 --> 00:08:15.200
It's partially orange.

00:08:15.200 --> 00:08:17.920
And that the CSS
file-- example.css--

00:08:17.920 --> 00:08:21.620
has been discovered, but it
hasn't really been loaded yet.

00:08:21.620 --> 00:08:23.760
So this means that we
actually cannot display text

00:08:23.760 --> 00:08:26.510
on the screen because we don't
have the styling properties

00:08:26.510 --> 00:08:29.080
that define that
text on the screen.

00:08:29.080 --> 00:08:31.130
It's not until the
CSS data actually

00:08:31.130 --> 00:08:34.460
gets loaded that we can build
the CSS version of the DOM

00:08:34.460 --> 00:08:36.650
and actually begin
constructing the render tree.

00:08:36.650 --> 00:08:39.510
Now notice that the DOM still
hasn't finished loading yet.

00:08:39.510 --> 00:08:40.730
That's OK.

00:08:40.730 --> 00:08:43.510
If we can actually partially
load and partially display

00:08:43.510 --> 00:08:46.680
the top part of the DOM, or
what people typically say

00:08:46.680 --> 00:08:49.440
is above the fold, that allows
us to actually start rendering

00:08:49.440 --> 00:08:51.980
pixels on the screen while
the bottom part of the page

00:08:51.980 --> 00:08:52.904
is still loading.

00:08:52.904 --> 00:08:54.320
So now that the
CSS has loaded, we

00:08:54.320 --> 00:08:56.340
can actually get some
text on the screen.

00:08:56.340 --> 00:08:58.670
Now notice that layout
and paint are halfway

00:08:58.670 --> 00:09:01.140
through their process because
they don't have all the data.

00:09:01.140 --> 00:09:03.530
Loading up next should be
WebP, and then of course,

00:09:03.530 --> 00:09:06.050
last.js is defined
there as well.

00:09:06.050 --> 00:09:08.825
Now, because of the
file sizes, last.js

00:09:08.825 --> 00:09:11.430
is smaller than the image,
and so it can actually

00:09:11.430 --> 00:09:13.880
get loaded and parsed
before the image

00:09:13.880 --> 00:09:15.720
data actually gets
to the screen.

00:09:15.720 --> 00:09:18.767
Now, if last.js kicks
off some style chains

00:09:18.767 --> 00:09:20.850
or some other information
that needs to be loaded,

00:09:20.850 --> 00:09:22.580
it can go back
and change the DOM

00:09:22.580 --> 00:09:25.600
and change the CSS properties,
forcing page reflows,

00:09:25.600 --> 00:09:28.330
forcing page repaints, and other
sorts of chaos in your loading

00:09:28.330 --> 00:09:28.830
time.

00:09:28.830 --> 00:09:31.550
But notice the image still
hasn't been loaded yet

00:09:31.550 --> 00:09:33.380
and hasn't been
displayed on screen.

00:09:33.380 --> 00:09:35.680
It's not until the final
complete bits come off

00:09:35.680 --> 00:09:38.140
the wire, the CSS
DOM can be finished,

00:09:38.140 --> 00:09:39.900
the render tree can
actually be completed,

00:09:39.900 --> 00:09:41.650
and finally, we can
actually get the image

00:09:41.650 --> 00:09:43.487
on the screen showing
you what we're

00:09:43.487 --> 00:09:45.070
trying to show you
in the first place.

00:09:45.070 --> 00:09:46.611
Now, what you should
gather from this

00:09:46.611 --> 00:09:49.800
is that while images make up the
bulk of the content on the web,

00:09:49.800 --> 00:09:52.350
it's really the
textual base data that

00:09:52.350 --> 00:09:56.204
drives how and when
pixels get to your screen.

00:09:56.204 --> 00:09:57.620
And when you're
trying to optimize

00:09:57.620 --> 00:10:01.637
for fast path and critical path
rendering on mobile devices,

00:10:01.637 --> 00:10:03.053
it's really the
text data you need

00:10:03.053 --> 00:10:06.550
to get off the wire
as fast as possible.

00:10:06.550 --> 00:10:08.055
Now, a lot of you
kids out there are

00:10:08.055 --> 00:10:09.680
talking about some
really cool internet

00:10:09.680 --> 00:10:11.080
technology called Emscripten.

00:10:11.080 --> 00:10:14.430
Now, this is a library has
been developed open source that

00:10:14.430 --> 00:10:18.130
allows developers to take
existing C and C++ code

00:10:18.130 --> 00:10:21.150
and trans-compile that
to JavaScript data.

00:10:21.150 --> 00:10:23.010
Probably one of the
most impressive examples

00:10:23.010 --> 00:10:25.840
of this technology was debuted
a little bit earlier this year,

00:10:25.840 --> 00:10:28.380
sometime in March at the game
developer conference, where

00:10:28.380 --> 00:10:30.820
Nvidia and Mozilla and a
bunch of the open source

00:10:30.820 --> 00:10:34.850
community at Emscripten actually
debuted the Unreal 3 graphics

00:10:34.850 --> 00:10:38.540
engine running on
top of Emscripten.

00:10:38.540 --> 00:10:40.880
Now, this means that you
get this rich, full graphics

00:10:40.880 --> 00:10:42.360
engine running
inside of a browser.

00:10:42.360 --> 00:10:43.901
It runs inside of
JavaScript, so it's

00:10:43.901 --> 00:10:45.780
spec compliant and all
this other fun stuff.

00:10:45.780 --> 00:10:47.695
Now, I'm from the
games industry,

00:10:47.695 --> 00:10:50.070
and I've been working with
Unreal 3 for a number of years

00:10:50.070 --> 00:10:50.410
now.

00:10:50.410 --> 00:10:52.090
And one of the things
Unreal has never

00:10:52.090 --> 00:10:55.600
been popular for is
being small in size.

00:10:55.600 --> 00:10:57.910
You can actually see that
when we move the source

00:10:57.910 --> 00:11:00.320
code and the compiled
data to the web,

00:11:00.320 --> 00:11:02.300
this problem still consists.

00:11:02.300 --> 00:11:06.480
So if you look at the loading
time of this application,

00:11:06.480 --> 00:11:08.360
you can actually see
that the core JavaScript

00:11:08.360 --> 00:11:11.610
file for this Emscripten-based
port of Unreal

00:11:11.610 --> 00:11:14.120
is about 50 megabytes of data.

00:11:14.120 --> 00:11:16.810
50 megabytes to bring
down a JavaScript

00:11:16.810 --> 00:11:18.980
file to actually load this game.

00:11:18.980 --> 00:11:21.080
Now to be fair, it
is served compressed,

00:11:21.080 --> 00:11:22.496
which means that
you actually only

00:11:22.496 --> 00:11:24.350
have to pull down
five megabytes of data

00:11:24.350 --> 00:11:27.780
across the wire for this
single JavaScript file.

00:11:27.780 --> 00:11:29.040
But still, five megabytes.

00:11:29.040 --> 00:11:31.360
That's five times larger than
any of our other websites

00:11:31.360 --> 00:11:31.860
out there.

00:11:31.860 --> 00:11:34.410
And by the way, this isn't
counting the 18 megabytes

00:11:34.410 --> 00:11:37.540
worth of data that has to
be pulled down as well.

00:11:37.540 --> 00:11:39.670
So when you look at these
trends-- and the more

00:11:39.670 --> 00:11:42.160
developers that are trying to
move towards high performance

00:11:42.160 --> 00:11:45.000
JavaScript execution using
things like Emscripten

00:11:45.000 --> 00:11:48.170
and asm.js-- what you
start seeing is a trend.

00:11:48.170 --> 00:11:51.370
The more web applications
that produce source code

00:11:51.370 --> 00:11:53.540
in JavaScript that come
from other languages,

00:11:53.540 --> 00:11:57.041
we'll start seeing bloated
and larger and larger

00:11:57.041 --> 00:11:59.540
JavaScript files that need to
be downloaded by their clients

00:11:59.540 --> 00:12:01.510
before pixels can get
on the screen and a game

00:12:01.510 --> 00:12:02.270
can be played.

00:12:02.270 --> 00:12:05.400
This trend is not going
to diminish over time.

00:12:05.400 --> 00:12:07.640
This means that our
text data is going

00:12:07.640 --> 00:12:09.870
to continue growing larger.

00:12:09.870 --> 00:12:12.230
As such, as a
developer, it's your job

00:12:12.230 --> 00:12:14.660
to figure out how to
minimize, compress, and reduce

00:12:14.660 --> 00:12:17.390
the number of bits on the wire.

00:12:17.390 --> 00:12:19.410
And so with that,
let's take a look

00:12:19.410 --> 00:12:21.876
at the good old
boy known as GZIP,

00:12:21.876 --> 00:12:24.000
sort of the backbone of
compression on the internet

00:12:24.000 --> 00:12:24.940
today.

00:12:24.940 --> 00:12:29.570
Now, I want to do a quick side
to note that compression is not

00:12:29.570 --> 00:12:32.830
the same as minimization
or minification,

00:12:32.830 --> 00:12:34.880
depending on how purist
you are on the term.

00:12:34.880 --> 00:12:36.960
So let's take a look at
the two really quick.

00:12:36.960 --> 00:12:40.010
Minification in the
web platform is the art

00:12:40.010 --> 00:12:43.880
of removing redundant
information from textual data

00:12:43.880 --> 00:12:48.510
such that we can still
symbolically parse and process

00:12:48.510 --> 00:12:51.241
it when we pass it off
to the underlying system.

00:12:51.241 --> 00:12:52.740
So if you look at
this example here,

00:12:52.740 --> 00:12:54.780
we've got a function
that adds two numbers,

00:12:54.780 --> 00:12:57.610
but there seems like
there's a lot of information

00:12:57.610 --> 00:12:58.460
that you don't see.

00:12:58.460 --> 00:13:01.930
There's line returns, there's
extra spacing information,

00:13:01.930 --> 00:13:04.610
perhaps the variable names
are actually too long.

00:13:04.610 --> 00:13:07.710
The process of minification
actually reduces all of this

00:13:07.710 --> 00:13:12.610
to give you the least number of
completely processing and valid

00:13:12.610 --> 00:13:15.040
bytes for this file
representation.

00:13:15.040 --> 00:13:17.290
Again here, we can
process this ahead of time

00:13:17.290 --> 00:13:19.850
and actually pass this right
to the underlying systems

00:13:19.850 --> 00:13:22.781
to be processed and actually
get stuff on your screen.

00:13:22.781 --> 00:13:24.280
Now, compression,
on the other hand,

00:13:24.280 --> 00:13:26.060
offers something
completely different.

00:13:26.060 --> 00:13:29.240
Compression is the act of
modifying your data so that it

00:13:29.240 --> 00:13:32.770
has to be reconstructed before
being passed to the underlying

00:13:32.770 --> 00:13:33.600
processing system.

00:13:33.600 --> 00:13:35.570
So again, if we take
the minimized form

00:13:35.570 --> 00:13:37.620
of that some
function, compression

00:13:37.620 --> 00:13:40.900
will actually turn it into
a bit stream of information

00:13:40.900 --> 00:13:44.360
that then has to be deflated,
or actually decompressed

00:13:44.360 --> 00:13:46.280
into the original form
before it can actually

00:13:46.280 --> 00:13:48.260
be passed off to the
underlying systems.

00:13:48.260 --> 00:13:51.480
These two technologies
work as a one-two punch.

00:13:51.480 --> 00:13:54.600
So you actually have to add
minification to your technology

00:13:54.600 --> 00:13:57.020
before you allow GZIP-ing
to occur in order

00:13:57.020 --> 00:13:58.851
to get the fewest
bytes on the wire.

00:13:58.851 --> 00:14:00.350
Now for the rest
of this talk, we're

00:14:00.350 --> 00:14:03.410
actually going to be addressing
some of the issues and pros

00:14:03.410 --> 00:14:05.050
and cons of GZIP,
but before we do

00:14:05.050 --> 00:14:07.174
that, I need to make sure
that we understand what's

00:14:07.174 --> 00:14:09.060
going on under the
hood of this algorithm.

00:14:09.060 --> 00:14:11.450
You see, GZIP is a
compression format

00:14:11.450 --> 00:14:13.880
that actually uses two
interplaying technologies

00:14:13.880 --> 00:14:14.760
together.

00:14:14.760 --> 00:14:17.646
The first one is a
technology known as LZ77.

00:14:17.646 --> 00:14:20.830
Now, this is a
dictionary-based transform

00:14:20.830 --> 00:14:23.130
that will actually take
a given data stream

00:14:23.130 --> 00:14:26.170
and convert it into
a sequence of tuples.

00:14:26.170 --> 00:14:30.350
Now, in each tuple, we actually
have a position and a length

00:14:30.350 --> 00:14:30.850
value.

00:14:30.850 --> 00:14:32.330
So I know that's confusing.

00:14:32.330 --> 00:14:34.220
Let's take a look at
a little example here.

00:14:34.220 --> 00:14:35.761
So we've got this
string up top which

00:14:35.761 --> 00:14:37.570
is comprised of
various A, B's, and C's

00:14:37.570 --> 00:14:39.080
in some sort of random order.

00:14:39.080 --> 00:14:41.420
What happens is if we
start parsing this string,

00:14:41.420 --> 00:14:43.870
we're going to look at
each symbol individually

00:14:43.870 --> 00:14:47.220
and we're going to do a
scan backwards to find out

00:14:47.220 --> 00:14:51.000
when previously we've
seen this symbol before,

00:14:51.000 --> 00:14:52.930
because then that allows
us to actually encode

00:14:52.930 --> 00:14:56.470
this symbol rather as a
single piece of information,

00:14:56.470 --> 00:14:59.180
but actually as a relative
piece of information.

00:14:59.180 --> 00:15:01.620
The goal here in
LZ77 is actually

00:15:01.620 --> 00:15:04.464
to create a lot of
redundant types of tuples

00:15:04.464 --> 00:15:06.380
that we can then compress
a little bit better.

00:15:06.380 --> 00:15:08.560
So let's walk through
this just a little bit

00:15:08.560 --> 00:15:10.170
so you can see what's going on.

00:15:10.170 --> 00:15:12.330
So you see if we start
with the first A here

00:15:12.330 --> 00:15:13.830
and we actually
parse it, well, it's

00:15:13.830 --> 00:15:15.871
the beginning of the
string, so we haven't really

00:15:15.871 --> 00:15:17.219
seen anything before yet.

00:15:17.219 --> 00:15:18.760
Therefore, we actually
have to output

00:15:18.760 --> 00:15:20.540
a position and length of 0, 0.

00:15:20.540 --> 00:15:22.150
It represents that
we're not actually

00:15:22.150 --> 00:15:23.690
seeing any other characters.

00:15:23.690 --> 00:15:25.730
We're just looking
at the A itself.

00:15:25.730 --> 00:15:29.380
Now, when we get to the second
A, we start our backwards scan

00:15:29.380 --> 00:15:31.670
and we find that the
first A we encounter

00:15:31.670 --> 00:15:35.290
was actually one symbol ago
at a length of one symbol.

00:15:35.290 --> 00:15:37.880
So this means we can actually
output the tuple, 1, 1.

00:15:37.880 --> 00:15:39.620
Now B, we haven't
seen any B's before,

00:15:39.620 --> 00:15:41.150
so we have to output 0, 0.

00:15:41.150 --> 00:15:43.240
And C, same thing, 0, 0.

00:15:43.240 --> 00:15:45.126
But now we've reached
another B. So it's

00:15:45.126 --> 00:15:46.500
going to start
scanning backwards

00:15:46.500 --> 00:15:49.430
and actually finds that the last
B we encountered in the stream

00:15:49.430 --> 00:15:51.000
was two symbols ago.

00:15:51.000 --> 00:15:52.585
And again, we only
want length of one.

00:15:52.585 --> 00:15:54.960
Now, the important and probably
the more interesting part

00:15:54.960 --> 00:15:56.590
of this particular
example is when

00:15:56.590 --> 00:16:00.010
we get to the end of the string
where we see A, B, C. Now,

00:16:00.010 --> 00:16:01.900
when we scan backwards
from A, B, C,

00:16:01.900 --> 00:16:05.040
we can actually find that we
found this exact three symbol

00:16:05.040 --> 00:16:08.490
value previously in
our stream, and exactly

00:16:08.490 --> 00:16:11.280
about five character
positions back.

00:16:11.280 --> 00:16:14.140
So this means we're going
to output the tuple of 5, 3.

00:16:14.140 --> 00:16:16.450
So we output a tuple rather
than the three characters

00:16:16.450 --> 00:16:17.010
themselves.

00:16:17.010 --> 00:16:21.590
Now again, the point of
LZ77 is to actually create

00:16:21.590 --> 00:16:24.930
duplicate and high
redundancy tuples.

00:16:24.930 --> 00:16:26.750
These tuples and the
redundancy that we

00:16:26.750 --> 00:16:29.300
create with them are actually
very important to pass off

00:16:29.300 --> 00:16:31.630
to the next step of
the GZIP algorithm

00:16:31.630 --> 00:16:33.520
known as Huffman compression.

00:16:33.520 --> 00:16:36.020
Now, for those of you who don't
remember Huffman compression

00:16:36.020 --> 00:16:38.510
or probably have blocked
it out of your mind back

00:16:38.510 --> 00:16:41.310
from the Computer Science
101 days at your university,

00:16:41.310 --> 00:16:46.710
Huffman works by assigning
variable length bit codes

00:16:46.710 --> 00:16:50.530
to symbols in your stream
based on probability.

00:16:50.530 --> 00:16:53.540
The idea here is that the more
probable and more frequent

00:16:53.540 --> 00:16:56.300
a symbol is in your
stream, the least bits

00:16:56.300 --> 00:16:57.820
you should use to represent it.

00:16:57.820 --> 00:17:01.710
A perfect real world example of
Huffman encoding is Morse code.

00:17:01.710 --> 00:17:03.590
In the American
language, the letter E

00:17:03.590 --> 00:17:05.490
is the most dominant
of all our words.

00:17:05.490 --> 00:17:07.980
Therefore, it's
assigned a single beep

00:17:07.980 --> 00:17:09.880
to represent its value.

00:17:09.880 --> 00:17:11.818
Huffman compression
works in a similar way.

00:17:11.818 --> 00:17:14.109
Now, we're going to spare
you the knowledge of building

00:17:14.109 --> 00:17:15.567
a Huffman tree and
everything else.

00:17:15.567 --> 00:17:17.520
This is in tons of
data structures books.

00:17:17.520 --> 00:17:19.050
Instead, we'll just
show you that we

00:17:19.050 --> 00:17:23.230
can see after we parse our
newly tokenized tuple string,

00:17:23.230 --> 00:17:26.558
you can see that 0, 0 is
the most dominant tuple set.

00:17:26.558 --> 00:17:28.099
And of course, we
can assign that one

00:17:28.099 --> 00:17:30.210
bit, being a single zero.

00:17:30.210 --> 00:17:32.350
Therefore, the next
set is actually 1, 1.

00:17:32.350 --> 00:17:33.517
We have two symbols of that.

00:17:33.517 --> 00:17:35.474
And because of the way
our tree is constructed,

00:17:35.474 --> 00:17:37.890
we actually have to provide
that with two bit symbols.

00:17:37.890 --> 00:17:40.600
We continue on and continue
on and effectively keep

00:17:40.600 --> 00:17:44.240
assigning variable bit
code words to symbols,

00:17:44.240 --> 00:17:46.240
creating a compressed
version of our data.

00:17:46.240 --> 00:17:49.150
Now, these two characters
or these two algorithms

00:17:49.150 --> 00:17:51.300
operating back and
forth with each other

00:17:51.300 --> 00:17:53.770
actually have a very
beautiful ballet

00:17:53.770 --> 00:17:55.350
in the way that
content is created

00:17:55.350 --> 00:17:57.750
and how it's compressed with
the statistical encoding.

00:17:57.750 --> 00:17:59.390
This is the cornerstone
of everything

00:17:59.390 --> 00:18:02.660
we're going to talk about for
the rest of this conversation.

00:18:02.660 --> 00:18:04.630
Now, it's worth pointing
out that GZIP is not

00:18:04.630 --> 00:18:06.160
the only kid on the block.

00:18:06.160 --> 00:18:10.648
In fact, GZIP is about 20
something years old now.

00:18:10.648 --> 00:18:12.022
If it were a real
human being, it

00:18:12.022 --> 00:18:13.700
would be able to
drink and drive and do

00:18:13.700 --> 00:18:16.100
all other sorts of fun
stuff on the internet.

00:18:16.100 --> 00:18:18.190
But other compression
algorithms which

00:18:18.190 --> 00:18:20.690
may be a little bit newer
and a little bit different

00:18:20.690 --> 00:18:22.773
can actually give you some
interesting trade-offs.

00:18:22.773 --> 00:18:25.140
So here are four of the more
popular encoders out there.

00:18:25.140 --> 00:18:27.220
The first is one known as LZMA.

00:18:27.220 --> 00:18:28.930
Some of you may know
this more popularly

00:18:28.930 --> 00:18:32.000
as 7-Zip, a very
popular compression

00:18:32.000 --> 00:18:33.540
archive tool out there.

00:18:33.540 --> 00:18:35.860
You can see that LZMA
actually gives you

00:18:35.860 --> 00:18:38.040
smaller compression than GZIP.

00:18:38.040 --> 00:18:40.960
This is actually due to
a lot of higher order

00:18:40.960 --> 00:18:44.290
searching and finding algorithms
built around that LZ77

00:18:44.290 --> 00:18:45.230
algorithm.

00:18:45.230 --> 00:18:48.310
In fact, LZMA can actually be
considered a distant cousin

00:18:48.310 --> 00:18:50.950
to GZIP because it's actually
very similar in the way

00:18:50.950 --> 00:18:52.410
that it compresses its data.

00:18:52.410 --> 00:18:54.680
However, because it uses
more modern heuristics

00:18:54.680 --> 00:18:57.150
and algorithms, it can
actually get better results.

00:18:57.150 --> 00:18:59.960
Now, below LZMA, you see LPAQ.

00:18:59.960 --> 00:19:03.210
This is a context
mixing based encoder.

00:19:03.210 --> 00:19:05.630
It's effectively
mostly a neural net

00:19:05.630 --> 00:19:08.320
in practice of how
it matches symbols.

00:19:08.320 --> 00:19:09.900
Now notice LPAQ
actually gives us

00:19:09.900 --> 00:19:12.760
the smallest file size
compared to everything else

00:19:12.760 --> 00:19:15.940
at 0.35 megabytes.

00:19:15.940 --> 00:19:18.209
I'll get into that a little
bit more in a second.

00:19:18.209 --> 00:19:20.500
Now, the final compressor we
look at is actually BZIP2.

00:19:20.500 --> 00:19:25.160
Now, BZIP is a modern variant
of the Burrows-Wheeler transform

00:19:25.160 --> 00:19:27.810
assigned also with a
move to front transform

00:19:27.810 --> 00:19:30.270
and either a Huffman
or an arithmetic

00:19:30.270 --> 00:19:31.580
encoder on the back end.

00:19:31.580 --> 00:19:34.980
Now, BZIP2 fundamentally changes
the way that compression works.

00:19:34.980 --> 00:19:35.950
It's very different.

00:19:35.950 --> 00:19:37.530
Doesn't use LZ77 at all.

00:19:37.530 --> 00:19:40.430
Instead, it uses
a semi-block based

00:19:40.430 --> 00:19:42.640
sorting transform
to actually increase

00:19:42.640 --> 00:19:45.404
redundancy or adjacent
redundancy in data.

00:19:45.404 --> 00:19:47.570
This redundancy allows it
to get better compression.

00:19:47.570 --> 00:19:50.220
Again, you can see here
where GZIP actually gives us

00:19:50.220 --> 00:19:54.061
0.48 in terms of megabytes,
BZIP actually beats this.

00:19:54.061 --> 00:19:56.310
Now, what you're actually
seeing here in terms of data

00:19:56.310 --> 00:20:00.206
sizes is actually a scrape I
did of the amazon.com home page.

00:20:00.206 --> 00:20:01.580
So if I take all
of the text data

00:20:01.580 --> 00:20:05.000
from that-- HTML, CSS,
JSON, and JS data--

00:20:05.000 --> 00:20:07.315
it actually comes out
to about 1.64 megs.

00:20:07.315 --> 00:20:09.430
So you can see how
these compressors relate

00:20:09.430 --> 00:20:12.770
to that specific
memory footprint.

00:20:12.770 --> 00:20:14.680
Now, when you're
comparing compressors,

00:20:14.680 --> 00:20:17.030
the size of the data is
not the only heuristic.

00:20:17.030 --> 00:20:19.030
You have two other
heuristics that you typically

00:20:19.030 --> 00:20:20.030
throw into the mix.

00:20:20.030 --> 00:20:22.350
The first one is encoding
time and the second one

00:20:22.350 --> 00:20:25.740
is decoding time, which often is
the more important of the two.

00:20:25.740 --> 00:20:27.530
So if we look at the
encoding time column,

00:20:27.530 --> 00:20:31.100
you can see that GZIP does
probably the fastest in terms

00:20:31.100 --> 00:20:34.550
of encoding speed
at 0.79 seconds.

00:20:34.550 --> 00:20:38.460
LZMA, being a distant cousin
with more search heuristics,

00:20:38.460 --> 00:20:40.750
actually comes in
pretty close at 1.26.

00:20:40.750 --> 00:20:44.860
You can see that the wins
it gets in compression size

00:20:44.860 --> 00:20:47.810
actually come from
more preprocessing

00:20:47.810 --> 00:20:51.140
at the front of
the encoding step.

00:20:51.140 --> 00:20:53.320
Now, LPAQ gives you
amazing compression,

00:20:53.320 --> 00:20:56.620
but notice it actually takes 11
seconds to compress your data.

00:20:56.620 --> 00:20:59.070
Now again, LPAQ is
a modern wonder.

00:20:59.070 --> 00:21:01.460
It actually uses
various forms of context

00:21:01.460 --> 00:21:04.490
mixing in different
sort of combinations,

00:21:04.490 --> 00:21:07.240
again, resulting in pretty
much an artificial intelligence

00:21:07.240 --> 00:21:09.780
neural net algorithm to
find compression wins.

00:21:09.780 --> 00:21:11.410
So it makes sense
that this actually

00:21:11.410 --> 00:21:13.927
oscillates at about 11 seconds
worth of processing time

00:21:13.927 --> 00:21:16.260
because there's a lot of
things going on under the hood.

00:21:16.260 --> 00:21:19.620
Meanwhile, the Burrows-Wheeler
transform powered BZIP2

00:21:19.620 --> 00:21:21.750
doesn't stray too
far from the mark,

00:21:21.750 --> 00:21:24.235
but gives us about 2.1
seconds in encoding time.

00:21:24.235 --> 00:21:25.610
And there's a
whole set of papers

00:21:25.610 --> 00:21:28.630
that talk about how that
clustering transform actually

00:21:28.630 --> 00:21:30.560
affects memory and
processing and whatnot.

00:21:30.560 --> 00:21:33.280
Now, decoding time is probably
the most important part here.

00:21:33.280 --> 00:21:35.530
You can actually see
that GZIP and LZMA are,

00:21:35.530 --> 00:21:38.520
for all practical
purposes, identical.

00:21:38.520 --> 00:21:41.230
So while LZMA takes a
little bit longer to encode,

00:21:41.230 --> 00:21:44.410
it seems to regularly produce
smaller files than GZIP,

00:21:44.410 --> 00:21:47.340
and the decoding time tends
to be almost identical.

00:21:47.340 --> 00:21:49.540
Now, LPAQ, again, because
it's running a neural net,

00:21:49.540 --> 00:21:51.690
the decode time actually
equals almost what

00:21:51.690 --> 00:21:54.000
the encode time was, so you
look at about 11 seconds

00:21:54.000 --> 00:21:56.490
there, while BZIP
actually is a faster

00:21:56.490 --> 00:21:57.970
decode than it is an encode.

00:21:57.970 --> 00:21:59.500
The whole point of this
slide-- the only thing

00:21:59.500 --> 00:22:00.958
I want you to take
away from here--

00:22:00.958 --> 00:22:03.630
is that GZIP is really
good at a couple things,

00:22:03.630 --> 00:22:05.480
specifically encoding time.

00:22:05.480 --> 00:22:07.920
It gets beat in a couple
places, specifically

00:22:07.920 --> 00:22:12.070
with the size of the compression
that it achieves, and then ties

00:22:12.070 --> 00:22:14.020
in a couple other
places in decoding time.

00:22:14.020 --> 00:22:16.470
So it's not really the
only algorithm on the block

00:22:16.470 --> 00:22:19.620
there in terms of
compression ratios.

00:22:19.620 --> 00:22:21.370
So you can see here
that we've highlighted

00:22:21.370 --> 00:22:23.120
a couple of these
specific instances.

00:22:23.120 --> 00:22:24.820
LPAQ wins in size.

00:22:24.820 --> 00:22:26.930
Encoding time is, of
course, dominated by GZIP,

00:22:26.930 --> 00:22:30.710
and decoding time is, of
course, dominated by LZMA.

00:22:30.710 --> 00:22:34.980
Now, this can lead a lot of
people on the web tech platform

00:22:34.980 --> 00:22:37.010
to actually come at me
with burning pitchforks

00:22:37.010 --> 00:22:39.530
and actually say, hey, GZIP
is actually pretty good.

00:22:39.530 --> 00:22:41.180
That data just showed
us that we really

00:22:41.180 --> 00:22:44.030
don't need any other compression
algorithms because it gives us

00:22:44.030 --> 00:22:47.120
decent size compression,
decent time for encoding,

00:22:47.120 --> 00:22:49.207
and decent decoding time.

00:22:49.207 --> 00:22:51.290
Well, I'm here to tell you
that's a good argument,

00:22:51.290 --> 00:22:53.860
but let's dig a
little bit deeper into

00:22:53.860 --> 00:22:55.920
whether or not GZIP should
be the only thing we

00:22:55.920 --> 00:22:57.239
allow on the web.

00:22:57.239 --> 00:22:59.530
So first off, you should
understand that there's really

00:22:59.530 --> 00:23:02.320
no silver bullet
for compression.

00:23:02.320 --> 00:23:04.840
Depending on what your
data is, how frequent

00:23:04.840 --> 00:23:08.500
it is, the relationships it
has with local information,

00:23:08.500 --> 00:23:11.520
all relate to how well it
can be compressed at the end,

00:23:11.520 --> 00:23:13.500
and different compressors
will handle this data

00:23:13.500 --> 00:23:14.470
in different ways.

00:23:14.470 --> 00:23:17.830
For example, if you actually
applied JPEG style compression

00:23:17.830 --> 00:23:19.456
to text data, you're
not going to be

00:23:19.456 --> 00:23:21.330
able to decompress your
text in the right way

00:23:21.330 --> 00:23:24.380
because it's lossy in terms
of removing information

00:23:24.380 --> 00:23:26.156
bits from the stream itself.

00:23:26.156 --> 00:23:28.030
So let's take a perfect
example that actually

00:23:28.030 --> 00:23:31.230
shows to beat GZIP without
really any modification.

00:23:31.230 --> 00:23:33.480
So let's say we have a
string of integers here.

00:23:33.480 --> 00:23:35.470
Now, this string of
integers was actually

00:23:35.470 --> 00:23:40.880
created as an index system into
some higher order database.

00:23:40.880 --> 00:23:44.010
So there's basically 10
things that are being indexed,

00:23:44.010 --> 00:23:45.270
and we list them here.

00:23:45.270 --> 00:23:46.980
Now, if we just GZIP
this, we actually

00:23:46.980 --> 00:23:48.406
don't get a lot of savings.

00:23:48.406 --> 00:23:49.780
This is because,
if you remember,

00:23:49.780 --> 00:23:53.100
the LZ77 algorithm
isn't going to find

00:23:53.100 --> 00:23:55.920
any duplicate indexes
in this array.

00:23:55.920 --> 00:23:59.220
Nine is only existing once,
and so are the other numbers.

00:23:59.220 --> 00:24:03.670
Meanwhile, that means that all
of the statistical probability

00:24:03.670 --> 00:24:05.710
for these symbols is
pretty much equal.

00:24:05.710 --> 00:24:07.430
There's no difference.

00:24:07.430 --> 00:24:09.190
Nothing is more frequent
or less frequent,

00:24:09.190 --> 00:24:11.180
which means the Huffman
algorithm is going

00:24:11.180 --> 00:24:13.350
to pretty much assign
equal bit lengths to all

00:24:13.350 --> 00:24:14.560
the symbols themselves.

00:24:14.560 --> 00:24:17.490
Because of this, we really don't
get any compression savings

00:24:17.490 --> 00:24:20.430
at all from the GZIP algorithm
for this particular data

00:24:20.430 --> 00:24:21.880
stream.

00:24:21.880 --> 00:24:25.780
This means that you have
to look at your data

00:24:25.780 --> 00:24:27.790
and go, what is
GZIP doing to it?

00:24:27.790 --> 00:24:30.710
If we, however, take a
little bit different swing

00:24:30.710 --> 00:24:32.710
at the problem itself,
we can actually

00:24:32.710 --> 00:24:35.560
beat what GZIP is doing with a
little knowledge of our data.

00:24:35.560 --> 00:24:38.100
So let's say we take
the original array,

00:24:38.100 --> 00:24:40.014
and instead of just
leaving it by itself,

00:24:40.014 --> 00:24:42.180
we actually notice the
property that we can actually

00:24:42.180 --> 00:24:44.990
sort this information because
we don't need the order

00:24:44.990 --> 00:24:46.517
to come out on the backside.

00:24:46.517 --> 00:24:48.100
So if we sort this,
we actually end up

00:24:48.100 --> 00:24:51.560
with a pretty incrementing
run of numbers-- zero to nine

00:24:51.560 --> 00:24:53.955
without any gaps in the middle.

00:24:53.955 --> 00:24:56.080
We can take this string
and apply a technique known

00:24:56.080 --> 00:25:00.680
as delta encoding to actually
create a different symbol set.

00:25:00.680 --> 00:25:03.660
Now, delta encoding
works by taking a symbol

00:25:03.660 --> 00:25:05.860
and finding the
difference in the symbol

00:25:05.860 --> 00:25:08.922
between the previous symbol,
and it encodes the difference

00:25:08.922 --> 00:25:10.130
rather than the actual value.

00:25:10.130 --> 00:25:12.160
So for our specific
example here,

00:25:12.160 --> 00:25:14.700
we start with the number
zero, and the number one

00:25:14.700 --> 00:25:16.920
is one more, so we
add one to that.

00:25:16.920 --> 00:25:19.600
Two is greater than
one by a single value,

00:25:19.600 --> 00:25:21.290
so we had one, one,
one, one, one, one.

00:25:21.290 --> 00:25:22.790
And you can see
with delta encoding,

00:25:22.790 --> 00:25:26.040
all we have to do is encode
one and then eight zeros

00:25:26.040 --> 00:25:29.280
to actually represent the
original string we have.

00:25:29.280 --> 00:25:32.160
Now, the delta encoded
version, notice

00:25:32.160 --> 00:25:34.210
that it does have a lot
of duplicate symbols,

00:25:34.210 --> 00:25:36.400
and there is a
particular symbol which

00:25:36.400 --> 00:25:38.890
is more dominant than
the other symbols.

00:25:38.890 --> 00:25:43.970
Now, once we've encoded
our data into this form,

00:25:43.970 --> 00:25:45.890
GZIP actually has a field day.

00:25:45.890 --> 00:25:48.940
The LZ77 algorithm finds lots
of matches in its window,

00:25:48.940 --> 00:25:50.690
and the arithmetic
compressor can actually

00:25:50.690 --> 00:25:53.710
come through and assign
smaller bit codes

00:25:53.710 --> 00:25:55.677
to more frequent streams.

00:25:55.677 --> 00:25:57.760
What we're showing you
here is that you can't just

00:25:57.760 --> 00:25:59.790
throw arbitrary data
at GZIP and expect

00:25:59.790 --> 00:26:03.330
to get the most perfect,
amazing form of compression.

00:26:03.330 --> 00:26:05.120
Instead, a little
bit of preprocessing

00:26:05.120 --> 00:26:07.520
can actually change the
way your compression works,

00:26:07.520 --> 00:26:09.830
most of the time for the better.

00:26:09.830 --> 00:26:12.012
Now, let's look at
another perfect example

00:26:12.012 --> 00:26:13.970
of where GZIP can actually
cause some problems.

00:26:13.970 --> 00:26:17.030
So if you look at my website, I
did a little bit of an analysis

00:26:17.030 --> 00:26:21.250
a while ago on various forms of
CSS minimization technologies.

00:26:21.250 --> 00:26:23.070
There was a
fantastic set of code

00:26:23.070 --> 00:26:25.820
out there by someone who
was using genetic algorithms

00:26:25.820 --> 00:26:29.754
to figure out optimal
ways to minify CSS data.

00:26:29.754 --> 00:26:31.420
So let's take a look
at this table here.

00:26:31.420 --> 00:26:35.000
So if we take two CSS files from
StackOverflow and Bootstrap,

00:26:35.000 --> 00:26:37.090
and we actually look at
their minified forms--

00:26:37.090 --> 00:26:42.390
so this has already been run
through Closure or YUI or Clean

00:26:42.390 --> 00:26:47.420
CSS, one of these things-- we
actually get about 90 and 97k,

00:26:47.420 --> 00:26:48.570
respectively.

00:26:48.570 --> 00:26:51.510
Now, using the genetic
algorithm version

00:26:51.510 --> 00:26:54.880
of the minifier actually
produced about 3% savings

00:26:54.880 --> 00:26:56.420
in both of these
files, which meant

00:26:56.420 --> 00:26:59.780
that we can use a different
variant of minification to get

00:26:59.780 --> 00:27:02.390
wins over what we're
typically already doing.

00:27:02.390 --> 00:27:03.880
So this is a good idea.

00:27:03.880 --> 00:27:07.290
If we find a better algorithm
that minifies our data smaller,

00:27:07.290 --> 00:27:09.030
we should be embracing that.

00:27:09.030 --> 00:27:11.550
This is where GZIP actually
causes some trouble.

00:27:11.550 --> 00:27:15.540
When we actually run these
genetically algorithmically

00:27:15.540 --> 00:27:19.340
minimized data through
the GZIP compressor,

00:27:19.340 --> 00:27:21.280
you can see the results
are actually negative.

00:27:21.280 --> 00:27:23.770
That means it's actually
growing the data.

00:27:23.770 --> 00:27:25.540
So we have our minified.GZIP.

00:27:25.540 --> 00:27:29.590
So if we just zip the data
that comes from Clean CSS

00:27:29.590 --> 00:27:31.920
or Closure, we get
about 18 and 15k.

00:27:31.920 --> 00:27:34.060
However, when we
zip the data that's

00:27:34.060 --> 00:27:36.850
been generated from
our genetic algorithm,

00:27:36.850 --> 00:27:38.780
you can see that we're
actually increasing

00:27:38.780 --> 00:27:40.580
the size of the file.

00:27:40.580 --> 00:27:43.000
Basically, what you're
seeing is that the data is so

00:27:43.000 --> 00:27:45.480
minimized coming from
the genetic algorithm

00:27:45.480 --> 00:27:47.620
that GZIP is actually
inflating the data.

00:27:47.620 --> 00:27:49.860
It's making your
file larger than it

00:27:49.860 --> 00:27:53.320
should be because it's only
using the LZ77 and Huffman

00:27:53.320 --> 00:27:54.500
encoding steps.

00:27:54.500 --> 00:27:56.450
Now, this should
scare a lot of people,

00:27:56.450 --> 00:27:59.550
hopefully, into thinking
how often this is actually

00:27:59.550 --> 00:28:02.030
occurring on the internet
for various size of files.

00:28:02.030 --> 00:28:04.180
Now, I can tell you
it's not that frequent,

00:28:04.180 --> 00:28:06.638
but it is something to keep an
eye on because it definitely

00:28:06.638 --> 00:28:08.720
shows that GZIP is
not a silver bullet

00:28:08.720 --> 00:28:10.970
and that it can actually
do harmful things in very

00:28:10.970 --> 00:28:12.842
certain circumstances.

00:28:12.842 --> 00:28:14.300
Now of course, a
lot of people here

00:28:14.300 --> 00:28:16.640
would then say, well,
this means we shouldn't

00:28:16.640 --> 00:28:19.190
be using this genetic
algorithm to minimize our CSS.

00:28:19.190 --> 00:28:20.579
Obviously, that's a flop.

00:28:20.579 --> 00:28:22.620
And I still think that
that's the wrong argument,

00:28:22.620 --> 00:28:24.530
but let's move on.

00:28:24.530 --> 00:28:26.320
Now, let's talk
about PNG for minute.

00:28:26.320 --> 00:28:28.960
Now, PNG is a format
that you can actually

00:28:28.960 --> 00:28:30.570
export in a compressed form.

00:28:30.570 --> 00:28:33.800
Now internal to PNG, the
compression algorithm it uses

00:28:33.800 --> 00:28:38.240
is called deflate, pretty much
the backbone of what GZIP uses.

00:28:38.240 --> 00:28:41.809
It's LZ77 coupled with a
Huffman compressor step.

00:28:41.809 --> 00:28:44.350
Now, what you're looking at on
the screen here is two images.

00:28:44.350 --> 00:28:47.970
Effectively, I took a 90
pixel by 90 pixel sprite,

00:28:47.970 --> 00:28:52.140
and I tiled it vertically,
creating a 90 by 270 image.

00:28:52.140 --> 00:28:55.490
Now, I resized that
image just a little bit

00:28:55.490 --> 00:28:59.380
and added two columns of
pixels, so we now have a 92

00:28:59.380 --> 00:29:02.490
by 270 image instead
of a 90 by 270.

00:29:02.490 --> 00:29:05.924
So we've barely altered
the size of the data.

00:29:05.924 --> 00:29:07.340
However, you can
see at the bottom

00:29:07.340 --> 00:29:13.340
there that the sizes post-GZIP
are drastically different.

00:29:13.340 --> 00:29:16.190
The 90 by 270
image is about 20k,

00:29:16.190 --> 00:29:19.790
while the 92 pixel image is 41k.

00:29:19.790 --> 00:29:23.650
That's twice as large for
only two columns of pixels

00:29:23.650 --> 00:29:24.720
being added to the image.

00:29:24.720 --> 00:29:26.730
And again, the
compression algorithm

00:29:26.730 --> 00:29:29.660
that's being used under the
hood here is effectively GZIP.

00:29:29.660 --> 00:29:31.560
So let's take a look at
why this is occurring

00:29:31.560 --> 00:29:33.320
and what's going on.

00:29:33.320 --> 00:29:35.550
So let's say we've got a
bunch of pixel colors here.

00:29:35.550 --> 00:29:37.014
Well remember,
the LZ77 algorithm

00:29:37.014 --> 00:29:38.430
is going to come
through, and it's

00:29:38.430 --> 00:29:39.638
going to try to find matches.

00:29:39.638 --> 00:29:42.480
So we've got a particular
green pixel here

00:29:42.480 --> 00:29:44.660
and another green
pixel that's identical

00:29:44.660 --> 00:29:47.220
to this somewhere previous
in our data stream.

00:29:47.220 --> 00:29:52.390
Now, how this works is that
LZ77 won't scan infinitely

00:29:52.390 --> 00:29:53.820
to try to find matches.

00:29:53.820 --> 00:29:56.050
Instead, it actually
operates in a window

00:29:56.050 --> 00:29:59.390
of 32k worth of information.

00:29:59.390 --> 00:30:02.050
Now, that means if I
encounter a symbol that's

00:30:02.050 --> 00:30:05.230
outside of that window, then
I'm not going to find a match.

00:30:05.230 --> 00:30:08.190
It only matches things
inside of a 32k window.

00:30:08.190 --> 00:30:10.850
So if we look at
our 90 by 90 images

00:30:10.850 --> 00:30:13.690
that we created for
our tile sheet there,

00:30:13.690 --> 00:30:17.810
90 times 90 times
4 bytes per pixel

00:30:17.810 --> 00:30:21.160
is roughly about 32k in size.

00:30:21.160 --> 00:30:23.590
Now, if we actually
look at 32 times 1,024,

00:30:23.590 --> 00:30:25.770
the real definition
of 32k, it's,

00:30:25.770 --> 00:30:29.950
again, about 300 bytes
larger than our 90 by 90.

00:30:29.950 --> 00:30:33.140
However, when we added
those two columns of pixels,

00:30:33.140 --> 00:30:36.690
we actually changed the size
and the stride of our images.

00:30:36.690 --> 00:30:41.674
90 by 92 by 4 is
actually 33k, not 32k.

00:30:41.674 --> 00:30:43.090
What this means
is that when we're

00:30:43.090 --> 00:30:46.110
trying to find duplicate
pixels in this image,

00:30:46.110 --> 00:30:50.590
we're just far enough away that
we can't find the exact pixel

00:30:50.590 --> 00:30:51.732
that we had seen before.

00:30:51.732 --> 00:30:53.190
Let's take a look
at this visually.

00:30:53.190 --> 00:30:56.670
So again, we have our two
images, the 20k and the 41k,

00:30:56.670 --> 00:30:58.540
and we can actually
create a heat map

00:30:58.540 --> 00:31:00.120
for this image representation.

00:31:00.120 --> 00:31:03.780
Now, this heat map shows
us in dark blue pixels

00:31:03.780 --> 00:31:05.580
being the ones that
are highly compressed.

00:31:05.580 --> 00:31:07.450
This is where matches
have been found.

00:31:07.450 --> 00:31:10.600
They're very small, encoded
pixels at this point.

00:31:10.600 --> 00:31:14.550
Meanwhile, reds and yellows mean
that we didn't find a match,

00:31:14.550 --> 00:31:18.030
and to encode this particular
pixel took a lot more bits.

00:31:18.030 --> 00:31:21.542
So you can see on the left hand
side, where we're 90 by 270,

00:31:21.542 --> 00:31:23.000
we actually get a
lot more matches.

00:31:23.000 --> 00:31:25.780
The dominant form of
the image is dark blue.

00:31:25.780 --> 00:31:29.280
You can see once we tiled that
image in the lower two regions,

00:31:29.280 --> 00:31:33.640
that we find an exact pixel
matching 32k pixels away.

00:31:33.640 --> 00:31:35.850
However, when we increase
the size by two columns,

00:31:35.850 --> 00:31:40.090
we've actually bumped out the
window of pixels to look at.

00:31:40.090 --> 00:31:42.620
Therefore, we're not actually
finding exact matches.

00:31:42.620 --> 00:31:44.680
In fact, we have to
look for near matches,

00:31:44.680 --> 00:31:47.500
and that's why the second
image has a lot more misses

00:31:47.500 --> 00:31:51.230
in the LZ77 cache,
resulting in worse encoding.

00:31:51.230 --> 00:31:55.460
Now, this basically shows
us that with small changes

00:31:55.460 --> 00:31:57.900
to our image data, the
GZIP algorithm actually

00:31:57.900 --> 00:31:59.960
falls completely on its face.

00:31:59.960 --> 00:32:02.516
Again, it's far from
being a silver bullet.

00:32:02.516 --> 00:32:04.890
Now, the truth is that at this
time you should be saying,

00:32:04.890 --> 00:32:05.830
well hey, it'd be
great if we could

00:32:05.830 --> 00:32:07.163
use one of the newer algorithms.

00:32:07.163 --> 00:32:10.640
But in reality,
we're stuck with GZIP

00:32:10.640 --> 00:32:12.570
for some indefinite future.

00:32:12.570 --> 00:32:13.334
Here's why.

00:32:13.334 --> 00:32:15.250
If you actually go to
the Chromium source code

00:32:15.250 --> 00:32:17.710
and look at the bug system
that's provided there,

00:32:17.710 --> 00:32:22.160
you'll find an interesting
tale of how Chrome actually

00:32:22.160 --> 00:32:25.232
adopted the BZIP2
compression format.

00:32:25.232 --> 00:32:26.940
Effectively, Chrome
added support for it,

00:32:26.940 --> 00:32:28.780
and a lot of servers--
Apache and whatnot--

00:32:28.780 --> 00:32:31.030
added support to send
out BZIP2 content.

00:32:31.030 --> 00:32:34.660
So effectively, a server creates
a BZIP2 file instead of GZIP

00:32:34.660 --> 00:32:36.720
and sends it off to servers.

00:32:36.720 --> 00:32:38.862
However, what started
happening in the wild

00:32:38.862 --> 00:32:40.320
was a little bit
interesting to us,

00:32:40.320 --> 00:32:41.861
and the results made
us actually have

00:32:41.861 --> 00:32:45.460
to remove the support
for BZIP2 from Chrome.

00:32:45.460 --> 00:32:48.120
You see, what was happening
was a lot of these middle boxes

00:32:48.120 --> 00:32:50.370
that are out there in
the wild didn't ever

00:32:50.370 --> 00:32:53.660
expect that we would do
anything besides GZIP,

00:32:53.660 --> 00:32:58.370
and they actually had hard coded
paths in there to look and say,

00:32:58.370 --> 00:33:01.030
if this is not GZIP
header compressed,

00:33:01.030 --> 00:33:02.880
compress it with GZIP.

00:33:02.880 --> 00:33:05.460
So effectively, what occurred
was some servers, when

00:33:05.460 --> 00:33:09.120
they received the BZIP2
archive or the BZIP2 header,

00:33:09.120 --> 00:33:11.120
and these middle boxes
would look at it and say,

00:33:11.120 --> 00:33:12.500
hey, this isn't GZIP.

00:33:12.500 --> 00:33:14.840
Strip the header, try
to GZIP the information,

00:33:14.840 --> 00:33:15.860
and then send it out.

00:33:15.860 --> 00:33:19.620
This means Chrome would
actually receive a BZIP2 data

00:33:19.620 --> 00:33:21.640
package whose header
had been stripped

00:33:21.640 --> 00:33:23.270
and then it had been re-gzipped.

00:33:23.270 --> 00:33:25.940
So we would have no idea what
the actual format of the data

00:33:25.940 --> 00:33:27.240
is that we're receiving.

00:33:27.240 --> 00:33:29.115
It could be binary data,
or it could actually

00:33:29.115 --> 00:33:30.100
be valid information.

00:33:30.100 --> 00:33:32.760
The problem was this was
so systemic out there

00:33:32.760 --> 00:33:35.342
in the wild for various types
of firmware and middle boxes

00:33:35.342 --> 00:33:37.050
that it was actually
too difficult for us

00:33:37.050 --> 00:33:40.030
to fix on the fly, which
means it was a smarter idea

00:33:40.030 --> 00:33:41.879
to actually just remove
this from Chrome.

00:33:41.879 --> 00:33:43.670
So when you start
talking about the ability

00:33:43.670 --> 00:33:46.960
to add other compression
algorithms to the base browser,

00:33:46.960 --> 00:33:48.760
you're going to run
into the same problem.

00:33:48.760 --> 00:33:50.290
There's a lot of
systems out there

00:33:50.290 --> 00:33:52.640
who just don't
understand and haven't

00:33:52.640 --> 00:33:55.520
been updated to expand to
different sorts of compression

00:33:55.520 --> 00:33:57.740
technologies.

00:33:57.740 --> 00:34:00.320
So this gets us to an
interesting idea of well,

00:34:00.320 --> 00:34:04.060
if we're stuck with GZIP, but
size of text data is a problem,

00:34:04.060 --> 00:34:06.320
how do we actually create
smaller GZIP files?

00:34:06.320 --> 00:34:07.695
Well lucky for
you, I've actually

00:34:07.695 --> 00:34:10.179
spent the past three or four
months focusing explicitly

00:34:10.179 --> 00:34:11.720
on this problem,
and you can actually

00:34:11.720 --> 00:34:14.520
see a lot of the results on my
blog at mainroach.blogspot.com.

00:34:14.520 --> 00:34:18.340
But first, let's dive into
a couple of these ideas,

00:34:18.340 --> 00:34:19.870
and then you can
go there and read

00:34:19.870 --> 00:34:21.880
about all the awesome things.

00:34:21.880 --> 00:34:26.050
So first off, we can actually
generate better GZIP files.

00:34:26.050 --> 00:34:27.920
A lot of people don't
understand this,

00:34:27.920 --> 00:34:31.130
but when your server creates
your GZIP file, a lot of them

00:34:31.130 --> 00:34:33.880
are only tuned to compress
it at a factor of six.

00:34:40.230 --> 00:34:42.230
The command line parameter
that you pass to GZIP

00:34:42.230 --> 00:34:45.340
allows you to pass in
between zero and nine,

00:34:45.340 --> 00:34:47.511
where most of the servers
are default set to six.

00:34:47.511 --> 00:34:49.219
This seems to be an
interesting trade-off

00:34:49.219 --> 00:34:52.730
between compression speed
and compression size.

00:34:52.730 --> 00:34:55.230
Now, this is mostly
because web developers

00:34:55.230 --> 00:34:57.950
tend to just upload raw
assets to the server,

00:34:57.950 --> 00:34:59.590
not wanting to deal
with compression.

00:34:59.590 --> 00:35:01.673
The server actually is
responsible for compressing

00:35:01.673 --> 00:35:04.192
the content and then
sending it off to requests.

00:35:04.192 --> 00:35:05.650
Now, what's
interesting about this,

00:35:05.650 --> 00:35:08.080
though, is that you
can modify your files

00:35:08.080 --> 00:35:10.790
and actually GZIP them offline.

00:35:10.790 --> 00:35:12.570
So effectively,
part of your build

00:35:12.570 --> 00:35:15.040
step would be to take
your text information,

00:35:15.040 --> 00:35:18.262
GZIP it with some better
compressor to produce a smaller

00:35:18.262 --> 00:35:20.095
GZIP file, and then
actually tell the server

00:35:20.095 --> 00:35:22.180
to just pass that data through.

00:35:22.180 --> 00:35:25.100
There's two applications out
there that could actually

00:35:25.100 --> 00:35:27.510
produce this type of
smaller GZIP file.

00:35:27.510 --> 00:35:28.870
The first one is 7-Zip.

00:35:28.870 --> 00:35:32.190
This is actually a
very nice command line

00:35:32.190 --> 00:35:35.240
tool which can produce BZIP2
archives as well as GZIP

00:35:35.240 --> 00:35:38.160
archives, but it does
so taking advantage

00:35:38.160 --> 00:35:41.500
of the more modern,
more powerful searching

00:35:41.500 --> 00:35:44.330
and dictionary based
compression algorithms

00:35:44.330 --> 00:35:45.560
that it's going to be using.

00:35:45.560 --> 00:35:49.060
So 7-Zip can actually produce
smaller versions of GZIP files

00:35:49.060 --> 00:35:51.510
than the standard command
line GZIP archiver

00:35:51.510 --> 00:35:54.200
that ships with most
firmware in servers.

00:35:54.200 --> 00:35:56.582
Another fantastic tool out
there is one called Zopfli.

00:35:56.582 --> 00:35:58.540
Now, this was actually
created by the engineers

00:35:58.540 --> 00:36:01.060
here at Google to solve
this same problem.

00:36:01.060 --> 00:36:04.670
Now, Zopfli uses a lot more
memory and a lot more advanced

00:36:04.670 --> 00:36:08.800
algorithms than even 7-Zip to
find better matches in smaller

00:36:08.800 --> 00:36:12.524
spaces to produce smaller
GZIP files as well.

00:36:12.524 --> 00:36:13.690
It's an open source project.

00:36:13.690 --> 00:36:14.690
You can go check it out.

00:36:14.690 --> 00:36:16.720
I highly recommend it
if you're interested.

00:36:16.720 --> 00:36:19.250
So the cool thing is we can
use either 7-Zip or Zopfli

00:36:19.250 --> 00:36:21.940
as part of your build process to
actually generate smaller text

00:36:21.940 --> 00:36:24.770
files and have them sent
along on behalf of the user.

00:36:24.770 --> 00:36:26.400
And again, these are
GZIPs, so they're

00:36:26.400 --> 00:36:28.400
going to be accepted by
the middle boxes as well

00:36:28.400 --> 00:36:29.450
as the browsers.

00:36:29.450 --> 00:36:32.310
Now, if you're wondering
how these preprocessing

00:36:32.310 --> 00:36:35.030
systems actually fare
against the standard GZIP,

00:36:35.030 --> 00:36:36.695
here's a great graph to look at.

00:36:36.695 --> 00:36:38.320
So you can actually
see the blue column

00:36:38.320 --> 00:36:41.480
across the bunch of files is
actually the standard GZIP

00:36:41.480 --> 00:36:42.290
algorithm.

00:36:42.290 --> 00:36:45.430
The red one is actually
Zopfli, and the green one

00:36:45.430 --> 00:36:46.870
is actually 7-Zip.

00:36:46.870 --> 00:36:50.740
So you can see
across these 42 files

00:36:50.740 --> 00:36:54.690
that on average, in fact,
most of the time, both Zopfli

00:36:54.690 --> 00:36:59.990
and 7-Zip regularly beat GZIP
by somewhere between 2% to 10%.

00:36:59.990 --> 00:37:02.220
And if you actually
increase your parameters

00:37:02.220 --> 00:37:05.166
and allow Zopfli and 7-Zip to
spend more time doing matching

00:37:05.166 --> 00:37:06.790
and doing compression,
you can actually

00:37:06.790 --> 00:37:09.770
get it into the 15%
ratio, which is fantastic.

00:37:09.770 --> 00:37:12.390
Now, the cost of this,
though, is enormous.

00:37:12.390 --> 00:37:15.660
A lot of these algorithms
took probably 20 minutes

00:37:15.660 --> 00:37:19.460
to run-- on the light
side-- to find 1% to 5%

00:37:19.460 --> 00:37:20.607
worth of compression wins.

00:37:20.607 --> 00:37:22.190
Basically, what we
found is that we're

00:37:22.190 --> 00:37:25.230
at a local minima
for compression.

00:37:25.230 --> 00:37:28.550
The more time we spend trying
to compress the content

00:37:28.550 --> 00:37:31.330
yields less and less
and less actual savings.

00:37:31.330 --> 00:37:32.970
You could spend
six hours of cloud

00:37:32.970 --> 00:37:36.720
compute time to get 2% savings
in your compression algorithm.

00:37:36.720 --> 00:37:38.990
So at this point, you
have to look and say,

00:37:38.990 --> 00:37:40.630
well, if we're stuck
with GZIP, yet it

00:37:40.630 --> 00:37:43.880
takes additional hours of
time to compute smaller GZIP

00:37:43.880 --> 00:37:46.660
files, what the heck is
this entire talk about?

00:37:46.660 --> 00:37:48.500
Aren't we just stuck in this?

00:37:48.500 --> 00:37:49.680
Fret not, my good friends.

00:37:49.680 --> 00:37:51.430
You're actually not
stuck, and it actually

00:37:51.430 --> 00:37:53.460
comes down to you
owning your data.

00:37:53.460 --> 00:37:56.480
You see, you can create
smaller GZIP files

00:37:56.480 --> 00:37:58.680
by actually preprocessing
your information

00:37:58.680 --> 00:38:00.150
before handing it off to GZIP.

00:38:00.150 --> 00:38:01.990
Much like the example
I gave earlier

00:38:01.990 --> 00:38:04.705
where we took a set of numbers
and then delta compressed them

00:38:04.705 --> 00:38:07.080
to create something that was
highly repetitive and highly

00:38:07.080 --> 00:38:09.400
compressible, you can
apply these techniques

00:38:09.400 --> 00:38:11.640
to a lot of other portions
of your code base.

00:38:11.640 --> 00:38:13.750
So let's dive into some
interesting algorithms

00:38:13.750 --> 00:38:17.920
you can use in your
projects today.

00:38:17.920 --> 00:38:20.400
The first one happens to be
close and near to my heart.

00:38:20.400 --> 00:38:22.330
Now, many applications
and many websites

00:38:22.330 --> 00:38:25.390
out there use JSON as
a format to transfer

00:38:25.390 --> 00:38:27.190
data between client and server.

00:38:27.190 --> 00:38:29.237
Social media
information is typically

00:38:29.237 --> 00:38:30.570
sent around in this information.

00:38:30.570 --> 00:38:33.695
It's a very nice, very
adopted file format

00:38:33.695 --> 00:38:36.070
for sending information around,
particularly because it's

00:38:36.070 --> 00:38:38.220
built off the
JavaScript standard.

00:38:38.220 --> 00:38:41.110
Now, when you're sending
this data around,

00:38:41.110 --> 00:38:43.820
though, most of the time,
users and developers

00:38:43.820 --> 00:38:48.060
don't think about how to
modify the data being sent such

00:38:48.060 --> 00:38:49.370
that it compresses better.

00:38:49.370 --> 00:38:51.747
So a user asks for
some search query.

00:38:51.747 --> 00:38:53.580
The server goes and
computes the information

00:38:53.580 --> 00:38:56.110
and then returns
the JSON blob back,

00:38:56.110 --> 00:38:58.887
normally optimizing for
return round trip time.

00:38:58.887 --> 00:39:00.720
It's trying to get the
data back to the user

00:39:00.720 --> 00:39:01.707
as fast as possible.

00:39:01.707 --> 00:39:03.790
But what they don't
understand is if they actually

00:39:03.790 --> 00:39:06.150
have that GZIP flag
turned on, GZIP

00:39:06.150 --> 00:39:08.440
is going to stop the
operation and zip the content

00:39:08.440 --> 00:39:10.420
before sending it down
to the client anyway.

00:39:10.420 --> 00:39:11.940
Now, if you're the
type of developer

00:39:11.940 --> 00:39:14.920
that's dealing in third world
countries or other types

00:39:14.920 --> 00:39:18.620
of connectivity that may
be sparse or intermittent,

00:39:18.620 --> 00:39:22.240
this is actually a huge problem
because your client device--

00:39:22.240 --> 00:39:25.040
usually a mobile device
on some 1G or 2G network

00:39:25.040 --> 00:39:27.900
that may or may not
stay consistent--

00:39:27.900 --> 00:39:29.332
is sending off
requests, and then

00:39:29.332 --> 00:39:30.790
what's hurting them
is that they're

00:39:30.790 --> 00:39:32.720
getting a larger
payload coming down.

00:39:32.720 --> 00:39:35.250
So basically, what we're
talking about now is,

00:39:35.250 --> 00:39:38.212
how can you process
your JSON blobs that

00:39:38.212 --> 00:39:40.420
are being returned to these
individuals in such a way

00:39:40.420 --> 00:39:41.924
that GZIP can
compress it further?

00:39:41.924 --> 00:39:43.340
And it all starts
with the ability

00:39:43.340 --> 00:39:48.079
to transpose your structured
data in your JSON file.

00:39:48.079 --> 00:39:49.620
So let's take a look
at this example.

00:39:49.620 --> 00:39:51.500
Now, I've scraped
a lot of websites

00:39:51.500 --> 00:39:52.930
and a lot of JSON
responses, and I

00:39:52.930 --> 00:39:55.520
see this pattern occur
on a lot of websites.

00:39:55.520 --> 00:39:57.280
Effectively, what
you're looking at

00:39:57.280 --> 00:40:03.470
is a list of dictionary
structures of name value pairs.

00:40:03.470 --> 00:40:06.960
So if someone requests a
search for a particular item

00:40:06.960 --> 00:40:09.160
on a shopping website,
it's very common

00:40:09.160 --> 00:40:13.540
to actually return to them a
dictionary item that actually

00:40:13.540 --> 00:40:16.430
contains what the product
name is, what the price is,

00:40:16.430 --> 00:40:18.750
what thumbnail image to use,
et cetera, and then just

00:40:18.750 --> 00:40:21.860
list these linearly
inside of an array.

00:40:21.860 --> 00:40:24.830
So what this creates, though,
as an interesting problem,

00:40:24.830 --> 00:40:27.450
is that similar based
data is actually

00:40:27.450 --> 00:40:28.977
strided away from each other.

00:40:28.977 --> 00:40:30.060
It's actually interleaved.

00:40:30.060 --> 00:40:31.977
So you may have a
name and then a price,

00:40:31.977 --> 00:40:34.560
which could be a floating point
value, and then a description,

00:40:34.560 --> 00:40:36.410
which may be a long
form block of text,

00:40:36.410 --> 00:40:39.870
and then a URL, which has its
own specific characteristics.

00:40:39.870 --> 00:40:42.000
What we're proposing
in transposing our data

00:40:42.000 --> 00:40:44.495
is actually turning
this name value pair

00:40:44.495 --> 00:40:48.660
and actually de-interleaving it,
and instead actually grouping

00:40:48.660 --> 00:40:51.460
similar values for
property names together.

00:40:51.460 --> 00:40:54.200
So example here is we have these
two dictionary objects that

00:40:54.200 --> 00:40:56.370
both have name and
position values,

00:40:56.370 --> 00:40:58.490
and instead, we can
transpose that such

00:40:58.490 --> 00:41:01.060
that we have an
array of name answers

00:41:01.060 --> 00:41:03.050
and an array of pos answers.

00:41:03.050 --> 00:41:05.360
Now, I know this
looks weird at JSON,

00:41:05.360 --> 00:41:07.109
so let's take a
graphical look at things.

00:41:07.109 --> 00:41:08.650
So let's say on the
top here, we have

00:41:08.650 --> 00:41:10.660
an array where we have
a list of objects,

00:41:10.660 --> 00:41:12.250
and each one of
the colored blocks

00:41:12.250 --> 00:41:14.000
represents a property
on that object.

00:41:14.000 --> 00:41:17.320
So we've got green is a name,
and blue may be a price point,

00:41:17.320 --> 00:41:19.424
and again, red
might be some URL.

00:41:19.424 --> 00:41:21.090
What we can do, then,
is we can actually

00:41:21.090 --> 00:41:24.410
transpose this and align all of
the green blocks, blue blocks,

00:41:24.410 --> 00:41:29.610
and red blocks together,
allowing homogeneous data

00:41:29.610 --> 00:41:33.070
to reside in an array with
other homogeneous data.

00:41:33.070 --> 00:41:35.232
Now, I'm going to explain
why this is actually

00:41:35.232 --> 00:41:36.940
an improvement in a
second, but let's see

00:41:36.940 --> 00:41:40.462
if we can take a look at whether
or not this saves us any data.

00:41:40.462 --> 00:41:41.920
So if we take a
bunch of JSON files

00:41:41.920 --> 00:41:43.580
that are returned from
very common websites--

00:41:43.580 --> 00:41:45.790
so I took some responses
from Amazon and Pinterest

00:41:45.790 --> 00:41:48.206
and whatnot-- you can see that
the first column represents

00:41:48.206 --> 00:41:50.130
the source size in
bytes, the second column

00:41:50.130 --> 00:41:53.420
represents the source
data actually gzipped.

00:41:53.420 --> 00:41:56.070
The third is actually
the size of the data

00:41:56.070 --> 00:41:57.670
after it's been transposed.

00:41:57.670 --> 00:42:00.126
You can see that there's
some variability in numbers

00:42:00.126 --> 00:42:02.000
there because we're
actually removing symbols

00:42:02.000 --> 00:42:03.460
from the string at this point.

00:42:03.460 --> 00:42:05.400
And you can see the
final column is actually

00:42:05.400 --> 00:42:07.550
the gzipped version of
the transposed data.

00:42:07.550 --> 00:42:09.890
Now, we're actually in an
interesting situation here,

00:42:09.890 --> 00:42:14.990
is that for some JSON files,
the transposed gzipped data

00:42:14.990 --> 00:42:17.770
is actually smaller than the
source gzipped data, which

00:42:17.770 --> 00:42:20.690
means we actually get a win
by applying this process.

00:42:20.690 --> 00:42:22.760
Thankfully, we're not
falling into that area

00:42:22.760 --> 00:42:26.150
where genetic algorithm
minimized CSS is actually

00:42:26.150 --> 00:42:27.617
causing GZIP to
inflate the data.

00:42:27.617 --> 00:42:28.700
We don't want to be there.

00:42:28.700 --> 00:42:30.533
So the transpose operation
actually gives us

00:42:30.533 --> 00:42:32.420
smaller files,
which is fantastic.

00:42:32.420 --> 00:42:34.580
Now, the reason that
this actually works

00:42:34.580 --> 00:42:37.150
has to do with the
32k window that LZ77

00:42:37.150 --> 00:42:39.150
uses for its matching.

00:42:39.150 --> 00:42:41.175
So if we have our
values here that

00:42:41.175 --> 00:42:43.550
are all interleaved-- red,
green, blue, red, green, blue,

00:42:43.550 --> 00:42:48.020
red, green, blue-- again, we
have to go farther to search

00:42:48.020 --> 00:42:50.390
for a piece of content
that may be an exact match.

00:42:50.390 --> 00:42:54.070
However, when we transpose
our data, we de-interleave it

00:42:54.070 --> 00:42:56.124
and group similar
types of data together.

00:42:56.124 --> 00:42:57.540
So let's say we're
actually trying

00:42:57.540 --> 00:43:00.360
to find a match for one
of these green values.

00:43:00.360 --> 00:43:03.860
In the top array, you can see
that an entire listing of data,

00:43:03.860 --> 00:43:06.850
the green may not fit
inside the 32k window.

00:43:06.850 --> 00:43:09.440
Meanwhile, once we transpose
it and group homogeneous data

00:43:09.440 --> 00:43:11.750
together, the entirety
of the green data

00:43:11.750 --> 00:43:13.370
actually fits in
a single window.

00:43:13.370 --> 00:43:16.530
This is going to allow you
to find better matches, which

00:43:16.530 --> 00:43:20.500
is going to result in
smaller compression values.

00:43:20.500 --> 00:43:23.120
Now this actually comes to
another step here, which is OK,

00:43:23.120 --> 00:43:25.180
well, if we can
transpose our data,

00:43:25.180 --> 00:43:27.310
how else might we
modify our content

00:43:27.310 --> 00:43:28.310
so that we can get wins?

00:43:28.310 --> 00:43:30.500
Now, this is a really,
really cool algorithm

00:43:30.500 --> 00:43:33.230
that I found digging
through the archives of IEEE

00:43:33.230 --> 00:43:34.655
known as compression boosting.

00:43:34.655 --> 00:43:36.570
Now, it operates on
the same parameters.

00:43:36.570 --> 00:43:39.775
How do we preprocess things
for better compression?

00:43:39.775 --> 00:43:41.900
So the first one we're
going to take a look at here

00:43:41.900 --> 00:43:43.760
is actually something
called Dense Codes.

00:43:43.760 --> 00:43:46.680
Now, this is some great
research out of some academics

00:43:46.680 --> 00:43:48.600
in Argentina, and
effectively, it

00:43:48.600 --> 00:43:50.860
allows us to take a text
based file and preprocess

00:43:50.860 --> 00:43:52.380
it and hand it off to GZIP.

00:43:52.380 --> 00:43:53.990
Now, the preprocessing
is actually

00:43:53.990 --> 00:43:54.990
the important part here.

00:43:54.990 --> 00:43:56.430
We're not actually
transposing it,

00:43:56.430 --> 00:43:59.660
but instead, were using a
modified dictionary index

00:43:59.660 --> 00:44:00.770
lookup scheme.

00:44:00.770 --> 00:44:03.900
So let's say we parse
our text and our seen,

00:44:03.900 --> 00:44:06.740
and we create a
dictionary index.

00:44:06.740 --> 00:44:09.600
So once a word is seen, we
provide it to the dictionary,

00:44:09.600 --> 00:44:11.290
and then every
reference to that word

00:44:11.290 --> 00:44:13.930
is replaced with an
index value to the array.

00:44:13.930 --> 00:44:16.750
So let's say we have an array
here of, "How much would could

00:44:16.750 --> 00:44:20.170
a woodchuck chuck," and we
have 400 symbols before that.

00:44:20.170 --> 00:44:24.140
So we see that "how" is at
location 400, 401 is "much,"

00:44:24.140 --> 00:44:26.720
402 is "wood," 403 is "could."

00:44:26.720 --> 00:44:28.890
So when we're creating
a stream, we're

00:44:28.890 --> 00:44:33.750
going to get values that point
into these element arrays.

00:44:33.750 --> 00:44:37.240
So the problem with this is that
if we only have 256 symbols,

00:44:37.240 --> 00:44:39.400
we can only use eight
bits per pointer.

00:44:39.400 --> 00:44:41.210
However, if we go
above 256, we have

00:44:41.210 --> 00:44:44.180
to start using 16 bits per
pointer, which is actually

00:44:44.180 --> 00:44:48.210
a problem because if the
symbols are weighted such

00:44:48.210 --> 00:44:51.440
that the most probable
and most visible

00:44:51.440 --> 00:44:53.899
symbols are actually closer to
the front of the dictionary,

00:44:53.899 --> 00:44:56.023
you're actually going to
be wasting a lot of space.

00:44:56.023 --> 00:44:58.670
You're going to have a lot of
symbols and a lot of indexes

00:44:58.670 --> 00:45:01.130
where the first upper
eight bits are actually

00:45:01.130 --> 00:45:03.599
going to be zeros for the
entire dominant side of it.

00:45:03.599 --> 00:45:05.140
So you're actually
inflating the size

00:45:05.140 --> 00:45:06.973
of your stream at this
point because there's

00:45:06.973 --> 00:45:09.060
a lot of bits that
aren't being used.

00:45:09.060 --> 00:45:12.100
Now, the way that
Dense Codes work is it

00:45:12.100 --> 00:45:16.730
actually allows you to modify
the way the token is used

00:45:16.730 --> 00:45:18.660
to create the ability
to actually do

00:45:18.660 --> 00:45:21.940
variable length coding
in string, which means

00:45:21.940 --> 00:45:23.540
that for the first
three numbers,

00:45:23.540 --> 00:45:26.490
we actually use 16 bits
to represent the indexes,

00:45:26.490 --> 00:45:28.950
but the second three numbers,
because their indexes

00:45:28.950 --> 00:45:31.170
are lower than 256,
we can actually

00:45:31.170 --> 00:45:33.250
use eight bits instead of 16.

00:45:33.250 --> 00:45:35.819
This is actually going to
create a smaller stream for us

00:45:35.819 --> 00:45:37.485
to actually compress
a little bit later.

00:45:40.280 --> 00:45:41.810
Now, the wins from
this are pretty

00:45:41.810 --> 00:45:43.664
interesting to look
at because there's

00:45:43.664 --> 00:45:45.330
a couple things in
flight here, and I've

00:45:45.330 --> 00:45:49.749
got an entire 10-page writeup
on my blog about this algorithm

00:45:49.749 --> 00:45:52.290
and the interesting things that
go back and forth between it,

00:45:52.290 --> 00:45:54.248
some caveats, and some
things you need to know,

00:45:54.248 --> 00:45:56.492
but it all boils
down to this table.

00:45:56.492 --> 00:45:57.950
Now basically, what
this table says

00:45:57.950 --> 00:46:01.230
is that we've tried the
source data being gzipped,

00:46:01.230 --> 00:46:03.720
and then we've
tried the data being

00:46:03.720 --> 00:46:05.220
run through our
dense codes-- that's

00:46:05.220 --> 00:46:09.380
the ETDC column-- and then the
gzipping of that information.

00:46:09.380 --> 00:46:11.940
Now, next to it, I actually
compare the other compressors

00:46:11.940 --> 00:46:16.170
that we've mentioned today--
Zopfli, 7-Zip, and BZIP2.

00:46:16.170 --> 00:46:17.800
So what I'm trying
to see here is

00:46:17.800 --> 00:46:21.310
once I do this preprocessing,
what compression algorithm is

00:46:21.310 --> 00:46:23.740
actually going to give
us the best results?

00:46:23.740 --> 00:46:26.950
You can see that hands
down, GZIP really

00:46:26.950 --> 00:46:30.027
doesn't produce savings with
this preprocessing method

00:46:30.027 --> 00:46:31.610
for the majority of
the data that I've

00:46:31.610 --> 00:46:33.402
shown here-- JSON files, CSSs.

00:46:33.402 --> 00:46:34.860
And I've actually
run this probably

00:46:34.860 --> 00:46:37.480
against 25,000,
30,000 files, and you

00:46:37.480 --> 00:46:40.550
see this similar pattern,
that just doing standard GZIP

00:46:40.550 --> 00:46:42.970
against this information
doesn't really give you wins.

00:46:42.970 --> 00:46:45.150
However, you see when
you start using Zopfli,

00:46:45.150 --> 00:46:46.710
you start getting some wins.

00:46:46.710 --> 00:46:50.370
The advanced pattern matching
and more memory usage

00:46:50.370 --> 00:46:52.970
characteristics that it
has inside of an encoder

00:46:52.970 --> 00:46:54.860
allows you to get better
matches with this,

00:46:54.860 --> 00:46:56.200
producing smaller files.

00:46:56.200 --> 00:46:58.120
The clear winner here is 7-Zip.

00:46:58.120 --> 00:47:00.620
Something inside of the way
it's using its algorithms

00:47:00.620 --> 00:47:04.710
consistently produces smaller
dense code compressed files

00:47:04.710 --> 00:47:06.350
as opposed to the
source data, which

00:47:06.350 --> 00:47:08.156
means this is interesting,
that if you have

00:47:08.156 --> 00:47:09.530
data that's above
a certain size.

00:47:09.530 --> 00:47:12.860
Let's say you're returning a 20k
blob of data for some reason.

00:47:12.860 --> 00:47:15.800
Preprocessing that
text with dense codes

00:47:15.800 --> 00:47:22.980
and then using 7-Zip as
your compressor of choice

00:47:22.980 --> 00:47:25.859
can actually produce smaller
GZIP files consistently.

00:47:25.859 --> 00:47:27.400
Now, the downside
of this, of course,

00:47:27.400 --> 00:47:29.875
is that you have to reconstruct
your dense code transform

00:47:29.875 --> 00:47:32.082
at the client, but that's
not necessarily a big deal

00:47:32.082 --> 00:47:33.540
depending on what
type of trade-off

00:47:33.540 --> 00:47:35.760
you're willing to make.

00:47:35.760 --> 00:47:38.150
Now, these are all
preprocessing schemes.

00:47:38.150 --> 00:47:40.950
There's another form of
processing your data that

00:47:40.950 --> 00:47:42.870
can actually get wins
that these can't touch.

00:47:42.870 --> 00:47:44.370
I mean, the types
of wins that we're

00:47:44.370 --> 00:47:46.630
going to talk about
for Delta.js blow

00:47:46.630 --> 00:47:50.445
these preprocessing schemes out
of the water, but be warned.

00:47:50.445 --> 00:47:53.560
Thar be dragons here, mostly
in the form of madness.

00:47:53.560 --> 00:47:56.320
So let's dive into what
I'm talking about here.

00:47:56.320 --> 00:47:59.680
So in 2012, the
Gmail team actually

00:47:59.680 --> 00:48:02.410
did a fantastic
presentation for the W3C,

00:48:02.410 --> 00:48:03.910
and actually put
up some slides that

00:48:03.910 --> 00:48:07.460
are publicly available to other
information, that was proposing

00:48:07.460 --> 00:48:09.100
a solution to a common problem.

00:48:09.100 --> 00:48:13.000
You see, Gmail users
see about 61 years

00:48:13.000 --> 00:48:16.730
of loading bar inside of
Gmail about every day.

00:48:16.730 --> 00:48:20.130
This is a lot of time that
users are sitting around,

00:48:20.130 --> 00:48:22.140
waiting for JavaScript
to be streamed out.

00:48:22.140 --> 00:48:25.030
What they proposed was
a new form of ability

00:48:25.030 --> 00:48:28.540
to, instead of transferring the
large files every single time,

00:48:28.540 --> 00:48:30.910
they can actually start
transferring the difference

00:48:30.910 --> 00:48:32.940
between the file
that the user has

00:48:32.940 --> 00:48:35.470
and the new file
that the user needs.

00:48:35.470 --> 00:48:36.720
Now, this isn't a new concept.

00:48:36.720 --> 00:48:39.160
We've seen these sorts
of patching-based systems

00:48:39.160 --> 00:48:42.147
everywhere in computing
since the late 1970s,

00:48:42.147 --> 00:48:43.730
but it's never been
able to be applied

00:48:43.730 --> 00:48:47.497
to the web due to some of
the architecture involved.

00:48:47.497 --> 00:48:48.830
How the algorithm works is this.

00:48:48.830 --> 00:48:52.900
So let's say we have a
file, file.CSS version zero,

00:48:52.900 --> 00:48:54.510
and we've made an update to it.

00:48:54.510 --> 00:48:57.000
This happens quite a
bit in large projects.

00:48:57.000 --> 00:48:59.660
Now, when we make that update,
the majority of the file

00:48:59.660 --> 00:49:01.180
is the same as it used to be.

00:49:01.180 --> 00:49:02.800
There may be a
function that's added,

00:49:02.800 --> 00:49:05.960
or some comments were placed
in, or some things were removed,

00:49:05.960 --> 00:49:07.480
but for the majority
of the file,

00:49:07.480 --> 00:49:08.910
it's almost
identical, which means

00:49:08.910 --> 00:49:12.000
we can represent the
second file, the new file,

00:49:12.000 --> 00:49:15.410
as a difference from the
file that we've already seen.

00:49:15.410 --> 00:49:18.410
This is the concept,
again, of delta encoding.

00:49:18.410 --> 00:49:20.350
Now, once we've delta
encoded this file,

00:49:20.350 --> 00:49:23.400
the patch-- basically the
difference between the two--

00:49:23.400 --> 00:49:27.790
is generally much, much smaller
than the updated version

00:49:27.790 --> 00:49:30.180
of the file, which
means we can represent

00:49:30.180 --> 00:49:36.110
version one as a patch
operation of version zero.

00:49:36.110 --> 00:49:38.070
This means that if
the user already

00:49:38.070 --> 00:49:41.530
has version one on their
machine, all we have to do

00:49:41.530 --> 00:49:44.040
is send them the
patch, and it allows

00:49:44.040 --> 00:49:47.817
them to reconstruct and create
the new version of the data.

00:49:47.817 --> 00:49:49.900
Now, this is actually a
pretty interesting concept

00:49:49.900 --> 00:49:51.566
because this means
we don't have to send

00:49:51.566 --> 00:49:53.770
full files to the
clients all the time.

00:49:53.770 --> 00:49:57.470
Instead, we can produce and send
down highly, highly minimized

00:49:57.470 --> 00:49:58.890
content to these guys.

00:49:58.890 --> 00:50:01.650
Now of course, this comes
with a bit of overhead

00:50:01.650 --> 00:50:03.790
in terms of communication.

00:50:03.790 --> 00:50:05.660
You see, in order
to get this working,

00:50:05.660 --> 00:50:07.410
we have to have a
communication process

00:50:07.410 --> 00:50:09.219
between the client
and the server.

00:50:09.219 --> 00:50:11.010
So let's say we have
our mobile device here

00:50:11.010 --> 00:50:13.330
and the user is going
to load a website.

00:50:13.330 --> 00:50:15.520
Well, the client
actually needs to notify

00:50:15.520 --> 00:50:19.260
the server on what version
of the file it has cached.

00:50:19.260 --> 00:50:21.070
The server can then
take this information,

00:50:21.070 --> 00:50:22.700
look up in its
array, and figure out

00:50:22.700 --> 00:50:26.310
what patch file it needs
to send to the client

00:50:26.310 --> 00:50:27.882
in order to get them up to date.

00:50:27.882 --> 00:50:30.340
Once it's figured this out, it
passes it off to the client,

00:50:30.340 --> 00:50:32.400
and then the client is
going to use this patch

00:50:32.400 --> 00:50:35.390
to construct the new version
of the file and cache that

00:50:35.390 --> 00:50:36.930
and use that appropriately.

00:50:36.930 --> 00:50:40.410
Effectively, this technique
is trading network requests

00:50:40.410 --> 00:50:42.530
for smaller file sizes.

00:50:42.530 --> 00:50:44.530
Sending a couple byte
request to determine

00:50:44.530 --> 00:50:46.210
what version of
the file you have

00:50:46.210 --> 00:50:48.750
is going to be night and day
difference than actually doing

00:50:48.750 --> 00:50:51.750
the entire file being sent
to the user multiple times.

00:50:51.750 --> 00:50:54.900
Now, if this sounds like
craziness to you, just hold on.

00:50:54.900 --> 00:50:57.230
You have to realize
this is a common problem

00:50:57.230 --> 00:51:00.250
for many large, very
industrial websites

00:51:00.250 --> 00:51:02.600
that serve millions
of users a day.

00:51:02.600 --> 00:51:04.720
You see, when the
Gmail people actually

00:51:04.720 --> 00:51:06.680
looked at the type
of savings they

00:51:06.680 --> 00:51:08.350
can get from this
type of algorithm,

00:51:08.350 --> 00:51:11.700
they saw a massive potential
for improvement here.

00:51:11.700 --> 00:51:14.780
You see, when they
compared the number of revs

00:51:14.780 --> 00:51:18.190
they do to a single file
over a month against the size

00:51:18.190 --> 00:51:20.370
of the deltas if they
were using this scheme,

00:51:20.370 --> 00:51:23.050
they actually saw that a
whole month's worth of changes

00:51:23.050 --> 00:51:28.120
was about 9%-- lower than
9%-- of the size of the assets

00:51:28.120 --> 00:51:30.160
altogether, which
means they would only

00:51:30.160 --> 00:51:34.300
have to send 9% of the content
as opposed to the new full file

00:51:34.300 --> 00:51:35.190
each time.

00:51:35.190 --> 00:51:36.260
This is a huge win.

00:51:36.260 --> 00:51:37.970
We're not talking
about 10% improvement.

00:51:37.970 --> 00:51:39.636
We're not talking
about 50% improvement.

00:51:39.636 --> 00:51:44.780
This is 90% size decrease by
using this delta algorithm.

00:51:44.780 --> 00:51:48.724
So obviously, this was a
proposal to the W3C spec.

00:51:48.724 --> 00:51:50.390
I've talked with some
of the Gmail guys.

00:51:50.390 --> 00:51:52.764
It doesn't look like this is
actually live in the servers

00:51:52.764 --> 00:51:53.323
right now.

00:51:53.323 --> 00:51:55.406
If that's changed, I hope
to be wrong because this

00:51:55.406 --> 00:51:57.430
is a fantastic
piece of technology

00:51:57.430 --> 00:51:59.750
that I hope to see
rolling out in some form

00:51:59.750 --> 00:52:02.660
to a lot of other
distributors on the internet.

00:52:02.660 --> 00:52:04.697
Now, there's another
form of this compression

00:52:04.697 --> 00:52:06.530
that I've actually been
playing around with.

00:52:06.530 --> 00:52:08.863
It actually comes in the form
of horizontal compression.

00:52:08.863 --> 00:52:11.200
Typically, when we think of
delta encoding for files,

00:52:11.200 --> 00:52:12.990
we think in terms of patches.

00:52:12.990 --> 00:52:14.659
I've got version
A and version B,

00:52:14.659 --> 00:52:16.700
and I want to generate
the patch between the two.

00:52:16.700 --> 00:52:18.920
This is especially common
in game development.

00:52:18.920 --> 00:52:20.962
However, there exists a
form called horizontal.

00:52:20.962 --> 00:52:22.420
How this works is
let's say we have

00:52:22.420 --> 00:52:25.480
a cascade of files that may
be similar on a website.

00:52:25.480 --> 00:52:29.540
So this particular website
uses three CSS files.

00:52:29.540 --> 00:52:31.740
Let's say it's Bootstrap
or something like that.

00:52:31.740 --> 00:52:35.277
Well, the interesting thing is
that these CSS files generally

00:52:35.277 --> 00:52:36.860
aren't that different
from each other.

00:52:36.860 --> 00:52:39.620
There's actually a lot a shared
syntax on a website for a given

00:52:39.620 --> 00:52:41.600
CSS file, which
means that instead

00:52:41.600 --> 00:52:45.190
of doing delta compression
between versions of the file,

00:52:45.190 --> 00:52:48.857
we can actually do delta
compression between the files

00:52:48.857 --> 00:52:50.440
that are going to
be sent to the user.

00:52:50.440 --> 00:52:53.010
So we can actually do a
difference between file zero

00:52:53.010 --> 00:52:56.290
and file one, and then
file one and file two.

00:52:56.290 --> 00:52:58.200
This actually allows
us to create patches

00:52:58.200 --> 00:52:59.780
for each one of
these, and when we

00:52:59.780 --> 00:53:02.560
combine that with the
source file to your server,

00:53:02.560 --> 00:53:05.690
the size of the assets that need
to be sent down to the client

00:53:05.690 --> 00:53:07.710
is drastically reduced.

00:53:07.710 --> 00:53:10.110
We can actually send them
the entire application

00:53:10.110 --> 00:53:13.540
and all the content required
as deltas from base files.

00:53:13.540 --> 00:53:15.630
This, again, is just
an extrapolation

00:53:15.630 --> 00:53:17.590
of various forms
of delta algorithms

00:53:17.590 --> 00:53:19.790
that are already
used out in the wild.

00:53:19.790 --> 00:53:22.030
Now, how this works
on the client side

00:53:22.030 --> 00:53:24.150
is that the server
will, of course, provide

00:53:24.150 --> 00:53:27.760
to the client the base file
and the set of patches,

00:53:27.760 --> 00:53:30.350
and then the client is
responsible for reconstructing

00:53:30.350 --> 00:53:33.160
each one of those files and
then caching them locally.

00:53:33.160 --> 00:53:35.040
This is a really
neat idea if you're

00:53:35.040 --> 00:53:38.470
trying to optimize first
time load for users.

00:53:38.470 --> 00:53:40.950
The Gmail proposed
specification requires

00:53:40.950 --> 00:53:44.530
that the user has to download
version zero of the file, which

00:53:44.530 --> 00:53:48.870
in some cases actually
could be 300k of CSS data.

00:53:48.870 --> 00:53:50.530
Meanwhile, horizontal
compression

00:53:50.530 --> 00:53:52.570
suggests that it may
not have to do that.

00:53:52.570 --> 00:53:57.700
It may only have to send down
bites or chunks of 50k data

00:53:57.700 --> 00:54:00.520
when you actually represent
that CSS as a delta.

00:54:00.520 --> 00:54:03.420
Effectively, what you're doing
in this horizontal scheme is

00:54:03.420 --> 00:54:07.551
you're trading client side
processing for smaller transfer

00:54:07.551 --> 00:54:08.050
sizes.

00:54:08.050 --> 00:54:10.780
This is because it actually
takes a lot of CPU cycles

00:54:10.780 --> 00:54:14.050
on the client to reconstruct
these files before passing them

00:54:14.050 --> 00:54:15.930
off to the processing
system to create

00:54:15.930 --> 00:54:17.340
your DOM and everything else.

00:54:17.340 --> 00:54:20.785
So there's this interesting
trade-off between Delta.js

00:54:20.785 --> 00:54:24.340
or vertical delta encoding for
files and the horizontal delta

00:54:24.340 --> 00:54:26.830
encoding for files as well.

00:54:26.830 --> 00:54:29.099
Now, we can see when I've
applied this technique

00:54:29.099 --> 00:54:30.640
to some various
sites on the internet

00:54:30.640 --> 00:54:32.170
that we actually get some
interesting numbers out of it.

00:54:32.170 --> 00:54:34.830
So I took all of the CSS
from a Gmail session,

00:54:34.830 --> 00:54:36.960
all of the JavaScript
from a Gmail session,

00:54:36.960 --> 00:54:39.760
and all of the CSS
from an Amazon session,

00:54:39.760 --> 00:54:41.260
and I ran it through
this technique.

00:54:41.260 --> 00:54:43.440
So you can see we've
got the source, the GZIP

00:54:43.440 --> 00:54:45.960
source, and then
the size of the data

00:54:45.960 --> 00:54:47.920
once it's been delta
encoded, and then

00:54:47.920 --> 00:54:50.720
of course, the gzipping
of the delta encoded data

00:54:50.720 --> 00:54:52.840
because we can't just stop
at the delta encoding.

00:54:52.840 --> 00:54:54.480
So you can see for
Gmail, we actually

00:54:54.480 --> 00:54:58.730
get some pretty amazing
savings, 31% and 12%,

00:54:58.730 --> 00:55:02.334
by using this horizontal
delta compression encoding.

00:55:02.334 --> 00:55:04.750
Meanwhile, something weird's
going on with the Amazon data

00:55:04.750 --> 00:55:08.800
where we actually get an
increase of size by 13%.

00:55:08.800 --> 00:55:10.540
Now, I'd like to
actually point out

00:55:10.540 --> 00:55:15.332
that this data is highly, highly
minimized and highly redundant.

00:55:15.332 --> 00:55:17.540
All of these files that are
creating multiple network

00:55:17.540 --> 00:55:19.300
requests tend to
be self similar,

00:55:19.300 --> 00:55:22.160
and that's actually due to the
minification processes that

00:55:22.160 --> 00:55:24.550
are being applied to
these files on the web.

00:55:24.550 --> 00:55:27.390
To give you an example of
how important minimization

00:55:27.390 --> 00:55:29.760
is to horizontal
delta compression,

00:55:29.760 --> 00:55:32.437
let's take a look at the game
library known as Impact.js.

00:55:32.437 --> 00:55:33.270
I love this library.

00:55:33.270 --> 00:55:34.079
It's fantastic.

00:55:34.079 --> 00:55:35.620
If you're a game
developer and you're

00:55:35.620 --> 00:55:37.470
looking to make HTML5
games, definitely

00:55:37.470 --> 00:55:39.140
give Impact.js a look.

00:55:39.140 --> 00:55:40.990
Effectively, I took
the source file

00:55:40.990 --> 00:55:45.520
and left them as loose files.

00:55:45.520 --> 00:55:47.520
I did not combine them
into a single large file.

00:55:47.520 --> 00:55:50.510
I actually created the delta
between all these files

00:55:50.510 --> 00:55:54.100
and then did the
gzipped version of that.

00:55:54.100 --> 00:55:55.530
Now, you can see
that it actually

00:55:55.530 --> 00:55:57.780
goes down from about 70k to 21k.

00:55:57.780 --> 00:56:00.040
However, when I minimized
all of the files

00:56:00.040 --> 00:56:02.470
before doing delta
compression, I actually

00:56:02.470 --> 00:56:03.360
got a better savings.

00:56:03.360 --> 00:56:05.100
I got down to about 14k.

00:56:05.100 --> 00:56:06.900
This is because the
minimization techniques

00:56:06.900 --> 00:56:08.441
we're using on the
web today actually

00:56:08.441 --> 00:56:10.290
produce a lot of
duplicate symbols

00:56:10.290 --> 00:56:12.510
that could be used
in various places.

00:56:12.510 --> 00:56:14.730
Again, when you see
those original examples

00:56:14.730 --> 00:56:18.070
of taking some and we
renamed the parameters that

00:56:18.070 --> 00:56:20.690
are being passed
to it as ABC, we

00:56:20.690 --> 00:56:23.086
tend to see that pattern
occur for all the JavaScript

00:56:23.086 --> 00:56:24.710
for every function
that's been defined,

00:56:24.710 --> 00:56:26.520
meaning we're going
to see more matches.

00:56:26.520 --> 00:56:29.680
More matches result in better
statistical probability,

00:56:29.680 --> 00:56:32.720
which results in
smaller file sizes.

00:56:32.720 --> 00:56:35.540
So by minimizing our data before
we do our delta compression,

00:56:35.540 --> 00:56:39.650
we actually get very,
very important wins.

00:56:39.650 --> 00:56:42.290
So what we've talked
about today is

00:56:42.290 --> 00:56:44.340
where GZIP sits on
the web platform.

00:56:44.340 --> 00:56:46.770
We've got a lot of text data
that's about to blow up.

00:56:46.770 --> 00:56:48.060
We've got mobile devices.

00:56:48.060 --> 00:56:49.930
We've got fragmented hardware.

00:56:49.930 --> 00:56:52.300
We've got different
connectivity all over the world,

00:56:52.300 --> 00:56:56.040
but some things here allow you
to take advantage and actually

00:56:56.040 --> 00:56:58.840
address these issues
today rather than waiting

00:56:58.840 --> 00:57:00.090
for network times to increase.

00:57:00.090 --> 00:57:01.631
So for example, we
looked at the fact

00:57:01.631 --> 00:57:03.050
that GZIP is not
a silver bullet.

00:57:03.050 --> 00:57:04.650
By preprocessing your
data, you can actually

00:57:04.650 --> 00:57:05.790
get some pretty big wins.

00:57:05.790 --> 00:57:08.027
We've seen where GZIP
actually falls on its face.

00:57:08.027 --> 00:57:09.610
If you've done a lot
of preprocessing,

00:57:09.610 --> 00:57:11.526
it may not compress your
data, and in fact, it

00:57:11.526 --> 00:57:12.420
might inflate it.

00:57:12.420 --> 00:57:16.120
Or in the case of PNGs,
modifying the window matching

00:57:16.120 --> 00:57:17.840
by slight parameters
can actually

00:57:17.840 --> 00:57:21.270
upset how well GZIP
attaches to your data.

00:57:21.270 --> 00:57:23.630
We also looked at how to
preprocess your content using

00:57:23.630 --> 00:57:27.930
various forms of other command
line algorithms, like Zopfli

00:57:27.930 --> 00:57:30.880
or 7-Zip, and then how to
transpose your JSON data, which

00:57:30.880 --> 00:57:33.190
is fantastic for a lot
of your shopping sites.

00:57:33.190 --> 00:57:35.980
Also, when you combine that
with the dense code boosting,

00:57:35.980 --> 00:57:39.440
which is more cutting edge
text preprocessing data,

00:57:39.440 --> 00:57:41.970
you start getting a sense
that the web is not done yet.

00:57:41.970 --> 00:57:44.252
You're not locked into
your form that you have.

00:57:44.252 --> 00:57:46.710
Then you can start looking at
the delta compression methods

00:57:46.710 --> 00:57:48.751
and start looking at the
ways to actually combine

00:57:48.751 --> 00:57:53.410
your data in different
ways to reduce duplication

00:57:53.410 --> 00:57:55.529
and complexity in the
content that you have.

00:57:55.529 --> 00:57:57.070
And when you apply
all this together,

00:57:57.070 --> 00:57:59.500
you start getting
a vision of the web

00:57:59.500 --> 00:58:01.770
that we can actually
control more of our data

00:58:01.770 --> 00:58:05.130
to reduce the sizes and actually
get around a lot of the hitches

00:58:05.130 --> 00:58:07.380
and problems that
GZIP presents to us.

00:58:07.380 --> 00:58:09.830
So with that, thank you
for your time today.

00:58:09.830 --> 00:58:11.690
I really appreciate
you listening

00:58:11.690 --> 00:58:14.050
to this ramble on some very
hard compression stuff.

00:58:14.050 --> 00:58:15.930
If you're interested
in more, I highly

00:58:15.930 --> 00:58:17.930
recommend you check
out html4rocks.com.

00:58:17.930 --> 00:58:20.920
I recently put up
two articles there

00:58:20.920 --> 00:58:22.800
on text compression
for web developers

00:58:22.800 --> 00:58:25.100
as well as image compression
for web developers,

00:58:25.100 --> 00:58:27.740
and these are generally
meant to be opener tutorials

00:58:27.740 --> 00:58:30.200
to introduce you to different
terminology and different

00:58:30.200 --> 00:58:31.800
algorithms and how
they're being used.

00:58:31.800 --> 00:58:34.227
Once again, definitely check
out the #perfmatters hashtag.

00:58:34.227 --> 00:58:35.560
A lot of smart people are there.

00:58:35.560 --> 00:58:38.902
And join the Google+ Web
Performance community.

00:58:38.902 --> 00:58:41.110
You can actually see the
short link there on the side

00:58:41.110 --> 00:58:43.380
as goo.gl/webperf.

00:58:43.380 --> 00:58:45.750
Again, a great place to talk
about performance problems

00:58:45.750 --> 00:58:46.830
and find issues.

00:58:46.830 --> 00:58:47.800
That's it for me.

00:58:47.800 --> 00:58:48.962
My name is Colt McAnlis.

00:58:48.962 --> 00:58:50.670
Here's how you get a
hold of me for email

00:58:50.670 --> 00:58:52.380
and other various
social media times.

00:58:52.380 --> 00:58:54.910
Thanks, once again, for tuning
in to this episode of Google

00:58:54.910 --> 00:58:56.000
Developers Live.

00:58:56.000 --> 00:58:57.410
I hope to see you again soon.

00:58:57.410 --> 00:58:58.960
Thanks.

