WEBVTT
Kind: captions
Language: en

00:03:55.655 --> 00:03:57.655
AI Adventures

00:04:01.054 --> 00:04:02.900
-Art, Science, and Tools of 
Machine Learning

00:05:03.914 --> 00:05:05.954
May 10, 2018.
     8:30 a.m.

00:05:09.062 --> 00:05:11.062
PT.

00:05:14.783 --> 00:05:16.783
AI Adventures-Art, Science, and 

00:05:19.271 --> 00:05:21.271
Tools of Machine Learning.

00:06:18.146 --> 00:06:21.430
May 10, 2018.
     8:30 a.m. PT.

00:06:28.602 --> 00:06:30.602
AI Adventures- Art, Science, and

00:06:33.438 --> 00:06:35.438
Tools of Machine Learning

00:11:41.511 --> 00:11:43.511
.

00:11:47.850 --> 00:11:49.850
.

00:23:17.173 --> 00:23:20.163
.

00:24:19.001 --> 00:24:21.001
.

00:24:23.454 --> 00:24:25.454
(captioner standing by)

00:24:45.250 --> 00:24:47.250
.

00:24:49.082 --> 00:24:51.082
(music)

00:24:52.236 --> 00:24:53.275
.
     &gt;&gt; YUFENG GUO:  Good 

00:24:53.276 --> 00:24:55.321
morning.
     (Applause).

00:24:59.187 --> 00:25:01.187
&gt;&gt; YUFENG GUO:  May name is 
Yufeng.

00:25:02.656 --> 00:25:03.917
I host a video series on YouTube
called AI Adventures, we explore

00:25:03.918 --> 00:25:07.396
the art, science, and tools of 
Machine Learning.  I wanted to 

00:25:07.397 --> 00:25:09.397
bring a little piece of 

00:25:12.313 --> 00:25:14.313
that here today in person, so 
let's go 

00:25:17.256 --> 00:25:18.885
on an AI adventure together.
     For the purposes of this 

00:25:18.886 --> 00:25:22.949
session, let's start with just a
short definition of Machine 

00:25:22.950 --> 00:25:25.588
Learning and use that to build 
up kind of a workflow and get 

00:25:25.589 --> 00:25:30.283
into the tools.
     We'll then use that today 

00:25:30.284 --> 00:25:32.284
to help 

00:25:33.761 --> 00:25:36.196
us see how we can solve Machine 
Learning problems and see how we

00:25:36.197 --> 00:25:38.197
can use that to 

00:25:40.675 --> 00:25:42.675
apply to your problems outside 
of I/O.

00:25:42.923 --> 00:25:44.923
And so we'll say that Machine 

00:25:46.100 --> 00:25:48.540
Learning is programming with 
data, and we'll use that data to

00:25:48.541 --> 00:25:50.541
create a system 

00:25:53.058 --> 00:25:55.058
or model which is trained to do 
some 

00:25:56.532 --> 00:25:58.364
task, and we train this model, 
very importantly, using data so 

00:25:58.365 --> 00:26:00.365
everything comes back to the 
data.

00:26:03.069 --> 00:26:05.134
So that's really the first step 
of Machine Learning, gathering 

00:26:05.135 --> 00:26:07.135
your data.

00:26:09.001 --> 00:26:10.855
It all centers and starts from 
there.  Next, there is typically

00:26:10.856 --> 00:26:15.935
some kind of preparation needed 
of that data.  The raw data is 

00:26:15.936 --> 00:26:17.975
often not so suitable for use, 
and so we need to prepare it a 

00:26:19.820 --> 00:26:21.820
little before it's ready for 
Machine Learning.

00:26:22.475 --> 00:26:24.475
Third, we'll need to choose a 
model 

00:26:26.532 --> 00:26:28.532
we want to use, and then train 
it up and 

00:26:31.236 --> 00:26:33.677
evaluate its performance, do 
some fine tuning, and finally 

00:26:33.678 --> 00:26:35.678
make those predictions and do 
those tasks.

00:26:36.742 --> 00:26:39.586
So we'll go through each of 
these seven steps in detail over

00:26:39.587 --> 00:26:44.134
the course of this session.  So 
first, let's see what question 

00:26:44.135 --> 00:26:46.135
we are trying to use our model 
to address 

00:26:49.448 --> 00:26:51.448
in the first place.

00:26:53.003 --> 00:26:55.911
We'll walk through kind of a 
simple example dataset and go 

00:26:55.912 --> 00:26:57.912
through these 

00:26:59.141 --> 00:27:00.785
seven steps using the task to 
categorize some data about 

00:27:00.786 --> 00:27:05.758
animals.  Okay.  We're going to 
say, what kind of animal is this

00:27:05.759 --> 00:27:08.608
based on some input data, is it 
a bird, mammal, reptile, fish, 

00:27:12.418 --> 00:27:14.418
amphibian, bug,

00:27:16.560 --> 00:27:18.206
vertebrae, one of these seven 
types based on a variety of 

00:27:18.207 --> 00:27:20.841
stats about a given animal.
     So our model will take in 

00:27:20.842 --> 00:27:22.842
this 

00:27:24.318 --> 00:27:26.348
data, this structured data, and 
use that to try to make a 

00:27:26.349 --> 00:27:28.814
prediction, one of these seven 
types of animals.

00:27:30.243 --> 00:27:31.917
Now that we know what kind of 
situation that we're modeling, 

00:27:31.918 --> 00:27:35.208
let's dive into each of these 
steps in more detail.

00:27:39.153 --> 00:27:40.781
First up is gathering data.  
Just because we know what 

00:27:40.782 --> 00:27:45.047
problem we are trying to solve 
for doesn't always mean that we 

00:27:45.048 --> 00:27:47.048
have the data needed to 

00:27:48.702 --> 00:27:50.702
solve it, and without data your 
model is 

00:27:51.751 --> 00:27:53.751
going nowhere.

00:27:56.260 --> 00:27:58.260
So ask yourself this, where does
my 

00:27:59.725 --> 00:28:01.725
data live and in what format, 
and how can I get to it?

00:28:04.108 --> 00:28:06.967
These are often times the early 
stumbling blocks as we kind of 

00:28:06.968 --> 00:28:08.968
try to 

00:28:10.449 --> 00:28:12.449
use data to solve and address 
our problems.

00:28:14.735 --> 00:28:15.958
And perhaps you're actually in a
situation where you don't have 

00:28:15.959 --> 00:28:18.401
all the data that you need, and 
you need to collect your own 

00:28:18.402 --> 00:28:22.514
data.  This is also a common use
case and you can get creative in

00:28:22.515 --> 00:28:25.164
solving this problem by building
some systems for data 

00:28:28.436 --> 00:28:30.670
collection, perhaps you make it 
a game.  One really interesting 

00:28:30.671 --> 00:28:32.671
example of this is Quick  Draw.

00:28:35.982 --> 00:28:38.449
Quick Draw is an online game 
that lets you draw a specific 

00:28:38.450 --> 00:28:40.450
picture and tries to predict 
what you're drawing.

00:28:43.786 --> 00:28:46.254
So it tells you to draw a 
basketball or draw a boat.  It's

00:28:46.255 --> 00:28:49.126
kind of fun and people like it, 
and as a result the game has 

00:28:49.127 --> 00:28:51.127
generated 

00:28:54.685 --> 00:28:56.342
over 1 billion doodles of 
hand-drawn images from around 

00:28:56.343 --> 00:28:58.375
the world and this dataset is 
Open Sourced on GitHub and 

00:29:01.451 --> 00:29:03.292
there is actually an AI 
Adventures episode that we just 

00:29:03.293 --> 00:29:05.293
put out if you want to learn 
more about using this dataset 

00:29:05.523 --> 00:29:08.409
and playing with that.
     Back to our steps, we've 

00:29:08.410 --> 00:29:12.095
gathered our data, so what's 
next?  Well, we got to prepare 

00:29:12.096 --> 00:29:14.096
that data.

00:29:15.155 --> 00:29:16.589
Data preparation could be its 
own separate talk entirely, so 

00:29:16.590 --> 00:29:19.716
we'll just touch on a few 
aspects here.

00:29:21.958 --> 00:29:23.786
Exploring your data before you 
do any kind of Machine Learning 

00:29:23.787 --> 00:29:28.101
can really help you understand 
your data better, and this is 

00:29:28.102 --> 00:29:30.765
fundamental because it gives you
this intuition about your 

00:29:30.766 --> 00:29:33.400
dataset and can save you a ton 
of time and 

00:29:36.882 --> 00:29:38.728
headache later on in the 
process, and so I really 

00:29:38.729 --> 00:29:40.729
encourage folks to clean their 

00:29:42.395 --> 00:29:43.829
data, to look at the data and 
kind of get some intuition 

00:29:43.830 --> 00:29:45.830
around it.

00:29:49.110 --> 00:29:51.739
It helps you identify gaps and 
you'll learn a lot about what 

00:29:51.740 --> 00:29:56.015
you're working with along the 
way, maybe you'll learn you need

00:29:56.016 --> 00:29:58.481
more data or certain pieces of 
data you collected weren't so 

00:29:58.482 --> 00:30:00.530
useful afterall.
     One toy I like to play 

00:30:00.531 --> 00:30:03.997
there to do this is called 
facets, we'll take a little 

00:30:03.998 --> 00:30:06.870
brief detour and talk about 
that.  Facets is a Open Sourced 

00:30:06.871 --> 00:30:10.350
data visualization tool from 
Google Research and it can help 

00:30:10.351 --> 00:30:12.351
you get a better sense 

00:30:13.828 --> 00:30:15.859
of your data distribution.
     Yes, there are lots of 

00:30:15.860 --> 00:30:17.860
tools that enable you to do very
similar things, 

00:30:20.981 --> 00:30:22.822
you can write out commands and 
call functions, but this one 

00:30:22.823 --> 00:30:27.537
kind of puts it all in one place
and because it's a UI, you won't

00:30:27.538 --> 00:30:29.377
forget to compute a particular 
statistic or look into some 

00:30:29.378 --> 00:30:33.410
corner of the data because it's 
all computed for you.  It just 

00:30:33.411 --> 00:30:36.568
shows up, and so you can just 
kind of browse it.  And so here 

00:30:36.569 --> 00:30:38.569
we can see  Facets applied 

00:30:40.657 --> 00:30:43.512
to our dataset of these animals 
from the  zoo, and most of them 

00:30:43.513 --> 00:30:45.513
are kind of 1s 

00:30:47.219 --> 00:30:48.848
and 0s datasets, does it brief, 
does it have a tail, things 

00:30:48.849 --> 00:30:50.931
like,  that and so that's why we
see this kind of funny 

00:30:53.397 --> 00:30:55.832
distribution on 0 to  1, but a 
lot of times with numerical 

00:30:55.833 --> 00:30:57.496
distributions you'll see 
something far more interesting.

00:30:57.497 --> 00:31:01.580
     Now, let's move on to our 
third step of Machine Learning, 

00:31:01.581 --> 00:31:05.694
choosing a model.  And we'll 
discuss a little bit about 

00:31:09.689 --> 00:31:11.735
model selection, how we might go
about doing this using the least

00:31:11.736 --> 00:31:16.418
effort possible, and first, 
let's build an intuition around 

00:31:16.419 --> 00:31:18.419
what is happening when 

00:31:21.353 --> 00:31:23.353
a Machine Learning model is 
working well.

00:31:24.012 --> 00:31:26.251
So here we see how the model is 
trying to divide up a region, 

00:31:26.252 --> 00:31:30.307
trying to say which side is blue
and which side is orange to 

00:31:30.308 --> 00:31:31.985
match the data, which are the 
dots.  It's not the hardest 

00:31:31.986 --> 00:31:36.254
example in the world, right, 
everything is kind of already 

00:31:36.255 --> 00:31:37.470
separated out, but let's look at
something a little more 

00:31:37.471 --> 00:31:41.350
interesting.  Here we see the 
model struggle for a little bit 

00:31:41.351 --> 00:31:43.211
as it tries to find a way to 
draw that enclosing circle 

00:31:43.212 --> 00:31:47.513
around the blue dots, and it's 
really need to see that these 

00:31:47.514 --> 00:31:49.763
tools are available to you today
which are sophisticated enough 

00:31:49.764 --> 00:31:51.764
to 

00:31:54.646 --> 00:31:56.879
handle all of the minutia for 
you so you don't really need to 

00:31:56.880 --> 00:32:01.006
worry about the details of how 
it's getting about it, it's just

00:32:01.007 --> 00:32:03.007
a matter of is it  achieving the
task, but it is fun to see.

00:32:04.494 --> 00:32:05.907
Different models can model 
different kinds of data and 

00:32:05.908 --> 00:32:07.908
different 

00:32:08.937 --> 00:32:12.910
degrees degrees of complexity, 
there's what monoselection is 

00:32:12.911 --> 00:32:16.587
really about, it's about seeing 
what model is the right fit for 

00:32:16.588 --> 00:32:17.595
your particular dataset and the 
particular problem you're trying

00:32:17.596 --> 00:32:19.596
to solve.

00:32:21.658 --> 00:32:23.723
Now, concretely, what we're 
going to use is use TensorFlow 

00:32:23.724 --> 00:32:26.584
to achieve this, right.  So 
there is lots of talks about 

00:32:28.431 --> 00:32:30.431
TensorFlow here at I/O this year
so I 

00:32:32.137 --> 00:32:33.961
want rehash those bits, but 
we'll see an example, perhaps, 

00:32:33.962 --> 00:32:35.962
of what is might look like to 
use TensorFlow to kind of do 

00:32:37.815 --> 00:32:39.654
model selection to actually 
write that code, especially if 

00:32:39.655 --> 00:32:41.655
you haven't had too much 
hands-on experience with 

00:32:42.949 --> 00:32:44.949
TensorFlow, this may be kind of 
a useful 

00:32:46.603 --> 00:32:48.603
look at just how accessible it 
can be.

00:32:49.267 --> 00:32:51.700
So this is an example of a 
linear classifier.  It's similar

00:32:51.701 --> 00:32:54.583
to the first animation we saw.  
It's good for drawing a kind of 

00:32:54.584 --> 00:32:59.744
linear line between two kind of 
spaces.  We can see it's pretty 

00:33:01.158 --> 00:33:04.850
straightforward, but what if we 
had more complex data, right?  

00:33:04.851 --> 00:33:07.321
This one-line call, well it kind
of stays a one-line call when 

00:33:07.322 --> 00:33:09.322
you try to try it out on a 
different model, so 

00:33:12.416 --> 00:33:14.649
we're going to replace this with
a Deep Neural Network and it 

00:33:14.650 --> 00:33:19.158
only takes a few small tweaks, 
and in this case we just have to

00:33:19.159 --> 00:33:21.603
add one additional argument 
called hidden units, and this 

00:33:21.604 --> 00:33:24.258
defines the structure of our 
network.

00:33:27.936 --> 00:33:30.376
In this case, we have three 
layers with 30, 20, and 10 

00:33:30.377 --> 00:33:34.665
neurons in them.  And so all we 
needed to do is literally just 

00:33:34.666 --> 00:33:37.570
change the name, just replace 
that text and add one argument.

00:33:42.002 --> 00:33:44.002
You don't have to re replumb 
your entire system and don't 

00:33:45.782 --> 00:33:47.418
have to delete everything and 
start from scratch.  That's 

00:33:47.419 --> 00:33:49.464
really neat, and it's similar 
for a lot of models you can try.

00:33:51.545 --> 00:33:53.774
A lot of them just involve one 
flip of the method call and you 

00:33:53.775 --> 00:33:56.276
can basically play around with 
these different menu of choices.

00:33:56.509 --> 00:33:59.163
     And so in that paradigm, it
allows 

00:34:02.478 --> 00:34:05.526
you to not have to commit to a 
model kind of beforehand.  You 

00:34:05.527 --> 00:34:08.404
can try one out, see how it 
does, and then come back, so 

00:34:08.405 --> 00:34:11.720
these seven steps, you work 
through it and can iterate back,

00:34:11.721 --> 00:34:13.721
you can always go back and cycle
through.

00:34:13.971 --> 00:34:16.210
And so that brings us to our 
next step.  We've  gathered our 

00:34:16.211 --> 00:34:18.211
data, we've 

00:34:19.671 --> 00:34:21.500
prepared it, we've chosen a 
model for now at least, and now 

00:34:21.501 --> 00:34:24.761
we can do the training.
     Training gets a lot of 

00:34:24.762 --> 00:34:29.254
attention because in some ways 
that's where the magic happens, 

00:34:29.255 --> 00:34:33.122
but in practice it's often very 
straightforward.  You just let 

00:34:33.123 --> 00:34:35.123
it run and you wait.  So let's 
take a look at what happens 

00:34:37.032 --> 00:34:39.032
during the training and then we 
can see it in code.

00:34:40.754 --> 00:34:43.275
Conceptually when training a 
model we pass in our data and we

00:34:43.276 --> 00:34:48.572
ask the model right away to try 
to make a prediction.  But the 

00:34:48.573 --> 00:34:50.573
prediction will be pretty bad 

00:34:52.814 --> 00:34:54.814
because our model

00:34:55.967 --> 00:34:57.193
has initiallyized randomly, but 
we make this prediction and take

00:34:57.194 --> 00:34:59.828
it and compare it to the true 
answer, the correct answer that 

00:34:59.829 --> 00:35:01.829
we know.

00:35:04.541 --> 00:35:06.541
We use that information to 
update the 

00:35:09.430 --> 00:35:11.430
model, and so each time we go 
through 

00:35:12.703 --> 00:35:14.972
and update the model with a new 
piece of data as it works its 

00:35:14.973 --> 00:35:19.037
way through the training set, it
allows the model to get better 

00:35:19.038 --> 00:35:21.038
and better over time with each 

00:35:22.694 --> 00:35:24.121
training loop, so that's kind of
what the training process looks 

00:35:24.122 --> 00:35:25.963
like.
     But in practice when you 

00:35:25.964 --> 00:35:27.964
actually 

00:35:29.042 --> 00:35:30.693
need to code it up, we take our 
function, we've created the 

00:35:30.694 --> 00:35:32.694
model, and 

00:35:35.189 --> 00:35:37.189
we just call dot train on it, 
it's literally one line.

00:35:39.310 --> 00:35:40.932
You just need to provide it a 
function that gives it the 

00:35:40.933 --> 00:35:45.250
training data inputs and that's 
it.  It's one line.  And so once

00:35:45.251 --> 00:35:47.251
the training completes, you run 
this function and you go off and

00:35:50.141 --> 00:35:51.772
you grab a sandwich and you come
back and it's time to do 

00:35:51.773 --> 00:35:56.289
evaluation.  We got to evaluate 
our models to see how accurate 

00:35:56.290 --> 00:35:58.290
it is, see if it's doing a 

00:36:00.611 --> 00:36:02.611
good job of performing its 
primary task.

00:36:03.673 --> 00:36:06.206
So we can show it some examples 
of things that we know the 

00:36:06.207 --> 00:36:08.461
correct answers to, and the 
model did not see those 

00:36:09.074 --> 00:36:10.905
questions in training.  It 
didn't see that data in 

00:36:10.906 --> 00:36:15.843
training, so we use this 
evaluation data that we set 

00:36:15.844 --> 00:36:17.844
aside to measure the performance
of 

00:36:19.954 --> 00:36:20.974
our model, and conceptually it 
looks very similar to training, 

00:36:20.975 --> 00:36:24.458
right.  It's still this same 
shape, but the important 

00:36:24.459 --> 00:36:26.918
difference is that there is no 
arrow at the top, it's not a 

00:36:26.919 --> 00:36:28.919
closed loop.

00:36:31.643 --> 00:36:33.659
The model does not get updated 
once we check its prediction 

00:36:33.660 --> 00:36:37.984
results, and so if we did update
the model, then it would be no 

00:36:37.985 --> 00:36:40.030
different than training and we'd
be essentially cheating, right.

00:36:40.239 --> 00:36:45.396
     So the evaluation data is 
kind of precious in that way.  

00:36:45.397 --> 00:36:47.397
We don't use it to update the 
model.

00:36:49.487 --> 00:36:51.487
We hold it aside and we reserve 
it for 

00:36:53.156 --> 00:36:55.156
evaluating the performance of 
our model.

00:36:56.205 --> 00:36:58.205
In code it's just as simple as 
everything else we've seen.

00:37:02.333 --> 00:37:05.006
We just call dot evaluate.  You 
want to evaluate, you call.

00:37:05.810 --> 00:37:11.113
evaluate.  Notice we're passing 
in evaluation  data, right, it's

00:37:11.114 --> 00:37:13.114
not the training data, and if we
were using the training data, 

00:37:14.790 --> 00:37:17.217
let's say we copy that line down
and swopped out train for 

00:37:17.218 --> 00:37:21.312
evaluate and we forgot to change
the data source, now we're 

00:37:21.313 --> 00:37:23.313
evaluating the performance of 
our 

00:37:24.593 --> 00:37:27.228
model using the training data 
and that would be a big mistake 

00:37:27.229 --> 00:37:29.229
because the model 

00:37:30.901 --> 00:37:32.901
has optimized itself for the 
training 

00:37:34.214 --> 00:37:36.214
data, and so if you evaluate 
using that 

00:37:37.488 --> 00:37:38.912
same data what happens is you've
misrepresented the true 

00:37:38.913 --> 00:37:40.913
performance of your model and it
will likely perform 

00:37:46.446 --> 00:37:48.898
poorly on  real  real-world data
once you deploy it since its 

00:37:48.899 --> 00:37:51.343
never seen and you never 
measured how it does with 

00:37:52.157 --> 00:37:54.157
previously unseen data.

00:37:57.651 --> 00:37:59.696
So this brings us to our next 
point.  How can we actually 

00:37:59.697 --> 00:38:01.951
improve our model then?  We ran 
the training and did the thing 

00:38:03.645 --> 00:38:06.714
and ran evaluation, now what?  
Do we run training some more?  

00:38:06.715 --> 00:38:09.181
How can we tweak it further?  We
could always swop out for a new 

00:38:11.015 --> 00:38:13.044
model, but what if we decided on
this model but we want to 

00:38:13.045 --> 00:38:15.887
improve it as is?  This is where
hyperparameter tuning comes in.

00:38:19.590 --> 00:38:22.490
These models have parameters of 
their own, but what value should

00:38:22.491 --> 00:38:26.368
we use?  How do we pick them?
     Much like how you can try 

00:38:26.369 --> 00:38:31.151
out different models, you can 
also try out lots of different 

00:38:31.152 --> 00:38:35.864
model parameters.  This process 
called hyperparameter tuning, in

00:38:35.865 --> 00:38:37.895
some ways, still remains a 
active area of research, but 

00:38:37.896 --> 00:38:40.778
that doesn't mean that you can't
take advantage of it.

00:38:43.658 --> 00:38:45.334
Conceptually, you would take our
training and instead of just 

00:38:45.335 --> 00:38:48.041
training one model, we'll tweak 
that model and 

00:38:51.092 --> 00:38:53.543
create a couple of different 
variants, and so we'll run 

00:38:53.544 --> 00:38:57.627
training on different variants 
of the same underlying model but

00:38:57.628 --> 00:39:00.475
with different model parameters 
and see how that  affects our 

00:39:00.476 --> 00:39:05.204
accuracy.  So we'll train and 
evaluate all of these different 

00:39:05.205 --> 00:39:07.442
variants of the same or similar 
models and then see which one 

00:39:07.860 --> 00:39:13.161
performs best.  That's how it 
informs our parameter choice.  

00:39:13.162 --> 00:39:15.162
But you see, a lot of that is 
experimental.

00:39:16.656 --> 00:39:18.656
You got to try them to see what 
works, 

00:39:20.139 --> 00:39:22.139
so you might have to break out 
your four loop.

00:39:22.420 --> 00:39:24.420
And so yeah, the task of 
choosing 

00:39:25.923 --> 00:39:27.923
good  hyperparameters is 
something 

00:39:29.578 --> 00:39:31.211
perhaps that you've intuited 
this, you could probably turn 

00:39:31.212 --> 00:39:33.439
into a Machine Learning problem 
itself, optimizing the 

00:39:34.670 --> 00:39:35.899
parameters you're using for 
Machine Learning, but we'll have

00:39:35.900 --> 00:39:40.168
to save that for another talk.
     All right.  So we've 

00:39:40.169 --> 00:39:41.797
gathered the data, we've  
prepared it, chosen a model and 

00:39:41.798 --> 00:39:43.798
trained 

00:39:44.831 --> 00:39:47.940
it, evaluated the data, and we 
tuned the hyperparameters.  At 

00:39:47.941 --> 00:39:51.849
long last we've reached our last
step, making those predictions. 

00:39:51.850 --> 00:39:53.268
This is the whole point of 
training out the model in the 

00:39:53.269 --> 00:39:55.269
first place, right.  It wasn't 
just to get a really good 

00:40:00.008 --> 00:40:02.718
accuracy or to create heat on 
our server.  It was to actually 

00:40:02.719 --> 00:40:04.719
do something useful with it.

00:40:07.624 --> 00:40:10.078
And so making predictions, we 
take the model and we deploy it 

00:40:10.079 --> 00:40:12.079
somewhere or kind 

00:40:13.351 --> 00:40:15.351
of isolate it out, and we can 
show the 

00:40:16.609 --> 00:40:18.292
model some data that it hasn't 
been trained on and then see 

00:40:18.293 --> 00:40:20.293
what kind of outputs it 
predicts.

00:40:23.532 --> 00:40:25.532
And so that's

00:40:33.188 --> 00:40:35.431
our our seven steps, conceptual 
seven steps.  Let's see what 

00:40:35.432 --> 00:40:39.779
happens in practice, look at the
tooling side and figure out what

00:40:39.780 --> 00:40:42.233
you use besides TensorFlow and 
how you might actually achieve 

00:40:42.234 --> 00:40:46.301
the seven steps.  So we're going
to go and look at some code, and

00:40:46.302 --> 00:40:48.335
in particular I'll feature a 
couple of tools to get hands on 

00:40:48.336 --> 00:40:50.802
with and I'll point you to some 
additional resources as well.

00:40:55.074 --> 00:40:57.074
First up is a tool called 
codelab.

00:40:58.788 --> 00:41:01.075
If you've ever worked with any 
sort of notebook environment or 

00:41:01.076 --> 00:41:03.105
repple on the web before, this 
is basically a way to 

00:41:06.363 --> 00:41:09.038
run Python in the browser, but 
not just run Python.  It's a 

00:41:09.039 --> 00:41:13.710
whole  notebook environment.  
You can put Mark down.  The code

00:41:13.711 --> 00:41:15.967
actually executes, and because 
it's hosted essentially on 

00:41:18.002 --> 00:41:20.002
Google Drive, you can share it 
with other people.

00:41:20.694 --> 00:41:22.694
So let's switch over to the demo

00:41:26.216 --> 00:41:28.667
and I can show you what is looks
like to use Codelab.

00:41:32.527 --> 00:41:35.019
So here we see and I've loaded 
up Codelab and we can run 

00:41:35.020 --> 00:41:37.020
various commands, right, all the
things that we would expect.

00:41:39.958 --> 00:41:42.801
So let's see, we're still 
connected and in Codelab because

00:41:42.802 --> 00:41:46.268
it's a notebook environment, you
have to upload your own files 

00:41:46.269 --> 00:41:49.128
and it connects to the Internet 
so you can also authenticate and

00:41:49.129 --> 00:41:54.469
pull things down, but in my case
I have the dataset loaded on the

00:41:54.470 --> 00:41:57.350
machine so we can download that.
I've got pandas loaded here so 

00:41:57.351 --> 00:42:02.313
we can take a lock at your data 
decision  set, as promised it's 

00:42:02.314 --> 00:42:03.941
a bunch of animals and 
statistics, and at the very end 

00:42:03.942 --> 00:42:05.942
there is a class type.

00:42:09.839 --> 00:42:11.839
And so Codelab really lets you 
just 

00:42:13.120 --> 00:42:15.771
execute the code on the screen 
in, well, in realtime.  And 

00:42:15.772 --> 00:42:17.772
moreover, because it's all 
hosted 

00:42:18.863 --> 00:42:20.863
on Google Drive I don't have to 
spin up 

00:42:22.138 --> 00:42:26.038
any machines in the background, 
it just works seamlessly.  You 

00:42:26.039 --> 00:42:28.039
can put comments in and share 
your 

00:42:31.945 --> 00:42:34.788
work with others, and -- there 
we go, and do collaborative 

00:42:34.789 --> 00:42:36.789
research or just 

00:42:37.895 --> 00:42:39.895
work in general, so that's 
really neat.

00:42:42.848 --> 00:42:44.897
And so let's see, I think we 
have some time so let's actually

00:42:44.898 --> 00:42:47.127
walk through a little bit of 
kind of what I've ended up doing

00:42:47.128 --> 00:42:49.768
here.  We took our data and what
I'm doing is 

00:42:53.650 --> 00:42:55.083
shuffling it and then splitting 
the data.  We had a whole 

00:42:55.084 --> 00:42:57.084
dataset, right.

00:42:58.540 --> 00:43:00.185
This whole dataset happens to 
just be 101 different animal, so

00:43:00.186 --> 00:43:04.463
it's probably the smallest  
dataset that I've probably come 

00:43:04.464 --> 00:43:06.512
across, but you still need to do
the same types of best 

00:43:06.513 --> 00:43:08.513
practices.

00:43:09.592 --> 00:43:11.592
In this case I've taken it and 
split 

00:43:13.477 --> 00:43:15.742
it into training and evaluation 
data because if I use all 101 to

00:43:15.743 --> 00:43:18.455
do training, what am I going to 
do to evaluate my model?

00:43:20.705 --> 00:43:22.705
So we'll split it, and in this 
case 

00:43:23.768 --> 00:43:25.800
I chose a fraction of 60% and 
40%, but that ratio can be 

00:43:25.801 --> 00:43:27.801
adjusted, and so we 

00:43:29.663 --> 00:43:31.701
can see we have our training 
data, all 60 values, and then we

00:43:31.702 --> 00:43:37.019
have our evaluation data below.
     And I do a little bit of 

00:43:38.637 --> 00:43:40.637
preprocessing so this is the 
processing 

00:43:42.117 --> 00:43:44.555
your data bit, and notably the 
last column of the dataset is 

00:43:44.556 --> 00:43:46.556
the label, it 

00:43:48.250 --> 00:43:49.884
is the answer one through seven 
of what the correct type of 

00:43:49.885 --> 00:43:52.556
animal it is, reptile, 
amphibian, mammal, et cetera.

00:43:52.557 --> 00:43:57.658
     The problem is the labels 
go from one to seven and I need 

00:43:57.659 --> 00:44:01.928
them to be zero through six in 
this particular case, so I shift

00:44:01.929 --> 00:44:04.845
it by one by subtracting one and
that's it.  So pretty simple 

00:44:04.846 --> 00:44:06.846
preprocessing, is and I would 
expect that a bigger dataset 

00:44:07.900 --> 00:44:09.900
perhaps would do more things.

00:44:12.378 --> 00:44:14.378
And so we can see that our data 

00:44:16.245 --> 00:44:19.500
apparently I didn't make this 
run.  Run.  Always run your 

00:44:19.501 --> 00:44:21.501
code.

00:44:23.165 --> 00:44:25.165
And our input function here is 
pretty straightforward.

00:44:28.255 --> 00:44:30.689
It takes our raw data, and if we
want it to be shuffled we can 

00:44:30.690 --> 00:44:33.127
call shuffle on our dataset and 
I just let it repeat and 

00:44:36.524 --> 00:44:38.524
batch into

00:44:40.957 --> 00:44:43.553
chunks as needed and it will fit
into the training.  I believe 

00:44:43.554 --> 00:44:45.383
there was a talk on this 
specifically earlier at I/O so 

00:44:45.384 --> 00:44:47.384
if you 

00:44:48.477 --> 00:44:50.121
missed that you can catch the 
recording or roll back the 

00:44:50.122 --> 00:44:52.122
livestream.

00:44:53.624 --> 00:44:55.880
So we have our dataset or our 
input function.  I think I 

00:44:55.881 --> 00:44:58.525
forgot to run this cell again.  
And this is a little cell I have

00:44:58.526 --> 00:45:00.526
here 

00:45:03.415 --> 00:45:05.706
to just kind of try out my input
function, make sure it's 

00:45:05.707 --> 00:45:09.385
working, always good to test 
things.  We can see my input 

00:45:09.386 --> 00:45:13.276
function is indeed returning all
of the data of each type of the 

00:45:13.277 --> 00:45:15.919
feathers, whether it's eggs, 
whether it's airborn, does it 

00:45:15.920 --> 00:45:17.920
have a backbone, things like 
that.

00:45:20.425 --> 00:45:22.905
So each kind of these arrays 
represent a batch of data.  I 

00:45:22.906 --> 00:45:25.161
was also playing around with 
this to check out the unique 

00:45:25.162 --> 00:45:27.162
values for each 

00:45:29.636 --> 00:45:31.682
column just to make sure, most 
are 1s and 0s, and but there are

00:45:31.683 --> 00:45:33.683
a couple that are strange, for 
example, there are 

00:45:36.409 --> 00:45:38.409
animals with five legs 
apparently so not 

00:45:39.893 --> 00:45:41.124
all datasets are perfect.
     TensorFlow uses this notion

00:45:41.125 --> 00:45:46.658
of feature columns to represent 
the incoming data.  Models are 

00:45:46.659 --> 00:45:48.884
pretty generic, and so by using 
feature columns it allows you to

00:45:50.567 --> 00:45:52.567
customize it for your particular
dataset.

00:45:54.865 --> 00:45:56.905
In our case, I just loop over 
all the columns, all 17 of them 

00:45:56.906 --> 00:46:02.246
which happen to be numeric and 
set that as numeric, so it's 

00:46:02.247 --> 00:46:04.479
really just a configuration to 
let TensorFlow know how many 

00:46:04.480 --> 00:46:07.119
columns and of what type are 
coming in.

00:46:10.641 --> 00:46:12.270
So we'll run that and if things 
break it's because I forgot to 

00:46:12.271 --> 00:46:17.981
run the cell, so yell at me if 
that happens.  And so here is 

00:46:17.982 --> 00:46:20.829
that line that we saw before, 
right, with the creating a 

00:46:23.068 --> 00:46:24.725
model, so here we'll create a 
linear classifier and pull in 

00:46:24.726 --> 00:46:28.792
those feature columns and say 
that there are seven different 

00:46:28.793 --> 00:46:30.793
classes, seven different 

00:46:31.861 --> 00:46:33.861
possible values for the animals.

00:46:36.380 --> 00:46:38.380
And I've combined the train and 
ee 

00:46:39.654 --> 00:46:41.088
value, train and evaluate calls 
in as a function for convenience

00:46:41.089 --> 00:46:43.089
so they both run together.  I 
want to run the training on the 

00:46:44.954 --> 00:46:46.579
training data and evaluation on 
the evaluation data, so wrapping

00:46:46.580 --> 00:46:51.057
this together helps make sure 
you don't introduce bugs as you 

00:46:51.058 --> 00:46:53.058
re-run cells as you go along.

00:46:55.124 --> 00:46:57.632
So let's say we run the linear 
model and we'll just let that 

00:46:57.633 --> 00:46:59.633
churn, and 

00:47:01.086 --> 00:47:03.086
so this is all backed by Google 
Servers 

00:47:04.139 --> 00:47:06.139
and apparently it is taking its 
sweet time.

00:47:07.791 --> 00:47:10.035
While it's do that -- well this 
one is done, so we got 90%, 

00:47:10.036 --> 00:47:13.753
which is like okay, right.  
There were only 100 rows, we 

00:47:13.754 --> 00:47:18.665
took 60 for training and used 40
for evaluation, so it's not the 

00:47:18.666 --> 00:47:21.531
most reliable metric.  The idea 
here is the tooling is there, 

00:47:23.372 --> 00:47:25.051
the code is there, and you 
should replace this with your 

00:47:25.052 --> 00:47:28.353
own data which will be much 
better than this dataset and get

00:47:28.354 --> 00:47:30.383
awesome results.
     So as promised, the Deep 

00:47:30.384 --> 00:47:34.059
Neural Network is very similar. 
It's literally the same as 

00:47:34.060 --> 00:47:38.143
before but I replaced it with 
hidden units.  I guess there is 

00:47:38.144 --> 00:47:42.937
one notable difference, is that 
I do take the old feature 

00:47:42.938 --> 00:47:45.000
columns that we had from before 
and we wrap it in an indicator 

00:47:45.001 --> 00:47:47.001
column, 

00:47:50.803 --> 00:47:52.631
so that's just a space to put 
the linear networks data into a 

00:47:52.632 --> 00:47:56.297
way that the Deep Neural Network
can represent, so it's a little 

00:47:56.298 --> 00:47:58.298
bit out of scope for this talk, 

00:48:00.162 --> 00:48:01.591
but it's an adjustment just for 
Deep Neural Networks that linear

00:48:01.592 --> 00:48:03.592
networks don't have to deal 
with.

00:48:05.726 --> 00:48:10.398
So we'll let that get created.  
Oops, just created a new cell.  

00:48:10.399 --> 00:48:12.399
So I guess that's also a good 
thing to 

00:48:13.462 --> 00:48:15.298
show off here, so we can very 
easily create, you know, code 

00:48:15.299 --> 00:48:17.299
blocks and cell blocks in 
between and can you have all 

00:48:20.796 --> 00:48:22.796
sorts of great editing kind of 
abilities 

00:48:26.459 --> 00:48:28.836
here, and you can turn it into 
-- and you execute the cell and 

00:48:28.837 --> 00:48:30.837
it will be there.

00:48:33.950 --> 00:48:36.851
So with the deep network, I 
forgot if I ran this.  Maybe 

00:48:36.852 --> 00:48:38.852
we'll run it again.

00:48:39.900 --> 00:48:41.900
We can also like push the model 
further, right.

00:48:44.001 --> 00:48:46.235
The percentages here are 
sometimes wonky because they are

00:48:46.236 --> 00:48:50.137
-- there is only 40 values in 
the evaluation data and this is 

00:48:50.138 --> 00:48:52.138
effectively, you know, something

00:48:54.028 --> 00:48:56.699
over 40 is getting it right and 
the other ones are wrong, so in 

00:48:56.700 --> 00:48:58.700
this case it 

00:48:59.941 --> 00:49:01.364
looks like that 10%, probably 
four are wrong or three or four 

00:49:01.365 --> 00:49:04.038
are incorrect.  So let's make 
some predictions and see what 

00:49:04.039 --> 00:49:06.039
got missed.

00:49:07.738 --> 00:49:09.738
TensorFlow also has a.

00:49:12.212 --> 00:49:14.438
predict function so you can pass
in, I just took evaluation data 

00:49:14.439 --> 00:49:18.827
and sliced out a couple of 
examples to take a look at.  We 

00:49:18.828 --> 00:49:21.300
can see here that when we do 
that you get, you know, you get 

00:49:21.301 --> 00:49:25.991
the prediction and the correct 
answer, so in this case those 

00:49:25.992 --> 00:49:27.992
five that I arbitrarily chose 
happened to work out but what if

00:49:29.485 --> 00:49:32.532
we wanted to see the exact ones 
that we got wrong?  This was a 

00:49:32.533 --> 00:49:34.533
little experiment I ran just 
because I was curious, you know,

00:49:37.275 --> 00:49:39.505
which ones were the linear 
network getting wrong and which 

00:49:39.506 --> 00:49:42.163
were the deep network getting 
wrong?  In this case they're 

00:49:42.164 --> 00:49:44.164
different, right.

00:49:45.219 --> 00:49:47.261
They got the same number of 
incorrect predictions.  They all

00:49:47.262 --> 00:49:51.128
had four wrong, but they're 
actually different examples.  So

00:49:51.129 --> 00:49:53.360
this would be a opportunity to 
dig in further and play around 

00:49:53.361 --> 00:49:55.810
with that.
     One final thing I'll adhere

00:49:55.811 --> 00:49:57.811
is that 

00:50:01.106 --> 00:50:03.106
Codelab or Colaboratory has GPU 
support 

00:50:04.370 --> 00:50:07.212
and so you can toggle on GPU so 
if you have big datasets and 

00:50:07.213 --> 00:50:09.655
fancy models and you want to 
access that kind of stuff, go 

00:50:09.656 --> 00:50:12.124
and get that GPU.
     So let's switch back to the

00:50:12.125 --> 00:50:14.125
slides 

00:50:15.191 --> 00:50:17.191
briefly and take a look at 
another tool.

00:50:21.298 --> 00:50:22.933
Aside from Codelab there is 
another tool that we have that 

00:50:22.934 --> 00:50:26.421
is very similar and it's also  
notebook based.  You might have 

00:50:26.422 --> 00:50:28.445
heard of it.  It's called 
Kaggle.

00:50:32.334 --> 00:50:34.334
And while Kaggle is most known 
for its 

00:50:35.420 --> 00:50:38.023
competitions or discussion 
forums and datasets, it also has

00:50:38.024 --> 00:50:40.066
a feature called kernels and 
kernels is really just a 

00:50:44.144 --> 00:50:45.594
fancy name for notebook, and the
kernels look something like  

00:50:45.595 --> 00:50:47.595
this, this might 

00:50:48.685 --> 00:50:50.685
start to look familiar from 
Codelab except it's blue.

00:50:52.969 --> 00:50:56.679
Kernels is different in a couple
of subtle ways.  Firstly, I want

00:50:56.680 --> 00:50:58.680
to -- let's just 

00:51:00.365 --> 00:51:01.793
switch over to the demo for 
Kaggle kernels and we'll see 

00:51:01.794 --> 00:51:03.794
what that looks like.  How is 
that.

00:51:05.453 --> 00:51:07.453
So I took the notebook that we 
had 

00:51:10.745 --> 00:51:13.590
earlier in Codelab and I 
downloaded the notebook itself 

00:51:13.591 --> 00:51:15.640
as a notebook file.  Since 
Kaggle has datasets we actually 

00:51:18.297 --> 00:51:20.740
have a zoo animal classification
dataset, how convenient that I 

00:51:20.741 --> 00:51:22.986
chose the exact dataset that 
already exists on Kaggle, right.

00:51:23.190 --> 00:51:28.469
     And so we can click new 
kernel, and I'm going to choose 

00:51:28.470 --> 00:51:30.726
notebook, and Kaggle kernel is 
not only run in Python but you 

00:51:32.554 --> 00:51:35.435
can also choose to run them in R
for those of you who prefer R.

00:51:38.729 --> 00:51:40.729
And what I can do is I can 
actually 

00:51:43.217 --> 00:51:45.306
upload a notebook of -- well, 
first I need to download that 

00:51:45.307 --> 00:51:47.307
notebook, so let's do that.

00:51:50.139 --> 00:51:52.139
So from Codelab

00:51:53.669 --> 00:51:55.669
we will download -- this is what
I get for zooming in a lot.

00:51:58.186 --> 00:52:00.186
We download the notebook and 
then once 

00:52:01.293 --> 00:52:03.509
it's downloaded we can upload 
that notebook back into Kaggle 

00:52:03.510 --> 00:52:06.187
kernels and so boom, we have 
that same notebook now in 

00:52:06.389 --> 00:52:11.716
kernels.
     There is one small tweak.  

00:52:11.717 --> 00:52:13.717
Because we had to upload a file 
from 

00:52:14.766 --> 00:52:17.820
the local drive for Codelab 
we're going to get rid of that. 

00:52:17.821 --> 00:52:20.249
And the only other tweak is that
in Kaggle kernels the data lives

00:52:20.250 --> 00:52:24.326
in one directory up called 
Input.  So if I run the first 

00:52:24.327 --> 00:52:29.893
cell and then we'll work our way
down, we'll see that the data 

00:52:29.894 --> 00:52:32.213
lives in input and it's the same
kind of thing.  And since we 

00:52:32.214 --> 00:52:33.869
already see the rest of the 
notebook, it's literally the 

00:52:33.870 --> 00:52:36.303
same thing.
     What is interesting though 

00:52:36.304 --> 00:52:38.304
is that 

00:52:40.830 --> 00:52:43.100
in the kernels, we can -- let's 
see.  Let's give it a name.

00:52:51.042 --> 00:52:53.695
We can commit that notebook, and
what Kaggle kernels does is it 

00:52:53.696 --> 00:52:55.696
will run your 

00:52:57.180 --> 00:52:59.658
notebook in a new fresh 
environment, separate from the 

00:52:59.659 --> 00:53:02.114
session that you're in.  This 
will generate a notebook with 

00:53:02.115 --> 00:53:04.364
all the outputs that we've been 
seeing in a 

00:53:07.629 --> 00:53:09.479
kind of nice view-only format, 
which is really useful for 

00:53:09.480 --> 00:53:12.341
sharing because what if you want
to share your notebook as 

00:53:16.032 --> 00:53:18.269
well as how it was executed to 
others?  You want reproducible  

00:53:18.270 --> 00:53:20.270
notebooks so that's kind of what
we see here.

00:53:23.576 --> 00:53:25.615
We ran the notebook, it's done, 
and we can view that snapshot, 

00:53:25.616 --> 00:53:29.738
so it's kind of like a GitHub 
model of like you can commit 

00:53:29.739 --> 00:53:31.965
versions of your notebook.
     And so by default, your 

00:53:31.966 --> 00:53:37.911
notebooks are private and we can
see here that things ran.  The 

00:53:37.912 --> 00:53:39.912
outputs are shown, and I didn't 

00:53:41.393 --> 00:53:44.041
have to go through and run these
one by one, right.  If I ran 

00:53:44.042 --> 00:53:46.282
these cells out of order, 
manually earlier to yield my 

00:53:46.283 --> 00:53:48.343
results, running them top to 
bottom like this 

00:53:51.604 --> 00:53:54.063
will help catch those kind of 
bugs that would make it hard to 

00:53:54.064 --> 00:53:58.938
reproduce my performance down 
the road when I come back to 

00:53:58.939 --> 00:54:02.212
this next week or next month.
     And if you share your 

00:54:02.213 --> 00:54:07.526
notebooks, you can then fork 
them, you can fork them and you 

00:54:07.527 --> 00:54:09.970
can also get others to fork your
notebooks as well as add 

00:54:11.592 --> 00:54:14.036
collaborators to your  
notebooks, so you can add users 

00:54:14.037 --> 00:54:15.672
to join you on the same notebook
rather than just forking them.

00:54:15.673 --> 00:54:17.673
     So there is a lot of great 

00:54:20.786 --> 00:54:22.413
collaboration models across 
Colaboratory and Kaggle 

00:54:22.414 --> 00:54:24.414
depending on your particular 

00:54:26.276 --> 00:54:28.276
use case, and I guess it would 
be good 

00:54:29.341 --> 00:54:31.341
to also just briefly mention 
that like 

00:54:34.447 --> 00:54:36.474
Codelab, we can also enable GPUs
in Kaggle kernels as well, so 

00:54:36.475 --> 00:54:39.145
don't let that be a deciding 
factor for you.

00:54:42.439 --> 00:54:44.063
So that's Kaggle kernels, and 
let's switch back over to the 

00:54:44.064 --> 00:54:48.551
slides and talk about just a few
other kind of little tools and 

00:54:48.552 --> 00:54:50.804
tips and tricks before we kind 
of wrap things up.

00:54:56.100 --> 00:54:58.100
And if we could switch back to 
the slides.  Awesome.

00:55:01.213 --> 00:55:04.061
So let's say you don't want 
notebooks.  Let's say you try  

00:55:04.062 --> 00:55:06.312
Notebooks and they just weren't 
for you or you have a bigger 

00:55:06.313 --> 00:55:08.313
workload.

00:55:09.561 --> 00:55:11.796
You need to run a long running 
job, it's got to run for hours, 

00:55:11.797 --> 00:55:15.859
or you have a really big dataset
and it doesn't fit in Kaggle 

00:55:15.860 --> 00:55:17.480
kernels or doesn't fit on your 
local directory so you're not 

00:55:17.481 --> 00:55:19.744
going to upload it to Drive 
manually.

00:55:22.605 --> 00:55:24.424
Then you can use something like 
Cloud Machine Learning Engine 

00:55:24.425 --> 00:55:26.425
where you 

00:55:27.692 --> 00:55:30.535
can kick off a job to run a long
running job that maybe runs 

00:55:30.536 --> 00:55:32.778
across a set of distributed 
machines, all of them 

00:55:34.608 --> 00:55:36.051
potentially with GPUs attached. 
And once you're done, you might 

00:55:36.052 --> 00:55:38.306
want to serve your model,  
right.

00:55:41.362 --> 00:55:43.177
You want to make those 
prediction, but perhaps you want

00:55:43.178 --> 00:55:45.178
to do it at scale.  You're 
building a app, you train a 

00:55:47.513 --> 00:55:49.152
model, and you want to rest a 
REST endpoint that can serve 

00:55:49.153 --> 00:55:51.205
predictions to the world, and so
Machine Learning 

00:55:54.485 --> 00:55:56.485
Engine also includes a 
auto-scaling prediction service.

00:55:57.362 --> 00:55:59.362
You can literally just take your

00:56:01.019 --> 00:56:04.280
TensorFlow model, give it a 
name, and be done.  Literally, 

00:56:04.281 --> 00:56:07.135
point to the file and give it a 
name.  There aren't any other 

00:56:07.136 --> 00:56:09.136
steps because you can't do 
anything else in terms of 

00:56:10.843 --> 00:56:12.843
creating a model to serve, which
is really neat.

00:56:14.529 --> 00:56:16.529
And if you're working with 
things 

00:56:18.208 --> 00:56:20.068
like sigh kit learn or XGBoost 
and you want to use those as 

00:56:20.069 --> 00:56:22.069
well, we'll take those too, and 
then you don't have to 

00:56:24.963 --> 00:56:26.999
deal with the ops aspect of 
Machine Learning, play with the 

00:56:27.000 --> 00:56:30.055
data, tweak your model, train 
it, and deploy with ease.

00:56:32.735 --> 00:56:35.609
And I think most -- not most -- 
but many of you are also perhaps

00:56:35.610 --> 00:56:38.663
aware of the Machine Learning 
APIs that are also available.

00:56:41.703 --> 00:56:44.356
These are pre-trained APIs for 
doing various tasks.  They're a 

00:56:44.357 --> 00:56:46.357
little more canned, right.  You 
don't provide your own data, but

00:56:48.229 --> 00:56:51.884
it does mean that everything 
just works out of the box.  You 

00:56:51.885 --> 00:56:53.509
need  something -- you need a 
picture to be turned into a 

00:56:53.510 --> 00:56:57.390
description, you need audio to 
be turned into text, it just 

00:56:57.391 --> 00:56:59.621
works so that's kind of nice, 
but the limitations, of course, 

00:56:59.622 --> 00:57:04.784
are that you can't customize it 
for your specific use case.  

00:57:04.785 --> 00:57:07.039
That is still something that 
you'll have to just wait a 

00:57:07.040 --> 00:57:09.698
little bit longer for, with the 
one notable exception of 

00:57:13.989 --> 00:57:15.621
Vision, so the Auto ML Vision is
available right now in alpha so 

00:57:15.622 --> 00:57:20.711
you can apply for that and 
supply your own datasets to 

00:57:20.712 --> 00:57:23.394
train and customize your own 
Vision API, so that's kind of 

00:57:23.395 --> 00:57:27.055
like a neat side tangent.
     The animations that we saw 

00:57:27.056 --> 00:57:29.056
earlier 

00:57:31.572 --> 00:57:33.623
that I showed with the orange 
and blue dots came from the 

00:57:33.624 --> 00:57:36.095
TensorFlow  Playground.  So if 
you want to play around with a 

00:57:37.931 --> 00:57:40.582
neural network in your browser 
and just like toggle things on 

00:57:40.583 --> 00:57:44.499
and off and mess around, head 
over to playground.tensorFlow.

00:57:47.163 --> 00:57:48.991
org and you can do that right 
there.

00:57:48.992 --> 00:57:50.813
And don't worry, you can't break
it.  It's just through your 

00:57:50.814 --> 00:57:54.538
browser.  You can only break 
your own machine.  (Laughing).

00:57:54.539 --> 00:57:56.600
     And so what's next?

00:58:01.066 --> 00:58:03.066
The code that we just saw, I 
made it 

00:58:04.544 --> 00:58:06.544
public and it's on Kaggle at 
Kaggle.

00:58:07.799 --> 00:58:11.084
com/ Kaggle.
com/yufengg/zoo-demo.  If you 

00:58:11.085 --> 00:58:13.085
want to learn more about 
TensorFlow head to TensorFlow.

00:58:14.961 --> 00:58:16.789
org and there is a Machine 
Learning crash course Google 

00:58:16.790 --> 00:58:19.052
released recently, so if you 
want to really dive into the 

00:58:20.065 --> 00:58:21.707
concepts of Machine Learning and
go further than what we've 

00:58:21.708 --> 00:58:25.824
talked about today, head over 
there because there is basically

00:58:25.825 --> 00:58:28.087
a whole curriculum of videos and
kind of assignments that you can

00:58:28.088 --> 00:58:31.559
do and really build up your 
Machine Learning knowledge.

00:58:33.192 --> 00:58:35.039
Finally, if you're interested in
doing Machine Learning in the 

00:58:35.040 --> 00:58:37.040
Cloud you 

00:58:38.315 --> 00:58:40.315
can head over to the Cloud Dome,
the 

00:58:41.585 --> 00:58:44.045
white tent next to the Google 
Assistant Dome or head to Cloud.

00:58:44.858 --> 00:58:46.858
google.

00:58:48.739 --> 00:58:53.011
com/ML.  I host a video series 
by the same name of thissation. 

00:58:53.012 --> 00:58:55.457
AI Adventures, we explore some 
interesting nugget in each 

00:58:55.458 --> 00:58:58.959
episode about Machine Learning 
and try to do some hands 

00:58:58.960 --> 00:59:00.591
on-demos once in a while and do 
some interviews with interesting

00:59:00.592 --> 00:59:02.440
folks.
     So hopefully you'll check 

00:59:02.441 --> 00:59:06.506
it out and subscribe.
     So I want to thank you for 

00:59:06.507 --> 00:59:10.782
joining me in this  session, and
we really -- you know, I really 

00:59:10.783 --> 00:59:13.040
appreciate feedback on the 
session, the information, so 

00:59:14.874 --> 00:59:17.718
please head on over to the 
schedule and you can log in and 

00:59:17.719 --> 00:59:19.719
give some feedback.  Thanks a 
lot.

00:59:19.954 --> 00:59:21.954
(Applause).

00:59:27.570 --> 00:59:29.603
     &gt;&gt; Thank you for joining 
this session.

00:59:32.871 --> 00:59:34.293
Grand ambassadors will assist 
with directing you through the 

00:59:34.294 --> 00:59:37.553
designated exits.  We'll be 
making room for those who have 

00:59:37.554 --> 00:59:40.197
registered for the next session.
If you've registered for the 

00:59:40.198 --> 00:59:43.495
next session in this room, we 
ask that you please clear the 

00:59:43.496 --> 00:59:46.146
room and return via the 
registration line outside.  

00:59:46.147 --> 00:59:48.147
Thank you.

01:02:42.245 --> 01:02:44.245
May 10, 2018

01:02:45.515 --> 01:02:47.515
9:30 a.m.

01:02:50.361 --> 01:02:52.361
PT

01:03:07.343 --> 01:03:09.343
How to Kotlin

01:18:20.016 --> 01:18:22.016
The

01:21:39.703 --> 01:21:44.992
     &gt;&gt; At this time please find
your seat.  Our session will 

01:21:44.993 --> 01:21:46.993
begin soon.

01:24:04.800 --> 01:24:09.058
&gt;&gt; Hello and good morning!  It 
seems like the mic is not on.  

01:24:09.059 --> 01:24:13.129
Hello?  Hello?
     (Applause)

01:24:14.614 --> 01:24:17.318
     &gt;&gt; Hello.  The mic is not 
on.

01:24:21.791 --> 01:24:25.253
Hello!  This is better.  Thank 
you!  Thanks for being here this

01:24:25.254 --> 01:24:27.254
morning.

01:24:28.639 --> 01:24:32.738
My name is James and I'm part of
the Kotlin team at Google.  

01:24:32.739 --> 01:24:34.739
Today I have the pleasure of 

01:24:38.490 --> 01:24:40.321
introducing a very special guest
from Jetbrains who really 

01:24:40.322 --> 01:24:42.322
requires no introduction.

01:24:47.665 --> 01:24:49.665
Now, all of you know that Kotlin
is 

01:24:50.996 --> 01:24:53.063
now one of the most loved 
programming languages in the 

01:24:53.064 --> 01:24:55.064
world.
     (Applause)

01:25:01.433 --> 01:25:04.079
And at Google I/O it's very rare
for us to have external 

01:25:04.080 --> 01:25:06.080
speakers, but this 

01:25:07.326 --> 01:25:09.779
person was here last year and we
invited him back because he 

01:25:09.780 --> 01:25:11.780
couldn't think of 

01:25:13.870 --> 01:25:14.887
anybody else to better teach 
Kotlin other than one of the 

01:25:14.888 --> 01:25:16.888
people who 

01:25:19.369 --> 01:25:21.369
invented it, so please help me 
welcome 

01:25:24.522 --> 01:25:26.768
the Lead  Language Designer for 
Kotlin.  Andrey Breslav

01:25:27.571 --> 01:25:28.428
(Applause)

01:25:28.429 --> 01:25:30.429
&gt;&gt; ANDREA FOEGLER:  Thank you, 
James.

01:25:33.095 --> 01:25:38.374
     &gt;&gt; Hello, everybody.  I've 
very glad to be here.  Today I'm

01:25:38.375 --> 01:25:41.238
going to talk about what can it 
be, Kotlin, I guess, and I'm 

01:25:41.239 --> 01:25:46.396
really going to do a live demo 
so please bring my demo on.

01:25:47.819 --> 01:25:49.819
So the reason why I have this 

01:25:51.482 --> 01:25:53.482
horrible code in the slides is 
that we 

01:25:54.735 --> 01:25:57.400
are all learning and our old 
habits sometimes get in the way,

01:25:57.401 --> 01:25:59.401
so I'll be 

01:26:00.433 --> 01:26:03.317
presenting today on the topic of
how you get out of your Java 

01:26:03.318 --> 01:26:05.813
habits and get to your Kotlin 
habits, so we all come from 

01:26:07.244 --> 01:26:09.244
different backrounds, of course,
and 

01:26:10.926 --> 01:26:12.954
many of us started with the Java
Programming Language and built 

01:26:12.955 --> 01:26:17.037
up our knowledge of  programming
through this, and so we remember

01:26:17.038 --> 01:26:19.279
many things.  The thing is 
Kotlin has been inspired 

01:26:23.307 --> 01:26:25.307
by many languages complu 
languages including the Java 

01:26:26.717 --> 01:26:28.717
Programming Language, so many of
the 

01:26:30.587 --> 01:26:32.441
constructs in Java will work in 
Kotlin and you can get your job 

01:26:32.442 --> 01:26:35.487
done in this way, but in many 
cases it can be improved 

01:26:35.488 --> 01:26:36.950
dramatically.
     This particular example is 

01:26:36.951 --> 01:26:40.651
about declaring classes, and can
you see here that I have a 

01:26:40.652 --> 01:26:43.327
Kotlin class on the left and a 
Java class on the right and they

01:26:44.551 --> 01:26:46.551
look very similar, but this is 

01:26:48.048 --> 01:26:50.114
definitely not how we write 
Kotlin code.  What you're 

01:26:50.115 --> 01:26:52.149
actually supposed to do is like 
remove all the unnecessary 

01:26:52.150 --> 01:26:54.150
stuff.

01:26:57.045 --> 01:26:58.666
What I have to say here is one 
class, that's it, I can try to 

01:26:58.667 --> 01:27:00.696
transform by hand, but I 
actually want to show off a 

01:27:03.768 --> 01:27:05.627
nice tool and simply copy and 
paste the code from the Java 

01:27:05.628 --> 01:27:07.628
site to Kotlin site 

01:27:09.693 --> 01:27:12.369
and so it will use the Java to 
Kotlin converter built in and do

01:27:12.370 --> 01:27:14.848
it for me, so boom, there it is 
a single line that's 

01:27:18.522 --> 01:27:20.522
actually all you needed --
     (Applause).

01:27:21.160 --> 01:27:23.999
-- to declare one class and two 
properties, and that's it.  All 

01:27:24.000 --> 01:27:26.000
I have here is a class with a 

01:27:27.776 --> 01:27:29.419
primary constructer, so it has 
two parameters and both of them 

01:27:29.420 --> 01:27:31.420
are properties and that's all we
wanted to say.

01:27:33.536 --> 01:27:35.536
So this is one of the things 
that 

01:27:36.614 --> 01:27:38.807
demonstrates how cheap declaring
classes is in Kotlin, and there 

01:27:38.808 --> 01:27:44.980
is a consequence to this.  So 
look at this code.  Here it's 

01:27:44.981 --> 01:27:47.224
obviously not how you're 
supposed to write code in any 

01:27:47.225 --> 01:27:51.744
language, actually.  I wanted to
parse a full name into a first 

01:27:51.745 --> 01:27:54.379
name and last name, and so 
that's what I'm doing  here, but

01:27:54.380 --> 01:27:56.380
how do I pack 

01:27:57.629 --> 01:27:59.301
the results for the function?  I
don't have a way of returning 

01:27:59.302 --> 01:28:01.731
two things from a function.  I 
have to put it into one object, 

01:28:01.732 --> 01:28:03.732
and 

01:28:06.211 --> 01:28:08.047
I'm using a list here and then 
awkwardly taking out one and the

01:28:08.048 --> 01:28:11.980
other to make a first name and 
last name.  Don't do this in any

01:28:11.981 --> 01:28:13.981
language.
     But there is a kind of 

01:28:15.065 --> 01:28:17.497
psychological reason to doing 
there, at least in our old 

01:28:17.498 --> 01:28:21.607
habits because declaring classes
is expensive, right.  You have 

01:28:21.608 --> 01:28:24.054
to create a new file, put a lot 
of code in it, it's kind of 

01:28:24.055 --> 01:28:26.095
awkward.  But in Kotlin you 
don't have to do this.

01:28:30.640 --> 01:28:32.640
All you need to say is my class 

01:28:33.671 --> 01:28:35.671
fullname with first and last 
names as 

01:28:36.787 --> 01:28:38.787
properties, and then all I need 
to do 

01:28:40.995 --> 01:28:42.995
here is just return that, right.

01:28:45.487 --> 01:28:47.487
So my full name, here it goes, 
and 

01:28:49.769 --> 01:28:51.769
now instead of indices I can say
first 

01:28:53.636 --> 01:28:55.258
and last right here so that's 
the idea.  Classes being cheap 

01:28:55.259 --> 01:28:59.520
is not only saving you time at 
the declaration site, but it's 

01:28:59.521 --> 01:29:01.354
saving you mental effort.  You 
can represent your multiple 

01:29:01.355 --> 01:29:04.213
return as a class and you 
doesn't cost you anything.

01:29:07.682 --> 01:29:08.929
So around this you see that my 
equals doesn't work, obviously, 

01:29:08.930 --> 01:29:10.930
because that's 

01:29:12.244 --> 01:29:14.244
a single line class and so now 
I'll go 

01:29:15.250 --> 01:29:17.250
to

01:29:20.024 --> 01:29:21.701
declare equal there and then 
hash there but really I don't 

01:29:21.702 --> 01:29:23.702
need to do this in Kotlin 
because you probably know there 

01:29:24.793 --> 01:29:26.022
is something called data 
classes, right, who knows data 

01:29:26.023 --> 01:29:28.023
classes?  Many people.  Good.

01:29:30.736 --> 01:29:32.765
So you know that I simply put 
this single keyword there and it

01:29:32.766 --> 01:29:37.247
generates many things for me, 
equals,  hashcode, string, and 

01:29:37.248 --> 01:29:40.919
many other convenient methods, 
so that's it.  Change your mind 

01:29:40.920 --> 01:29:42.920
about how expensive a class  is.

01:29:47.237 --> 01:29:49.237
You can use it easily in all of 
your abstractions.

01:29:52.316 --> 01:29:55.174
So more or less done with the 
warmup, so let's look at 

01:29:55.175 --> 01:29:57.612
something else, properties.
     So we talked about classes,

01:29:57.613 --> 01:29:59.859
we'll go through  properties and
then go over to functions.

01:30:02.944 --> 01:30:05.809
So here is a property done the 
way you shouldn't do it in 

01:30:05.810 --> 01:30:07.430
Kotlin, again, and so the 
properties I showed you before 

01:30:07.431 --> 01:30:09.431
were 

01:30:12.121 --> 01:30:13.946
kind of  one-liners were both is
trivial.  If you want a custom  

01:30:13.947 --> 01:30:15.947
setter you definitely don't 
define functions for that.

01:30:19.261 --> 01:30:20.897
You have your syntax as you 
probably know, if you know data 

01:30:20.898 --> 01:30:22.898
classes you know 

01:30:28.212 --> 01:30:30.053
that, and so inside you have 
field to write to your backend 

01:30:30.054 --> 01:30:32.694
storage, but that's it.  You 
don't need to introduce extra 

01:30:34.145 --> 01:30:35.161
names and anything else, so 
that's straightforward, right.

01:30:38.242 --> 01:30:40.242
But then look at this code.

01:30:41.707 --> 01:30:43.571
So here is already some sensible
logic.  I have two properties 

01:30:43.572 --> 01:30:45.572
and one of them 

01:30:49.476 --> 01:30:51.502
is private and nullable and 
mutable, and on my first access 

01:30:51.503 --> 01:30:53.541
I'm checking if that's now and 
then I compute a value 

01:30:57.004 --> 01:30:59.236
and write into it and then I 
output return from my getter.  

01:30:59.237 --> 01:31:01.312
So what is it?  It's a lazy 
property, right.

01:31:04.797 --> 01:31:06.830
I personally wrote dozens and 
dozens of those in Java and many

01:31:06.831 --> 01:31:08.831
other 

01:31:10.289 --> 01:31:13.126
languages so I got kind of bored
by that and that's why Kotlin 

01:31:13.127 --> 01:31:15.779
has delegation for property, so 
delegated properties let 

01:31:18.830 --> 01:31:21.506
you get a read of all the 
reputation of this lazy  logic. 

01:31:21.507 --> 01:31:24.007
All you care about is this 
expression here, so let's just 

01:31:24.008 --> 01:31:26.008
do it.

01:31:29.492 --> 01:31:31.492
Implement my property by just 
lazy of 

01:31:34.101 --> 01:31:38.920
all this.  This is it.  So what 
I'm having now, I'm saying 

01:31:42.549 --> 01:31:44.549
that my property is not simply

01:31:45.876 --> 01:31:48.116
simply initiallyized by 
something but delegated to this 

01:31:48.117 --> 01:31:50.366
lazy thing here and upon first 
access this lambda will be 

01:31:52.189 --> 01:31:54.018
executed and the rest will be 
stored by the library, so lazy 

01:31:54.019 --> 01:31:58.114
is not a language construct, but
it's just a library function.  

01:31:58.115 --> 01:32:00.115
You can define your own, and the

01:32:01.178 --> 01:32:03.178
library provides you with many 
other things.

01:32:04.687 --> 01:32:06.687
The takeaway here is that if you

01:32:07.944 --> 01:32:09.571
have a common kind of property 
like observable, for example, 

01:32:09.572 --> 01:32:14.273
when you need to be notified 
that something was modified, use

01:32:14.274 --> 01:32:16.274
a library or write your own.

01:32:17.523 --> 01:32:19.523
So here delegates.

01:32:21.791 --> 01:32:23.219
observable does the job from the
library, but if you like you 

01:32:23.220 --> 01:32:25.220
don't have 

01:32:26.278 --> 01:32:27.903
to write code like this when you
have one property and then the 

01:32:27.904 --> 01:32:31.378
other property and the other 
doing the same thing over and 

01:32:31.379 --> 01:32:33.379
over again.

01:32:34.434 --> 01:32:36.434
All you need to do is this, 
actually, 

01:32:38.085 --> 01:32:40.120
declare a single class that 
encapsulates the logic of your 

01:32:40.121 --> 01:32:42.121
property like your 

01:32:43.169 --> 01:32:45.169
generic getter and generic 
setter and that's it.

01:32:46.827 --> 01:32:48.651
Now you can simply refer to this
class in many properties and get

01:32:48.652 --> 01:32:50.652
your business 

01:32:52.999 --> 01:32:54.824
logic database access, all kinds
of validation, anything you like

01:32:54.825 --> 01:33:00.123
can be extracted as a library 
and then reused across your 

01:33:00.124 --> 01:33:03.622
project.  Does that make sense? 
Who uses this already?

01:33:07.071 --> 01:33:08.708
So many people, you actually  
should.  I'm sure you can 

01:33:08.709 --> 01:33:12.864
benefit from this.
     Okay.  So this is more or 

01:33:12.865 --> 01:33:15.884
less it about properties and now
let's get to functions.  

01:33:15.885 --> 01:33:17.885
Functions are very important, 
right.

01:33:19.764 --> 01:33:21.792
So again, this is very horrible 
code.  Don't try code like this 

01:33:21.793 --> 01:33:25.690
in Kotlin, please.  This is very
much inspired by our habits in 

01:33:25.691 --> 01:33:27.315
the Java programming language.  
When you have to put everything 

01:33:27.316 --> 01:33:29.316
into a class, right.

01:33:32.386 --> 01:33:34.386
So string, does your project has
its 

01:33:36.881 --> 01:33:38.708
own string class, if it doesn't 
it's just a very new project, 

01:33:38.709 --> 01:33:40.709
right?
     (laughter).

01:33:42.369 --> 01:33:44.205
So any of my projects have them,
but the thing is in Kotlin it's 

01:33:44.206 --> 01:33:48.881
a little different.  You don't 
have to use a class.  First of 

01:33:48.882 --> 01:33:50.882
all, a Kotlin class, they don't 
have statics so to use these 

01:33:53.150 --> 01:33:56.010
functions from this class you 
have to say string into 

01:33:56.011 --> 01:33:59.089
parentheses which makes a new 
object, right.  I don't want a 

01:33:59.090 --> 01:34:01.526
new object every time, I want it
like this, so I turn this class 

01:34:01.527 --> 01:34:06.016
into an object.  It's a little 
bit of an improvement in my 

01:34:06.017 --> 01:34:07.451
insanity, right, so I was 
creating an object every time I 

01:34:07.452 --> 01:34:09.452
wanted to call a function, 
that's crazy.

01:34:11.925 --> 01:34:13.763
But really in Kotlin, I don't 
need any enclosing container at 

01:34:13.764 --> 01:34:15.786
all because I have top-level 
functions.

01:34:18.646 --> 01:34:21.499
So this may seem obvious, like 
functions, what are they?  

01:34:21.500 --> 01:34:23.564
They're just declarations, 
right?  But some  languages, you

01:34:23.565 --> 01:34:27.218
know, have them all in classes 
and many people learned this and

01:34:27.219 --> 01:34:29.461
rely on this.  So this is a lot 
more of a Kotlin way, 

01:34:33.402 --> 01:34:36.056
but it's still not great in 
terms of what you can achieve 

01:34:36.057 --> 01:34:38.057
with Kotlin because here you 
have two overloads, right.

01:34:42.370 --> 01:34:44.598
So getfirstword is supposed to 
parse a string, find a space and

01:34:44.599 --> 01:34:47.853
take the first word returned, 
right.  But what if the 

01:34:47.854 --> 01:34:49.854
separator is not a 

01:34:51.057 --> 01:34:55.210
space but a comma or something, 
so here is a more full featured 

01:34:55.211 --> 01:34:57.278
version and this is how you'll 
call it actually in most 

01:34:57.690 --> 01:34:59.315
contexts, right.
     So what I wanted to express

01:34:59.316 --> 01:35:03.598
here is just a default value.  
In Java we are used to using 

01:35:03.599 --> 01:35:05.635
overloads for this, and also 
some people use 

01:35:08.907 --> 01:35:10.542
nullable parameters like tasks 
and null here and I'll give you 

01:35:10.543 --> 01:35:14.805
a default value.  Don't do this 
in Kotlin.  You don't need to.  

01:35:14.806 --> 01:35:19.313
All you need to do, actually, is
simply specify your default.  My

01:35:19.314 --> 01:35:23.177
default is space here, and 
that's it.  All right.  So there

01:35:23.178 --> 01:35:25.178
is no need to emulate defaults.

01:35:27.256 --> 01:35:29.091
They are built into the 
language.

01:35:29.092 --> 01:35:30.706
And same for when you have many,
many default parameters with 

01:35:30.707 --> 01:35:32.707
different 

01:35:35.432 --> 01:35:37.862
values like multiple Boolean and
so on and so forth, you can just

01:35:37.863 --> 01:35:40.960
use named parameter syntax to 
express which you actually need 

01:35:40.961 --> 01:35:44.462
and all the rest will be used by
default.  So this makes 

01:35:44.463 --> 01:35:46.899
functions fewer in the first 
place, and then a lot more 

01:35:47.091 --> 01:35:50.967
expressive.
     Okay.  Good with functions,

01:35:50.968 --> 01:35:52.968
right?

01:35:54.004 --> 01:35:55.435
Well, actually, this is kind of 
-- this function is kind of 

01:35:55.436 --> 01:35:57.436
midway between 

01:35:59.717 --> 01:36:01.154
like the Kotlin style and the 
Java style because it's actually

01:36:01.155 --> 01:36:05.621
working on strengths, right.  
Very much a good idea to put 

01:36:05.622 --> 01:36:09.139
this into a string class -- oh, 
it's not because the string 

01:36:09.140 --> 01:36:11.140
class is not controlled by 

01:36:13.040 --> 01:36:14.466
you and we can put everything 
into the string class and if you

01:36:14.467 --> 01:36:18.558
really want to keep the string 
API minimal, so what I would 

01:36:18.559 --> 01:36:20.559
really like to do is something 
like this.

01:36:23.270 --> 01:36:24.887
Where I can say my string 
getfirstword and that's it.  

01:36:24.888 --> 01:36:29.398
Right.  So it looks like a 
method, it's called an extension

01:36:29.399 --> 01:36:33.907
function, actually.  It's not 
sitting in a string class.  I 

01:36:33.908 --> 01:36:35.908
didn't go into the  JDK and 
alter 

01:36:36.955 --> 01:36:38.955
the class I can control, but 
still it works like this.

01:36:41.073 --> 01:36:43.736
So this is a mechanism you can 
use.  I'll do it manually to 

01:36:43.737 --> 01:36:46.781
illustrate how it works here.  
So I have a receiver, I've typed

01:36:49.238 --> 01:36:51.238
string, now I don't need this 
parameter 

01:36:54.563 --> 01:36:56.563
anymore, and I can say that this
here 

01:37:00.067 --> 01:37:01.321
and use my "this" here or omit 
"this" on the left-hand side so 

01:37:01.322 --> 01:37:05.413
now I'll be able to use it this 
way.  Make sense?

01:37:08.616 --> 01:37:10.500
Ky
     I can do the same with a 

01:37:10.501 --> 01:37:14.240
property, actually it would be 
very nice to do it this way.  

01:37:14.241 --> 01:37:17.162
Just have first word as a 
property name, and you can have 

01:37:17.163 --> 01:37:21.657
an extension "property," but of 
course there will be no 

01:37:21.658 --> 01:37:23.696
customization for the separator,
but otherwise you're good to go.

01:37:24.506 --> 01:37:27.592
Yep.  I'll just need to put a 
space here.  And that's it.

01:37:29.925 --> 01:37:31.754
So extension functions, 
extension properties, it's 

01:37:31.755 --> 01:37:33.755
actually very important 

01:37:35.841 --> 01:37:38.157
idea, it's not only just salines
convenience.  It allows you to 

01:37:38.158 --> 01:37:40.604
keep your classes very minimal. 
Look at the string class in 

01:37:40.605 --> 01:37:45.756
Kotlin.  It's only five methods.
If you can compare that to Java 

01:37:45.757 --> 01:37:47.757
it's screens and screens of 
declarations, so 

01:37:50.633 --> 01:37:52.633
you can keep your API minimal 
and all 

01:37:53.960 --> 01:37:56.019
the utility functions can be 
extensions, and certain 

01:37:56.020 --> 01:37:58.686
different libraries can be 
modular like this and that's a 

01:37:58.687 --> 01:38:01.572
very important tool for 
designing APIs.

01:38:03.591 --> 01:38:07.870
Do you have questions?  Okay.  I
couldn't take them anyway.

01:38:10.330 --> 01:38:14.032
(laughter).
     Okay.  Now, let's have a 

01:38:14.033 --> 01:38:16.033
look at this.  Here I'm doing 
something very typical 

01:38:19.291 --> 01:38:21.291
and pro verse

01:38:24.650 --> 01:38:26.282
traversing a hierarchy, 
containers and leaf elements, 

01:38:26.283 --> 01:38:28.522
and I want to extract all the 
text from this hierarchy.  

01:38:28.523 --> 01:38:30.523
Pretty straightforward.

01:38:32.811 --> 01:38:34.650
So my classes are three lines of
code.  Not much, there is an 

01:38:34.651 --> 01:38:36.651
element, a 

01:38:38.539 --> 01:38:40.539
container, and with the list of 
children there is text.

01:38:42.216 --> 01:38:43.855
Now I'm traversing this so I'm 
using extensions function, 

01:38:43.856 --> 01:38:47.518
top-level functions, everything 
as I told you, and so all right 

01:38:47.519 --> 01:38:49.519
but I don't like this code.
     Why don't I like it?

01:38:54.428 --> 01:38:57.314
Here to traverse the hierarchy I
need recursion so I need to pass

01:38:57.315 --> 01:39:01.792
the string builder down the 
stack and add to it as I'm going

01:39:01.793 --> 01:39:06.929
down the tree.  But then I end 
up with a top-level function 

01:39:06.930 --> 01:39:08.930
that's only needed by this one 

01:39:10.988 --> 01:39:12.620
here, right, so this one is not 
really anywhere but inside this 

01:39:12.621 --> 01:39:14.621
function, so what I'd really 
like to do is just put 

01:39:19.271 --> 01:39:21.271
it inside

01:39:22.845 --> 01:39:24.845
, just go here and make it a 
local function.

01:39:24.883 --> 01:39:26.883
So again, it's just expressing 
that nobody else needs this.

01:39:30.023 --> 01:39:31.036
You don't need private helpers 
anymore or look for local 

01:39:31.037 --> 01:39:34.493
helpers.  It can be improved a 
little bit and you can actually 

01:39:34.494 --> 01:39:36.494
make use of closure, so 

01:39:38.154 --> 01:39:41.001
I can create my string builder 
right here and get rid of all of

01:39:41.002 --> 01:39:43.002
this so I 

01:39:44.452 --> 01:39:46.452
don't need to return or take 
parameters here.

01:39:48.966 --> 01:39:50.966
All I need here is to use 
whatever is declared above.

01:39:54.898 --> 01:39:56.898
And then I just do extract text 
of 

01:39:59.220 --> 01:40:03.692
e right here and return string 
builder to string.  I'm sorry, 

01:40:03.693 --> 01:40:05.693
that's an extension function, 
right.

01:40:09.425 --> 01:40:13.910
No, sorry.  Yeah.  So here is 
how it goes.  Like, you can turn

01:40:13.911 --> 01:40:18.376
something into a local function 
and a leverage closure, so this 

01:40:18.377 --> 01:40:20.664
variable is declared outside my 
function, it's not accessible to

01:40:20.665 --> 01:40:22.665
anyone 

01:40:23.917 --> 01:40:25.917
outside the outer, and I'm using
here and that's it.

01:40:28.413 --> 01:40:31.258
Now, local functions, extension 
functions, deep level functions,

01:40:31.259 --> 01:40:34.313
default parameters use these, 
they will make your code nicer.

01:40:34.314 --> 01:40:39.602
     Now, let's look at what's 
still there.  You see gray code?

01:40:40.827 --> 01:40:42.827
Gray code is useless.  The ID in
the compiler show you that 

01:40:44.550 --> 01:40:49.335
something is not needed there 
and it actually isn't.  This is 

01:40:49.336 --> 01:40:52.084
redundant because we have this 
extract  here, right, so you 

01:40:52.085 --> 01:40:54.085
simply can remove this.  I don't
know if you see it -- oh, yeah 

01:40:56.622 --> 01:40:58.622
you do, but the text variable 
has gone green.  Why it's green?

01:41:00.888 --> 01:41:03.326
Because the compiler can figure 
out the cast for you.  It's 

01:41:03.327 --> 01:41:06.989
actually much safer.  It's not 
only convenient, but I'm really 

01:41:06.990 --> 01:41:08.990
annoyed at my  casts all over 
the place, right.

01:41:10.655 --> 01:41:12.655
I know it's text, why don't you 
know?

01:41:14.331 --> 01:41:17.188
Well, now it knows, and actually
you don't need this variable 

01:41:17.189 --> 01:41:19.189
either because 

01:41:20.644 --> 01:41:22.810
it's the only usage, right.  The
same thing here and then my 

01:41:24.096 --> 01:41:26.096
container can be in line as 
well, and so 

01:41:27.173 --> 01:41:29.173
here it is.

01:41:30.428 --> 01:41:32.680
Smart cast, makes your code 
safer and more concise, and 

01:41:32.681 --> 01:41:34.681
actually it makes all the casts 
that are still in your program 

01:41:36.163 --> 01:41:39.009
meaningful, so when you see an 
as operator in Kotlin now you 

01:41:39.010 --> 01:41:41.040
know it means something, it's 
not just a useless 

01:41:44.701 --> 01:41:46.701
complement to the is check 
above.

01:41:46.754 --> 01:41:48.754
Also, this thing here is kind of

01:41:51.183 --> 01:41:54.687
stupid because what I'm doing is
applying the same function for 

01:41:54.688 --> 01:41:56.688
everything as a single function,
so what I want to do is 

01:41:56.931 --> 01:41:58.931
something like this.

01:42:01.474 --> 01:42:03.474
It's a little bit nicer looking.

01:42:05.556 --> 01:42:08.192
And then let's look at what we 
have.  We're  traversing 

01:42:08.193 --> 01:42:11.625
hierarchy, I have my leaves, I 
have my  containers, and that's 

01:42:11.626 --> 01:42:13.704
what I want to express, right?  
I'm checking different case, so 

01:42:13.705 --> 01:42:15.705
to do 

01:42:17.559 --> 01:42:20.223
that it's a lot nicer to use a 
"when" statement, so when can 

01:42:20.224 --> 01:42:24.309
switch in types right here, but 
there is an annoying thing about

01:42:24.310 --> 01:42:26.310
it, and it's again coming 

01:42:29.993 --> 01:42:32.432
from my old habits on declaring 
at close hierarchy I have only 

01:42:32.433 --> 01:42:34.433
containers and text and I don't 
have anything else, right.

01:42:38.590 --> 01:42:40.624
Now I have this pretty annoying 
"else case" right here.  The 

01:42:40.625 --> 01:42:42.708
compiler has no idea.  I don't 
have anything but continues and 

01:42:42.709 --> 01:42:44.709
text.

01:42:47.596 --> 01:42:50.469
It's just an abstract text and I
have some cases there.  You can 

01:42:50.470 --> 01:42:53.655
actually express this in Kotlin 
with sealed.  I can have a seal 

01:42:53.656 --> 01:42:56.586
class which means all the 
subclasses are known, you can 

01:43:00.576 --> 01:43:02.576
declare them outside of this 
file, and 

01:43:03.902 --> 01:43:06.837
this way the ID and compiler 
know this "else" is useless.

01:43:07.368 --> 01:43:08.122
We went from almost two screens 
of code to less than one, simply

01:43:08.123 --> 01:43:10.997
applying the idiom it's of 
Kotlin to this code.  Do you 

01:43:10.998 --> 01:43:14.264
have questions?
     I'm sorry.  (Laughing).

01:43:15.885 --> 01:43:17.885
All right.

01:43:19.356 --> 01:43:21.356
So now let's just continue with 
this 

01:43:22.403 --> 01:43:25.043
exercise and look at some more 
examples of expressions that are

01:43:25.044 --> 01:43:27.072
written like with old habits in 
mind and we'll try to 

01:43:30.529 --> 01:43:32.151
transform them into something 
better.  So first thing that 

01:43:32.152 --> 01:43:36.036
really stands out here is Var.  
I can't say never use Var.

01:43:39.705 --> 01:43:41.705
Vars are useful mutable 
variables, can 

01:43:43.799 --> 01:43:47.304
be used for many nice things, 
but it's kind of discouraged.  

01:43:47.305 --> 01:43:49.305
If you need a  Var, you need a 
very good reason.

01:43:51.428 --> 01:43:52.881
Here is now a good reason using 
a Val, definitely.  Let's look 

01:43:52.882 --> 01:43:56.348
at these three.  It's 
repetition.  Repetition is ugly,

01:43:56.349 --> 01:43:58.381
repetition is  errorprone, 
especially if this was not a 

01:44:01.638 --> 01:44:02.835
single name but many things 
chained, so I would like to get 

01:44:02.836 --> 01:44:04.836
rid of this repetition.

01:44:10.086 --> 01:44:11.698
What I can do is say with ex -- 
does anybody remember Pascal?  

01:44:11.699 --> 01:44:14.761
Anyone?  Good.  Good.  I started
in Pascal almost.

01:44:18.647 --> 01:44:20.069
So it hid this thing which is a 
building construct, and in 

01:44:20.070 --> 01:44:24.586
Kotlin it's a function and we 
can use it and here we can get 

01:44:24.587 --> 01:44:28.253
rid of all the ex things here.  
Just like this.  And now it 

01:44:28.254 --> 01:44:30.310
looks even more stupid, right.  
I'm just  assigning to the same 

01:44:32.539 --> 01:44:34.539
variables, don't do that.
     Okay.

01:44:37.212 --> 01:44:38.857
So now I have a printline with 
string plus some things, string 

01:44:38.858 --> 01:44:42.728
plus some things, string plus 
some things.  It's awkward.  

01:44:42.729 --> 01:44:44.354
Most languages now have string 
interpolation and Kotlin has 

01:44:44.355 --> 01:44:48.606
that as well, so what you 
actually need here is this.

01:44:49.575 --> 01:44:51.575
Okay.

01:44:52.701 --> 01:44:54.956
Done with this one.  Import 
things into your scope with 

01:44:56.784 --> 01:44:58.784
"with" use string interpolation,
it's nice.

01:45:00.442 --> 01:45:04.099
Now here I'm creating a map, the
old way.  I can kind of make it 

01:45:04.100 --> 01:45:06.100
a little nicer 

01:45:08.790 --> 01:45:11.256
like this by using my operators,
which it's really much nicer if 

01:45:11.257 --> 01:45:13.900
I just use a builder function, 
so what I can do here 

01:45:18.731 --> 01:45:20.731
is replace

01:45:26.846 --> 01:45:28.670
my things with pairs -- oh -- 
not pairs, but pair.  I'm sorry,

01:45:28.671 --> 01:45:30.671
typing when talking is 
difficult.

01:45:31.324 --> 01:45:34.182
Yeah, so a map can be 
constructed of pairs, right.  A 

01:45:34.183 --> 01:45:37.028
map is only a set of pairs from 
key to value, but actually pairs

01:45:37.029 --> 01:45:39.029
are kind of 

01:45:42.555 --> 01:45:46.653
redundant in this so we're 
usually using the to function.  

01:45:46.654 --> 01:45:48.258
It's not a built in operator but
just a library function here and

01:45:48.259 --> 01:45:50.259
this is how you create a map.

01:45:51.958 --> 01:45:53.958
When you want to traverse the 
map, you 

01:45:56.867 --> 01:45:58.867
can say here key, and value, and
just a 

01:46:00.162 --> 01:46:02.162
few variables like this which 
makes four 

01:46:05.373 --> 01:46:07.373
loops a lot more concise.

01:46:09.377 --> 01:46:10.646
This example of code with my 
"if" statement is something I 

01:46:10.647 --> 01:46:12.647
really hate about my code in 
Java because it's like 

01:46:15.495 --> 01:46:17.495
these assignments here, they all
fall 

01:46:18.759 --> 01:46:20.759
apart so easily, so I really 
like to do 

01:46:24.070 --> 01:46:25.525
things like this in Kotlin -- so
if -- and many other things are 

01:46:25.526 --> 01:46:30.422
actually expressions.  This is 
something pretty unfamiliar for 

01:46:30.423 --> 01:46:33.325
this C language family.  We're 
used to dividing our code into 

01:46:34.340 --> 01:46:36.773
statements and  expresses, 
right.  Statements are things 

01:46:36.774 --> 01:46:40.454
that have effects and 
expressions are things that have

01:46:40.455 --> 01:46:41.893
value, so you assign expressions
to variables and write 

01:46:41.894 --> 01:46:43.894
statements to assign things to 
things.

01:46:45.574 --> 01:46:47.574
So Kotlin is halfway between 
this 

01:46:49.241 --> 01:46:51.736
procedural tradition and 
functional tradition but has a 

01:46:51.737 --> 01:46:55.417
lot more expressions than you're
used to in other languages.  You

01:46:55.418 --> 01:46:57.418
can do this here and of course 
you 

01:46:59.094 --> 01:47:01.094
don't have to use a Var, you 
don't have to make it a 

01:47:03.581 --> 01:47:06.230
different line, you can assign 
it right aso if expression, make

01:47:06.231 --> 01:47:09.484
it nicer, and the result of the 
expression is the all lasting in

01:47:09.485 --> 01:47:11.485
the block.

01:47:13.143 --> 01:47:15.616
So the same for When.  When is 
not simply switch case on 

01:47:18.878 --> 01:47:20.878
steroid, it's largely 
importantly an 

01:47:23.175 --> 01:47:26.017
expression, so you can also do 
it like this, right.  So not 

01:47:26.018 --> 01:47:29.753
many returns here but one return
here will be a lot nicer.  Also,

01:47:29.754 --> 01:47:32.167
you're going to story to repeat 
yourself, of course, this much 

01:47:32.168 --> 01:47:34.168
and you 

01:47:36.980 --> 01:47:39.042
can say even this.
     By the way, if you want to 

01:47:39.043 --> 01:47:41.229
check if something is odd and 
even, don't do it like me.  It's

01:47:41.230 --> 01:47:43.473
only for demo  purposes.  Don't 
try this at home.  It will hurt.

01:47:43.690 --> 01:47:49.456
     (laughter).
     Yeah, so this one can be 

01:47:49.457 --> 01:47:51.457
further 

01:47:52.924 --> 01:47:55.798
simplified like  this, so again 
you're trying to remove the 

01:47:55.799 --> 01:48:00.475
noise.  When you see code like 
this, just try to get rid of the

01:48:00.476 --> 01:48:02.504
noise.  Noise is harmful for 
your brain.

01:48:05.159 --> 01:48:08.880
Last thing just a quick thing 
with what you do with nullable. 

01:48:08.881 --> 01:48:11.350
These question marks are with 
nullable types in Kotlin.  How 

01:48:11.351 --> 01:48:14.201
many people, I'll go really, 
really quick, so you can have 

01:48:14.202 --> 01:48:18.075
nullable types and compiler 
makes you do things like this, 

01:48:18.076 --> 01:48:20.156
and so it's an error now.  The 
string is nullable you can't 

01:48:20.991 --> 01:48:22.991
reference it.

01:48:24.127 --> 01:48:25.954
You can either do this, which 
says just safely reference me, 

01:48:25.955 --> 01:48:28.597
which you by the way can do here
as well, right, so 

01:48:34.925 --> 01:48:36.925
you don't have to write an  "if"
around 

01:48:39.017 --> 01:48:41.017
it and can simplify it like 
this.

01:48:43.520 --> 01:48:45.971
Another nice thing is you can 
use an alias operator like this,

01:48:45.972 --> 01:48:50.347
so to simplify your like longer 
if statements into something, 

01:48:50.348 --> 01:48:52.348
and this is kind of curious 
because this is definitely the 

01:48:55.039 --> 01:48:57.060
expression position, right, so 
how Elvis works.  Elvis takes an

01:48:57.061 --> 01:49:01.145
expression on the left-hand side
of string and asks, are you a 

01:49:01.146 --> 01:49:03.146
null?

01:49:04.415 --> 01:49:05.071
Really nicely, and if it's a 
null it evaluates the right-hand

01:49:05.072 --> 01:49:07.072
side.

01:49:08.131 --> 01:49:09.956
But the right-hand side has to 
be an expression, right.  

01:49:09.957 --> 01:49:12.620
Basically, it's supposed to be a
default, so like if you are null

01:49:12.621 --> 01:49:14.669
on the left-hand side, like use 
a default on 

01:49:19.159 --> 01:49:20.782
the right-hand side, but your 
default can't be just a return, 

01:49:20.783 --> 01:49:23.233
which means you don't compute 
any value there, you just jump 

01:49:23.234 --> 01:49:25.060
out of the function.
     That's a quite an 

01:49:25.061 --> 01:49:29.549
interesting thing from the type 
system standpoint but I'm not 

01:49:29.550 --> 01:49:31.550
giving a lecture here, I'm doing
a demo.

01:49:31.803 --> 01:49:35.858
Okay.  We're good with 
expressions.  Let's look at some

01:49:35.859 --> 01:49:37.859
functional style.

01:49:38.924 --> 01:49:41.372
So people very often refer to 
Kotlin as a functional language.

01:49:42.996 --> 01:49:44.996
I don't think it is, actually.

01:49:47.067 --> 01:49:48.285
I think Kotlin is a 
multiparadigm language that 

01:49:48.286 --> 01:49:49.927
supports functional style.  You 
don't have to write functional 

01:49:49.928 --> 01:49:51.928
in Kotlin, but it's often times 
very nice 

01:49:54.596 --> 01:49:56.596
to do it, so let's have a look 
at this.

01:49:57.652 --> 01:49:59.652
So in my Java old days in mind, 
I 

01:50:04.560 --> 01:50:06.641
wrote this code which just goes 
over a list of numbers and picks

01:50:06.642 --> 01:50:08.642
those 

01:50:12.316 --> 01:50:14.804
devicable by 16 and converts 
them to X.  So what it actually 

01:50:14.805 --> 01:50:18.047
does is filter map, right, map 
is this one and filter is this 

01:50:18.048 --> 01:50:20.048
one.

01:50:21.551 --> 01:50:24.412
So what I can do even with the 
help of my ID, I can do this.

01:50:28.487 --> 01:50:29.520
So newer versions of all 
programming languages have 

01:50:29.521 --> 01:50:31.958
something like this.  I can 
definitely leverage this, and so

01:50:35.022 --> 01:50:38.681
this filter is a function, this 
lambda is a function value.  You

01:50:38.682 --> 01:50:42.781
don't have, by the way, to 
declare it as a  variable.  You 

01:50:42.782 --> 01:50:45.239
can get rid of it.  And so 
that's a lamb ka parameter.

01:50:49.157 --> 01:50:51.404
Kotlin has some nice semi 
functional things like you can 

01:50:51.405 --> 01:50:54.249
say anywhere in the code, it can
say also, and have this 

01:50:57.534 --> 01:50:59.534
value also do this for me, 
please, like 

01:51:01.846 --> 01:51:04.132
print this list for me and then 
proceed with what you were 

01:51:04.133 --> 01:51:08.005
doing, like nevermind this just 
debug output or some side effect

01:51:08.006 --> 01:51:11.496
I want to insert.  Side effects 
are not functional on the one 

01:51:11.497 --> 01:51:13.335
hand, on the other hand this is 
handy for debugging, you don't 

01:51:13.336 --> 01:51:17.619
have to break your clain apart 
and so on and so  forth.  Also 

01:51:17.620 --> 01:51:19.620
use let, use run, and so on and 
is forth.

01:51:22.103 --> 01:51:24.741
There is one very deep thing 
about functional abstractions in

01:51:25.355 --> 01:51:27.384
non-functional languages.  When 
I do something like this, I have

01:51:28.627 --> 01:51:30.688
my repeat function right here, 
right.  So what it does, it 

01:51:30.689 --> 01:51:34.598
takes a number of times I want 
to repeat something and this 

01:51:34.599 --> 01:51:36.219
something is a function, and by 
the way you don't have to invent

01:51:36.220 --> 01:51:39.478
your own function interface 
every time, just use the 

01:51:39.479 --> 01:51:41.905
function types here.  It's a 
function that takes an INT 

01:51:42.987 --> 01:51:44.286
returns a unit, and unit is 
something that you don't care 

01:51:44.287 --> 01:51:46.510
about.  Then it simply repeats 
it, right.

01:51:50.402 --> 01:51:52.402
So when I say repeat, I'm also 
very 

01:51:54.131 --> 01:51:56.196
much conscious about like what's
it going to cost me?  Right?

01:51:58.835 --> 01:52:00.668
So it's a function, it takes a 
lambda as a parameter, and so 

01:52:00.669 --> 01:52:06.184
it's actually just another 
parameter.  The Kotlin custom is

01:52:06.185 --> 01:52:10.457
to write outside of parentheses 
because it looks more like a 

01:52:10.458 --> 01:52:12.458
language construct like this, 
but 

01:52:14.117 --> 01:52:16.559
then, okay, I'm running this and
I have to create a lambda 

01:52:16.560 --> 01:52:20.191
object, right.  I have to create
a lambda object every time I do 

01:52:20.192 --> 01:52:22.192
anything like this, and so there
is a cost to this abstraction.

01:52:24.945 --> 01:52:27.620
It's nice code, I can reuse 
things, I can raise the level of

01:52:27.621 --> 01:52:32.754
abstraction in my code but there
is a toll on that.  Actually in 

01:52:32.755 --> 01:52:34.755
Kotlin, you can very often 

01:52:37.929 --> 01:52:40.196
get rid of the upgrading 
lambdas, lambda objects for you 

01:52:40.197 --> 01:52:42.426
by just using inline functions. 
When I say inline, my code 

01:52:42.427 --> 01:52:44.883
doesn't change, right.  So here 
nothing happened at the call 

01:52:46.096 --> 01:52:48.096
site that I can see.

01:52:51.170 --> 01:52:53.170
But if I say

01:52:54.673 --> 01:52:56.105
show common byte code and just 
decompile this into Java just to

01:52:56.106 --> 01:52:58.106
scare 

01:53:00.439 --> 01:53:02.439
you a little bit, it was much --
yeah.  Easy talk so far.

01:53:04.304 --> 01:53:06.304
So if I do this, here is goes.

01:53:08.768 --> 01:53:11.045
It's a simple for loop.  Where 
did my lambda go?

01:53:14.293 --> 01:53:16.293
Well, the compiler simply 
optimized it away.

01:53:19.882 --> 01:53:21.880
You don't need lambda, right.  
So if you simply have your loop 

01:53:21.881 --> 01:53:23.881
here 

01:53:25.742 --> 01:53:27.226
and you inline everything, you 
end up with a loop and that's 

01:53:27.227 --> 01:53:29.227
it.

01:53:30.886 --> 01:53:31.322
So the big difference in the 
mindset when you go from the 

01:53:31.323 --> 01:53:34.101
Java Programming Language to the
Kotlin Programming  Language is 

01:53:34.102 --> 01:53:35.894
you still use lambdas but some 
of the lambdas are really free, 

01:53:35.895 --> 01:53:37.895
and by the way these are all 
free, too.

01:53:40.978 --> 01:53:43.414
Many, many lambdas in the 
library are free abstractions, 

01:53:43.415 --> 01:53:45.664
you don't have to pay for 
calling them.  It's just code 

01:53:45.665 --> 01:53:47.712
generated for you.
     So functional in Kotlin is 

01:53:47.713 --> 01:53:49.713
not only 

01:53:50.979 --> 01:53:53.638
convenient but also quite cheap.
Speaking of cheap, by the way, 

01:53:53.639 --> 01:53:55.927
let's look at this example.

01:53:58.986 --> 01:54:00.986
So here I'm trying to do a 
parallel computation.

01:54:04.316 --> 01:54:06.767
Well, it's a stupid sample and 
nobody does variable 

01:54:06.768 --> 01:54:08.768
computation, but I want to 
illustrate a point.

01:54:09.249 --> 01:54:11.249
What I'm doing here is again 
with 

01:54:14.978 --> 01:54:19.897
my old habits in mind, I'm 
creating 100,000 threads.  

01:54:19.898 --> 01:54:22.541
100,000 threads each of which 
does some work.  Actually it 

01:54:22.542 --> 01:54:25.250
sleeps for one second and then 
prints a number, and then I have

01:54:25.251 --> 01:54:27.914
to join all of these threads to 
my main thread.

01:54:30.801 --> 01:54:34.711
So if I run this -- oh, oh.  
There was an exception.  What 

01:54:34.712 --> 01:54:36.712
was that?

01:54:38.371 --> 01:54:40.628
The Java out of memory error.  
Basically, what it's telling me 

01:54:40.629 --> 01:54:42.629
is 

01:54:44.906 --> 01:54:47.343
hey, you cannot create 100,000 
threads.  Are you crazy there?

01:54:53.104 --> 01:54:54.940
It's 100,000 stacks, it doesn't 
fit into memory, just get 

01:54:54.941 --> 01:54:56.941
reasonable.

01:54:59.643 --> 01:55:01.066
And that's fair, like OS threads
are not cheap, you have to 

01:55:01.067 --> 01:55:03.067
allocate 

01:55:05.349 --> 01:55:05.880
resources for threads, so you 
don't do such silly things with 

01:55:05.881 --> 01:55:07.881
thread, but I 

01:55:10.139 --> 01:55:12.285
have this example down 
coroutines, who knows about them

01:55:12.894 --> 01:55:18.208
in Kotlin?  Oh, good.  Who uses 
them in production?  Okay.  Soon

01:55:18.209 --> 01:55:20.446
enough you will all be using 
them, I'm sure.  So have a look 

01:55:20.447 --> 01:55:22.447
here, it's very much 

01:55:23.703 --> 01:55:25.703
the same code so I'll just put 
it side by side here.

01:55:28.989 --> 01:55:31.453
It's very much the same code, 
but instead of threads here I'm 

01:55:31.454 --> 01:55:35.953
creating async tasks that are 
using co-routines underneath so 

01:55:35.954 --> 01:55:37.954
I'm still waiting for one 

01:55:39.478 --> 01:55:40.917
second and  printing, and if I 
run this there is no out of 

01:55:40.918 --> 01:55:45.381
memory.  It's printing all the 
numbers and I'm good.

01:55:49.056 --> 01:55:51.088
So again, Kotlin introduced 
coroutines as means of making 

01:55:51.089 --> 01:55:53.089
your 

01:55:54.178 --> 01:55:56.019
asynchronous computations nicer 
and that  workers, but what's 

01:55:56.020 --> 01:56:00.343
the cost of that?  So the cost 
of that is at least cheaper than

01:56:00.344 --> 01:56:01.969
having a thread per each 
computation, of course nobody 

01:56:01.970 --> 01:56:06.068
does that exactly, but still 
like coroutines are very cheap.

01:56:11.638 --> 01:56:14.092
You can spin off 100,000  
coroutines and it doesn't costly

01:56:14.093 --> 01:56:17.135
near as much as threads.
     Let me illustrate something

01:56:17.136 --> 01:56:19.136
that 

01:56:20.962 --> 01:56:22.962
coroutines are really good for 
right here.

01:56:24.313 --> 01:56:26.313
So here is a legacy interface.

01:56:31.676 --> 01:56:34.728
Or I don't know, a motor 
interface, I don't know.  What 

01:56:34.729 --> 01:56:36.729
we very often have to do to make

01:56:39.619 --> 01:56:41.619
this asynchronous or reders very
denned

01:56:43.756 --> 01:56:45.584
our dependencies so on and so 
forth.  Ask me to do something 

01:56:45.585 --> 01:56:49.711
and I'll do it and let you know 
when I'm done.  Here is the mock

01:56:49.712 --> 01:56:51.762
service request and a callback 
function that's passed to it so 

01:56:55.624 --> 01:56:57.857
when the work in the comments 
are done, I'm calling the 

01:56:57.858 --> 01:56:59.858
callback and just passing my 
answer there.

01:57:00.503 --> 01:57:03.392
All right, so that's all right. 
It's working for everyone, 

01:57:03.393 --> 01:57:08.071
right.  But this is what the 
code looks like when I want to 

01:57:08.072 --> 01:57:10.072
exchange messages between two 
services, and so I just want to 

01:57:13.168 --> 01:57:15.248
basically send two messages in 
sequence, and here is what I 

01:57:15.249 --> 01:57:17.249
have to do.

01:57:18.524 --> 01:57:20.363
First, request and then a  
callback.  This is a result of 

01:57:20.364 --> 01:57:22.364
the request I printed, and then 
next request inside 

01:57:25.268 --> 01:57:27.268
that callback and then print 
inside, so 

01:57:29.485 --> 01:57:33.171
you see the stairways right here
staircase right here, right.  

01:57:33.172 --> 01:57:35.865
One step, two step, three step, 
and you can actually get quite 

01:57:35.866 --> 01:57:38.336
deep down this staircase which 
is not nice.

01:57:40.189 --> 01:57:42.189
And so what I would really love 
to do is something a lot more 

01:57:43.449 --> 01:57:45.691
straightforward, but so this is 
kind of tolerable but what if, 

01:57:45.692 --> 01:57:47.692
just imagine, 

01:57:49.140 --> 01:57:51.140
what if you needed to do like N 
calls?

01:57:52.814 --> 01:57:55.482
Just a number of like make a 
list of calls.  So this is the 

01:57:55.483 --> 01:57:57.924
code I came up with, which isn't
nice at all, so I definitely 

01:58:03.699 --> 01:58:04.715
need that there because I need 
to nest a callback inside a 

01:58:04.716 --> 01:58:08.981
callback.  This is the shortest 
code I came up with, it copies 

01:58:08.982 --> 01:58:10.982
arrays, don't do that.

01:58:12.307 --> 01:58:13.316
It's  wasteful in terms of 
memories and time, it's 

01:58:13.317 --> 01:58:16.183
quadratic.
     Basically I have to come up

01:58:16.184 --> 01:58:19.609
with something like this, so you
can't just say repeat this five 

01:58:19.610 --> 01:58:21.610
times, right.

01:58:24.329 --> 01:58:27.031
So what I really want to do to 
be able to do is something like 

01:58:27.032 --> 01:58:29.897
this where I just say, okay, 
send one request, wait 

01:58:33.367 --> 01:58:35.367
for results, send the other 
request, and 

01:58:37.299 --> 01:58:38.768
thenfy then if I want to repeat 
something, just repeat it with a

01:58:38.769 --> 01:58:41.217
for loop, right.
     So this code here is 

01:58:41.218 --> 01:58:43.665
actually using the same  
callbacks.

01:58:48.381 --> 01:58:49.810
Only the coroutine abstractions 
are pushing this away from me, 

01:58:49.811 --> 01:58:51.811
so actually 

01:58:53.120 --> 01:58:55.780
you can take any callback based 
API that you have now and turn 

01:58:55.781 --> 01:58:57.781
it into this, like 

01:58:59.469 --> 01:59:01.301
make it straightforward with 
just a few lines of code and 

01:59:01.302 --> 01:59:03.302
I'll show you.  So this is 
calling the same services 

01:59:06.843 --> 01:59:08.670
because I have this function 
right here.

01:59:08.671 --> 01:59:10.671
So what I'm doing is I'm just 

01:59:12.391 --> 01:59:14.850
turning the request into a 
suspension function through this

01:59:14.851 --> 01:59:16.851
simple construct.  That's an 
extension function of my 

01:59:17.692 --> 01:59:20.350
callback service.  I  say, the 
first thing I say there is 

01:59:23.217 --> 01:59:25.217
suspend my coroutine, so I'm 
assuming 

01:59:26.477 --> 01:59:28.709
I'm until a coroutine, suspect 
spend it right away, I get the 

01:59:28.710 --> 01:59:30.771
continuation, I do my request, 
that's it I'm suspended and 

01:59:31.987 --> 01:59:33.987
waiting for a request, and so 
there it 

01:59:35.851 --> 01:59:38.496
is, and when the request is 
done, I just say resume to my  

01:59:38.497 --> 01:59:40.497
coroutine and that's it.

01:59:42.364 --> 01:59:44.364
So there simple lines of code 
turns 

01:59:45.642 --> 01:59:48.118
your callback based API into a 
coroutine API and makes -- so it

01:59:48.119 --> 01:59:50.119
makes this -- oh, I'm sorry.

01:59:51.362 --> 01:59:53.628
It makes this code into this, 
which is a lot more readable to 

01:59:53.629 --> 01:59:57.331
my sense.  How do you like it?
     (Applause).

01:59:57.332 --> 01:59:59.604
     I see some nods in the 
audience.  Thank you.

02:00:04.085 --> 02:00:06.085
Yeah, well, actually if you want
to be 

02:00:07.168 --> 02:00:09.224
a lot more prudent here and I'm 
sure you want to, you need to 

02:00:09.225 --> 02:00:11.225
catch exceptions, 

02:00:14.987 --> 02:00:16.827
and so you know, capturing -- 
handling your exceptions is very

02:00:16.828 --> 02:00:18.828
important and 

02:00:21.757 --> 02:00:23.376
that's as easy as this, just 
catch your exception and 

02:00:23.377 --> 02:00:25.377
whatever happens on the -- oh, 
I'm sorry.  Not here, of course.

02:00:30.544 --> 02:00:32.544
Whatever happens with your 
request, 

02:00:35.219 --> 02:00:36.856
just catch it and resume with 
exception, so this will 

02:00:36.857 --> 02:00:39.506
propagate exceptions through 
your coroutines very nicely and 

02:00:45.252 --> 02:00:47.483
you'll able to have try catch 
around here, surround this with 

02:00:47.484 --> 02:00:50.739
try catch, or I'm sorry, 
whatever, and catch exceptions 

02:00:50.740 --> 02:00:53.597
there as if it was a sequential 
code but underneath it's all 

02:00:53.802 --> 02:00:55.802
asynchronous.

02:00:57.699 --> 02:00:59.323
You can do HTTP requests like 
this, async I/O file systems, 

02:00:59.324 --> 02:01:01.324
background 

02:01:02.803 --> 02:01:04.803
threads, everything you need.  
Isn't it nice?

02:01:06.514 --> 02:01:08.393
And I guess the last example 
I'll be showing you today is 

02:01:08.394 --> 02:01:10.394
this one.

02:01:14.132 --> 02:01:17.236
It's just another showoff for 
how  coroutines can help you.  

02:01:17.237 --> 02:01:19.237
Take a look.

02:01:21.143 --> 02:01:23.193
So what I wanted to do is to 
create an infinite stream of 

02:01:23.194 --> 02:01:28.127
numbers.  Who likes infinite 
streams of numbers?  I eat them 

02:01:28.128 --> 02:01:30.128
for breakfast.  Okay.

02:01:32.411 --> 02:01:33.830
So I want just a sequence to be 
generated here and then I can 

02:01:33.831 --> 02:01:35.831
take 20 of 

02:01:37.511 --> 02:01:39.511
them here, the sequence of 20 
and I can 

02:01:41.440 --> 02:01:43.440
take 200, 2,000, filter, map, 
slice, whatever.

02:01:44.314 --> 02:01:46.828
And so this Build Sequence 
function is a library function 

02:01:46.829 --> 02:01:48.829
and in the Kotlin standard 
library and it's actually based 

02:01:50.711 --> 02:01:52.336
on the same mechanism as 
coroutines.  It doesn't do any 

02:01:52.337 --> 02:01:54.337
background processing.  It's all
in the same thread.

02:01:56.611 --> 02:01:57.831
What it does, it takes all the 
yield statements from here and 

02:01:57.832 --> 02:02:00.096
just puts them in a sequence, 
and so if I want to yield 

02:02:04.420 --> 02:02:06.420
something here, I just do it 
like I 

02:02:09.546 --> 02:02:11.546
insert 2 into my sequence.

02:02:13.448 --> 02:02:15.302
If I want to say say if TMP is 
greater than 10, continue, I can

02:02:15.303 --> 02:02:17.303
skip 

02:02:20.463 --> 02:02:22.710
pieces of my logic, so that's as
straightforward as any coroutine

02:02:22.711 --> 02:02:25.956
it gives you a lazy sequence.
     Okay.

02:02:29.226 --> 02:02:31.672
So takeaways, classes are cheap,
functions are top level or 

02:02:31.673 --> 02:02:33.673
local, no 

02:02:35.145 --> 02:02:37.585
overloading to emulate your 
default values, use properties, 

02:02:37.586 --> 02:02:42.496
use delegated properties, use  
coroutines, have a nice Kotlin.

02:02:45.558 --> 02:02:49.668
And I want to advertise some 
more activities today.  So if 

02:02:49.669 --> 02:02:51.704
you still have questions that I 
couldn't take, you can come over

02:02:51.705 --> 02:02:53.705
to an 

02:02:55.172 --> 02:02:57.172
office hour we're having at 
12:30.

02:02:58.649 --> 02:03:01.307
You can come over to the sandbox
area C where we're at the Kotlin

02:03:01.308 --> 02:03:04.005
booth some of the day at least, 
and right after my 

02:03:07.493 --> 02:03:09.493
talk there will be a talk by 
Jake Warden 

02:03:11.208 --> 02:03:12.631
about Android KDX, very exciting
on Stage 2, so you're welcome 

02:03:12.632 --> 02:03:14.632
there.  Thank you very much for 
your attention.

02:03:15.521 --> 02:03:17.521
(Applause).

02:03:31.860 --> 02:03:37.050
     &gt;&gt; Thank you for joining 
this session.  Grand ambassadors

02:03:37.051 --> 02:03:40.526
will assist with directing you 
through the designated exits.  

02:03:40.527 --> 02:03:41.351
We'll be making room for those 
who have registered for the next

02:03:41.352 --> 02:03:45.039
session.  If you've registered 
for the next session in this 

02:03:45.040 --> 02:03:46.988
room, we ask that you please 
clear the room and return via 

02:03:46.989 --> 02:03:51.315
the registration line outside.  
Thank you.

02:07:17.356 --> 02:07:21.887
May 10, 2018
     10:30 a.m. PT

02:07:26.210 --> 02:07:27.866
TensorFlow Lite for Mobile 
Developers

02:24:24.372 --> 02:24:29.082
morning, everyone.  Thank you so
much for coming to our session 

02:24:29.083 --> 02:24:31.083
this morning.  I'm Sarah 
Sirajuddin.

02:24:33.963 --> 02:24:35.963
I'll on the TensorFlow Lite 
Team, and 

02:24:37.893 --> 02:24:40.236
we work on bringing Machine 
Learning to mobile and small 

02:24:40.237 --> 02:24:42.277
devices.  Later on I will 
introduce my colleague 

02:24:45.333 --> 02:24:46.182
Andrew Selle who will be doing 
the second half of this talk.

02:24:46.183 --> 02:24:51.877
     So the last couple of days 
have been really fun for me.  

02:24:51.878 --> 02:24:53.878
I've gotten to meet and speak 
with 

02:24:54.933 --> 02:24:56.768
many of you, and it's been 
really nice to see the 

02:24:56.769 --> 02:24:58.769
excitement around TensorFlow 
Lite.

02:25:01.480 --> 02:25:04.330
Today I'm happy to be here and 
talk to you about all the work 

02:25:04.331 --> 02:25:07.010
that our team is doing to make 
Machine Learning on small 

02:25:09.702 --> 02:25:11.702
devices possible and easy.

02:25:13.151 --> 02:25:16.406
So in today's talk we'll cover 
three areas.  First, we'll talk 

02:25:16.407 --> 02:25:19.269
about why Machine Learning 
directly on device is important 

02:25:21.930 --> 02:25:25.292
and how it's different than what
you may do on the server.  

02:25:25.293 --> 02:25:28.145
Second, we'll walk you through 
what we have built with 

02:25:28.146 --> 02:25:30.146
TensorFlow Lite.

02:25:31.196 --> 02:25:33.196
And lastly, we'll show you how 
you can 

02:25:36.848 --> 02:25:38.848
use TensorFlow Lite in your own 
apps.

02:25:39.352 --> 02:25:41.352
First, let's talk about devices 
for a bit.

02:25:42.814 --> 02:25:44.837
What do you mean when we say a 
device?  Usually our mobile 

02:25:44.838 --> 02:25:47.703
devices are basically our 
phones.  Our phones are with us 

02:25:47.704 --> 02:25:51.597
all the time, we interact with 
them so many times during the 

02:25:51.598 --> 02:25:53.657
day.  And modern phones come 
with a large 

02:25:57.119 --> 02:25:58.550
number of sensors on them which 
give us really rich data about 

02:25:58.551 --> 02:26:01.615
the physical world around us.

02:26:06.561 --> 02:26:08.704
Another category of devices is 
what we call edge devices and 

02:26:08.705 --> 02:26:13.574
this industry has seen a huge 
Explosion in the last few years,

02:26:13.575 --> 02:26:15.575
so some examples are Smart 

02:26:18.083 --> 02:26:20.954
Speakers, Smart  Watches, Smart 
Cameras, and as this market has 

02:26:20.955 --> 02:26:24.435
grown, we see the technology 
which only used to be available 

02:26:24.436 --> 02:26:26.436
on more expensive devices is 

02:26:30.711 --> 02:26:32.711
now available on far cheaper 
ones.

02:26:33.833 --> 02:26:35.244
So now we see that there is this
massive growth in devices, 

02:26:35.245 --> 02:26:37.522
they're becoming increasingly 
capable, both 

02:26:41.210 --> 02:26:43.210
mobile and edge, and there is 
opening up 

02:26:45.498 --> 02:26:46.109
many opportunities for novel 
applications for Machine 

02:26:46.110 --> 02:26:48.964
Learning.
     I expect that many of you 

02:26:48.965 --> 02:26:54.065
are already familiar with the 
basic idea of Machine Learning, 

02:26:54.066 --> 02:26:56.066
but for those that 

02:26:57.943 --> 02:26:59.943
aren't I'm going to quickly 
cover the core concept.

02:27:01.044 --> 02:27:02.911
Let's start with an example of 
something we may want to do, 

02:27:02.912 --> 02:27:06.573
let's say classification of 
images.  So how do we do this?

02:27:08.636 --> 02:27:10.891
     In the past what we would 
have done 

02:27:13.937 --> 02:27:16.403
was to write a lot of rules that
were hard coded, very specific 

02:27:16.404 --> 02:27:18.404
about some characteristics that 
we expected to see 

02:27:24.377 --> 02:27:26.453
in parts of the the im image.  
This was time consuming, hard to

02:27:26.454 --> 02:27:29.920
do, and frankly didn't work all 
that well.  This is where 

02:27:29.921 --> 02:27:31.982
Machine Learning comes in.
     With Machine Learning we 

02:27:31.983 --> 02:27:35.837
learn based on examples, and so 
a simple way to think about 

02:27:35.838 --> 02:27:37.838
Machine Learning is that 

02:27:40.149 --> 02:27:42.415
we use algorithms to learn from 
data and then we make 

02:27:42.416 --> 02:27:44.866
predictions about similar data 
that has not been seen before.

02:27:49.013 --> 02:27:51.057
It's a two-step process, first 
the model learns and then we use

02:27:51.058 --> 02:27:53.133
it to make predictions.
     The process of model 

02:27:53.134 --> 02:27:55.134
learning is 

02:27:56.202 --> 02:27:58.045
what we typically call training 
and when the model is making 

02:27:58.046 --> 02:28:03.159
predictions about data is what 
we call inference.

02:28:06.639 --> 02:28:07.454
This is a high-level view of 
what's happening during 

02:28:07.455 --> 02:28:11.343
training.  The model is passed 
in labeled data, that is input 

02:28:11.344 --> 02:28:13.344
data along with the associated 
prediction.

02:28:17.935 --> 02:28:19.570
And since in this case, we know 
what the right answer is, we're 

02:28:19.571 --> 02:28:23.889
able to calculate the error, 
that is how many times is the 

02:28:23.890 --> 02:28:26.748
model getting it wrong and by 
how much?  We use these errors 

02:28:26.749 --> 02:28:30.463
to improve the model.  This 
process is repeated many, many 

02:28:32.706 --> 02:28:34.800
times until we reach the point 
that we think the model is good 

02:28:34.801 --> 02:28:37.251
enough or that this is the best 
fit we can do.

02:28:42.530 --> 02:28:44.530
This involves a lot of steps and

02:28:47.977 --> 02:28:49.977
coordination and that is why we 
need a 

02:28:59.965 --> 02:29:04.031
framework to make this easyier, 
and this is where TensorFlow 

02:28:59.965 --> 02:29:02.010
comes in.  It's Google's 
framework for Machine Learning, 

02:29:02.011 --> 02:29:04.097
it makes it easy to train and 
build neural networks, and it is

02:29:04.098 --> 02:29:05.949
cross-platform, works on CPUs, 
GPU, TPUs, as well as mobile and

02:29:05.950 --> 02:29:07.767
embedded platforms.
     The mobile and embedded 

02:29:07.768 --> 02:29:11.049
piece of TensorFlow, which we 
call TensorFlow Lite is what we 

02:29:11.050 --> 02:29:15.324
are going to be focusing on in 
our talk today.

02:29:16.961 --> 02:29:18.961
So now I want to talk about, why
would you consider doing Machine

02:29:20.211 --> 02:29:22.211
Learning directly on device?

02:29:24.437 --> 02:29:26.437
And there are several reasons 
that you may consider, but 

02:29:27.758 --> 02:29:30.212
probably the most important one 
is latency.  If the processing 

02:29:30.213 --> 02:29:34.530
is happening on the device, then
you're not sending data back and

02:29:34.531 --> 02:29:36.768
forth to the server, so if your 
use case involves  realtime 

02:29:36.769 --> 02:29:38.769
processing 

02:29:39.829 --> 02:29:42.059
of data such as audio or video, 
then it's quite likely that you 

02:29:42.060 --> 02:29:44.060
would consider doing this.

02:29:46.348 --> 02:29:48.388
Other reasons are that your 
processing can happen even when 

02:29:48.389 --> 02:29:50.389
your 

02:29:52.491 --> 02:29:55.165
device is not connected to the 
Internet, the data stays 

02:29:55.166 --> 02:29:57.166
on-device and this is really 
useful if you're working with 

02:29:58.260 --> 02:30:00.260
sensitive user data which you 
don't want 

02:30:02.777 --> 02:30:05.209
to put on servers, it's more 
power efficient because your 

02:30:05.210 --> 02:30:07.210
device is not spending power 
transmitting data back 

02:30:10.363 --> 02:30:12.409
and forth, and lastly, we are in
a position to take advantage of 

02:30:12.410 --> 02:30:14.410
all the 

02:30:15.669 --> 02:30:17.703
sensor data that's already 
available and  accessible on the

02:30:17.704 --> 02:30:19.704
device.

02:30:21.995 --> 02:30:23.219
So this is all great but there 
is a catch like there always is,

02:30:23.220 --> 02:30:25.220
and the 

02:30:27.492 --> 02:30:30.172
catch is that doing on-device ML
is  hard.  Many of these devices

02:30:30.173 --> 02:30:32.173
have some pretty 

02:30:33.232 --> 02:30:35.697
tight constraints, they have 
small batteries, tight memory, 

02:30:35.698 --> 02:30:39.769
and very little computation 
power.  TensorFlow was built for

02:30:39.770 --> 02:30:42.401
processing on the server and it 
wasn't a great fit for 

02:30:45.462 --> 02:30:47.526
these use cases, and that is the
reason that we built TensorFlow 

02:30:47.527 --> 02:30:52.025
Lite.  It's a lightweight 
Machine Learning library for 

02:30:52.026 --> 02:30:55.507
mobile and embedded platforms.
     So this is a high-level 

02:30:55.508 --> 02:30:59.784
overview of the system.  It 
consists of a converter where we

02:31:03.508 --> 02:31:05.322
convert models from TensorFlow 
format to TensorFlow Lite 

02:31:05.323 --> 02:31:07.323
format, and for efficiency 
reasons we use a format which 

02:31:10.260 --> 02:31:12.751
is different.  It consists an 
interpreter which runs 

02:31:14.984 --> 02:31:17.633
on-device, there are library of 
apps and kernel, and then we 

02:31:17.634 --> 02:31:20.124
have APIs that allow us to take 
advantage of hardware 

02:31:21.552 --> 02:31:23.552
acceleration whenever it is 
available.

02:31:24.822 --> 02:31:26.822
TensorFlow Lite is 
cross-platform 

02:31:29.543 --> 02:31:31.569
so it works on Android, iOS, 
Linux, and a high-level 

02:31:31.570 --> 02:31:34.239
developer workflow here would be
to take a trained TensorFlow 

02:31:36.678 --> 02:31:39.146
model, convert it to TensorFlow 
Lite format and then update your

02:31:39.147 --> 02:31:43.444
apps to use the TensorFlow Lite 
interpreter using the 

02:31:43.445 --> 02:31:45.445
appropriate API.

02:31:48.366 --> 02:31:50.610
On iOS, developers also have the
option of using CoreML instead 

02:31:50.611 --> 02:31:52.611
and what 

02:31:54.084 --> 02:31:56.320
they would do here is to take 
that trained TensorFlow model 

02:31:56.321 --> 02:31:58.321
and convert it 

02:32:00.397 --> 02:32:01.859
to CoreML using the TensorFlow 
to CoreML converter and use the 

02:32:01.860 --> 02:32:03.860
converted model 

02:32:05.680 --> 02:32:07.680
with the

02:32:09.835 --> 02:32:11.671
CoreML runtime.
     The common questions we get

02:32:11.672 --> 02:32:13.672
when we 

02:32:15.962 --> 02:32:18.604
talk to  developers is it small 
and is it fast?  Let's talk 

02:32:18.605 --> 02:32:20.649
about the first question.  One 
of the design goals with 

02:32:22.294 --> 02:32:24.533
TensorFlow Lite was to keep the 
memory and binary size small, 

02:32:24.534 --> 02:32:29.442
and I'm happy to say that the 
size of our core interpreter is 

02:32:29.443 --> 02:32:31.443
only 75 kilobytes and 

02:32:35.594 --> 02:32:37.845
when you include all the 
supported ops, the size is 400 

02:32:37.846 --> 02:32:39.846
kilobytes.
     How did we do this?

02:32:42.558 --> 02:32:44.637
First of all, we've been really 
careful about which dependencies

02:32:44.638 --> 02:32:46.638
we include.

02:32:48.854 --> 02:32:50.854
Secondly, TensorFlow Lite uses 
flood 

02:32:53.711 --> 02:32:55.229
buffers which are far more 
memory than protocol buffers 

02:32:55.230 --> 02:32:58.289
are.  Another thing is  
selective registration and that 

02:32:58.290 --> 02:33:02.587
allows developers to only use 
the ops that their model needs 

02:33:02.588 --> 02:33:04.588
and thus they can keep the 
footprint small.

02:33:07.298 --> 02:33:11.620
Now, moving on to the second 
question which is of speed.  We 

02:33:11.621 --> 02:33:13.678
made several design choices 
throughout the system to enable 

02:33:13.679 --> 02:33:17.764
fast startup, low latency, and 
high throughput.

02:33:22.937 --> 02:33:24.937
Let's start with the model 
format.

02:33:27.048 --> 02:33:28.877
TensorFlow Lite uses FlatBuffers
like I said and it's a 

02:33:28.878 --> 02:33:31.533
cross-platform efficiency 
realization library.  It was 

02:33:31.534 --> 02:33:32.948
originally created at Google for
game development and is now 

02:33:32.949 --> 02:33:34.949
being 

02:33:37.259 --> 02:33:39.259
used for other performance 
sensitive applications.

02:33:41.739 --> 02:33:43.819
The advantage of using 
FlatBuffers is very can  

02:33:43.820 --> 02:33:45.820
directly access the data without
doing parsing or unparsing of 

02:33:48.566 --> 02:33:50.415
the large files which contain 
weights.  Another thing that we 

02:33:50.416 --> 02:33:52.416
do at the time 

02:33:55.569 --> 02:33:57.837
of conversion is that we pre 
pre-fuse the activations and 

02:33:57.838 --> 02:34:00.899
biases and it leads to faster 
execution later at runtime.

02:34:02.944 --> 02:34:04.944
The TensorFlow Lite interpreter 
uses a static memory and static 

02:34:06.007 --> 02:34:08.007
execution plan.

02:34:11.545 --> 02:34:13.232
This leads to faster load times 
times.  Many of the kernels that

02:34:13.233 --> 02:34:15.912
TensorFlow Lite comes from have 
been especially 

02:34:21.625 --> 02:34:23.245
optimized to run fast on NEON 
and ARM CPUs.

02:34:23.246 --> 02:34:25.891
Let's talk about hardware 
acceleration.  As Machine 

02:34:25.892 --> 02:34:28.337
Learning has grown in 
prominence, it has sparked quite

02:34:28.338 --> 02:34:30.778
a bit of innovation at the 
silicon layer as 

02:34:33.855 --> 02:34:36.543
well and many hardware companies
are investing in building custom

02:34:36.544 --> 02:34:39.619
chips which can accelerate 
neural income processing.

02:34:43.103 --> 02:34:45.774
GPUs and DSPs which have been 
around for some time are also 

02:34:45.775 --> 02:34:50.489
now being increasingly used to 
do Machine Learning tasks.  

02:34:50.490 --> 02:34:53.148
TensorFlow Lite was designed to 
take advantage of hardware 

02:34:53.149 --> 02:34:55.149
acceleration 

02:34:56.658 --> 02:34:59.721
whether it is through GPUs, DSPs
or custom AI chips.

02:35:04.239 --> 02:35:06.897
On Android, the recently 
released Android Neural  Network

02:35:06.898 --> 02:35:10.781
API is an abstraction layer that
makes it easy for TensorFlow 

02:35:10.782 --> 02:35:14.095
Lite to take advantage of the 
underlying acceleration.  And 

02:35:14.096 --> 02:35:16.096
the way this works is that 

02:35:17.977 --> 02:35:20.422
hardware renders write 
specialized drivers or custom 

02:35:20.423 --> 02:35:22.423
acceleration code for their 
hardware platforms and integrate

02:35:25.067 --> 02:35:27.614
with the Android NN API.

02:35:30.874 --> 02:35:32.902
It integrates with the Android 
NN API via its internal 

02:35:32.903 --> 02:35:35.762
delegation API.
     A point to note here is 

02:35:35.763 --> 02:35:39.698
that developers only need to 
integrate their apps with 

02:35:39.699 --> 02:35:42.549
TensorFlow Lite.  TensorFlow 
Lite will take care of 

02:35:44.202 --> 02:35:46.265
abstracting away the details of 
hardware acceleration from them.

02:35:52.120 --> 02:35:53.956
In addition to the Android NN 
API we're also working on 

02:35:53.957 --> 02:35:56.844
building direct GPU acceleration
in TensorFlow Lite.

02:35:59.977 --> 02:36:02.413
GPUs are widely available and 
used and like I said before, 

02:36:02.414 --> 02:36:05.472
they're now being increasingly 
used for doing Machine Learning 

02:36:05.473 --> 02:36:07.473
tasks.

02:36:08.956 --> 02:36:10.582
Similar to NN API, developers 
only integrate with TensorFlow 

02:36:10.583 --> 02:36:12.632
Lite if they want to take 
advantage of the GPU 

02:36:14.292 --> 02:36:17.561
acceleration.
     So the last bit on 

02:36:17.562 --> 02:36:19.562
performance that 

02:36:22.803 --> 02:36:24.803
I want to talk about is called

02:36:26.786 --> 02:36:28.212
quantization and this cuts 
across several components in our

02:36:28.213 --> 02:36:30.213
system.

02:36:31.516 --> 02:36:32.945
First of all, what is 
quantization, a simple way to 

02:36:32.946 --> 02:36:34.979
think about it is that it refers
to techniques to store numbers 

02:36:38.081 --> 02:36:40.081
and to perform calculations of 
numbers 

02:36:43.362 --> 02:36:44.790
in formats that are more compact
than 32-byte floating 

02:36:44.791 --> 02:36:46.791
representations.  Why is this 
important?

02:36:49.076 --> 02:36:52.379
For two reasons.
     First, model size is a 

02:36:52.380 --> 02:36:54.415
concern for small devices, so 
the smaller the model the better

02:36:54.416 --> 02:36:56.416
it is.

02:36:58.282 --> 02:37:00.282
Secondly, there are many 
processers 

02:37:04.138 --> 02:37:06.881
which have specialized SIMD 
which process faster than the 

02:37:07.689 --> 02:37:09.736
floating point numbers.
     So the next question here 

02:37:09.737 --> 02:37:11.737
is how 

02:37:13.465 --> 02:37:16.309
much accuracy do we use if we're
using 8 bytes or 16 bytes 

02:37:16.310 --> 02:37:19.402
instead of the 32 bytes used for
representing floating point 

02:37:19.403 --> 02:37:24.334
numbers?  The answer obviously 
depends on which model that 

02:37:24.335 --> 02:37:26.335
we're using, but in general 

02:37:28.684 --> 02:37:30.518
the learning process is robust 
to noise and quantization can be

02:37:30.519 --> 02:37:32.519
thought of as a 

02:37:33.785 --> 02:37:36.394
form of noise, so what we find 
is that the accuracies tend to 

02:37:36.395 --> 02:37:38.395
be usually within 

02:37:39.702 --> 02:37:42.344
acceptable thresholds.
     A simple way of doing 

02:37:42.345 --> 02:37:44.345
quantization 

02:37:45.404 --> 02:37:48.333
is to shrink the weights and 
biases after training and we are

02:37:48.334 --> 02:37:50.334
shortly going to be releasing a 
tool which developers 

02:37:53.643 --> 02:37:56.888
can use to shrink the size of 
their models.  In addition to 

02:37:56.889 --> 02:38:01.984
that, we have been actively 
working on doing quantization at

02:38:01.985 --> 02:38:04.845
training time and this is an 
active area of ongoing research,

02:38:04.846 --> 02:38:09.939
and what we find here is that we
are able to get accuracies which

02:38:09.940 --> 02:38:12.440
are comparable to the floating 
point models for architectures 

02:38:15.098 --> 02:38:17.562
like MobileNet as well as 
Inception and we recently 

02:38:17.563 --> 02:38:19.603
released a tool which allows 
developers to use this, and we 

02:38:19.604 --> 02:38:24.300
are working on adding support 
for more models in this.

02:38:25.307 --> 02:38:28.778
Okay.  So I talked about a bunch
of performance optimizations, 

02:38:28.779 --> 02:38:32.296
now let's talk about what does 
it translate to in terms of 

02:38:32.297 --> 02:38:34.297
numbers.

02:38:35.956 --> 02:38:37.956
So we benchmarked two models, 

02:38:40.029 --> 02:38:42.029
MobileNet and Inception V3 are 
the pixel 

02:38:43.107 --> 02:38:45.107
tool and we're getting speedups 
of more than three times.

02:38:48.962 --> 02:38:50.962
When we compare

02:38:52.727 --> 02:38:54.164
quantized quantized models and 
these numbers do not include any

02:38:54.165 --> 02:38:57.023
hardware acceleration.  We've 
done some initial  benchmarking 

02:38:59.706 --> 02:39:01.706
with hardware acceleration and 
we see 

02:39:02.798 --> 02:39:05.060
additional speedups of 3 to 4 
times with that, which is really

02:39:05.061 --> 02:39:08.333
promising and exciting so stay 
tuned in the next few months to 

02:39:08.334 --> 02:39:12.803
hear more on that.

02:39:14.062 --> 02:39:15.299
Now that I've talked about the 
design of TensorFlow and 

02:39:15.300 --> 02:39:18.165
performance, I want to show you 
what TensorFlow Lite can do in 

02:39:18.166 --> 02:39:20.166
practice.

02:39:22.665 --> 02:39:24.665
Let's please roll the video.

02:39:29.642 --> 02:39:33.702
     So this is a demo demo 
application which is running the

02:39:33.703 --> 02:39:38.819
MobileNet classification model 
which we trained on common 

02:39:38.820 --> 02:39:41.045
office objects, and as you can 
see it's doing a good job 

02:39:41.046 --> 02:39:45.173
detecting them, even this 
TensorFlow logo that we trained 

02:39:45.174 --> 02:39:47.619
this model on.
     Like I said, it's 

02:39:47.620 --> 02:39:49.620
cross-platform so 

02:39:51.714 --> 02:39:53.714
it's running on iOS as well as 
Android 

02:39:55.387 --> 02:39:57.387
and we also are running it here 
on Android Pings.

02:39:59.289 --> 02:40:01.289
This was a simple demo.  We have
more exciting demos for you 

02:40:02.128 --> 02:40:04.568
later on in the talk.  Now, 
let's talk about production use 

02:40:04.779 --> 02:40:08.875
cases.  I'm happy to say that 
we've been working with partner 

02:40:08.876 --> 02:40:10.876
teams inside Google 

02:40:12.366 --> 02:40:14.366
to bring TensorFlow Lite to 
Google Apps, 

02:40:17.766 --> 02:40:19.766
so portrait mode on Android 
camera, Hey 

02:40:23.268 --> 02:40:25.268
Google in Google Assistant and

02:40:27.592 --> 02:40:31.891
SmartReply will be on TensorFlow
Lite in the next few months.  

02:40:31.892 --> 02:40:33.892
It's the version powering the 
custom 

02:40:35.775 --> 02:40:37.630
functionality in the newly 
announced ML Kit and for those 

02:40:37.631 --> 02:40:39.631
of you that may have 

02:40:41.713 --> 02:40:43.713
missed the announcement, ML Kit 
is a 

02:40:46.034 --> 02:40:47.478
Machine Learning SDK, used Cloud
powered APIs for Machine 

02:40:47.479 --> 02:40:49.479
Learning and as well as 

02:40:50.552 --> 02:40:52.552
your ability to bring your own 
custom models and use them.

02:40:53.881 --> 02:40:56.112
These are some examples of apps 
that are already using 

02:40:56.113 --> 02:40:58.113
TensorFlow Lite 

02:41:01.212 --> 02:41:03.212
via ML Kit, PicsArt a photo 
editing and 

02:41:04.708 --> 02:41:06.708
collage making app, and VSCO is 
a cool photography app.

02:41:07.962 --> 02:41:10.409
So back to TensorFlow Lite and 
what is currently supported, so 

02:41:10.410 --> 02:41:13.263
we have support for 50 commonly 
used operations 

02:41:16.725 --> 02:41:18.975
which developers can use in 
their own models.  I will point 

02:41:18.976 --> 02:41:20.834
out here that if you need an Op 
which is not currently 

02:41:20.835 --> 02:41:25.757
supported, you do have the 
option of using what we call a 

02:41:25.758 --> 02:41:28.200
custom Op and using that, and 
later on in this talk, Andrew 

02:41:28.201 --> 02:41:30.261
will show you how you can do 
that.

02:41:33.117 --> 02:41:35.573
Op support is currently limited 
to inference.  We will be 

02:41:35.574 --> 02:41:37.574
working on adding training 
support in the future.

02:41:40.301 --> 02:41:42.301
We support several common 
popular Open 

02:41:45.198 --> 02:41:45.827
Source models as well as the 
quantized counterparts for some 

02:41:45.828 --> 02:41:47.873
of them.
     With this I'm going to 

02:41:47.874 --> 02:41:51.546
invite my colleague, Andrew, to 
talk to you about how you can 

02:41:51.547 --> 02:41:53.547
use TensorFlow Lite in your 

02:41:54.816 --> 02:41:55.239
own apps.
     &gt;&gt; ANDREW SELLE:  Thanks, 

02:41:55.240 --> 02:41:57.240
Sarah.
     (Applause).

02:42:02.061 --> 02:42:05.109
     So now that you know what 
TensorFlow Lite is and what it 

02:42:05.110 --> 02:42:09.455
can do and where it can be run, 
I'm sure you know -- you want to

02:42:09.456 --> 02:42:11.901
know how to use it.  So we can 
break that up into four 

02:42:12.315 --> 02:42:15.378
important steps.  The first one 
and probably the most important 

02:42:15.379 --> 02:42:19.650
is get a model.  You need to 
decide what you want to do.  It 

02:42:19.651 --> 02:42:22.318
could be image classification, 
it could be object detection, or

02:42:22.319 --> 02:42:26.381
it could be even speech 
recognition.  Whatever that 

02:42:26.382 --> 02:42:28.903
model is, you need to train it 
yourself and can you do that 

02:42:30.108 --> 02:42:31.529
with TensorFlow just like you 
trained any other TensorFlow 

02:42:31.530 --> 02:42:34.788
model.  Or can you download a 
pre-trained model if you're not 

02:42:34.789 --> 02:42:36.789
ready to make your own model 
yet.

02:42:38.888 --> 02:42:40.888
Or if an existing model 
satisfies your needs.

02:42:41.127 --> 02:42:42.566
Second, you need to convert your
model from TensorFlow to 

02:42:42.567 --> 02:42:46.057
TensorFlow Lite and we'll show 
some examples of how to do that 

02:42:46.058 --> 02:42:49.123
in a second.
     Third, if there is any 

02:42:49.124 --> 02:42:53.013
custom Ops that you need to 
write, this could be because you

02:42:53.014 --> 02:42:55.014
want to spot optimize something 
with some special instructions 

02:42:56.498 --> 02:42:58.358
you know about, or it could be 
that you're using a piece of 

02:42:58.359 --> 02:43:00.359
functionality that we do not yet
support like a 

02:43:03.709 --> 02:43:06.157
specialized piece of signal 
processing.  Whatever it might 

02:43:06.158 --> 02:43:08.389
be, you can write your ops.  
This may not be necessary, of 

02:43:08.390 --> 02:43:10.656
course.
     The next step is to write 

02:43:10.657 --> 02:43:14.383
an app and you use whatever 
client API is appropriate for 

02:43:14.384 --> 02:43:18.479
the target platform, so let's 
dive into some code.  Converting

02:43:18.480 --> 02:43:20.480
your TensorFlow model, once 
you're done with the TensorFlow 

02:43:21.725 --> 02:43:23.725
training, you typically have a 
saved 

02:43:25.910 --> 02:43:27.533
model or you might have a 
graphdev, what you need to do 

02:43:27.534 --> 02:43:29.166
first is put this through the 
converter so here I'm showing 

02:43:29.167 --> 02:43:33.256
how to do this within Python, so
if you download the normal 

02:43:33.257 --> 02:43:35.700
TensorFlow tooling that's 
pre-compiled like the PIP, 

02:43:35.701 --> 02:43:37.701
you're 

02:43:40.211 --> 02:43:42.045
able to run the converter and it
just takes a save model 

02:43:42.046 --> 02:43:44.046
directory in or 

02:43:45.854 --> 02:43:47.854
frozen graphdev and you specify 
a file 

02:43:50.141 --> 02:43:52.614
name of what TFLite file you 
want and that inputs a 

02:43:52.615 --> 02:43:54.615
FlatBuffer on disk that you can 
now shift to whatever device you

02:43:54.639 --> 02:43:58.985
want.  How might you get it to 
the device?  You could put it 

02:43:58.986 --> 02:44:02.253
into the package, or you could 
also say distribute it through a

02:44:02.254 --> 02:44:04.484
Cloud service where you can 
update your model on the fly 

02:44:04.485 --> 02:44:06.485
without updating your core 
application.

02:44:09.388 --> 02:44:11.388
Whatever you want to do is 
possible.

02:44:13.945 --> 02:44:15.373
So next, once you've converted, 
you might actually run into some

02:44:15.374 --> 02:44:19.468
issues during the conversion 
because there is a couple of 

02:44:19.469 --> 02:44:20.907
things that can go wrong.  So 
the first one is you need to 

02:44:20.908 --> 02:44:25.795
make sure that you have a frozen
graphdev or save model.  Both of

02:44:25.796 --> 02:44:27.622
these are able to get rid of the
parts of the graph that are used

02:44:27.623 --> 02:44:31.548
for training, these are 
typically things like variable 

02:44:31.549 --> 02:44:33.174
assignment, variable  
initialization, optimization 

02:44:33.175 --> 02:44:37.894
passes.  These are not  strictly
necessary if you're doing 

02:44:37.895 --> 02:44:40.338
inference that is prediction, so
you want to get rid of those out

02:44:40.339 --> 02:44:41.769
of the graph because we don't 
want to support those operations

02:44:41.770 --> 02:44:45.903
right now because we want to 
have the smallest version of the

02:44:45.904 --> 02:44:48.336
runtime that can be distributed 
to keep your binary size small.

02:44:48.554 --> 02:44:52.813
     The second thing that you 
need to do is make sure that you

02:44:52.814 --> 02:44:56.089
write any custom operators that 
you need, and now I'll go into a

02:44:56.090 --> 02:44:59.155
little bit of an example of 
doing that.  Well before that, 

02:44:59.156 --> 02:45:01.156
let me tell you one 

02:45:02.226 --> 02:45:03.899
more thing, which is we also 
have some visualizers to let you

02:45:03.900 --> 02:45:05.900
understand the model that you've
transformed and the 

02:45:06.349 --> 02:45:08.387
transformation process, so take 
a look at those.  They're linked

02:45:08.388 --> 02:45:10.388
off of the documentation.

02:45:15.583 --> 02:45:19.877
So let's get into writing a 
custom op?  What might we do?  

02:45:19.878 --> 02:45:23.764
Here I have an example that is 
silly, but it's to return p.  

02:45:23.765 --> 02:45:26.602
The important thing when you 
write an op is to perform C 

02:45:26.603 --> 02:45:31.317
functions so we have a C define 
for writing operations and the 

02:45:31.318 --> 02:45:32.576
reason we do that is all of our 
operations are implemented this 

02:45:32.577 --> 02:45:34.577
way so they can run on devices 
that only 

02:45:37.268 --> 02:45:39.329
support C, eventually, you but 
you can write kernels in C++.

02:45:39.330 --> 02:45:41.330
     In this case what I'm doing
is 

02:45:44.663 --> 02:45:46.663
ignoring the input tensors and 
putting 

02:45:48.152 --> 02:45:50.800
in output tensor which is N pi. 
If you have input tensors and 

02:45:50.801 --> 02:45:54.872
you want an output tensor you 
could also read the input 

02:45:54.873 --> 02:45:56.709
tensors and say multiply by 
three and now I have a multiply 

02:45:56.710 --> 02:45:58.710
by three operation.  This is 
going to be application at the 

02:46:01.353 --> 02:46:04.898
pend dependent and as I said 
before you don't always need to 

02:46:04.899 --> 02:46:08.579
do this.  I'm laying it out 
because if there is some 

02:46:08.580 --> 02:46:10.580
functionality you need we are 
extensible.

02:46:11.805 --> 02:46:12.616
Okay, once you've converted your
model you need to use a client 

02:46:12.617 --> 02:46:16.503
API.  Let me start with the C++ 
API, but we have other language 

02:46:16.504 --> 02:46:18.504
bindings as well that I'll get 
to.

02:46:19.363 --> 02:46:21.660
But in any of the binding, it's 
going to follow the same basic 

02:46:21.661 --> 02:46:23.661
pattern.

02:46:24.764 --> 02:46:26.764
The pattern is create an 
interpreter 

02:46:30.725 --> 02:46:32.553
and load the model, fill in your
data, execute the model and read

02:46:32.554 --> 02:46:34.554
back the data.  Very simple.

02:46:36.778 --> 02:46:40.575
In the C++ first thing you do is
create an object, this is given 

02:46:40.576 --> 02:46:44.281
the file name of the TensorFlow 
Lite file and creates an object 

02:46:44.282 --> 02:46:46.929
that is going to hold that model
and M map it, so we use 

02:46:46.930 --> 02:46:51.420
FlatBuffers and the reason why 
is because we can M map the 

02:46:51.421 --> 02:46:53.447
buffers which means there is 
zero latency to start running 

02:46:53.448 --> 02:46:56.549
the model effectively.
     Second, if you have any 

02:46:56.550 --> 02:47:00.426
custom operations, you can 
register them, so basically at 

02:47:00.427 --> 02:47:02.427
this phase you're deciding 

02:47:04.125 --> 02:47:06.125
which operations to include into
your runtime.

02:47:08.807 --> 02:47:10.266
By default we provide a built in
Op resolver that includes all of

02:47:10.267 --> 02:47:12.295
our default operations.  You 
might also use selective 

02:47:14.975 --> 02:47:17.013
registration that we alluded to 
before where you include only a 

02:47:17.014 --> 02:47:20.712
subset of the operations, and in
this case you might provide a 

02:47:20.713 --> 02:47:22.335
minimal resolver and if you 
wanted to use the custom 

02:47:22.336 --> 02:47:24.336
operation that we had before, 
you would create a custom 

02:47:26.055 --> 02:47:28.498
resolver that would tell 
TensorFlow Lite how to find your

02:47:28.499 --> 02:47:30.499
custom operation.

02:47:33.184 --> 02:47:35.421
So now we know what our Ops are 
and where to get the code for 

02:47:35.422 --> 02:47:37.926
them, and we know our model.  
Now we need to create an 

02:47:37.927 --> 02:47:41.790
interpreter object, so we take 
the pair of model and resolver 

02:47:41.791 --> 02:47:43.820
and put it together and it 
returns an interpreter.  This 

02:47:43.821 --> 02:47:46.679
interpreter is going to be our 
handle for doing our execution, 

02:47:46.680 --> 02:47:51.151
so the next step is we're going 
to perform an execution, but 

02:47:51.152 --> 02:47:54.594
before we can do that we need to
fill the buffer.  So if you have

02:47:54.595 --> 02:47:57.679
a model like a classification 
model, that is something that 

02:47:57.680 --> 02:48:01.513
takes an image in.  Where are 
you going to get the image?  The

02:48:01.514 --> 02:48:03.514
obvious place you might get it 
is 

02:48:04.591 --> 02:48:06.865
maybe from your devices storage 
if it's an image file name, or 

02:48:06.866 --> 02:48:09.514
also commonly it might be a 
camera.  Whatever it might be, 

02:48:09.515 --> 02:48:11.515
you produce like 

02:48:13.428 --> 02:48:15.110
a buffer, and in this it's going
to be a float star and instar 

02:48:15.111 --> 02:48:17.352
buffer and you fill it into your
buffer.  Once you fill this 

02:48:17.353 --> 02:48:20.210
buffer, you're ready to run.  So
we filled our buffer, TensorFlow

02:48:22.632 --> 02:48:24.632
Lite has all the information it 
needs to 

02:48:27.154 --> 02:48:29.377
run the execution, and we just 
call invoke.  Now it's going to 

02:48:29.378 --> 02:48:33.304
block until the execution is 
done and then we're going to be 

02:48:33.305 --> 02:48:35.305
able to read the output of it in

02:48:36.870 --> 02:48:38.299
an analogous way to our input 
and so that is we can get a 

02:48:38.300 --> 02:48:40.331
float star buffer out which 
could represent the class 

02:48:40.749 --> 02:48:44.634
numbers.  And then you're free 
to do with that data whatever 

02:48:44.635 --> 02:48:47.078
you want, so for example in an 
image classification app that we

02:48:48.524 --> 02:48:50.563
showed before, you would read 
that index out, map it back to a

02:48:50.564 --> 02:48:52.564
string, and then 

02:48:53.609 --> 02:48:56.046
put it into your GUI display.
     Great, so now we know how 

02:48:56.047 --> 02:48:58.047
to use C++.

02:48:59.925 --> 02:49:01.941
What if you're using another 
platform, for example, 

02:49:01.942 --> 02:49:03.942
RaspberryPi.

02:49:05.069 --> 02:49:06.705
On RaspberryPi the most common 
thing to use is probably  

02:49:06.706 --> 02:49:09.755
Python, and again it's going to 
follow the same basic pattern.  

02:49:09.756 --> 02:49:11.583
First we create a interpreter 
object, the interpreter object 

02:49:11.584 --> 02:49:13.584
is now our handle, how do we 
feed data?

02:49:17.543 --> 02:49:18.587
Since it's Python, we can use 
num pi arrays and this is 

02:49:18.588 --> 02:49:21.032
convenient because if you need 
pre-processing or post 

02:49:22.676 --> 02:49:24.513
processing you can do it with 
primitives you're familiar with,

02:49:24.514 --> 02:49:26.514
and this is kind of a theme that
goes on we want to keep 

02:49:29.411 --> 02:49:31.046
our bindings as idiomatic in the
language that they are and also 

02:49:31.047 --> 02:49:33.086
keep the performance, so in this
case we put in 

02:49:36.170 --> 02:49:38.170
some mun pi array and take out 
some, so that's Python.

02:49:38.409 --> 02:49:40.260
What if you're writing an 
Android app or want to write an 

02:49:40.261 --> 02:49:45.373
Android Things application, then
you might use the Java API.  So 

02:49:45.374 --> 02:49:48.060
in this case, it's the same 
thing.  You take and build an 

02:49:48.061 --> 02:49:51.116
interpreter, you give it the 
file name of the interpreter, it

02:49:51.117 --> 02:49:52.752
might be from a resource if 
you're doing an Android 

02:49:52.753 --> 02:49:58.069
application, and then finally 
you're going to fill the inputs 

02:49:58.070 --> 02:49:59.501
in and call run.
     So one of the things that 

02:49:59.502 --> 02:50:03.363
we did for the Java API is that 
we know that many Java 

02:50:03.364 --> 02:50:05.605
programmers don't really want to
deal with building their own 

02:50:05.606 --> 02:50:10.707
native library, so in that case 
you can just use our Gradle file

02:50:10.708 --> 02:50:12.708
here which will include our 
precompiled version of 

02:50:14.534 --> 02:50:16.944
TensorFlow Lite.  You don't have
to download our source code, and

02:50:16.945 --> 02:50:18.945
even for the tooling parts where
you do the conversion from 

02:50:19.201 --> 02:50:21.655
TensorFlow to TensorFlow Lite, 
you can download the 

02:50:21.656 --> 02:50:23.685
pre-compiled version of 
TensorFlow as I alluded to 

02:50:23.686 --> 02:50:27.757
before.
     Great.  So whether a if 

02:50:27.758 --> 02:50:29.758
you're doing iOS?

02:50:31.007 --> 02:50:33.665
This that case use the C++ API 
and you can also use the 

02:50:33.666 --> 02:50:35.666
Objective C API, but 

02:50:37.001 --> 02:50:39.001
again we provide a precompiled 
binary in 

02:50:40.816 --> 02:50:42.816
the form of a could he

02:50:46.342 --> 02:50:49.863
.  Cocoapod.
     I want to tell you what's 

02:50:49.864 --> 02:50:53.336
coming up in TensorFlow Lite.  
One thing we're asked for is 

02:50:53.337 --> 02:50:57.045
more operation, the more 
operations we have the more 

02:50:57.046 --> 02:50:59.321
models that can be run from 
TensorFlow out of the box.  The 

02:50:59.322 --> 02:51:00.348
other thing that happens with 
Machine Learning that is often 

02:51:00.349 --> 02:51:02.993
difficult is researchers come up
with new techniques all the 

02:51:02.994 --> 02:51:04.994
time, and that means that 
TensorFlow is always adding 

02:51:06.054 --> 02:51:08.721
operations and that means that 
we're going to continue to 

02:51:08.722 --> 02:51:10.777
follow TensorFlow as it adds 
important operations and add 

02:51:13.250 --> 02:51:14.270
them into TensorFlow Lite as 
well.

02:51:14.271 --> 02:51:17.336
Okay.  The second thing we're 
going to do is we're going to 

02:51:17.337 --> 02:51:20.198
improve the tooling, provide 
better documentation, and 

02:51:22.456 --> 02:51:24.304
tutorials and try to focus on 
ease of use, so it's really easy

02:51:24.305 --> 02:51:26.752
for you to understand on 
end-to-end examples how to 

02:51:27.774 --> 02:51:30.666
integrate TensorFlow Lite.
     And the third thing, which 

02:51:30.667 --> 02:51:33.944
Sarah already mentioned but I'll
mention again, is that we're 

02:51:33.945 --> 02:51:36.399
excited about  on-device 
training.  On-device training is

02:51:36.400 --> 02:51:38.875
really exciting because it 
allows us to refine a model 

02:51:41.524 --> 02:51:44.173
based on a user's experience.  
It allows us to decouple that 

02:51:45.812 --> 02:51:48.685
experience from going to the 
Cloud, so if they're 

02:51:48.686 --> 02:51:50.123
disconnected, we can continue 
improving the model so there is 

02:51:50.124 --> 02:51:52.768
a lot of requests for this.  
This will of course require more

02:51:52.769 --> 02:51:54.769
and more computation on the 
device, but 

02:51:58.722 --> 02:51:59.765
we're excited about upcoming 
hardware accelerators that will 

02:51:59.766 --> 02:52:01.766
make this more and more 
possible.

02:52:02.409 --> 02:52:04.409
Okay.

02:52:06.467 --> 02:52:08.467
One more question before we get 
into some exciting demos.

02:52:09.483 --> 02:52:11.746
When should I use TensorFlow 
Lite?  As we alluded to before, 

02:52:11.747 --> 02:52:13.747
we're starting to use TensorFlow
Lite for 

02:52:17.215 --> 02:52:18.733
first-party applications and 
third-party applications are 

02:52:18.734 --> 02:52:22.029
also using it.  That means that 
what we're doing moving forward 

02:52:22.030 --> 02:52:23.074
is we're going to make 
TensorFlow Lite our standard 

02:52:23.075 --> 02:52:26.575
solution for running ML on small
devices and mobile  devices.

02:52:29.694 --> 02:52:32.369
TensorFlow Lite currently 
supports a subset of TensorFlow 

02:52:32.370 --> 02:52:34.370
ops and this means that our 
recommendation is that you 

02:52:36.871 --> 02:52:38.871
should use TensorFlow Lite if 
you can 

02:52:40.585 --> 02:52:42.585
and let us know --

02:52:46.094 --> 02:52:47.928
(overhead speaker -- let us know
of any missing functionality you

02:52:47.929 --> 02:52:51.991
might need.  It's probably not 
done.  With that I want to show 

02:52:51.992 --> 02:52:54.888
you a video of retrained models.
     We showed you the 

02:52:54.889 --> 02:52:56.889
TensorFlow logo being 
recognized, and this is a common

02:52:58.349 --> 02:53:00.381
theme that we get which is that 
people like our pre-trained 

02:53:00.382 --> 02:53:04.240
examples like MobileNet but they
may not have a application where

02:53:04.241 --> 02:53:08.326
they need to tell the difference
between five dog breeds and many

02:53:08.327 --> 02:53:10.773
zoo animals, they might have an 
office area where they have 

02:53:10.774 --> 02:53:13.226
markers and whiteboards and as 
we were testing the app we had 

02:53:13.227 --> 02:53:17.345
this issue too.  It's like we 
don't have the classes that are 

02:53:17.346 --> 02:53:18.960
in these pre-trained models.
     So one of the great things 

02:53:18.961 --> 02:53:23.249
one of our other TensorFlow 
members created is something 

02:53:23.250 --> 02:53:25.308
called TensorFlow  For Poets and
there was a Codelab about that 

02:53:25.309 --> 02:53:28.966
and it's available online as 
well, and it basically allows 

02:53:28.967 --> 02:53:32.644
you to take a pretrained image 
model with really good detector 

02:53:32.645 --> 02:53:34.645
ability and put your own classes
into it.

02:53:35.083 --> 02:53:37.126
I want to show you a demo app 
that we created that runs on the

02:53:37.127 --> 02:53:40.998
PC and creates TensorFlow Lite 
models for you, so can we go to 

02:53:40.999 --> 02:53:45.728
the video?
     Okay, so we showed you 

02:53:45.729 --> 02:53:47.729
before, can 

02:53:50.666 --> 02:53:53.326
we recognize scissors and 
post-it notes?  Try it out.  You

02:53:53.327 --> 02:53:55.789
always want to try the models.  
The scissors look good.  Okay.  

02:53:55.790 --> 02:53:59.089
Great.  Post-it notes also looks
good.  What if we had another 

02:53:59.090 --> 02:54:01.090
object?

02:54:02.339 --> 02:54:04.339
An object that is more common, 
more 

02:54:06.203 --> 02:54:09.722
important, like this medal 
TensorFlow logo?  Happens?  

02:54:09.723 --> 02:54:14.018
Everyday life, right?  Let's 
take a look at how this does.  

02:54:14.019 --> 02:54:15.476
It's labeled as other, that's 
not very good, but the great 

02:54:15.477 --> 02:54:19.554
thing about Machine Learning is 
we can fix it.  The way we fix 

02:54:19.555 --> 02:54:22.197
it is to add data.  We have the 
application and have gone to the

02:54:22.198 --> 02:54:25.256
training tab.  Now we're going 
to define a class called 

02:54:25.257 --> 02:54:27.257
TensorFlow, and this is 
basically 

02:54:28.321 --> 02:54:30.321
short for TensorFlow logo and 
now from 

02:54:32.247 --> 02:54:34.247
our website cam we're going to 
click 

02:54:35.303 --> 02:54:37.363
capture a couple different 
perspectives as many as we can, 

02:54:37.364 --> 02:54:39.364
and ideally on different 
backgrounds so it doesn't 

02:54:41.657 --> 02:54:44.096
associate the background with it
being a TensorFlow logo and then

02:54:44.097 --> 02:54:47.323
I click Train, and now it's 
using TensorFlow to train the 

02:54:47.324 --> 02:54:49.194
model and it's converging to a 
good validation accuracy, it's 

02:54:49.195 --> 02:54:52.449
going to reload the model, we're
testing in TensorFlow Lite 

02:54:52.450 --> 02:54:54.487
running on the PC right now and 
we see it's  recognizing 

02:54:55.911 --> 02:54:58.346
TensorFlow correctly, so it's 
that fast and easy, but also we 

02:54:58.347 --> 02:55:00.347
can take that 

02:55:01.612 --> 02:55:03.833
model and we can move to Android
and iOS and use the exact same 

02:55:03.834 --> 02:55:08.927
model and update it.  Thanks.
     Now, let's move to a live 

02:55:08.928 --> 02:55:11.007
demo, so I'm going to go over 
here to the podium.

02:55:12.460 --> 02:55:14.460
(Applause).

02:55:21.427 --> 02:55:25.495
All right.  So classification, 
what I just showed you is kind 

02:55:25.496 --> 02:55:27.974
of this idea that you have an 
image in and you put an image 

02:55:27.975 --> 02:55:31.244
out and you put classifications 
out.  What if you have multiple 

02:55:31.245 --> 02:55:33.376
objects in the scene or 
something in the corner of 

02:55:39.487 --> 02:55:41.487
an object?  You also want to 
know where in the 

02:55:45.401 --> 02:55:45.416
scene that object is, and that 
enters this model called  

02:55:45.417 --> 02:55:49.437
Singleshot detection, a type of 
model and turns out our friends 

02:55:49.438 --> 02:55:51.438
in TensorFlow released a package
called object detection as part 

02:55:53.414 --> 02:55:56.320
of the TensorFlow models and it 
allows us to use the pre-trained

02:55:56.321 --> 02:55:59.796
model to recognize many classes.
     What I've done is I want to

02:55:59.797 --> 02:56:01.797
load it on a small  device.

02:56:05.314 --> 02:56:07.811
We've shown you a lot of things 
with small  devices.  I'm going 

02:56:07.812 --> 02:56:11.483
to show you another thing.  This
is a RaspberryPi.  It's another 

02:56:11.484 --> 02:56:13.484
cool example of a device because
it's cheap and easy to get so 

02:56:14.341 --> 02:56:15.761
any high school student can have
one of these, you can have many 

02:56:15.762 --> 02:56:20.245
of these and just use them for a
dedicated project, but the other

02:56:20.246 --> 02:56:22.074
great thing about them is not 
only are they relatively 

02:56:22.075 --> 02:56:24.324
powerful, but they're also able 
to interface with 

02:56:28.018 --> 02:56:29.806
other hardware and have GPIO 
pins and this can be capitalized

02:56:29.807 --> 02:56:32.930
in a number of different ways, 
but one way is to run Linux, and

02:56:32.931 --> 02:56:35.181
that's what we're doing here.  
But you can also use Android 

02:56:35.182 --> 02:56:37.182
Things 

02:56:39.087 --> 02:56:41.146
which you can see that the 
sandbox has many examples doing 

02:56:41.147 --> 02:56:43.901
that, so can also do this with 
Android Things.

02:56:47.273 --> 02:56:47.881
In this case I have the system 
board right here and it's 

02:56:47.882 --> 02:56:49.882
connected to a 

02:56:51.925 --> 02:56:55.458
motor controller, and this motor
controller is just a 

02:56:51.925 --> 02:56:53.925
microcontroller 

02:56:55.422 --> 02:56:57.678
that interfaces to server 
motors, and the servers motors 

02:56:57.679 --> 02:57:01.553
can go left and right and up and
down and aim the camera.  Now 

02:57:01.554 --> 02:57:03.786
we're going to load the object 
detection model on to this 

02:57:03.787 --> 02:57:05.787
device and we're actually going 
to run it in order 

02:57:09.713 --> 02:57:11.970
to recognize different objects, 
so let's go to the demo feed, 

02:57:11.971 --> 02:57:14.006
please.
     We can see my app, you can 

02:57:14.007 --> 02:57:17.514
tell by the beautiful nature of 
this app that I'm not a great 

02:57:17.515 --> 02:57:19.515
app developer, but this is what 
I can do on a weekend, so give 

02:57:20.378 --> 02:57:23.451
me a little bit of slack.
     Okay.  So here if we hold 

02:57:23.452 --> 02:57:25.452
up the apple, it's 

02:57:27.134 --> 02:57:27.948
rising the apple and it's 
telling us what probability and 

02:57:27.949 --> 02:57:30.631
where the object is.  Now, 
that's all good and fine but 

02:57:30.632 --> 02:57:35.362
when we couple it with the 
ability to move, where I'm going

02:57:35.363 --> 02:57:37.363
to turn on the motors now and 
bring back the apple.

02:57:40.263 --> 02:57:41.681
What I'm going to do is move the
apple in the screen and it's 

02:57:41.682 --> 02:57:44.127
going to try to keep it 
centered, so as I move this 

02:57:45.757 --> 02:57:47.196
apple, it's basically going to 
try to keep it centered, it's 

02:57:47.197 --> 02:57:50.066
like a little virtual camera 
person, and this works on 

02:57:53.570 --> 02:57:55.450
other objects like this banana 
here, hopefully.  Oh, there we 

02:57:55.451 --> 02:57:57.451
go, and it's going to keep that 
centered.

02:57:58.308 --> 02:58:00.948
If you put two objects in the 
screen, it's going to try to 

02:58:00.949 --> 02:58:02.949
keep them both in.

02:58:05.436 --> 02:58:07.066
Okay, so we have a little bit of
false detection, but it's 

02:58:07.067 --> 02:58:09.067
basically going to try to keep 
them both centered.

02:58:10.333 --> 02:58:11.766
So this is really a fun 
application, and I bet you can 

02:58:11.767 --> 02:58:15.664
come up with many different 
applications that are really 

02:58:15.665 --> 02:58:17.716
exciting to do with this type of
application, so can I go back to

02:58:17.717 --> 02:58:20.564
the slides again?  I'll get my 
clicker.

02:58:21.985 --> 02:58:27.724
(Applause).
       So like I said, this is 

02:58:27.725 --> 02:58:31.612
basically what I can do on a 
weekend, but I imagine great app

02:58:31.613 --> 02:58:33.613
developers and people with a 

02:58:34.710 --> 02:58:36.987
lot of creativity about 
connecting devices and 

02:58:36.988 --> 02:58:39.236
connecting software can do many 
interesting things.

02:58:41.268 --> 02:58:43.268
So what I want to do now is I 
want 

02:58:44.588 --> 02:58:45.807
to tell you in summary, 
TensorFlow Lite, you've seen a 

02:58:45.808 --> 02:58:50.939
lot about it.  Basically, we 
feel TensorFlow Lite makes 

02:58:50.940 --> 02:58:52.995
on-device ML  small, fast, and 
easy, and we think that you're 

02:58:52.996 --> 02:58:57.058
going to find it very useful in 
all of your applications and 

02:58:57.059 --> 02:58:59.059
we're excited to see what you 
build.

02:58:59.939 --> 02:59:05.054
Come talk to us.  I'm going to 
be in office hours at 12:30.  

02:59:05.055 --> 02:59:07.055
You can come talk to me.  In 
addition, you can come to our 

02:59:08.755 --> 02:59:10.384
sandbox if you haven't already, 
and we have, of course the 

02:59:10.385 --> 02:59:12.385
examples that I 

02:59:13.453 --> 02:59:15.519
showed you here, we have the 
tracking camera, we also have 

02:59:15.520 --> 02:59:17.520
the object classification on 
mobile device, but 

02:59:20.410 --> 02:59:22.286
another cool thing that we have 
is the donkey cars and this was 

02:59:22.287 --> 02:59:24.976
done by a group outside of 
Google and we converted them 

02:59:27.219 --> 02:59:29.219
over to TensorFlow Lite and we 
are 

02:59:30.274 --> 02:59:31.692
excited to see that their 
application works really well 

02:59:31.693 --> 02:59:34.150
with TensorFlow Lite as well.
     So with that, I hope you 

02:59:34.151 --> 02:59:37.835
check these things out.  I want 
to tell you that if you want to 

02:59:40.196 --> 02:59:42.036
get started, you can go to our 
documentation page, you can go 

02:59:42.037 --> 02:59:44.037
to the  TensorFlow.

02:59:47.405 --> 02:59:49.405
org page and there is a TFLite 
page that you can find more 

02:59:49.728 --> 02:59:54.402
information.  Our code is all 
Open Source, get it on GitHub, 

02:59:54.403 --> 02:59:56.263
download it, modify it, submit a
pull request, and of course file

02:59:56.264 --> 02:59:58.264
any issues that you have while 
using it.

02:59:59.162 --> 03:00:01.204
In addition, if you want to talk
about TensorFlow Lite, talk 

03:00:01.205 --> 03:00:03.205
about your applications, ask us 
about feature 

03:00:06.706 --> 03:00:08.706
requests, please send to

03:00:10.075 --> 03:00:12.075
our mailing list.

03:00:15.177 --> 03:00:17.243
This community is exciting 
exciting, in Open Sourcing 

03:00:17.244 --> 03:00:19.244
TensorFlow, people got really 
exciting and we made it better 

03:00:20.708 --> 03:00:22.973
forever for people inside and 
outside of Google and we hope 

03:00:22.974 --> 03:00:25.637
you'll engage TensorFlow Lite in
the same way TensorFlow has been

03:00:25.638 --> 03:00:27.062
engaged.
     With that I want to thank 

03:00:27.063 --> 03:00:29.063
you for 

03:00:30.558 --> 03:00:31.597
your attention, for coming to 
I/O, for listening to our talk 

03:00:31.598 --> 03:00:35.291
about TensorFlow Lite, and I 
also want to thank you -- thank 

03:00:35.292 --> 03:00:37.147
our Google partners.  This 
product didn't come out of 

03:00:37.148 --> 03:00:41.504
isolation.  It came from all of 
our experience building mobile 

03:00:41.505 --> 03:00:43.999
apps with machine intelligence, 
and as we gained experience we 

03:00:44.000 --> 03:00:46.877
found that there was a common 
need and that was the genesis of

03:00:49.336 --> 03:00:51.336
TensorFlow Lite, so all of our 
partners 

03:00:53.649 --> 03:00:55.486
provided provided application, 
provided feedback, even provided

03:00:55.487 --> 03:01:00.383
code and help with our model, so
thank you so much to you and to 

03:01:00.384 --> 03:01:02.443
them and enjoy the rest of I/O.
     (Applause).

03:01:20.017 --> 03:01:24.301
     &gt;&gt; Thank you for joining 
this session.  Grand ambassadors

03:01:24.302 --> 03:01:28.788
will assist with directing you 
through the designated exits.  

03:01:28.789 --> 03:01:30.006
We'll be making room for those 
who have registered for the next

03:01:30.007 --> 03:01:33.312
session.  If you've registered 
for the next session in this 

03:01:33.313 --> 03:01:35.356
room, we ask that you please 
clear the room and return via 

03:01:35.357 --> 03:01:39.894
the registration line outside.  
Thank you.

03:03:22.127 --> 03:03:24.127
May 10, 2018

03:03:26.407 --> 03:03:28.407
11:30 a.m. PT

03:03:35.594 --> 03:03:38.040
Don't Let Your App Drain Your 
User's Battery

03:10:28.400 --> 03:10:30.400
-

03:13:19.315 --> 03:13:21.315
-

03:24:07.647 --> 03:24:10.307
lunchish, welcome everybody and 
thank you for coming to talk 

03:24:10.308 --> 03:24:13.551
about battery.  It's another 
year and I'm here again to talk 

03:24:13.552 --> 03:24:15.552
to you about power.

03:24:17.424 --> 03:24:19.473
It's an ongoing project and as 
you've probably saw from the  

03:24:19.474 --> 03:24:21.474
keynote, Dave 

03:24:24.156 --> 03:24:26.156
talked about Maslow's or his own
version 

03:24:27.453 --> 03:24:29.501
of the pyramid and I think 
actually a lot is in this image.

03:24:31.116 --> 03:24:32.778
Battery is foundational to 
everything you're doing on your 

03:24:32.779 --> 03:24:36.449
phone, right.  If you don't have
power, you don't really have any

03:24:36.450 --> 03:24:38.450
of the cool feature, you don't 
have the camera, you don't have 

03:24:39.290 --> 03:24:41.953
the assistant.  Battery exists 
to power all the other 

03:24:45.410 --> 03:24:48.044
things and it's a struggle for 
us and a struggle for all of you

03:24:48.045 --> 03:24:51.743
as well.  You want to do awesome
things for users and you also 

03:24:51.744 --> 03:24:54.425
want to save power so they can 
do those things for longer, and 

03:24:56.084 --> 03:24:58.708
battery is actually a really 
easy problem to solve if you 

03:24:58.709 --> 03:25:01.782
just kill everything, fixed, 
I'll take my promotion, it's 

03:25:01.783 --> 03:25:03.223
great
     The problem is it's 

03:25:03.224 --> 03:25:07.909
obviously much more nuanced than
that and that is a struggle that

03:25:07.910 --> 03:25:09.552
we have internally every day 
talking about power and what are

03:25:09.553 --> 03:25:13.256
the right tradeoffs for users, 
and that's always a really hard 

03:25:13.257 --> 03:25:16.515
question and we're going to talk
a little bit about that and some

03:25:16.516 --> 03:25:18.516
of the things that we're 

03:25:19.599 --> 03:25:21.219
building into Android P to make 
it a lot easier, and so the 

03:25:21.220 --> 03:25:23.220
first thing I want to 

03:25:25.707 --> 03:25:28.559
talk about is where is it going.
     This is a horrifically 

03:25:28.560 --> 03:25:32.467
simplified, I'm sure some of the
engineering out there are like 

03:25:32.468 --> 03:25:34.730
oh, my God, he's showing this!  
You have hardware, things like 

03:25:34.731 --> 03:25:36.731
how 

03:25:38.268 --> 03:25:40.122
much RAM you have on your device
cause a persistent drain on 

03:25:40.123 --> 03:25:42.123
what's happening there.

03:25:43.613 --> 03:25:45.234
What hardware your OEM selected,
what you're using, how big your 

03:25:45.235 --> 03:25:47.486
screen is, how bright your 
screen is all play into that 

03:25:47.487 --> 03:25:49.487
effect

03:25:50.774 --> 03:25:52.821
You have the Os itself, is the 
kernel being efficient, waking 

03:25:52.822 --> 03:25:56.508
up a lot, does it have a lot of 
overhead?  Do you have disk 

03:25:56.509 --> 03:25:58.509
encryption, is it in software or
hardware?

03:26:01.656 --> 03:26:03.088
Those things all effect your 
overall power profile happening 

03:26:03.089 --> 03:26:04.530
under the hood
     The next thing is absented 

03:26:04.531 --> 03:26:07.836
services and this is where we're
going to spend a bit of time 

03:26:07.837 --> 03:26:11.096
talking about today.  Obviously,
what apps you have on your phone

03:26:11.097 --> 03:26:13.097
and services running that phone 

03:26:14.996 --> 03:26:16.863
drive power substantially, 
largely through the CPU and 

03:26:16.864 --> 03:26:18.691
network activity
     The last is the user's 

03:26:18.692 --> 03:26:22.996
interaction with the device 
itself, how they behave, what 

03:26:22.997 --> 03:26:25.648
applications they use, how 
quickly they respond to 

03:26:25.649 --> 03:26:27.691
notifications or don't, all 
impacts the overall picture of 

03:26:27.692 --> 03:26:29.692
where 

03:26:30.804 --> 03:26:35.529
battery ends up.  A really 
complicated topic, we're going 

03:26:35.530 --> 03:26:37.789
to talk largely about the bottom
left corner today, but the thing

03:26:37.790 --> 03:26:43.547
I want to say before we get into
that is what other things have 

03:26:43.548 --> 03:26:46.204
we done and how does the things 
we're going to talk about in P 

03:26:46.205 --> 03:26:48.205
affect the strategy?

03:26:49.696 --> 03:26:50.915
We started in lollipop with job 
scheduler and that was a 

03:26:50.916 --> 03:26:54.392
powerful tool to avoid apps 
having to hang out on  services 

03:26:54.393 --> 03:26:55.813
all the time processing in the 
background, but you could 

03:26:55.814 --> 03:26:58.671
schedule a piece of work and 
have it done a finite 

03:27:01.928 --> 03:27:03.793
amount of time and the Os could 
have variability on when to 

03:27:03.794 --> 03:27:05.794
schedule it.

03:27:08.701 --> 03:27:10.743
next was doze and that was 
targeted at you have the tablet 

03:27:10.744 --> 03:27:14.021
or phone, you put it down, not 
using it, you put it in a drawer

03:27:14.022 --> 03:27:19.158
and forget about it.  Hopefully 
when you pull it out and use it 

03:27:19.159 --> 03:27:22.626
again a few days later it still 
has a battery.  Doze was trying 

03:27:22.627 --> 03:27:24.627
to target that problem and was 
largely successful

03:27:27.134 --> 03:27:29.134
We had to go further, Doze on 
the 

03:27:31.444 --> 03:27:34.439
go or Does light we call it 
internally because it's faster. 

03:27:34.440 --> 03:27:35.595
That was targeted at phones.  I 
have my phone in my pocket, I'm 

03:27:35.596 --> 03:27:38.245
not using it, but I'm moving 
around and that's not in a 

03:27:38.246 --> 03:27:40.488
drawer.  That is like the in 
exstep, is how do 

03:27:44.768 --> 03:27:46.768
we get that to be the next

03:27:49.436 --> 03:27:51.578
scenario, and Doze was looking 
at that.

03:27:56.112 --> 03:27:58.146
You still don't want full Doze 
but a little backed off and

03:28:00.405 --> 03:28:02.489
We still needed to do more and 
go deeper, the next is 

03:28:02.490 --> 03:28:04.490
background limits, largely 
targeted at background services 

03:28:06.236 --> 03:28:07.052
as a whole, now we've had jobs 
around for quite a while, and 

03:28:07.053 --> 03:28:09.883
also we're looking at background
limits for location and trying 

03:28:09.884 --> 03:28:12.754
to figure out what's the right 
balance of applications looking 

03:28:12.755 --> 03:28:15.270
at location in the background 
versus the foreground and ensure

03:28:15.271 --> 03:28:17.271
there would be different 
thresholds of what is 

03:28:17.294 --> 03:28:19.968
appropriate
     Really good effect with 

03:28:19.969 --> 03:28:21.969
that, but 

03:28:23.247 --> 03:28:25.905
the only downside to what we did
with Oreo we needed apps to 

03:28:25.906 --> 03:28:29.422
target the Oreo  SDK, I'll talk 
about that in a moment, in order

03:28:29.423 --> 03:28:33.345
to support this.  P, since we're
here we're going to talk about 

03:28:33.346 --> 03:28:35.176
some more stuff that we're 
working n the one thing I wanted

03:28:35.177 --> 03:28:39.872
to take a moment because we're 
mostly going to talk about 

03:28:39.873 --> 03:28:41.873
really cool ML and cool things. 
There was a lot of brute force 

03:28:42.930 --> 03:28:44.930
optimization that the 
engineering team 

03:28:46.000 --> 03:28:47.826
has also done and this was 
looking at f2fs filesystem work 

03:28:47.827 --> 03:28:51.334
and how do we choose between 
whether a process should be on a

03:28:51.335 --> 03:28:53.335
small core or big core, and 

03:28:54.395 --> 03:28:56.648
looking at when to do CPU boosts
so you have buttery smooth 

03:28:56.649 --> 03:29:01.150
sliding but you're not just 
purning power when the screen is

03:29:01.151 --> 03:29:03.409
on in anticipation the user 
might interact with the screen. 

03:29:03.410 --> 03:29:05.700
These are all things done over 
the years.  This is a set we 

03:29:05.701 --> 03:29:07.550
just did on P, and there is 
probably still more we're trying

03:29:07.551 --> 03:29:09.776
to get done.
     And then the thing is there

03:29:09.777 --> 03:29:13.690
is still some of those big 
challenges that are remaining 

03:29:13.691 --> 03:29:15.691
and this is what we're going to 
talk about today.

03:29:18.491 --> 03:29:20.430
What we noticed was battery 
drain was roughly proportional 

03:29:20.431 --> 03:29:22.431
to the number of apps you have 
on the phone, not the 

03:29:25.124 --> 03:29:28.611
number of apps you are using, 
but how many you had installed. 

03:29:28.612 --> 03:29:30.029
And obviously that's not ideal, 
and so we wanted to try to 

03:29:30.030 --> 03:29:34.748
remedy that.  We also saw a lot 
of scenarios where apps are 

03:29:34.749 --> 03:29:36.749
accidently making mistakes, 

03:29:38.048 --> 03:29:40.302
like if you're working with 
wakelock you're kind of playing 

03:29:40.303 --> 03:29:43.618
with fire and what happens when 
the mistake happens?  Usually 

03:29:43.619 --> 03:29:46.895
the battery suffers.  Also some 
developers are really aggressive

03:29:46.896 --> 03:29:48.317
with the use case and trying to 
be like oh, I must be there all 

03:29:48.318 --> 03:29:51.614
the time ready to go at a 
moment's notice and sometimes 

03:29:51.615 --> 03:29:53.615
that might take a awful 

03:29:54.901 --> 03:29:56.119
lot of battery to achieve that 
and is that really in the 

03:29:56.120 --> 03:29:58.366
interest of the  user, and how 
can we help balance that back 

03:29:58.367 --> 03:30:00.623
out?
     The last part is, even when

03:30:00.624 --> 03:30:05.187
an app was aggressive or app had
a mistake, it wasn't obvious to 

03:30:05.188 --> 03:30:07.188
the user as to what to do.

03:30:08.917 --> 03:30:11.186
They didn't know is the battery 
menu going to help me, does it 

03:30:11.187 --> 03:30:14.846
tell me everything, does it 
catch all the  cases, none of 

03:30:14.847 --> 03:30:16.877
the case, some of the cases?  
Even when you saw a high battery

03:30:20.346 --> 03:30:22.208
value, now what is this do I 
uninstall, live with it, call up

03:30:22.209 --> 03:30:24.254
the developer and send a 
nastygram, that's something else

03:30:25.685 --> 03:30:26.915
we wanted to look into
     The next thing I mentioned 

03:30:26.916 --> 03:30:28.916
before 

03:30:30.224 --> 03:30:32.053
is the Oreo SDK, having apps 
targeting the SDK to say I 

03:30:32.054 --> 03:30:34.054
understand how the background 
limits work, I understand how 

03:30:36.347 --> 03:30:38.623
I can use Jobs and alarms 
instead of using services.  As 

03:30:38.624 --> 03:30:40.237
you probably saw there was an 
announcement late last year 

03:30:40.238 --> 03:30:44.582
talking about targeting 
requirements for applications on

03:30:44.583 --> 03:30:46.583
the Play Store, if you haven't 
seen that Google that and find 

03:30:47.499 --> 03:30:48.712
out about it, you should be 
looking at it because those 

03:30:48.713 --> 03:30:53.266
requirements are kicking in 
later this year and that's going

03:30:53.267 --> 03:30:54.467
to ensure then on P the majority
of apps on your phone are now 

03:30:54.468 --> 03:30:58.765
running the Oreo SDK and all of 
these features we're talking 

03:30:58.766 --> 03:31:00.766
about  work.

03:31:04.178 --> 03:31:06.178
So the high-level thing I want 
to 

03:31:07.429 --> 03:31:11.121
talk about is a feature called 
Adaptive Battery.  Battery Saver

03:31:11.122 --> 03:31:13.787
and trying to make it better, 
and additional restrictions 

03:31:15.819 --> 03:31:17.652
around background restrictions 
to help users understand when 

03:31:17.653 --> 03:31:22.183
can I make it stop, can I have 
another choice other than 

03:31:22.184 --> 03:31:24.184
uninstall, something in the 
middle 

03:31:25.669 --> 03:31:27.669
between live with it and r 
remove it

03:31:27.738 --> 03:31:29.361
This pyramid is trying to 
articulate, we want power 

03:31:29.362 --> 03:31:31.362
efficiency, we 

03:31:32.606 --> 03:31:34.675
like batteries that last a long 
time, we want cool apps and we 

03:31:34.676 --> 03:31:38.732
don't want the use tore have to 
manage between the things.  Cool

03:31:38.733 --> 03:31:40.794
apps or power efficiency?  No 
one wants to make that choice, 

03:31:40.795 --> 03:31:44.251
it's difficult, I don't want to 
make it, no one wants to make 

03:31:44.252 --> 03:31:46.252
it, we want it just to work and 
that's what this whole project 

03:31:47.301 --> 03:31:49.301
has been about, so with that I'm
going 

03:31:50.970 --> 03:31:52.385
to invite James up here to talk 
about Adaptive Battery.  Thanks.

03:31:52.386 --> 03:31:57.340
     (Applause)

03:31:57.341 --> 03:32:00.011
&gt;&gt; JAMES SMITH:  Thank you very 
much.  Hi, everyone.  Good 

03:32:00.012 --> 03:32:02.012
morning.  I'm James.

03:32:05.964 --> 03:32:07.788
I'm rep sending the DeepMind 
team today and we're pleased to 

03:32:07.789 --> 03:32:11.930
be here and working with Android
and excited about what we've 

03:32:11.931 --> 03:32:14.828
built together.
     So as a user, you shouldn't

03:32:14.829 --> 03:32:19.109
have to closely manage how your 
apps behave on your device.

03:32:22.193 --> 03:32:23.606
A modern intelligent operating 
system should just take care of 

03:32:23.607 --> 03:32:26.071
it, and that's where Machine 
Learning can help.

03:32:29.974 --> 03:32:31.802
We've co-developed this feature 
with Android called Adaptive 

03:32:31.803 --> 03:32:36.925
Battery.  It intelligently 
aligns app power consumption 

03:32:36.926 --> 03:32:39.792
with app usage.
     Apps can still run in the 

03:32:41.432 --> 03:32:43.432
background when they need to and
users 

03:32:45.744 --> 03:32:47.744
don't need to micromanage.

03:32:49.831 --> 03:32:51.831
So Adaptive Battery uses the 

03:32:55.322 --> 03:32:59.455
concept of app App Standby 
Buckets.  Every app is assigned 

03:32:59.456 --> 03:33:03.773
to the buckets.  Each bucket has
different limits on background 

03:33:03.774 --> 03:33:05.774
activity and there are four 

03:33:06.831 --> 03:33:08.831
buckets ranging from active to 
rare.

03:33:10.128 --> 03:33:12.401
Apps in the rare bucket have the
most restrictions and we use the

03:33:12.402 --> 03:33:14.402
ML model to 

03:33:15.496 --> 03:33:17.749
assign apps to buckets based on 
their predicted usage.

03:33:21.032 --> 03:33:22.447
So for example, apps that are 
predicted to be open in the next

03:33:22.448 --> 03:33:27.372
few hours, I'm here at I/O, so I
might be using the I/O app and 

03:33:27.373 --> 03:33:31.294
so the I/O app is going to be in
the active bucket.  But in a few

03:33:31.295 --> 03:33:33.295
days time when I'm back 

03:33:35.199 --> 03:33:36.885
at home,  Adaptive Battery is 
going to automatically determine

03:33:36.886 --> 03:33:38.918
that I'm unlikely to be using 
this app and put it 

03:33:41.996 --> 03:33:43.044
into the rare bucket so it's not
unnecessarily consuming 

03:33:43.045 --> 03:33:45.045
resources like battery.

03:33:49.599 --> 03:33:51.599
So what are the restrictions 
that are applied?

03:33:55.353 --> 03:33:57.806
Jobs,  Alarms, Firebase Cloud 
Messaging, and Network are all 

03:34:00.065 --> 03:34:02.316
restricted in Android P 
depending on which bucket the 

03:34:02.317 --> 03:34:04.317
app is in.

03:34:05.805 --> 03:34:07.805
Apps in the active bucket have 
no 

03:34:10.336 --> 03:34:13.020
restrictions, just like it is 
today.  Working set has 

03:34:13.021 --> 03:34:15.251
restrictions on  jobs and 
alarms, and frequent and rare 

03:34:16.479 --> 03:34:18.479
buckets introduce restrictions 
on the 

03:34:21.001 --> 03:34:22.843
number of high priority Firebase
Cloud  Messages.  Messages 

03:34:22.844 --> 03:34:24.872
beyond that cap will be treated 
as normal priority so that we 

03:34:26.512 --> 03:34:28.958
batch together with other 
messages to save battery.

03:34:32.426 --> 03:34:33.855
Generally, as developers you 
should assume the background 

03:34:33.856 --> 03:34:38.824
activity will be deferred and 
ensure that your app can work 

03:34:38.825 --> 03:34:41.268
under those conditions.  But one
thing to remember is once this 

03:34:44.159 --> 03:34:46.159
device is plugged into power, 
all of the 

03:34:48.990 --> 03:34:50.990
restrictions are lifted.

03:34:54.994 --> 03:34:57.087
There are ADB commands you can 
use to test apps in each buckets

03:34:57.088 --> 03:34:59.088
to make sure it performs as 
expected.

03:35:03.609 --> 03:35:05.641
Can you also use new framework 
APIs to to get your apps current

03:35:05.642 --> 03:35:07.642
bucket at runtime.

03:35:09.099 --> 03:35:10.991
Most apps should be fine if 
they're already following best 

03:35:10.992 --> 03:35:13.839
practices such as using Jobs for
background work and 

03:35:17.100 --> 03:35:18.962
targeting recent API levels like
Android Oreo.  As Ben just said,

03:35:18.963 --> 03:35:20.963
this is going to be 

03:35:23.062 --> 03:35:25.503
a requirement for app updates on
the Splay Store later this year 

03:35:25.504 --> 03:35:28.978
so please make sure you target 
the latest SDK  versions.

03:35:30.434 --> 03:35:32.434
So let's talk a little bit about
Machine Learning.

03:35:35.999 --> 03:35:37.999
The model was built by Android 
in 

03:35:40.522 --> 03:35:43.394
DeepMind to predict which apps 
go into which buckets.  We 

03:35:43.395 --> 03:35:44.817
should have done this with 
simple heuristics or we could 

03:35:44.818 --> 03:35:47.662
have even just used past 
behavior on the device but we 

03:35:49.898 --> 03:35:52.139
found that Machine Learning 
allows us to capture the nuance 

03:35:52.140 --> 03:35:54.172
of how users behave with their 
app, and I'll go into some 

03:35:55.805 --> 03:35:57.805
detail on how we built the model
in a second.

03:36:00.095 --> 03:36:01.745
For those of you who are 
interested in the architecture 

03:36:01.746 --> 03:36:03.746
of the model, we're 

03:36:05.035 --> 03:36:07.035
using a two-layer deep 
convolutional 

03:36:08.112 --> 03:36:09.756
neural nets with a feet forward 
neural net on top and this is 

03:36:09.757 --> 03:36:14.700
used to predict the probability 
that an app will be opened in a 

03:36:14.701 --> 03:36:17.784
given interval.
     You might have heard of 

03:36:20.217 --> 03:36:21.643
convolutional neural nets 
before, particularly they're 

03:36:21.644 --> 03:36:26.124
common in image classifiers, but
we've used them to here to 

03:36:26.125 --> 03:36:28.125
measure higher levels -- 
higher-level 

03:36:29.191 --> 03:36:31.063
patterns of app usage over time.
Turns out they're pretty good at

03:36:31.064 --> 03:36:33.064
that.

03:36:34.780 --> 03:36:36.028
Now all of there is happening 
on-device using TensorFlow, and 

03:36:36.029 --> 03:36:38.029
that's 

03:36:39.091 --> 03:36:41.123
actually a first for DeepMind.  
We never deployed production 

03:36:41.124 --> 03:36:43.124
models on 

03:36:44.186 --> 03:36:46.227
the compute pair of a single 
device, and a single mobile 

03:36:46.228 --> 03:36:48.228
device with the limited compute 
power that's available is a 

03:36:49.106 --> 03:36:50.364
particular set of challenges, 
and of course we're building 

03:36:50.365 --> 03:36:52.365
this Machine Learning model with
the intention of 

03:36:56.512 --> 03:36:58.512
attempting to save power, and if
we're 

03:37:00.006 --> 03:37:01.637
doing Machine Learning on-device
we have to be careful we're not 

03:37:01.638 --> 03:37:03.638
spending more power than we're 
saving.

03:37:04.731 --> 03:37:06.731
We're going to be making this 
model 

03:37:07.790 --> 03:37:09.826
available to all device 
manufacturers so they can take 

03:37:09.827 --> 03:37:11.827
it if they wish for their 
devices on Android P or they can

03:37:14.568 --> 03:37:15.996
implement their own or they can 
take an Android Open Source 

03:37:15.997 --> 03:37:22.557
version of it as well.
     So the model, we trained 

03:37:22.558 --> 03:37:24.558
the model 

03:37:26.038 --> 03:37:28.285
on millions of sequences of app 
opens and transitions to 

03:37:28.286 --> 03:37:30.286
discover the patterns 

03:37:32.816 --> 03:37:35.036
of how users behave, so an 
example would be if you use an 

03:37:35.037 --> 03:37:39.331
app at 8:00 in the morning every
morning, you're probably quite 

03:37:39.332 --> 03:37:44.463
likely to open it tomorrow at 
8:00 as well.  But if you only 

03:37:44.464 --> 03:37:46.913
use a certain app on weekends 
like a game or a travel app, 

03:37:49.399 --> 03:37:51.622
then you're probably not going 
to open it on Monday morning, 

03:37:51.623 --> 03:37:53.623
and the model is 

03:37:54.923 --> 03:37:57.184
able to capture this nuance.
     The model looks at the 

03:37:57.185 --> 03:38:00.890
user's behavior and outputs a 
probability of when you're next 

03:38:00.891 --> 03:38:04.563
going to open a particular app. 
The model compares the usage to 

03:38:04.564 --> 03:38:07.215
the patterns we've observed in 
the data and 

03:38:10.319 --> 03:38:12.756
the behavior on-device will 
influence the predictions given 

03:38:12.757 --> 03:38:14.757
by the model.

03:38:18.903 --> 03:38:20.903
So we've also built this model 
with certain principles in mind.

03:38:22.167 --> 03:38:24.167
We tried to make it fair.

03:38:25.843 --> 03:38:27.843
There is no favoritism of one 
app over another.

03:38:29.571 --> 03:38:30.994
If you use one particular app in
exactly the same way that I use 

03:38:30.995 --> 03:38:34.444
a different app, then the model 
will output the same predictions

03:38:34.445 --> 03:38:36.445
and they 

03:38:39.144 --> 03:38:41.144
will be assigned to the same 
buckets.

03:38:44.436 --> 03:38:45.656
It's sensitive, there is no 
personal identifiable 

03:38:45.657 --> 03:38:47.884
information used by the model, 
both when it's trained and when 

03:38:48.751 --> 03:38:50.751
it runs on the device.

03:38:51.800 --> 03:38:53.641
The model only compares the app 
usage on-device to our known 

03:38:53.642 --> 03:38:56.894
sequences of millions of app 
transitions and it predicts when

03:38:56.895 --> 03:39:00.368
you're next going to use that 
app.

03:39:04.921 --> 03:39:06.555
So if you combine the App 
Standby Buckets and their 

03:39:06.556 --> 03:39:08.556
restrictions with this model 
that predicts when you're next 

03:39:10.498 --> 03:39:14.352
going to open the app, that's 
the Adaptive Battery feature.  

03:39:14.353 --> 03:39:16.399
It's a system that adapts the 
phone to you.  The output of the

03:39:16.400 --> 03:39:21.931
model is used to personalize the
Os' behavior to adapt it to how 

03:39:21.932 --> 03:39:23.932
you use the phone, so the apps 
that you use get to run in the 

03:39:26.638 --> 03:39:28.638
background and the apps that you
don't,  don't.

03:39:29.905 --> 03:39:31.751
We hope this will create a more 
consistent battery experience 

03:39:31.752 --> 03:39:33.752
for users.

03:39:35.064 --> 03:39:36.892
I'm now going to pass it over to
Madan a product manager on 

03:39:36.893 --> 03:39:38.893
Android to talk 

03:39:40.795 --> 03:39:43.479
about the  Battery Saver future.
Thanks very much.

03:39:45.518 --> 03:39:47.510
(Applause).
     &gt;&gt; MADAN ANKAPURA:  Thank 

03:39:47.511 --> 03:39:49.511
you, James.

03:39:54.492 --> 03:39:56.492
So let's talk about Battery 
Saver.

03:39:58.993 --> 03:40:00.993
Many of us run through the day 
and as 

03:40:02.064 --> 03:40:04.064
we end the day we realize oh, I 
won't be 

03:40:05.149 --> 03:40:07.594
actually able to make it.  
Battery Saver is there to save 

03:40:07.595 --> 03:40:11.320
you.  Remember the red bars with
our animations that were janky 

03:40:11.321 --> 03:40:13.321
in Oreo?  Who liked those?

03:40:15.793 --> 03:40:17.793
So we got rid of them.

03:40:19.515 --> 03:40:21.515
We also actually turn off 
Location and that helps battery.

03:40:23.453 --> 03:40:25.453
In addition to that, many 

03:40:28.009 --> 03:40:29.021
device-specific features like 
display for example are also 

03:40:29.022 --> 03:40:31.022
turned off.  All of these add up
in order to save 

03:40:33.890 --> 03:40:35.522
battery and prolong the device 
battery so that you can go back 

03:40:35.523 --> 03:40:39.795
and charge the device.
     We have also made some 

03:40:39.796 --> 03:40:41.796
changes in 

03:40:44.889 --> 03:40:46.924
the UI so that you can live on 
this Moar for longer.  Now you 

03:40:46.925 --> 03:40:48.796
have a slider that you can 
actually increase all the way to

03:40:48.797 --> 03:40:51.231
the right, which probably allows
you to stay 

03:40:55.186 --> 03:40:57.231
on this mode if you choose to.  
But what's also interesting is 

03:40:57.232 --> 03:40:59.232
what we 

03:41:00.482 --> 03:41:02.482
found is that if an app is being
used by 

03:41:06.889 --> 03:41:10.899
the user in this mode, having a 
dark theme really helps.  Our 

03:41:10.900 --> 03:41:12.900
Internet tests show that apps 

03:41:13.937 --> 03:41:14.146
using dark theme save power 
compared with the  ones that 

03:41:14.147 --> 03:41:19.438
don't.  So if you are a 
developer that already supports 

03:41:19.439 --> 03:41:22.100
Dark Theme, please do think 
about switching to  Dark Theme 

03:41:22.101 --> 03:41:24.101
when you 

03:41:25.786 --> 03:41:27.786
detect that the device is in 
Battery  Saver.

03:41:27.826 --> 03:41:29.826
There are a few commands that 
you 

03:41:32.473 --> 03:41:34.473
can use

03:41:35.827 --> 03:41:37.661
through ADB for force the device
into Battery Saver and test 

03:41:37.662 --> 03:41:39.662
them, and for 

03:41:40.732 --> 03:41:42.732
those of you that already 
support that 

03:41:46.658 --> 03:41:48.483
mode, there are APIs like Power 
Save Mode and broadcast that you

03:41:48.484 --> 03:41:50.484
can use to 

03:41:51.567 --> 03:41:54.258
switch your device, or your app 
theme to dark.

03:41:57.117 --> 03:41:59.117
So let's jump into the battery 
settings itself.

03:42:01.633 --> 03:42:03.461
We think when the user actually 
goes into a battery setting, 

03:42:03.462 --> 03:42:05.729
probably they are having a bad 
battery day.

03:42:09.398 --> 03:42:11.671
You want to make the user very 
easy to take actions.

03:42:16.011 --> 03:42:18.011
You want to keep the UI 
simplified, 

03:42:20.145 --> 03:42:22.204
and we will tell you exactly 
what applications might be 

03:42:22.205 --> 03:42:26.522
causing the battery drain, so it
will be much more opinionated 

03:42:26.523 --> 03:42:29.170
about these things.
     In addition to that as you 

03:42:29.171 --> 03:42:32.090
scroll down, we'll also be able 
to see how long 

03:42:35.600 --> 03:42:37.600
your battery will last, and that
number 

03:42:39.085 --> 03:42:41.085
is also powered by an ML 
monitor.

03:42:42.375 --> 03:42:44.405
So let's jump into that 
opinionated background 

03:42:44.406 --> 03:42:49.601
restrictions.  So we have some 
principles upon which we have 

03:42:49.602 --> 03:42:51.865
built this particular feature.  
User asks for  controls, they 

03:42:51.866 --> 03:42:56.764
want to know like which app is 
doing what so that this he can 

03:42:56.765 --> 03:42:58.765
take a quick action on them.

03:43:00.703 --> 03:43:02.529
We also want to make sure that 
apps that don't necessarily 

03:43:02.530 --> 03:43:04.530
target Oreo are 

03:43:06.663 --> 03:43:08.663
also being well managed by the 
user, and 

03:43:11.384 --> 03:43:13.594
lastly our goal is to make sure 
that apps don't get restricted 

03:43:13.595 --> 03:43:17.033
in the first place, so that way 
apps fundamentally are a better 

03:43:17.034 --> 03:43:19.092
battery citizen.

03:43:22.244 --> 03:43:24.244
So with P, we will actually 
launch 

03:43:25.508 --> 03:43:27.508
this future with two specific 
criteria.

03:43:29.378 --> 03:43:32.023
The first one is if your app is 
still not targeting Oreo and has

03:43:32.024 --> 03:43:34.024
a background 

03:43:35.944 --> 03:43:37.944
service, the second one is if an
app 

03:43:40.292 --> 03:43:41.744
holds wakelocks or what call 
stuck wakelocks for more than an

03:43:41.745 --> 03:43:43.745
hour in the background, both of 
these are very well 

03:43:46.663 --> 03:43:48.663
understood causes for battery 
drain.

03:43:50.370 --> 03:43:52.370
There are more such reasons.

03:43:54.880 --> 03:43:56.880
We today present those reasons 
within 

03:43:57.971 --> 03:44:00.002
Play Developer Console and we 
call them Android Vitals so we 

03:44:00.003 --> 03:44:02.003
are in the future 

03:44:03.685 --> 03:44:05.927
going to be looking at many such
signals in order to incorporate 

03:44:05.928 --> 03:44:08.989
coming up with new rules for 
background restrictions.

03:44:13.498 --> 03:44:15.951
So what does it mean for your 
app in case if the user decides 

03:44:15.952 --> 03:44:17.952
to restrict them?

03:44:23.735 --> 03:44:25.765
As James talked about jobs, 
alarms, services, and network, 

03:44:25.766 --> 03:44:27.766
the apps will not 

03:44:28.903 --> 03:44:31.880
be able to do any of those in 
the background.  There are some 

03:44:31.881 --> 03:44:34.441
explicit intents, for example, 
location will not be delivered 

03:44:36.719 --> 03:44:38.719
to those applications, so we're 
trying 

03:44:39.987 --> 03:44:41.987
to minimize the reasons why an 
app 

03:44:43.023 --> 03:44:45.283
should be able to use device 
resources when it is in the 

03:44:45.284 --> 03:44:47.937
background.
     And lastly, if an app is in

03:44:47.938 --> 03:44:49.961
the background and restricted, 
it won't be 

03:44:54.063 --> 03:44:59.418
able to use foregone services.
     So as I spoke earlier, I 

03:44:59.419 --> 03:45:01.419
hope many 

03:45:02.676 --> 03:45:05.149
of you are already familiar with
Android Vitals, and if not I 

03:45:05.150 --> 03:45:07.150
would strongly recommend to you 
go take a look into 

03:45:09.711 --> 03:45:12.160
what signals do we actually show
in their there and how your app 

03:45:12.161 --> 03:45:15.030
is behaving.
     We have heard many success 

03:45:15.031 --> 03:45:17.031
stories of application 
developers using this 

03:45:20.378 --> 03:45:23.033
data in order to improve their 
applications, and we in fact use

03:45:23.034 --> 03:45:25.034
some of 

03:45:28.836 --> 03:45:30.676
these signals internally also to
make Os better.  Trying to 

03:45:30.677 --> 03:45:32.095
figure out like what kind of 
changes we could be making in 

03:45:32.096 --> 03:45:34.096
order to 

03:45:35.592 --> 03:45:37.592
have the right guardrails within
Os to 

03:45:39.045 --> 03:45:41.045
make sure that apps don't 
accidently drain battery.

03:45:42.992 --> 03:45:45.037
There was a session about it, if
you haven't had a chance to 

03:45:45.038 --> 03:45:48.340
listen to those, please take a 
look at the  recording.

03:45:56.942 --> 03:45:58.942
So we made the UI simplified, 
but 

03:46:00.600 --> 03:46:03.098
there might be several of you 
who would fall into as power 

03:46:03.099 --> 03:46:05.099
users, so you do like graphs and
you do like number of 

03:46:07.395 --> 03:46:09.395
percentages, so we do have it in
the overflow menu.

03:46:12.853 --> 03:46:14.853
Now the graph is improved with 
better predictions and the user 

03:46:17.089 --> 03:46:19.089
can take action from this screen
if you 

03:46:20.758 --> 03:46:22.379
see any app that is not 
necessarily surprising you as 

03:46:22.380 --> 03:46:24.380
being consuming more battery.

03:46:29.665 --> 03:46:31.665
So from here you can directly

03:46:32.869 --> 03:46:34.869
preemptively restrict 
applications or unrestrict them.

03:46:35.100 --> 03:46:37.100
This is a good user control for 
users to have.

03:46:40.449 --> 03:46:42.449
     So how do you test for 
this?

03:46:45.497 --> 03:46:47.497
Again,

03:46:52.565 --> 03:46:54.017
there is ADB to put them in the 
state and test them while 

03:46:54.018 --> 03:46:56.287
they're in the state.  Note that
even an app who has been 

03:46:57.723 --> 03:46:59.723
restricted could still be used 
by the 

03:47:01.411 --> 03:47:04.300
user by launching them 
explicitly so you want to make 

03:47:04.301 --> 03:47:07.378
sure that they still work.
     So we have gone through 

03:47:07.379 --> 03:47:09.379
some of the 

03:47:10.891 --> 03:47:12.344
key features in  P, and what 
does it mean for you as an app 

03:47:12.345 --> 03:47:15.220
developer to continue to 
actually deliver your 

03:47:19.038 --> 03:47:21.038
features using various

03:47:23.267 --> 03:47:25.097
things that are offered like 
Jobs, and so on and so forth, 

03:47:25.098 --> 03:47:27.098
what does it mean for you to do 
a background work?

03:47:28.408 --> 03:47:30.873
You might have actually seen 
this flowchart before.  Ben 

03:47:30.874 --> 03:47:32.518
presented it last year.  It 
literally went through what is 

03:47:32.519 --> 03:47:37.811
it that you want to do and 
offered some choices.  We want 

03:47:37.812 --> 03:47:39.812
to simplify our recommendation, 
and thanks to Jetpack, 

03:47:45.805 --> 03:47:48.970
in it we have a great tool.
     So it will simplify it now 

03:47:49.176 --> 03:47:53.050
hopefully.  If you want to think
about doing anything in the 

03:47:53.051 --> 03:47:55.051
background, we highly recommend 
you to actually evaluate Work 

03:47:56.365 --> 03:47:58.365
Manager as a go-to thing.  And 
if you think it's something that

03:48:02.484 --> 03:48:04.335
is really important and must 
happen now, of course you have 

03:48:04.336 --> 03:48:07.902
the ability to use foreground 
services.  But note that if you 

03:48:07.903 --> 03:48:12.207
use foreground services, you 
have to tell the user why you 

03:48:12.208 --> 03:48:14.666
are using it for because there 
will be a notification and 

03:48:14.667 --> 03:48:17.770
you'll have to justify yourself 
as to why you think it is 

03:48:17.771 --> 03:48:19.614
important.
     And now because of that, if

03:48:19.615 --> 03:48:21.615
you 

03:48:26.368 --> 03:48:28.368
change your mind, go back to 
WorkManager.

03:48:29.224 --> 03:48:32.107
So if none of this worked, think
about actually doing the work 

03:48:32.108 --> 03:48:36.032
when the app actually is 
launched and being  actively 

03:48:36.033 --> 03:48:38.033
used by the user, so the 

03:48:39.928 --> 03:48:42.417
choices are hopefully very 
simplified.  All of this really 

03:48:42.418 --> 03:48:44.918
helps us manage battery better 
and that will lead to 

03:48:49.212 --> 03:48:52.291
having a better battery life.
     There have been some 

03:48:52.292 --> 03:48:54.719
sessions that have already 
happened so I want to call 

03:48:57.827 --> 03:48:59.827
and shout out to Jetpack which 
talks 

03:49:01.477 --> 03:49:03.477
about WorkManager and there was 
also a 

03:49:05.365 --> 03:49:07.405
session on Android Vitals and 
there is a video.  There is a 

03:49:07.406 --> 03:49:09.647
detailed documentation of the 
features that we talked about.

03:49:13.877 --> 03:49:15.877
Please head over to d.android.

03:49:19.133 --> 03:49:21.133
com/power, and finally since we 
won't be 

03:49:22.614 --> 03:49:24.614
taking any questions, please do 
find us.

03:49:25.705 --> 03:49:28.185
All three of us will be in the 
office hours from 2:30 to 3:30 

03:49:28.186 --> 03:49:30.186
in the office 

03:49:31.513 --> 03:49:33.513
hour space, so look for Android 
Framework office hours.

03:49:36.052 --> 03:49:38.052
And finally, if you have any 

03:49:41.377 --> 03:49:43.005
feedback here is our URL and 
thank you.

03:49:43.006 --> 03:49:45.006
(Applause).

03:49:51.430 --> 03:49:55.364
     &gt;&gt; Thank you for joining 
this session.  Grand ambassadors

03:49:55.365 --> 03:49:58.891
will assist with directing you 
through the designated exits.  

03:49:58.892 --> 03:50:00.118
We'll be making room for those 
who have registered for the next

03:50:00.119 --> 03:50:03.601
session.  If you've registered 
for the next session in this 

03:50:03.602 --> 03:50:05.654
room, we ask that you please 
clear the room and return via 

03:50:05.655 --> 03:50:13.586
the registration line outside.  
Thank you.

06:14:42.953 --> 06:14:44.953
Advances in Machine Learning and

06:14:45.960 --> 06:14:50.911
TensorFlow

06:15:34.620 --> 06:15:36.820
Breakthroughs in Machine 
Learning

06:21:11.179 --> 06:21:15.101
   &gt;&gt; At this time, please find 
your seat. Our session will 

06:21:15.102 --> 06:21:17.102
begin soon.

06:24:01.029 --> 06:24:05.539
   &gt;&gt; Hi, everybody, and welcome
to this session, where we're 

06:24:05.540 --> 06:24:06.961
going to talk about 
breakthroughs in machine 

06:24:06.962 --> 06:24:10.840
learning. I'm Laurence Moroney, 
a developer advocate working on 

06:24:10.841 --> 06:24:15.316
TensorFlow. We're here today to 
talk about the revolution that's

06:24:15.317 --> 06:24:17.317
going on in machine learning and
how that revolution is 

06:24:17.401 --> 06:24:19.866
transformative. Now, I come from
a software development 

06:24:20.285 --> 06:24:22.285
background. Any software 
developers here?

06:24:25.166 --> 06:24:27.166
Given that it's I/O, sure. This 
transformation is particularly 

06:24:29.086 --> 06:24:31.122
from a developer's perspective, 
is really, really cool, because 

06:24:31.123 --> 06:24:35.810
it's giving us a whole new set 
of tools that we can use to 

06:24:35.811 --> 06:24:37.441
build scenarios and to build 
solutions for problems that may 

06:24:37.442 --> 06:24:41.537
have been too complex to even 
consider prior to this.

06:24:42.950 --> 06:24:45.020
It's leading to massive advances
in our understanding of things 

06:24:45.021 --> 06:24:48.088
like the universe around us. 
It's opening up new fields in 

06:24:48.089 --> 06:24:52.549
art. And it's impacting and 
revolutionizing things like 

06:24:52.550 --> 06:24:55.197
healthcare and so many more 
things. Should we take a look at

06:24:55.198 --> 06:24:57.198
some of these? First of all, 
astronomy.

06:25:00.086 --> 06:25:02.762
At school I studied physics. I'm
a physics and astronomy geek.

06:25:08.504 --> 06:25:10.504
It wasn't that long ago when we 
learned about new planets.

06:25:12.177 --> 06:25:13.802
The way we discovered it is, 
sometimes we would observe a 

06:25:13.803 --> 06:25:15.803
wobble in the star.

06:25:17.726 --> 06:25:19.564
That meant there was a large 
planet orbiting the star closely

06:25:19.565 --> 06:25:22.020
and causing a wobble because of 
the gravitational attraction.

06:25:22.625 --> 06:25:27.801
   The kinds of kind of planets 
we want to find are like Earth, 

06:25:27.802 --> 06:25:32.281
where there's a chance of 
finding life. And finding those 

06:25:32.282 --> 06:25:34.282
and discovering those was very, 
very difficult to do because 

06:25:37.604 --> 06:25:39.604
small ones, close to a star, you
wouldn't see.

06:25:41.698 --> 06:25:43.698
But, with research that's been 
going 

06:25:45.989 --> 06:25:47.989
on, they've recently discovered 
Kepler 

06:25:49.069 --> 06:25:50.940
90I by sifting through data and 
building models for using 

06:25:50.941 --> 06:25:52.941
machine learning and TensorFlow.

06:25:55.265 --> 06:25:57.265
And Kepler 90I is much closer to
its home star than Earth.

06:25:59.783 --> 06:26:01.783
The orbit is 14 days instead of 
365.

06:26:02.017 --> 06:26:03.864
And not only that, which I find 
really cool, they didn't just 

06:26:03.865 --> 06:26:07.971
find this as a single planet 
around that star. They've mapped

06:26:07.972 --> 06:26:09.972
and modeled the entire solar 
system of eight planets that are

06:26:10.314 --> 06:26:14.192
there. So these are some of the 
advances. To me, I find this a 

06:26:14.193 --> 06:26:16.193
wonderful time to 

06:26:17.657 --> 06:26:19.664
be alive, because technology's 
enabling us to discover these 

06:26:19.665 --> 06:26:23.475
great new things. Even closer to
home, we've also  discovered 

06:26:23.476 --> 06:26:25.950
that looking at scans of the 
human eye, as you would have 

06:26:25.951 --> 06:26:31.093
seen in the keynote, with 
machine learning trained models 

06:26:31.094 --> 06:26:32.123
on this, we've been able to 
discover things such as blood 

06:26:32.124 --> 06:26:36.657
pressure predictions, or being 
able to assess a person's risk 

06:26:36.658 --> 06:26:39.496
of a heart attack or a stroke.
   Now, just imagine if this 

06:26:39.497 --> 06:26:41.950
screening can be done on a small
mobile phone.

06:26:46.053 --> 06:26:48.520
How profound is the effect going
to be? The world will be able to

06:26:48.521 --> 06:26:50.521
access easy, 

06:26:51.925 --> 06:26:52.935
rapid, affordable, and 
noninvasive screening for things

06:26:52.936 --> 06:26:55.943
such as heart diseases. It will 
be saving many lives and 

06:26:58.591 --> 06:27:01.263
improving the quality of many, 
many more lives. Now, these are 

06:27:01.264 --> 06:27:04.525
just a few of the breakthroughs 
and advances that have been made

06:27:04.526 --> 06:27:06.386
because of TensorFlow. And 
TensorFlow, we've been working 

06:27:06.387 --> 06:27:10.935
hard with the community with all
of you to make this a machine 

06:27:10.936 --> 06:27:12.936
learning platform for everybody.

06:27:15.123 --> 06:27:16.167
So today we want to share a few 
of the new advances that have 

06:27:16.168 --> 06:27:18.168
been working 

06:27:23.414 --> 06:27:23.428
on this, including we'll be 
looking at robots. And Vincent 

06:27:23.429 --> 06:27:25.418
will come out to show us robots 
that learn and some of the work 

06:27:25.419 --> 06:27:27.419
that they've been doing to 
improve how robots learn.

06:27:32.182 --> 06:27:34.646
And then Debbie is going to be, 
from NERSC, showing us cosmology

06:27:36.102 --> 06:27:38.342
advancements, including how 
building a simulation of the 

06:27:38.343 --> 06:27:40.806
universe will help us understand
the nature of the unknowns in 

06:27:42.891 --> 06:27:45.124
our universe like dark matter.
   First of all, I would love to

06:27:45.125 --> 06:27:47.125
welcome 

06:27:49.021 --> 06:27:52.259
from the Magenta team, Doug, a 
principal scientist. Doug.

06:27:54.098 --> 06:27:54.719
&gt;&gt; DOUGLAS ECK: Thanks, 
Lawrence.

06:27:54.720 --> 06:27:55.519
(Applause)
   &gt;&gt; DOUGLAS ECK: Thank you 

06:27:55.520 --> 06:27:59.408
very much. All right. Day three.
We're getting there.

06:28:02.485 --> 06:28:04.509
Hi, everybody, I'm Doug, a 
scientist at Google working on a

06:28:04.510 --> 06:28:07.187
project called Magenta. Before 
we talk about modeling the 

06:28:11.181 --> 06:28:12.001
entire known universe, before we
talk about robots, I want to 

06:28:12.002 --> 06:28:15.733
talk about music and art, and 
how to use machine learning for 

06:28:15.734 --> 06:28:18.193
expressive purposes. So, I want 
to talk first about a 

06:28:21.250 --> 06:28:22.883
drawing project called sketch 
RNN, where we trained a neural 

06:28:22.884 --> 06:28:25.936
network to do something as 
important as draw the pig that 

06:28:25.937 --> 06:28:28.576
you see on the right there.
   And I want to use this as an 

06:28:28.577 --> 06:28:30.577
example 

06:28:32.496 --> 06:28:33.534
to highlight a few important 
machine learning concepts that 

06:28:33.535 --> 06:28:37.205
we're finding to be crucial for 
using machine learning in the 

06:28:37.206 --> 06:28:39.206
context of art and music.

06:28:42.526 --> 06:28:44.753
So, let's dive in. It's going to
get a little technical, 

06:28:45.776 --> 06:28:48.008
hopefully it will be fun. We're 
going to try to learn to draw 

06:28:48.009 --> 06:28:51.918
not by generating pixels, but 
pen strokes. This is a very 

06:28:51.919 --> 06:28:55.819
interesting representation to 
use because it's close do what 

06:28:55.820 --> 06:28:57.820
we do when we draw. Specifically
we're going to take the 

06:29:02.293 --> 06:29:04.293
data from the popular quick draw
game,.

06:29:05.630 --> 06:29:07.630
That was captured as delta X 
movements of the pen.

06:29:10.554 --> 06:29:13.816
We know when the pen is put down
and lifted up. That's our 

06:29:13.817 --> 06:29:15.817
training domain.

06:29:18.126 --> 06:29:21.389
One thing I would observe is we 
didn't need a lot of this data. 

06:29:21.390 --> 06:29:23.390
It fits the creative process.

06:29:24.473 --> 06:29:26.473
It's closer to drawing, I argue,
than pixels are.

06:29:29.006 --> 06:29:31.006
It's modeling the movement of 
the pen.

06:29:36.432 --> 06:29:38.432
Now, what we're going to do with
these 

06:29:42.415 --> 06:29:43.410
drawings is push them through an
auto-encoder. On the left, the 

06:29:43.411 --> 06:29:44.425
encoder network's job is to take
the strokes and encode them in 

06:29:44.426 --> 06:29:46.426
some way so that they can be 
stored 

06:29:47.429 --> 06:29:49.629
as a latent vector. The job of 
the decoder is to decode that 

06:29:49.630 --> 06:29:51.461
into a generated sketch.
   And the only point that you 

06:29:51.462 --> 06:29:55.326
really need to take away from 
this talk is that that latent 

06:29:55.327 --> 06:29:57.327
factor is worth everything to 
us.

06:29:59.411 --> 06:30:03.308
First, it's smaller in size than
the encoded or decoded drawing. 

06:30:03.309 --> 06:30:05.534
It can't memorize everything. 
Because of that, we get nice 

06:30:05.535 --> 06:30:10.212
effects. For example, you might 
notice if you look carefully 

06:30:10.213 --> 06:30:12.079
that the cat on the left, and 
which is actual data and has 

06:30:12.080 --> 06:30:16.356
been pushed through the trained 
model and decoded, is not the 

06:30:16.357 --> 06:30:18.357
same as the cat on the right, 
right?

06:30:21.294 --> 06:30:23.516
The cat on the left has five 
whiskers. The model regenerated 

06:30:23.517 --> 06:30:26.388
the sketch with six because 
that's what it usually sees. Six

06:30:26.389 --> 06:30:30.046
is general. It's normal to the 
model, whereas five is hard for 

06:30:30.047 --> 06:30:32.047
the model to make sense of.

06:30:33.498 --> 06:30:35.157
So this idea of having a tight, 
low-dimensional representation, 

06:30:35.158 --> 06:30:39.828
the latent vector that's been 
trained on lots of data, the 

06:30:39.829 --> 06:30:43.066
goal is the model might learn to
find generalities in a drawing, 

06:30:43.067 --> 06:30:45.934
learn general strategies.
   So here's an example of 

06:30:45.935 --> 06:30:51.282
starting each of the four 
corners with a drawing done by a

06:30:51.283 --> 06:30:53.317
human, David, the first author. 
And those are encoded in the 

06:30:53.318 --> 06:30:57.397
corners. Now we move linearly 
around the space -- not the 

06:30:57.398 --> 06:31:01.074
space of the strokes, but the 
space of the latent vector. If 

06:31:01.075 --> 06:31:03.114
you look closely, I think you'll
see that the movements and the 

06:31:03.115 --> 06:31:06.364
changes from these faces say 
from left to right, are actually

06:31:06.365 --> 06:31:09.434
quite smooth. The model has 
dreamt up all of those faces in 

06:31:09.435 --> 06:31:15.137
the middle. To my eye they 
really do fill the space of 

06:31:15.138 --> 06:31:18.798
possible drawings.
   Finally, as I pointed out 

06:31:18.799 --> 06:31:21.468
with the cat whiskers, the 
models generalize, not memorize.

06:31:22.934 --> 06:31:24.994
It's not interesting to memorize
a drawing. It's more interesting

06:31:24.995 --> 06:31:28.486
to learn general strategies for 
drawing. We see with the cat.

06:31:32.381 --> 06:31:34.225
I think more interestingly -- 
it's also suggestive -- we see 

06:31:34.226 --> 06:31:37.682
this with doing something like 
taking a model that's only seen 

06:31:37.683 --> 06:31:40.146
pigs and giving it a picture of 
a truck. What's that model going

06:31:40.147 --> 06:31:43.616
to do? It's going to find a pig 
truck, because that's all it 

06:31:43.617 --> 06:31:48.911
knows about. And if that seems 
silly, which I grant it is, in 

06:31:48.912 --> 06:31:50.912
your own mind think about how 

06:31:51.961 --> 06:31:53.179
hard it would be, for me, if 
someone says draw a truck that 

06:31:53.180 --> 06:31:55.009
looks like a pig.
   It's hard to make that 

06:31:55.010 --> 06:31:57.010
transformation.

06:31:58.066 --> 06:32:00.066
And these models do it.

06:32:03.172 --> 06:32:06.246
Finally, by the way, I get paid 
to do this. I just want to point

06:32:06.247 --> 06:32:10.541
that out as an aside. It's kind 
of nice. I said that last year. 

06:32:10.542 --> 06:32:13.385
It's still true. These latent 
space analogies, another 

06:32:13.789 --> 06:32:18.885
example. If you add and subtract
pen strokes, you're not going to

06:32:18.886 --> 06:32:22.566
get far with making something 
that's recognizable. But if you 

06:32:22.567 --> 06:32:27.462
have a look at these analogies, 
we take the latent vector for a 

06:32:27.463 --> 06:32:29.689
cat head and we add a pig body.
   And we subtract the pig head.

06:32:31.336 --> 06:32:32.771
And of course it stands to 
reason that you should get a cat

06:32:32.772 --> 06:32:37.653
body. We can do the same thing 
in reverse. This is real data. 

06:32:37.654 --> 06:32:42.348
This works. I mention it because
it shows that these models are 

06:32:42.349 --> 06:32:46.647
learning some of the geometric 
relations between the forms that

06:32:46.648 --> 06:32:48.648
people draw.

06:32:50.538 --> 06:32:52.538
I'm going to switch gears and 
move from 

06:32:54.207 --> 06:32:56.207
drawing to music, talk about a 
model 

06:33:01.532 --> 06:33:03.575
called  Nsynth, that generalizes
audio. You may have seen from 

06:33:03.576 --> 06:33:05.576
the beginning of I/O with 
bathing that this has been put 

06:33:07.080 --> 06:33:09.113
into a  hardware unit. How many 
people have heard of super?

06:33:12.385 --> 06:33:14.867
How many people want an ensign 
super? Good. That's possible, as

06:33:14.868 --> 06:33:16.691
you know.
   For those of you that didn't 

06:33:16.692 --> 06:33:18.692
see the opening, I have a short 
version of the 

06:33:21.777 --> 06:33:23.197
making of the NSynth to give you
an idea of what the model is up 

06:33:23.198 --> 06:33:25.198
to.

06:33:26.246 --> 06:33:28.503
Let's roll it.
   &gt;&gt; Here's a flute.

06:33:31.972 --> 06:33:36.266
Here's a snare. I guess in the 
middle, this is what it sounds 

06:33:36.267 --> 06:33:38.267
like.

06:33:39.678 --> 06:33:41.678
 

06:33:45.677 --> 06:33:49.550
   &gt;&gt; It does feel like what 
could be a new possibility.

06:33:53.061 --> 06:33:55.061
It could generate a sound that 
might inspire.

06:33:56.772 --> 06:33:58.599
&gt;&gt; The fun part is even though 
you think you know what you're 

06:33:58.600 --> 06:34:01.684
doing, there's some weird 
interaction happening that can 

06:34:01.685 --> 06:34:03.926
give you something totally 
unexpected.

06:34:06.565 --> 06:34:08.565
&gt;&gt; Wait, why did that happen 
that way?

06:34:09.830 --> 06:34:11.830
 
   &gt;&gt; DOUGLAS ECK: Okay.

06:34:13.743 --> 06:34:16.185
So, what you see here -- by the 
way, the last person was Jesse, 

06:34:16.186 --> 06:34:18.186
the main 

06:34:19.453 --> 06:34:21.730
scientist on the NSynth project.
This grid that you're seeing, 

06:34:21.731 --> 06:34:25.800
this square where you can move 
around the space, is exactly the

06:34:25.801 --> 06:34:29.096
same idea as we saw with those 
faces. You're moving around the 

06:34:29.097 --> 06:34:31.097
latent space, 

06:34:32.212 --> 06:34:34.464
able to discover sounds that 
have some similarity. Because 

06:34:34.465 --> 06:34:36.697
they're made up of learning what
makes humans -- how sound works 

06:34:36.698 --> 06:34:39.337
for us in the same way as a pig 
truck gives 

06:34:43.229 --> 06:34:45.229
us new ideas about how sound 
works.

06:34:46.278 --> 06:34:48.278
As you probably know, you can 
make 

06:34:50.622 --> 06:34:52.622
these yourself, which my 
favorite part about it.

06:34:54.359 --> 06:34:56.359
This is open source, GitHub.

06:34:57.626 --> 06:35:00.539
For those of you who like to 
tinker, give it a shot. If not 

06:35:00.540 --> 06:35:02.573
we'll see some coming from tons 
of people who are building them 

06:35:02.574 --> 06:35:04.574
on their own.

06:35:06.044 --> 06:35:08.971
So, I want to keep going with 
music. I want to move away from 

06:35:08.972 --> 06:35:11.866
audio to musical scores, musical
notes, something 

06:35:14.948 --> 06:35:16.948
that think of last night driving
the sequencer.

06:35:18.199 --> 06:35:20.864
And talk about basically the 
same idea. Can we learn a latent

06:35:20.865 --> 06:35:22.865
space where we 

06:35:23.917 --> 06:35:25.917
can move around what's possible 
in a musical score?

06:35:30.256 --> 06:35:32.281
So what you see here is some 
three-part musical thing on the 

06:35:32.282 --> 06:35:36.633
top, and some one-part musical 
thing on the bottom, and then 

06:35:36.634 --> 06:35:38.634
finding in a latent space 

06:35:39.887 --> 06:35:41.887
something that's in between, 
okay?

06:35:42.531 --> 06:35:44.969
And now, I put the faces 
underneath this. What you're 

06:35:44.970 --> 06:35:47.617
looking at now is a 
representation of a musical drum

06:35:47.618 --> 06:35:49.618
score 

06:35:51.472 --> 06:35:53.562
where time is passing left to 
right. I'm going to play this 

06:35:53.563 --> 06:35:55.563
for you. It's a little bit long.

06:35:59.088 --> 06:36:00.941
We're going to start with a drum
beat, one measure of drums, and 

06:36:00.942 --> 06:36:04.623
we're going to end with one 
measure of drums. You're going 

06:36:04.624 --> 06:36:06.453
to hear those first, A and B. 
Then you're going to hear this 

06:36:06.454 --> 06:36:12.188
latent space model try to figure
out how to get from A to B. And 

06:36:12.189 --> 06:36:14.628
everything in between is made up
by the model in exactly the same

06:36:14.629 --> 06:36:17.322
way that the faces in the middle
are made up by the model.

06:36:19.765 --> 06:36:21.211
So as you're listening, 
basically listen for whether it 

06:36:21.212 --> 06:36:24.676
makes musical sense or not, the 
intermediate drums. Let's give 

06:36:24.677 --> 06:36:28.078
it a roll.
     

06:37:30.714 --> 06:37:33.848
   &gt;&gt; DOUGLAS ECK: There you 
have it.

06:37:36.852 --> 06:37:40.383
(Applause)
   &gt;&gt; DOUGLAS ECK: Moving right 

06:37:40.384 --> 06:37:42.384
along, take a look at this 
command.

06:37:46.663 --> 06:37:47.963
This may make sense to some of 
you. We were surprised to learn 

06:37:47.964 --> 06:37:49.975
that this is not the right way 
to work with musicians and 

06:37:49.976 --> 06:37:53.945
artists. I laughed, too. We 
thought this is a great idea, 

06:37:53.946 --> 06:38:00.331
guys. Paste this into Terminal. 
They're like what's terminal? 

06:38:00.332 --> 06:38:03.185
You know you're in trouble. 
We've moved quite a bit towards 

06:38:03.186 --> 06:38:06.678
trying to build tools that 
musicians can use. This is a 

06:38:06.679 --> 06:38:08.679
drum machine that you can 

06:38:10.127 --> 06:38:12.775
play with online built around 
TensorFlow.JS. I have a short 

06:38:12.776 --> 06:38:15.025
clip of this being used.
   You're going to see all the 

06:38:15.026 --> 06:38:19.313
red is from you. You can play 
around with it. The blue is 

06:38:19.314 --> 06:38:21.314
generated by the model. Let's 
give this a roll.

06:38:23.820 --> 06:38:25.859
This one is quite a bit shorter.

06:38:28.868 --> 06:38:30.868
 

06:38:47.461 --> 06:38:49.461
   &gt;&gt; DOUGLAS ECK: So this is 
available 

06:38:52.390 --> 06:38:54.426
for you as a code pen which 
allows you to play around with 

06:38:54.427 --> 06:38:56.427
it, and really 

06:38:58.919 --> 06:39:02.179
amazing, a huge shoutout to Tero
who did this. He grabbed one of 

06:39:02.180 --> 06:39:04.180
our  training models 

06:39:05.850 --> 06:39:08.317
and used TensorFlow and hacked 
code and put it on Twitter. We 

06:39:08.318 --> 06:39:10.552
had no idea this was happening. 
And then we reached out to him 

06:39:10.553 --> 06:39:15.019
on Twitter. I said you're my 
hero. He said you care about 

06:39:15.020 --> 06:39:19.336
this? I'm like, of course,, this
is our dream, to have people 

06:39:19.337 --> 06:39:21.792
playing with this technology. I 
love that we've gotten there.

06:39:21.996 --> 06:39:25.499
   Part of what I want to talk 
about today -- actually close  

06:39:25.500 --> 06:39:30.405
with, we've cleaned up a lot of 
the code. Tero helped. We're 

06:39:30.406 --> 06:39:32.926
able to introduce Magenta.JS, 
very tightly integrated with 

06:39:36.339 --> 06:39:38.339
TensorFlow, and it allows you to
grab a 

06:39:40.017 --> 06:39:43.713
checkpointed model and set up a 
player, and start sampling. In 

06:39:43.714 --> 06:39:45.714
three lines of code you can set 
up a drum machine.

06:39:47.178 --> 06:39:49.178
We have the art side as well.

06:39:50.429 --> 06:39:52.658
And we've seen a lot of demos 
driven by this, a lot of 

06:39:52.659 --> 06:39:55.127
interesting work by  Googlers 
and people from the outside.

06:39:59.878 --> 06:40:03.358
And I think it highly aligns 
with what we're doing. We're 

06:40:03.359 --> 06:40:05.600
working to engage with musicians
and artists, very happy to see 

06:40:05.601 --> 06:40:09.085
the JavaScript stuff come along,
which seems to be the language 

06:40:09.086 --> 06:40:13.175
for that. Hoping to see better 
tools come, and heavy engagement

06:40:13.176 --> 06:40:15.855
with the open source community. 
If you want to learn more, 

06:40:15.856 --> 06:40:18.539
please visit the website. Also, 
you can follow my Twitter 

06:40:18.540 --> 06:40:21.418
account. I post regular updates 
and try to be a connector for 

06:40:21.419 --> 06:40:25.734
that. So  that's what I have for
you. Now I'd like to switch 

06:40:25.735 --> 06:40:27.735
gears and go to 

06:40:29.640 --> 06:40:29.835
robots, with my colleague, 
Vincent Vanhoucke. Thank you 

06:40:29.836 --> 06:40:31.879
very much.

06:40:34.902 --> 06:40:37.445
(Applause)
   &gt;&gt; VINCENT VANHOUCKE: Thanks,

06:40:37.446 --> 06:40:39.446
Doug.

06:40:42.585 --> 06:40:44.622
So, my name is Vincent and I 
lead the Brain robotics research

06:40:44.623 --> 06:40:46.623
team, the 

06:40:47.681 --> 06:40:49.681
robotics research team at 
Google.

06:40:51.954 --> 06:40:53.409
When you think about robots, you
may think about precision and 

06:40:53.410 --> 06:40:55.410
control.

06:40:56.478 --> 06:40:59.997
You may think about robots, you 
know, that live in factories. 

06:40:59.998 --> 06:41:02.248
They've got one very specific 
job to do and they've got to do 

06:41:02.249 --> 06:41:04.249
it over and over again.

06:41:05.731 --> 06:41:07.731
But as you saw in the keynote 
earlier, 

06:41:09.226 --> 06:41:12.472
more and more robots are about 
people. Right? They're 

06:41:12.473 --> 06:41:14.530
self-driving cars that are 
driving in our streets, 

06:41:14.531 --> 06:41:16.985
interacting with people.
   They essentially now live in 

06:41:16.986 --> 06:41:19.013
our world, not their world.

06:41:22.499 --> 06:41:24.648
And so they really have to adapt
and perceive the world around 

06:41:24.649 --> 06:41:26.649
them, and 

06:41:30.147 --> 06:41:32.722
learn how to operate in this 
human-centric environment. How 

06:41:32.723 --> 06:41:35.366
do we get robots to learn 
instead of having to program 

06:41:35.367 --> 06:41:38.013
them? This is what we've been 
embarking on.

06:41:41.751 --> 06:41:44.182
And it turns out we can get 
robots to learn. It takes a lot 

06:41:44.183 --> 06:41:46.183
of robots.

06:41:47.495 --> 06:41:49.495
It takes a lot of time.

06:41:53.422 --> 06:41:55.893
But we can actually improve on 
this if we teach robots how to 

06:41:55.894 --> 06:41:59.820
behave collaboratively.
   So this is an example of a 

06:41:59.821 --> 06:42:02.567
team of robots that are learning
together how to 

06:42:06.025 --> 06:42:08.025
do a very simple task like 
grasping objects, right?

06:42:10.309 --> 06:42:13.012
At the beginning, they have no 
idea what they're doing. They 

06:42:13.013 --> 06:42:14.461
try, and try, and try. And 
sometimes they will grasp 

06:42:14.462 --> 06:42:18.394
something. Every time they grasp
something, we give them a  

06:42:18.395 --> 06:42:23.973
reward. And over time, they get 
better and better at it. Of 

06:42:23.974 --> 06:42:25.974
course, we use deep learning for
this.

06:42:27.461 --> 06:42:30.320
We basically have a network that
maps those images that the 

06:42:30.321 --> 06:42:32.988
robots see of the  workspace in 
front of them to actions and 

06:42:32.989 --> 06:42:35.468
possible actions.
   And this collective learning 

06:42:35.469 --> 06:42:37.469
of 

06:42:39.167 --> 06:42:40.817
robots enables us to get to 
levels of performance that we 

06:42:40.818 --> 06:42:45.690
haven't seen before. But it 
takes a lot of robots. And in 

06:42:45.691 --> 06:42:48.143
fact, you know, this is Google. 
We would much rather use lots of

06:42:50.456 --> 06:42:53.287
computers if we could instead of
lots of robots. And so the 

06:42:53.288 --> 06:42:55.288
question becomes, could we 

06:42:56.344 --> 06:42:58.344
actually use a lot of simulated 
robots, 

06:42:59.620 --> 06:43:02.489
virtual robots, to do this kind 
of task and teach those robots 

06:43:02.490 --> 06:43:04.919
to perform tasks?
   And would it actually matter 

06:43:04.920 --> 06:43:08.850
in the real world? Would what 
they learn in simulation 

06:43:12.531 --> 06:43:14.971
actually apply to real tasks? 
And it turns out the key to 

06:43:14.972 --> 06:43:16.972
making this 

06:43:18.047 --> 06:43:20.922
work is to learn simulations 
that are more and more faithful 

06:43:20.923 --> 06:43:22.923
to reality.

06:43:23.972 --> 06:43:26.297
So on the right here you see 
what a typical simulation of a 

06:43:26.298 --> 06:43:28.298
robot would look like.

06:43:29.553 --> 06:43:31.553
This is a virtual robot trying 
to grasp objects in simulation.

06:43:34.435 --> 06:43:36.674
What you see on the other side 
here may look like a real robot 

06:43:36.675 --> 06:43:39.935
doing the same task, but in 
fact, it is completely simulated

06:43:39.936 --> 06:43:41.936
as well.

06:43:44.070 --> 06:43:46.717
We've learned a machine learning
model that maps those simulated 

06:43:46.718 --> 06:43:48.718
images 

06:43:51.641 --> 06:43:54.298
to real images, to real-looking 
images that are essentially 

06:43:54.299 --> 06:43:56.758
indistinguishable from what a 
real robot would see in the real

06:43:56.759 --> 06:43:58.759
world. And using this kind of 
data and 

06:44:02.494 --> 06:44:04.119
training a simulated model to 
accomplish tasks using those 

06:44:04.120 --> 06:44:07.595
images, we can transfer that 
information and make it work in 

06:44:07.596 --> 06:44:09.596
the real world as well.

06:44:11.106 --> 06:44:13.174
So there's lots of things we can
do with these kinds of simulated

06:44:13.175 --> 06:44:17.472
robots.
   This is Rainbow Dash, our 

06:44:17.473 --> 06:44:21.349
favorite little pony. And what 
you see here is him taking his 

06:44:23.600 --> 06:44:25.600
very first steps.

06:44:28.488 --> 06:44:30.752
Or very first hops, I should 
say. He's really good for 

06:44:30.753 --> 06:44:33.797
somebody who's just starting to 
learn how to walk. And the way 

06:44:33.798 --> 06:44:35.798
we accomplished this is by 

06:44:37.464 --> 06:44:39.464
having a virtual Rainbow Dash 
running in simulation.

06:44:40.922 --> 06:44:43.376
We train it using deep 
reinforcement learning to run 

06:44:43.377 --> 06:44:46.224
around in the simulator. And 
then we can basically download 

06:44:46.225 --> 06:44:50.130
the model that we've run in the 
simulation onto the real robot 

06:44:50.131 --> 06:44:52.131
and actually make it 

06:44:53.381 --> 06:44:58.077
work in the real world as well.
   There are many ways we can 

06:44:58.078 --> 06:45:03.594
scale up robotics and robotic  
learning in this way. One of the

06:45:03.595 --> 06:45:05.595
key ingredients turns out to 

06:45:07.468 --> 06:45:09.468
be learning by itself, 
self-supervision, self-learning.

06:45:10.721 --> 06:45:12.164
This is an example, for example,
what you see at the top here is 

06:45:12.165 --> 06:45:14.612
somebody driving a car.

06:45:18.082 --> 06:45:20.343
And what we're trying to learn 
in this instance is the 3D 

06:45:20.344 --> 06:45:23.014
structure of the world, the 
geometry of everything.

06:45:26.927 --> 06:45:29.425
What you see at the bottom here 
is a representation of how far 

06:45:29.426 --> 06:45:32.791
things are from the car. You 
probably are looking at avoiding

06:45:38.102 --> 06:45:38.921
obstacles and looking at other 
cars to not collide with them.

06:45:38.922 --> 06:45:42.616
   And so you want to learn 
about the 3D geometry based on 

06:45:42.617 --> 06:45:44.617
those videos.

06:45:45.763 --> 06:45:48.647
The traditional way that you 
would do this is by  involving, 

06:45:48.648 --> 06:45:53.178
for example, a 3D camera, or 
something that gives you a sense

06:45:53.179 --> 06:45:55.179
of depth. Here we're going to do
none of that.

06:45:58.134 --> 06:46:00.775
We're going to simply look at 
the video and learn directly 

06:46:00.776 --> 06:46:02.776
from the video the 3D structure 
of the world.

06:46:05.667 --> 06:46:08.116
And the way to do this is to 
look at the video and try to 

06:46:08.117 --> 06:46:12.402
predict the future of this 
video. You can imagine that if 

06:46:12.403 --> 06:46:17.124
you actually understand the 3D 
geometry of the world, you can 

06:46:17.125 --> 06:46:18.790
do a pretty good job at 
predicting what's going to 

06:46:18.791 --> 06:46:22.075
happen next in the video.
   So we're going to use that 

06:46:22.076 --> 06:46:27.163
signal that tells us how well 
we're doing at predicting the 

06:46:27.164 --> 06:46:29.001
future to learn what the 3D 
geometry of the world looks 

06:46:29.002 --> 06:46:31.002
like. So at the end of the day, 
what we end 

06:46:34.429 --> 06:46:36.833
up with is yet another big 
convolutional network that maps 

06:46:36.834 --> 06:46:41.124
what you see at the top to what 
you see at the bottom without 

06:46:41.125 --> 06:46:43.125
involving any 3D camera or 
anything like that.

06:46:45.438 --> 06:46:47.057
This idea of self-learning or 
just learning without any 

06:46:47.058 --> 06:46:49.922
supervision directly from the 
data is really, really powerful.

06:46:51.345 --> 06:46:54.885
   Another problem that we have 
when we're trying to teach 

06:46:54.886 --> 06:46:56.940
robots how to do things is that 
we have to communicate to 

06:47:00.283 --> 06:47:03.135
them what we want, what we care 
about, right? And the best way 

06:47:03.136 --> 06:47:05.170
you can do that is by simply 
showing them what you want them 

06:47:06.182 --> 06:47:08.182
to perform.

06:47:09.252 --> 06:47:10.884
So here is an example of one of 
my colleagues basically doing 

06:47:10.885 --> 06:47:12.885
the robot dance.

06:47:14.761 --> 06:47:17.626
And the robot that is just 
looking at him performing those 

06:47:17.627 --> 06:47:22.189
tasks, and trying to imitate 
visually what he is doing. And 

06:47:22.190 --> 06:47:23.829
what's remarkable here is that, 
you know, even though the robot,

06:47:23.830 --> 06:47:25.830
for example, doesn't have legs, 
it tries to 

06:47:28.753 --> 06:47:30.753
do this crouching motion as best
it can 

06:47:32.059 --> 06:47:34.059
given the degrees of freedom 
that it has available.

06:47:36.344 --> 06:47:38.344
And all of this is entirely 
self-supervised.

06:47:40.004 --> 06:47:42.004
The way we go about this is that
if you 

06:47:43.104 --> 06:47:45.593
think about imitating somebody 
else, for example, somebody 

06:47:45.594 --> 06:47:47.594
pouring a glass of 

06:47:49.478 --> 06:47:51.478
water, or a can of Coke, it all 
relies 

06:47:54.413 --> 06:47:56.644
on you being able to look at 
them from a third-party view and

06:47:56.645 --> 06:47:58.645
picturing yourself 

06:48:00.581 --> 06:48:02.206
doing the same thing from your 
point of view, what it would 

06:48:02.207 --> 06:48:06.776
look like if you did the same 
thing yourself, right? So we 

06:48:06.777 --> 06:48:08.188
collected some of this data that
looks like that where you have 

06:48:08.189 --> 06:48:10.656
somebody looking at somebody 
else do a task and 

06:48:14.391 --> 06:48:16.642
you end up with those two videos
of one taken by the person doing

06:48:16.643 --> 06:48:18.676
the task and another one taken 
by another  person.

06:48:20.095 --> 06:48:22.140
And what we want to teach the 
robots is that those two things 

06:48:22.141 --> 06:48:25.008
are actually the same thing.

06:48:28.079 --> 06:48:29.705
So we're going to use, again, 
machine learning to perform this

06:48:29.706 --> 06:48:34.173
matchup. We're going to have a 
machine learning model that is 

06:48:34.174 --> 06:48:36.209
going to tell us, okay, this 
image on the left is actually of

06:48:38.873 --> 06:48:40.739
the same task as this image on 
the right. And once we've 

06:48:40.740 --> 06:48:43.566
learned that correspondence, 
there lots of things we can do 

06:48:43.567 --> 06:48:48.310
with this. One of them is just 
imitation like this. Imagine you

06:48:48.311 --> 06:48:50.311
have somebody pouring a 

06:48:52.580 --> 06:48:54.580
glass of water, the robot sees 
them, 

06:48:56.601 --> 06:48:58.615
they try to picture themselves 
doing the same task, and try 

06:48:58.616 --> 06:49:00.616
best they can to imitate what 
they are doing.

06:49:01.489 --> 06:49:02.688
And so using, again, deep 
reinforcement learning, we can 

06:49:02.689 --> 06:49:04.689
train 

06:49:05.968 --> 06:49:07.968
robots to learn those kinds of 

06:49:09.238 --> 06:49:11.289
activities completely based on 
visual observation without any 

06:49:11.290 --> 06:49:13.290
programming of any kind.

06:49:17.457 --> 06:49:19.290
So I won't let that robot pour 
quite yet, but it's encouraging 

06:49:19.291 --> 06:49:22.973
that we can look at robots that 
understand essentially what the 

06:49:22.974 --> 06:49:28.115
nature, what the fundamentals of
the task is regardless of 

06:49:28.116 --> 06:49:30.116
whether they're pouring a 
liquid, or 

06:49:32.812 --> 06:49:35.708
they're pouring beads, or 
whatever the glasses look like 

06:49:35.709 --> 06:49:39.451
or the containers. All of that 
is  abstracted and the robot 

06:49:39.452 --> 06:49:41.452
understands deeply what the task
is about.

06:49:43.058 --> 06:49:45.058
So I'm very excited about this 
whole 

06:49:46.900 --> 06:49:48.936
perspective on teaching robots 
how to learn instead of having 

06:49:48.937 --> 06:49:50.937
to program them, right.

06:49:53.012 --> 06:49:55.678
At some point I would want to be
able to tell my Google assistant

06:49:55.679 --> 06:49:57.700
hey, okay, please go fold my 
laundry, right.

06:50:00.964 --> 06:50:03.224
And for that to happen, we're 
going to have to rebuild the 

06:50:03.225 --> 06:50:05.225
science of robotics from the 
ground up.

06:50:09.382 --> 06:50:12.046
We're going to have to base it 
on  understanding and machine 

06:50:12.047 --> 06:50:14.047
learning, and perception, and of
course we're going to 

06:50:16.566 --> 06:50:19.431
have to do that at Google scale.
   With that, I'm going to give 

06:50:19.432 --> 06:50:22.697
the stage to Debbie, who is 
going to talk to us about 

06:50:22.698 --> 06:50:28.333
cosmology. Thank you.
   (Applause)

06:50:30.361 --> 06:50:32.361
&gt;&gt; DEBORAH BARD: Thank you.

06:50:34.388 --> 06:50:36.388
Good afternoon, everyone.

06:50:38.957 --> 06:50:40.791
My name is Debbie Bard, and I'm 
going to be talking about 

06:50:40.792 --> 06:50:42.941
something a little bit 
different. So I lead the data 

06:50:42.942 --> 06:50:46.933
science engagement group at 
NERSC. And NERSC is the National

06:50:46.934 --> 06:50:48.934
Energy Research Scientific 
Computing Center.

06:50:51.538 --> 06:50:53.978
We're a supercomputing center at
Lawrence Berkeley National Lab 

06:50:53.979 --> 06:50:57.681
just over the bay from here. We 
are the mission computing center

06:50:57.682 --> 06:51:02.620
for the Department of Energy 
Office of Science. We have 

06:51:02.621 --> 06:51:04.703
something like 7,000 scientists 
using our supercomputers to work

06:51:04.704 --> 06:51:07.352
on some of the biggest questions
in science today.

06:51:08.776 --> 06:51:11.258
And what I think is really cool 
as well is that I get to work 

06:51:11.259 --> 06:51:13.259
with some of 

06:51:14.755 --> 06:51:16.596
the most powerful computers on 
the planet. One of the things 

06:51:16.597 --> 06:51:21.723
we're noticing is we've seen 
that scientists are increasingly

06:51:21.724 --> 06:51:22.743
turning to deep learning and 
machine learning methods to 

06:51:22.744 --> 06:51:26.004
solve big questions that they're
working on. We're seeing these 

06:51:26.005 --> 06:51:28.275
questions showing up in our 
workload on our supercomputers.

06:51:31.337 --> 06:51:34.386
So I want to focus on one 
particular topic area. It's very

06:51:34.387 --> 06:51:36.387
close to my heart, which is 

06:51:38.197 --> 06:51:40.197
cosmology, because I'm a

06:51:42.276 --> 06:51:44.276
cost moll

06:51:46.426 --> 06:51:49.126
cosmologist by training.
   I've always been interested 

06:51:49.127 --> 06:51:51.797
in the nature of the universe. 
Perhaps one of the most basic 

06:51:51.798 --> 06:51:53.798
questions 

06:51:54.874 --> 06:51:56.874
you can ask is, what is it made 
of?

06:51:57.929 --> 06:51:59.761
These days, we have a fairly 
good feel for how much dark 

06:51:59.762 --> 06:52:02.000
energy there is in the universe,
how much dark matter, how much 

06:52:03.652 --> 06:52:06.152
regular matter there is in the 
universe. And there's only about

06:52:06.153 --> 06:52:08.153
5% of regular matter, which is 
everything that you and 

06:52:11.261 --> 06:52:13.310
I, and all the stars, dust, gas,
and galaxies out there are made 

06:52:13.311 --> 06:52:17.277
of regular matter. That makes up
a pretty tiny proportion of the 

06:52:17.278 --> 06:52:19.346
content of the universe.
   The thing that I find really 

06:52:21.790 --> 06:52:25.859
interesting is we just don't 
know what the rest of it is. 

06:52:25.860 --> 06:52:27.860
Dark matter, we don't know what 
that's made of.

06:52:29.961 --> 06:52:32.598
But we see indirectly the 
gravitational effect it has. 

06:52:32.599 --> 06:52:35.309
Dark energy, we don't know what 
that is. It was recently 

06:52:35.310 --> 06:52:40.478
discovered. Dark energy is a 
name we give to an observation 

06:52:40.479 --> 06:52:44.171
which is the accelerated 
expansion of the universe. And 

06:52:44.172 --> 06:52:46.821
this is, I think, really 
exciting. The fact that there is

06:52:46.822 --> 06:52:51.140
so much that we have yet to 
discover means that there are 

06:52:51.141 --> 06:52:53.141
tremendous possibilities for new

06:52:54.190 --> 06:52:56.190
ways for us to understand our 
universe.

06:52:56.430 --> 06:52:59.906
And we are building a bigger and
better telescope. We're 

06:52:59.907 --> 06:53:01.907
collecting data all the time, 

06:53:02.961 --> 06:53:04.790
taking images and observations 
of the sky to get more data to 

06:53:04.791 --> 06:53:06.791
help us 

06:53:08.411 --> 06:53:12.963
understand this, because we only
have one universe to observe. We

06:53:12.964 --> 06:53:14.996
need to collect as much data as 
we can and extract all the 

06:53:14.997 --> 06:53:16.997
information we 

06:53:19.114 --> 06:53:21.954
can from our data, from our 
observations. And cosmologists 

06:53:21.955 --> 06:53:26.654
are turning to deep learning to 
extract meaning from our data. 

06:53:26.655 --> 06:53:27.667
I'm going to talk about a couple
of different ways we're doing 

06:53:27.668 --> 06:53:29.668
that. First of all, I want to 
ground this in 

06:53:32.968 --> 06:53:34.391
the background of how we 
actually do experiments in 

06:53:34.392 --> 06:53:38.676
cosmology, because cosmology is 
not an  experimental science in 

06:53:38.677 --> 06:53:40.677
the way that many of the 
physical  sciences are.

06:53:41.322 --> 06:53:44.816
There's not a lot we can do to 
experiment with the universe. We

06:53:44.817 --> 06:53:46.670
can't really do much to change 
the nature of space time, 

06:53:46.671 --> 06:53:50.951
although it would be fun if we 
could. Instead we have to run 

06:53:50.952 --> 06:53:53.602
simulations. We run simulations 
in super computers 

06:53:56.878 --> 06:53:58.103
off theoretical universes and to
different physical models and 

06:53:58.104 --> 06:54:01.987
the different parameters that 
control those physical models. 

06:54:01.988 --> 06:54:04.429
And that's how we experiment. We
run the simulated universes and 

06:54:06.071 --> 06:54:08.714
compare the outputs of the 
simulations to our observations 

06:54:08.715 --> 06:54:11.568
of the real universe around us.
   So when we make this 

06:54:11.569 --> 06:54:15.668
comparison, we're typically 
using some statistical measure, 

06:54:15.669 --> 06:54:18.351
some kind of reduced statistic 
like the power spectrum, which 

06:54:18.352 --> 06:54:20.352
is illustrated in this animation
here.

06:54:24.086 --> 06:54:26.125
The power spectrum is a measure 
of how matter is distributed 

06:54:26.126 --> 06:54:29.587
throughout the universe, whether
it's, kind of, distributed 

06:54:29.588 --> 06:54:32.421
fairly evenly throughout space 
or whether it's clustered on 

06:54:32.422 --> 06:54:34.422
small scales.

06:54:37.778 --> 06:54:40.483
And this is illustrated in the 
images on the top of the slide, 

06:54:40.484 --> 06:54:42.484
snapshots of a 

06:54:43.556 --> 06:54:45.556
simulated universe run in the 
super computer.

06:54:46.023 --> 06:54:48.484
And you can see that over time, 
gravity is pulling matter 

06:54:48.485 --> 06:54:53.413
together. And so that's dark 
matter and regular matter. 

06:54:53.414 --> 06:54:55.414
Gravity is acting upon that, 
collapsing 

06:54:56.693 --> 06:54:58.943
the matter into very typical 
cluster type structures, whereas

06:54:58.944 --> 06:55:03.528
dark energy is expanding space 
itself, expanding the volume of 

06:55:03.529 --> 06:55:05.529
this miniature universe.

06:55:06.598 --> 06:55:08.219
And so by looking at the 
distribution of matter, we can 

06:55:08.220 --> 06:55:10.891
start to learn something about 
the nature of the matter 

06:55:14.496 --> 06:55:16.504
itself, how gravity is acting on
that, and what dark energy is 

06:55:16.505 --> 06:55:17.938
doing.
   But as you can imagine, 

06:55:17.939 --> 06:55:19.939
running these 

06:55:22.013 --> 06:55:26.079
kinds of simulations is very 
computationally expensive, even 

06:55:22.013 --> 06:55:24.013
if 

06:55:25.462 --> 06:55:26.320
you're only simulating a tiny 
universe, it still requires a 

06:55:26.321 --> 06:55:29.002
tremendous amount of compute 
power. And we spend billions of 

06:55:29.003 --> 06:55:31.039
compute hours on supercomputers 
around the world on 

06:55:34.306 --> 06:55:35.758
these kinds of simulations, 
including the supercomputers 

06:55:35.759 --> 06:55:37.759
that I work with.

06:55:39.460 --> 06:55:41.460
And one of the ways that we're 
using 

06:55:42.953 --> 06:55:45.593
deep learning is to reduce the 
need for such expensive 

06:55:45.594 --> 06:55:47.640
simulations, similar to the 
previous speaker was talking 

06:55:47.641 --> 06:55:51.400
about in robotics.
   We're exploring using 

06:55:51.401 --> 06:55:53.401
generative 

06:55:55.453 --> 06:55:57.453
networks to produce, in this 
case, this 

06:55:59.147 --> 06:56:01.199
example, two-dimensional maps of
the universe. These are maps of 

06:56:01.200 --> 06:56:05.112
the mass concentration of the 
universe. You can imagine the 

06:56:05.113 --> 06:56:06.936
three dimensional volume 
collapsed into a  

06:56:06.937 --> 06:56:10.803
two-dimensional projection of 
the mass density in the universe

06:56:10.804 --> 06:56:12.804
as you're looking out at the 
sky.

06:56:18.174 --> 06:56:20.174
And we use a fairly standard DC 

06:56:22.859 --> 06:56:24.859
topology to produce new maps 
based on simulations.

06:56:26.926 --> 06:56:28.926
So this is an augmentation.

06:56:30.396 --> 06:56:33.500
We're using this network to 
augment an existing simulation. 

06:56:33.501 --> 06:56:36.606
And we see that it's doing a 
pretty good job. So just by 

06:56:36.607 --> 06:56:41.296
looking by eye at the generated 
images, they look pretty similar

06:56:41.297 --> 06:56:43.950
to the real simulated images. As
a scientist, squinting at 

06:56:43.951 --> 06:56:46.670
something and saying that looks 
about right is not good enough.

06:56:50.160 --> 06:56:52.160
What I want is to be able to 
quantify 

06:56:54.070 --> 06:56:56.358
this, how the network is working
and how like the real images our

06:56:56.359 --> 06:56:59.014
generated images are.
   And this is where scientific 

06:56:59.015 --> 06:57:01.711
data has a real advantage  
compared to natural 

06:57:06.796 --> 06:57:08.863
image data, because scientific 
data usually, very often, has 

06:57:08.864 --> 06:57:12.550
associated statistics with it. 
So statistics that you can use 

06:57:12.551 --> 06:57:17.478
to evaluate the success of your 
model. So in this case, we were 

06:57:17.479 --> 06:57:20.127
looking at reduced statistics 
that describe the 

06:57:24.213 --> 06:57:25.632
patterns in the maps, like the 
power spectra and other measures

06:57:25.633 --> 06:57:29.100
of the topology of the maps. We 
see that not only do the maps 

06:57:29.101 --> 06:57:34.405
look about right, but the 
statistics that are contained in

06:57:34.406 --> 06:57:36.406
those maps match those from the 
real simulations.

06:57:38.304 --> 06:57:40.755
So we can quantify the accuracy 
of our network. And this is 

06:57:40.756 --> 06:57:43.436
something that potentially could
be useful for the wider deep 

06:57:45.524 --> 06:57:47.148
learning community, using 
scientific data that has 

06:57:47.149 --> 06:57:49.149
statistics could be of real 
interest to deep learning 

06:57:50.822 --> 06:57:52.654
practitioners in trying to 
quantify how well your networks 

06:57:52.655 --> 06:57:55.379
are working. So I mentioned 
before that this is an 

06:57:57.203 --> 06:58:00.487
augmentation that we've been 
working on so far. It can 

06:58:00.488 --> 06:58:02.488
produce new maps based on a 

06:58:04.778 --> 06:58:06.833
physics model that it's already 
seen. We're working at producing

06:58:06.834 --> 06:58:11.434
physics models that the network 
has never seen, making this into

06:58:11.435 --> 06:58:14.317
a true emulator.
   This will help reduce the 

06:58:14.318 --> 06:58:16.318
need for 

06:58:20.054 --> 06:58:22.106
these expensive simulations and 
allow cosmologists to explore 

06:58:22.107 --> 06:58:26.240
space more freely. I'd like to 
explore a little bit further 

06:58:26.241 --> 06:58:28.469
what this network is actually  
learning. I saw an interesting 

06:58:28.470 --> 06:58:30.470
talk this morning 

06:58:32.787 --> 06:58:34.090
here touching on this kind of 
thing, how we can use machine 

06:58:34.091 --> 06:58:36.343
learning to gain insight into 
the nature of the data that 

06:58:37.969 --> 06:58:42.119
we're working with. So in the 
work that I'm showing here, we 

06:58:42.120 --> 06:58:43.966
were looking at which structures
in our mass maps are 

06:58:43.967 --> 06:58:46.843
contributing to the model, most 
strongly contributing, by 

06:58:49.334 --> 06:58:51.334
looking at a quantity called 
saliency.

06:58:53.871 --> 06:58:55.510
And so if you look at the map of
saliency, black and white, you 

06:58:55.511 --> 06:58:57.511
can see 

06:58:59.362 --> 06:59:02.742
the peaks in the saliency map 
correspond to peaks in the mass 

06:59:02.743 --> 06:59:04.743
map, which are concentrations of

06:59:06.007 --> 06:59:08.007
matter, and these correspond to 
galaxy clusters.

06:59:10.299 --> 06:59:12.299
And this isn't news to  
cosmologists.

06:59:14.450 --> 06:59:17.307
We've known for decades that 
galaxy clusters are a good way 

06:59:17.308 --> 06:59:19.799
of exploring cosmology. The 
shapes of the features that this

06:59:22.472 --> 06:59:24.102
network has learned are not 
round balls, they are irregular 

06:59:24.103 --> 06:59:26.103
and they're showing some 
structure.

06:59:26.998 --> 06:59:29.017
And this is really interesting 
to me. And there's also 

06:59:29.018 --> 06:59:31.071
indications that some of the 
smaller mass concentrations are 

06:59:32.914 --> 06:59:34.914
showing up as important features
in this network.

06:59:36.803 --> 06:59:39.454
And that's a little bit 
unexpected. So by taking this 

06:59:39.455 --> 06:59:41.528
kind of introspection into the 
features that our network is 

06:59:42.786 --> 06:59:45.080
learning we can start to learn 
something about the data and get

06:59:45.081 --> 06:59:47.728
insight into some of the 
physical processes that are 

06:59:47.729 --> 06:59:49.729
going 

06:59:52.208 --> 06:59:54.048
on in our data, and learn what 
kind of structures are most 

06:59:54.049 --> 06:59:56.495
sensitive to the interplay of 
gravity and dark energy.

06:59:58.332 --> 07:00:00.197
This is something that's a real 
strong point of deep learning, 

07:00:00.198 --> 07:00:04.084
when you are allowing the 
network to learn features for 

07:00:04.085 --> 07:00:06.120
itself rather than imposing 
features, doing feature 

07:00:06.121 --> 07:00:09.623
engineering or  telling it any 
particular statistics. You can 

07:00:09.624 --> 07:00:11.056
allow the network to tell you 
something about your data that 

07:00:11.057 --> 07:00:13.085
might surprise you.

07:00:17.411 --> 07:00:18.658
So far, that was looking at 
two-dimensional maps, but, of 

07:00:18.659 --> 07:00:23.365
course the group verse is not a 
two-dimensional place. It's at 

07:00:23.366 --> 07:00:25.688
least four dimensions, perhaps 
many more dimensions depending 

07:00:25.689 --> 07:00:27.689
on your favorite model of string
theory.

07:00:29.994 --> 07:00:32.291
But we've been looking at three 
dimensions and scaling this  up.

07:00:35.160 --> 07:00:38.012
The reason why three dimensions 
are interesting for us, in a 

07:00:38.822 --> 07:00:40.822
three-dimensional data volume, 
you're 

07:00:42.911 --> 07:00:44.911
looking at matrices, 
convolutions, it's 

07:00:45.176 --> 07:00:47.176
computationally expensive and it
can run 

07:00:48.921 --> 07:00:51.780
really well on a supercomputing 
architecture. A team recently 

07:00:51.781 --> 07:00:55.810
demonstrated for the first time 
that deep learning can be used 

07:00:55.811 --> 07:00:57.811
to determine the physical model 
of 

07:00:59.878 --> 07:01:01.094
the universe from  
three-dimensional simulations of

07:01:01.095 --> 07:01:03.960
the full matter distribution.
   This is the full matter 

07:01:03.961 --> 07:01:05.389
rather than a two-dimensional 
projection of the matter 

07:01:05.390 --> 07:01:09.687
density. And this work showed 
that the network was able to 

07:01:09.688 --> 07:01:11.688
make significantly better 
estimates of the parameters that

07:01:15.407 --> 07:01:16.855
describe the physics of this 
simulated universe compared to 

07:01:16.856 --> 07:01:20.177
traditional methods, where you 
might be looking at one of these

07:01:20.178 --> 07:01:23.047
statistics like the power 
spectrum. And so this is a 

07:01:23.048 --> 07:01:25.048
really nice example of 

07:01:27.530 --> 07:01:29.530
how the network was able to 
learn, and 

07:01:32.790 --> 07:01:34.087
what structures in this were 
important rather than just 

07:01:34.088 --> 07:01:37.961
looking at statistics that we in
advance thought were going to be

07:01:37.962 --> 07:01:40.230
useful.
   So we're working on scaling 

07:01:40.231 --> 07:01:42.231
this up 

07:01:43.563 --> 07:01:45.563
in collaboration with NERSC, UC 

07:01:48.883 --> 07:01:50.883
Berkeley, Intel, and Cray.

07:01:52.803 --> 07:01:54.890
We're using larger simulation 
volumes, more data, and 

07:01:54.891 --> 07:01:56.891
TensorFlow running on 

07:02:02.426 --> 07:02:06.445
those of CPU nodes achieving 
several petaflocks of data. 

07:02:06.446 --> 07:02:08.446
We're able to predict more 
physical 

07:02:09.933 --> 07:02:11.360
parameters with even greater 
accuracy by scaling up the 

07:02:11.361 --> 07:02:15.650
training. This is something 
we're really excited about. I 

07:02:15.651 --> 07:02:18.302
think it's worth talking a 
little bit more in technical 

07:02:18.303 --> 07:02:20.303
detail about how we achieved 
this performance, how we are 

07:02:22.391 --> 07:02:23.620
using TensorFlow to get this 
kind of performance and insight 

07:02:23.621 --> 07:02:27.332
into our data and our science.
   Now, supercomputers are 

07:02:27.333 --> 07:02:29.333
fairly specialized.

07:02:32.394 --> 07:02:33.597
We have  specialized hardware to
allow the tens of thousands of 

07:02:33.598 --> 07:02:36.707
compute nodes we have on these 
computers to act together as one

07:02:36.708 --> 07:02:39.996
compute machine. We want to use 
this machine as efficiently as 

07:02:39.997 --> 07:02:41.829
possible to train our network. 
We have a lot of performance 

07:02:41.830 --> 07:02:46.126
available to us. We want to be 
able to take advantage of that 

07:02:46.127 --> 07:02:49.003
when we're  running TensorFlow. 
The approach we take is using a 

07:02:49.004 --> 07:02:53.905
fully synchronous data parallel 
approach where each node is 

07:02:53.906 --> 07:02:55.906
training on a subset of the 
data.

07:02:56.762 --> 07:02:59.017
And we started off as many 
people do, using GRPC for this, 

07:02:59.018 --> 07:03:01.322
where each compute node is 
communicating with a parameter 

07:03:03.763 --> 07:03:05.627
server to send their parameter 
updates and have that sent back 

07:03:05.628 --> 07:03:10.341
and forward. But like many other
people have noted, this is not a

07:03:10.342 --> 07:03:12.580
very efficient way to run at 
scale. We found that if we were 

07:03:12.581 --> 07:03:17.286
running beyond a hundred nodes 
or so, then we had a real 

07:03:17.287 --> 07:03:19.287
communication bottleneck between
the compute nodes and the 

07:03:19.523 --> 07:03:21.523
parameter servers.

07:03:24.920 --> 07:03:27.115
So instead, we use NPI, which is
a message-passing interface to 

07:03:27.116 --> 07:03:29.772
allow our compute nodes to 
communicate with each other 

07:03:29.773 --> 07:03:34.707
directly, removing the need for 
parameter servers. And this also

07:03:34.708 --> 07:03:36.708
has the advantage that you can 
really take advantage of our  

07:03:38.452 --> 07:03:40.452
high-speed interconnect, the 
specialized 

07:03:41.523 --> 07:03:44.227
hardware that connects our 
compute nodes. So we use for 

07:03:44.228 --> 07:03:46.228
gradient aggregation for 

07:03:47.527 --> 07:03:49.527
this, we use a specialized NPI 

07:03:51.247 --> 07:03:53.728
collective, which is designed by
Cray, our partners with our 

07:03:53.729 --> 07:03:55.729
supercomputers.

07:03:58.820 --> 07:04:00.820
And this MPI is pretty neat.

07:04:01.888 --> 07:04:03.315
It's able to avoid imbalances in
the node performance, the 

07:04:03.316 --> 07:04:06.951
straggler effect that some of 
you might run into. It's 

07:04:06.952 --> 07:04:09.212
overlapping communication and 
compute in a way that allows 

07:04:09.213 --> 07:04:14.724
very effective scaling and we've
seen we're able to run 

07:04:14.725 --> 07:04:18.193
TensorFlow on thousands of nodes
with little drop in efficiency. 

07:04:18.194 --> 07:04:20.194
Something I've been excited to 
see here 

07:04:21.713 --> 07:04:23.615
is MPI reduce is coming soon in 
TensorFlow. And we're excited to

07:04:23.616 --> 07:04:26.477
see how this is going to work in
the larger community.

07:04:28.380 --> 07:04:30.833
So the three things I'd like you
to take away from this talk, 

07:04:30.834 --> 07:04:33.683
first, cosmology has cool 
science problems and cool deep 

07:04:33.684 --> 07:04:36.142
learning problems. The second is
that scientific data is 

07:04:39.287 --> 07:04:40.921
different from natural image 
data. The statistics that we 

07:04:40.922 --> 07:04:44.809
often have associated with 
scientific data could be of real

07:04:44.810 --> 07:04:48.874
use in the deep learning 
community. And the third thing 

07:04:48.875 --> 07:04:52.589
is that MPI all reduce is the 
optimal strategy for scaling 

07:04:52.590 --> 07:04:54.271
TensorFlow up to multiple nodes,
and we're looking forward to 

07:04:54.272 --> 07:04:56.517
seeing how the rest of the 
community is going to work with 

07:04:56.518 --> 07:04:58.518
this.

07:04:59.602 --> 07:05:01.421
So now I'll turn things back to 
Lawrence. Thank you.

07:05:01.422 --> 07:05:02.899
(Applause)
   &gt;&gt; LAURENCE MORONEY: Thank 

07:05:02.900 --> 07:05:07.402
you, Debbie. Great stuff. 
Actually simulating universes. 

07:05:07.403 --> 07:05:10.062
So, we're running very short on 
time, so I just want to share, 

07:05:10.063 --> 07:05:12.926
these are just three great 
stories. There are countless 

07:05:12.927 --> 07:05:15.174
more out there. This is a map 
that I created of people 

07:05:18.903 --> 07:05:19.718
who starred TensorFlow and 
GitHub and shared their 

07:05:19.719 --> 07:05:21.758
location. We have the people 
from Australia to 

07:05:25.244 --> 07:05:26.904
Ireland, from the north arctic 
circle in Norway all the way 

07:05:26.905 --> 07:05:32.024
down to deception island. There 
are countless stories being 

07:05:35.319 --> 07:05:35.927
created and new things being 
done with TensorFlow and machine

07:05:35.928 --> 07:05:38.581
learning.
   If some of those stories are 

07:05:38.582 --> 07:05:41.868
yours, we'd love to share them. 
So with that, I just want to say

07:05:41.869 --> 07:05:46.468
thank you very much for  
attending today. Enjoy what's 

07:05:46.469 --> 07:05:48.469
left of I/O, and have a safe 
journey home. Thank you.

07:05:50.892 --> 07:05:52.892
(Applause)

07:05:54.841 --> 07:05:56.877
   &gt;&gt; Thank you for joining this
session.

07:05:59.970 --> 07:06:02.231
Brand ambassadors will assist 
with directing you through the 

07:06:02.232 --> 07:06:04.877
designated exits. We'll be 
making room for those who have 

07:06:06.100 --> 07:06:07.940
registered for the next session.
If you have registered for the 

07:06:07.941 --> 07:06:09.975
next session in this room, we 
ask that you 

07:06:13.880 --> 07:06:14.484
please clear the room and return
via the registration line 

07:06:14.485 --> 07:06:16.485
outside. Thank you.

07:06:18.506 --> 07:06:20.506
 

07:07:46.685 --> 07:07:49.032
Android Fireside Chat

07:24:04.170 --> 07:24:06.170
(Cheering and applause)

07:24:13.414 --> 07:24:15.438
&gt;&gt; Hello, and welcome to the 
Android fireside chat. We have a

07:24:15.439 --> 07:24:19.136
lot of content. We're going to 
get through that first and see 

07:24:19.137 --> 07:24:21.163
if we can get to questions next.
If we could get to the next 

07:24:21.164 --> 07:24:25.479
slide. I guess we're ready for 
some questions. So we do have 

07:24:25.480 --> 07:24:30.587
one question to start off, if we
could go to that one. Someone 

07:24:30.588 --> 07:24:32.624
asked online on the Twitter 
thread why is it called a 

07:24:32.625 --> 07:24:35.456
fireside chat if there's no 
fire. I thought this was a 

07:24:35.457 --> 07:24:37.457
pretty good 

07:24:42.379 --> 07:24:44.379
question, so the answer to that 
would be . . .

07:24:45.730 --> 07:24:47.730
(Laughter)
   (Applause)

07:24:48.235 --> 07:24:51.509
&gt;&gt; MODERATOR: All right. So warm
yourselves by the fire. We're 

07:24:51.510 --> 07:24:53.140
going to ask some questions. 
You're going to ask some 

07:24:53.141 --> 07:24:57.828
questions. We're going to try to
answer those questions. So, we 

07:24:57.829 --> 07:24:59.685
pre-rolled some questions 
online. It was interesting to 

07:24:59.686 --> 07:25:04.397
see what people were thinking. 
We have some of those. We'll 

07:25:04.398 --> 07:25:07.461
interweave them with questions 
from the audience. I will show 

07:25:07.462 --> 07:25:10.506
you how that works in a 
question. First, I'm Chet Haase,

07:25:10.507 --> 07:25:14.173
Android toolkit team.
   &gt;&gt; DAVE BURKE: Dave, most 

07:25:14.174 --> 07:25:16.174
things.

07:25:17.590 --> 07:25:19.880
&gt;&gt; STEPHANIE SAAD: 
   &gt;&gt; I work on the developer 

07:25:19.881 --> 07:25:22.768
experience.
   &gt;&gt; I work on Kotlin.

07:25:25.042 --> 07:25:27.042
&gt;&gt; I manage the framework team.

07:25:28.511 --> 07:25:30.511
&gt;&gt; I'm Xavier, I work on the 
tools team.

07:25:30.542 --> 07:25:31.794
&gt;&gt; CHET HAASE: We have a bunch 
of other people from the 

07:25:31.795 --> 07:25:35.861
engineering team to my stage 
right here. So if one of these 

07:25:35.862 --> 07:25:37.862
people here can 

07:25:39.102 --> 07:25:41.342
answer it, maybe one of the 
people in the audience can't 

07:25:41.343 --> 07:25:45.896
answer, either. We have a couple
of questions to kick it off, and

07:25:45.897 --> 07:25:48.945
then we'll kick it to the 
audience. First of  all, what's 

07:25:48.946 --> 07:25:52.206
an exciting feature that you're 
working on for the next release?

07:25:55.267 --> 07:25:56.687
Romain, do you want to take that
one?

07:25:56.688 --> 07:25:58.512
&gt;&gt; ROMAIN GUY: No.
   &gt;&gt; CHET HAASE: That's a 

07:25:58.513 --> 07:26:00.747
really good example. We do not 
answer questions about future 

07:26:02.596 --> 07:26:04.596
development, kind of a policy 
thing that we have.

07:26:06.257 --> 07:26:08.103
I would say 90% of the questions
I got on the Twitter thread were

07:26:08.104 --> 07:26:11.441
about future development. That 
makes it easy. We don't have to 

07:26:11.442 --> 07:26:12.246
answer those.
   &gt;&gt; This is going really well.

07:26:12.247 --> 07:26:16.963
   &gt;&gt; CHET HAASE: We'll be done 
in the next two minutes. Let's 

07:26:16.964 --> 07:26:19.400
ask a more real question. I'll 
toss this to Romain.

07:26:22.646 --> 07:26:25.299
Will Android architecture 
components be a de facto 

07:26:25.300 --> 07:26:27.126
architecture choice with 
continuous support, like 

07:26:27.127 --> 07:26:29.127
continuous builds except for 
support.

07:26:30.841 --> 07:26:32.841
&gt;&gt; ROMAIN GUY: Yes.

07:26:34.511 --> 07:26:36.952
More seriously, we listened, you
were asking us to have an 

07:26:36.953 --> 07:26:40.252
opinion about architecture. For 
many years we did not have one. 

07:26:40.253 --> 07:26:42.524
You can do whatever you want. 
But we have a solution that's 

07:26:42.525 --> 07:26:47.234
our solution. We're going to 
support it. We have tooling. We 

07:26:47.235 --> 07:26:50.489
have more libraries, 
documentation, tons of talks. If

07:26:50.490 --> 07:26:52.733
you like it or don't know what 
architecture you want to use, go

07:26:52.734 --> 07:26:56.033
with that one.
   &gt;&gt; CHET HAASE: There you go. 

07:26:56.034 --> 07:26:58.689
That's an example. It was a real
question and kind of a real 

07:26:58.690 --> 07:27:02.149
answer as well. I would invite 
everybody in the audience to ask

07:27:02.150 --> 07:27:04.150
a question.

07:27:05.527 --> 07:27:06.953
there are two microphones. We 
were curious how long it would 

07:27:06.954 --> 07:27:08.954
take people to get to them.

07:27:11.282 --> 07:27:13.311
It's going to be kind of a 
Hunger Games thing. Come to the 

07:27:13.312 --> 07:27:15.551
mic, ask your question. If 
people in the audience don't ask

07:27:16.773 --> 07:27:19.815
questions, I have some here. I 
see someone walking up. Go 

07:27:19.816 --> 07:27:22.674
ahead.
   &gt;&gt; AUDIENCE: The mic is too 

07:27:22.675 --> 07:27:24.675
tall for me.

07:27:27.178 --> 07:27:29.420
So, following up the question 
about architecture components, I

07:27:29.421 --> 07:27:34.093
remember Diane posting online 
saying that, like, you know, 

07:27:34.094 --> 07:27:36.763
look, we gave you activity. We 
gave you content, we gave you 

07:27:36.764 --> 07:27:38.764
all these things.

07:27:41.415 --> 07:27:43.415
That's the agnostic platform, 
you can 

07:27:44.422 --> 07:27:45.433
do whatever you want with it. 
We're not going to impose any 

07:27:45.434 --> 07:27:48.485
choices on you. I'm sure there 
are other people on the Android 

07:27:48.486 --> 07:27:50.854
team who agreed with her. I 
wonder what Diane and those 

07:27:50.855 --> 07:27:55.750
other people might think about 
architecture components. Is this

07:27:55.751 --> 07:27:58.643
like a perversion of their 
vision of Android?

07:27:58.840 --> 07:28:01.484
(Laughter)
   &gt;&gt; All wrong.

07:28:01.689 --> 07:28:03.515
(Laughter)
   &gt;&gt; AUDIENCE: Or is this 

07:28:03.516 --> 07:28:07.606
something that like, I 
understand why some people might

07:28:07.607 --> 07:28:09.607
need it, but if you want to do 
things weirdly, you can.

07:28:13.111 --> 07:28:14.921
Was there a debate about 
architecture components?

07:28:14.922 --> 07:28:16.373
&gt;&gt; DIANNE HACKBORN: Basically, 
that all holds true as far as 

07:28:16.374 --> 07:28:18.374
the core platform is concerned.

07:28:21.483 --> 07:28:23.962
You can think of -- when I say 
the framework team, it's the 

07:28:23.963 --> 07:28:27.460
Core platform. This is stuff 
that's shipped on the device 

07:28:27.461 --> 07:28:29.461
that is, like, defines what you 
can do with the device.

07:28:34.107 --> 07:28:36.107
And we want to keep that still 
generic, 

07:28:37.394 --> 07:28:39.220
not enforcing any particular 
model. We want to have 

07:28:39.221 --> 07:28:42.881
flexibility for  applications to
do new things and all that kind 

07:28:42.882 --> 07:28:46.949
of stuff. The architecture 
components are basically a layer

07:28:46.950 --> 07:28:50.826
on top that it's very different,
because in the support library, 

07:28:50.827 --> 07:28:52.827
it can evolve rapidly with the 
applications.

07:28:53.464 --> 07:28:55.501
It can change and break because 
it's not going to break 

07:28:55.502 --> 07:28:58.975
applications when those changes 
happen. So we're basically 

07:28:58.976 --> 07:29:01.618
putting a new layer on top of 
the platform that gives our 

07:29:05.280 --> 07:29:06.106
very strong position on how we 
think applications should be 

07:29:06.107 --> 07:29:08.107
developed, which 

