WEBVTT
Kind: captions
Language: en

00:00:00.120 --> 00:00:01.670
GARRICK EVANS: My name's
Garrick Evans.

00:00:01.670 --> 00:00:04.480
I'm a software architect on
the Google Cloud Platform

00:00:04.480 --> 00:00:05.830
solutions team.

00:00:05.830 --> 00:00:08.270
And I want to tell you one of
the coolest parts about my job

00:00:08.270 --> 00:00:10.300
is the opportunity that comes
up once in a while to get

00:00:10.300 --> 00:00:13.910
involved with some projects
that partners are doing

00:00:13.910 --> 00:00:16.910
working on some very, very hard
problems and asking some

00:00:16.910 --> 00:00:18.820
of the most intriguing and
important questions of the

00:00:18.820 --> 00:00:22.190
world, in this particular
case, in the universe.

00:00:22.190 --> 00:00:24.030
So today, I'm actually pleased
to share with you

00:00:24.030 --> 00:00:24.990
one of these projects.

00:00:24.990 --> 00:00:27.580
It's the ATLAS experiment on
Google Compute Engine.

00:00:27.580 --> 00:00:30.240
And with this project, what we
wanted to demonstrate was

00:00:30.240 --> 00:00:33.470
actually tangible acceleration
of scientific research by

00:00:33.470 --> 00:00:35.560
leveraging and taking advantage
of Google's Cloud in

00:00:35.560 --> 00:00:38.010
a particular Compute Engine.

00:00:38.010 --> 00:00:40.060
The computational and data
demands of the ATLAS

00:00:40.060 --> 00:00:41.830
experiment are pretty
substantial.

00:00:41.830 --> 00:00:44.120
There's thousands of
collaborators running as a

00:00:44.120 --> 00:00:49.590
baseline hundreds of thousands
of jobs per day, peaking at

00:00:49.590 --> 00:00:51.210
over 10 times that amount.

00:00:51.210 --> 00:00:53.840
The experiment generates tens
of petabytes of data a year,

00:00:53.840 --> 00:00:55.950
and there's currently well over
100 petabytes of data

00:00:55.950 --> 00:00:58.150
under management coordination.

00:00:58.150 --> 00:01:00.580
So with me today are Dr.
Sergey Panitkin of the

00:01:00.580 --> 00:01:03.895
Brookhaven National Lab in New
York and Andrew Hanushevsy of

00:01:03.895 --> 00:01:06.600
the SLAC National Accelerator
Lab at Stanford.

00:01:06.600 --> 00:01:09.710
Sergey leads research and
development of cloud computing

00:01:09.710 --> 00:01:12.420
at the ATLAS experiment at
a Large Hadron Collider.

00:01:12.420 --> 00:01:14.790
And we'll talk about the project
itself, the compute

00:01:14.790 --> 00:01:16.800
clusters this team has
assembled, and share his

00:01:16.800 --> 00:01:18.570
results with you.

00:01:18.570 --> 00:01:21.870
Andy's an information systems
specialist who's designed and

00:01:21.870 --> 00:01:25.980
co-developed a data clustering
technology, XRootD, which was

00:01:25.980 --> 00:01:28.820
used to federate data between
ATLAS and the Cloud and will

00:01:28.820 --> 00:01:30.860
provide us an overview
of the technology.

00:01:30.860 --> 00:01:33.165
So with that, I'd like to
introduce you to Dr. Sergey

00:01:33.165 --> 00:01:35.975
Panitkin of Brookhaven
National Lab.

00:01:35.975 --> 00:01:43.750
[APPLAUSE]

00:01:43.750 --> 00:01:45.093
DR. SERGEY PANITKIN: Today
I will talk-- oh.

00:01:45.093 --> 00:01:46.020
[LAUGHTER]

00:01:46.020 --> 00:01:48.190
DR. SERGEY PANITKIN: Today I
will talk a little bit about

00:01:48.190 --> 00:01:51.940
ATLAS experiment, Large Hadron
Collider, ATLAS experiment

00:01:51.940 --> 00:01:55.730
computing the challenges of
a big data experiment, our

00:01:55.730 --> 00:01:59.090
interesting Clouds, and our
recent project Google Compute

00:01:59.090 --> 00:02:03.510
Engine, some conditional
clusters that we run on the

00:02:03.510 --> 00:02:08.500
grid and now we're running in
the Cloud, and we'll also

00:02:08.500 --> 00:02:10.419
describe the XRoot technology.

00:02:10.419 --> 00:02:12.980
Andy will go into more
details about that.

00:02:12.980 --> 00:02:15.140
We're saying that there are
products developed in the high

00:02:15.140 --> 00:02:16.190
energy community.

00:02:16.190 --> 00:02:19.080
Remember, worldwide web.

00:02:19.080 --> 00:02:21.630
There is a new generation of
technology developed that

00:02:21.630 --> 00:02:24.210
community, they're open source,
and we're saying

00:02:24.210 --> 00:02:26.960
they're ready to be shared with
people who are interested

00:02:26.960 --> 00:02:29.270
in Clouds and large
computational clustering,

00:02:29.270 --> 00:02:31.070
bridging virtual and real

00:02:31.070 --> 00:02:33.470
infrastructure, things like that.

00:02:37.310 --> 00:02:39.670
ATLAS is a multipurpose detector
and Large Hadron

00:02:39.670 --> 00:02:41.090
Collider at CERN.

00:02:41.090 --> 00:02:44.620
The ATLAS experiment itself
is a large international

00:02:44.620 --> 00:02:48.420
collaboration of about 3,000
scientists and engineers from

00:02:48.420 --> 00:02:51.920
many universities and labs
around the globe.

00:02:51.920 --> 00:02:54.480
It took almost 20 years to
design and build the

00:02:54.480 --> 00:02:58.760
apparatus, but nevertheless,
it's very young collaboration

00:02:58.760 --> 00:03:01.980
with more than 1,200 graduate
students working ATLAS and

00:03:01.980 --> 00:03:05.200
driving the analyses.

00:03:05.200 --> 00:03:11.050
This photo shows the outline of
the LHC tunnel on an aerial

00:03:11.050 --> 00:03:14.880
view of countryside near
Geneva, Switzerland.

00:03:14.880 --> 00:03:17.620
The LHC is one of the largest
scientific instruments ever

00:03:17.620 --> 00:03:21.790
built and certainly only one
of the most complex.

00:03:21.790 --> 00:03:25.060
And everything about the LHC
is extreme, its size, its

00:03:25.060 --> 00:03:27.140
energy, its detectors.

00:03:27.140 --> 00:03:32.220
It's the coldest and emptiest
place in the solar system.

00:03:32.220 --> 00:03:34.190
It's the hottest place in the
universe, with temperatures of

00:03:34.190 --> 00:03:37.700
particle created under collision
exceeding trillions

00:03:37.700 --> 00:03:38.950
degrees of Celsius.

00:03:42.660 --> 00:03:46.420
LHC tunnel is about 27
kilometers long.

00:03:46.420 --> 00:03:50.210
Thousands of superconducting
magnets working at near

00:03:50.210 --> 00:03:53.250
absolute zero temperature are
needed in order to accelerate

00:03:53.250 --> 00:03:56.970
and collide protons and heavy
ions at the highest

00:03:56.970 --> 00:03:59.890
temperatures ever achieved
in the lab.

00:03:59.890 --> 00:04:02.685
Such energies are needed in
order to explore the high

00:04:02.685 --> 00:04:05.580
energy frontier of modern
particle physics to discover

00:04:05.580 --> 00:04:08.600
things like Higgs boson, the
missing piece of the standard

00:04:08.600 --> 00:04:11.420
model, a particle that is
responsible for electric

00:04:11.420 --> 00:04:13.020
symmetry breaking
that generates

00:04:13.020 --> 00:04:15.310
massive elementary particles.

00:04:15.310 --> 00:04:18.930
LHC's aim to explore physics
beyond the standard model,

00:04:18.930 --> 00:04:21.690
things like supersymmetry,
possible existence of

00:04:21.690 --> 00:04:24.970
extradimensional, possible
candidates of dark matter,

00:04:24.970 --> 00:04:30.000
dark energy, and whatever
can be at that frontier.

00:04:32.680 --> 00:04:35.550
The ATLAS detector is a
multipurpose apparatus

00:04:35.550 --> 00:04:40.000
designed to detect particles
created in the collision of

00:04:40.000 --> 00:04:41.750
the LHC beams.

00:04:41.750 --> 00:04:44.830
It's the largest detector of its
kind, and also one of the

00:04:44.830 --> 00:04:47.150
most complex.

00:04:47.150 --> 00:04:50.630
It's built like a Russian doll
with detectors inside other

00:04:50.630 --> 00:04:54.320
detectors with several giant
superconducting magnets.

00:04:54.320 --> 00:04:59.080
All of this is needed to
accurately register particle

00:04:59.080 --> 00:05:01.080
and identify them.

00:05:01.080 --> 00:05:04.150
It has very high granularity of
sensing elements with about

00:05:04.150 --> 00:05:08.120
150 millions of sensors
of various kinds.

00:05:08.120 --> 00:05:10.520
And it's capable of taking
snapshots of the collision

00:05:10.520 --> 00:05:13.950
events at the rate of 40
megahertz, or every 25

00:05:13.950 --> 00:05:16.010
nanoseconds.

00:05:16.010 --> 00:05:19.250
And it weighs about 7,000
tons, and it's

00:05:19.250 --> 00:05:22.780
quite large in size.

00:05:22.780 --> 00:05:24.940
This slide gives you a
feeling of the size

00:05:24.940 --> 00:05:26.240
of the ATLAS detector.

00:05:26.240 --> 00:05:29.830
Here, outlines of ATLAS and our
sister detector, CMS, are

00:05:29.830 --> 00:05:33.470
superimposed with the image of
the ATLAS and CMS six-story

00:05:33.470 --> 00:05:35.400
office building at CERN.

00:05:35.400 --> 00:05:37.870
And ATLAS is taller than
that building.

00:05:37.870 --> 00:05:39.120
It's a huge apparatus.

00:05:41.860 --> 00:05:45.400
The detector itself sits in
the LHC tunnel about 100

00:05:45.400 --> 00:05:48.720
meters below the surface, and
it was assembled there piece

00:05:48.720 --> 00:05:51.390
by piece carefully like
a ship in the bottle.

00:05:51.390 --> 00:05:54.470
Quite extraordinary engineering
achievement.

00:05:54.470 --> 00:05:57.210
Especially taking into account
that the detector should be

00:05:57.210 --> 00:05:59.990
able to measure particle
positions with accuracy

00:05:59.990 --> 00:06:02.040
exceeding 100 microns.

00:06:02.040 --> 00:06:04.330
So it should be assembled with
a very high precision.

00:06:06.950 --> 00:06:09.650
And that's how it looks fully
assembled in the cover.

00:06:15.850 --> 00:06:18.810
As I mentioned before, LHC and
ATLAS were built to explore

00:06:18.810 --> 00:06:22.030
high energy frontier of modern
particle physics to search for

00:06:22.030 --> 00:06:25.300
new phenomena that may occur at
these energies to probe the

00:06:25.300 --> 00:06:27.960
very fundamental
laws of nature.

00:06:27.960 --> 00:06:32.020
In particular, to search for
Higgs boson, this missing

00:06:32.020 --> 00:06:34.940
piece of standard model
elementary particles

00:06:34.940 --> 00:06:37.440
responsible for electroweak
symmetry break and again

00:06:37.440 --> 00:06:40.570
generation of masses of other
elementary particles.

00:06:40.570 --> 00:06:43.240
And Higgs' mechanism was
suggested about 40 years ago,

00:06:43.240 --> 00:06:45.720
and since then physicists
around the world were

00:06:45.720 --> 00:06:48.530
searching for proof
of its existence.

00:06:48.530 --> 00:06:52.160
And recent discovery of a new
product at the LHC which looks

00:06:52.160 --> 00:06:54.240
like Higgs boson culminated
this search.

00:07:02.580 --> 00:07:05.180
Less than a year ago, two
experiments at LHC ATLAS and

00:07:05.180 --> 00:07:08.580
CMS announced discovery
of the new particle.

00:07:08.580 --> 00:07:11.800
It was called a giant leap for
science, the most important

00:07:11.800 --> 00:07:15.100
discovery of the last decade
in the particle physics.

00:07:15.100 --> 00:07:17.580
It generated a lot of public
interest and a lot of media

00:07:17.580 --> 00:07:20.525
attention, with thousands
and thousands of print

00:07:20.525 --> 00:07:22.090
stories and TV spots.

00:07:22.090 --> 00:07:26.130
And most likely you have heard
about this discovery already.

00:07:26.130 --> 00:07:30.010
But you probably haven't
seen my next slide.

00:07:30.010 --> 00:07:36.640
I must note here that typically
we do not detect

00:07:36.640 --> 00:07:40.050
elementary particles like
Higgs directly.

00:07:40.050 --> 00:07:43.740
Like many other subatomic
particles, Higgs is too heavy,

00:07:43.740 --> 00:07:47.570
too unstable, and
too short lived.

00:07:47.570 --> 00:07:49.870
When produced, it immediately
decays into wider, more

00:07:49.870 --> 00:07:52.310
stable, more long-lived
particles that are actually

00:07:52.310 --> 00:07:55.400
registered by our detectors,
like ATLAS.

00:07:55.400 --> 00:07:58.060
And by the way, such decays
may be multi-staged, and

00:07:58.060 --> 00:08:01.170
[INAUDIBLE] stable object
decay [INAUDIBLE].

00:08:01.170 --> 00:08:02.820
After several of such
decays, you're

00:08:02.820 --> 00:08:05.410
getting final state particles.

00:08:05.410 --> 00:08:09.740
By using collected information
about decayed products of

00:08:09.740 --> 00:08:13.190
Higgs, what we call the final
state particles, we can

00:08:13.190 --> 00:08:15.720
nevertheless prove the existence
of parent particles,

00:08:15.720 --> 00:08:17.550
like Higgs.

00:08:17.550 --> 00:08:21.080
Indirectly, yes, but without
any doubt, just by using

00:08:21.080 --> 00:08:24.090
simple laws of conservation of
energy and momentum familiar

00:08:24.090 --> 00:08:27.380
from freshman physics 101.

00:08:27.380 --> 00:08:30.080
Whatever energy existed before
the particle decayed should be

00:08:30.080 --> 00:08:32.900
conserved after the decay and
mass equivalent to energy, as

00:08:32.900 --> 00:08:37.010
was pointed out by Einstein,
and mass of the decaying

00:08:37.010 --> 00:08:38.410
parent particle doesn't
disappear.

00:08:38.410 --> 00:08:40.360
It transformed to masses
[INAUDIBLE]

00:08:40.360 --> 00:08:42.500
the decay product to
other particles.

00:08:42.500 --> 00:08:45.470
So again, by using the
information collected about

00:08:45.470 --> 00:08:47.480
final state particles,
we can reconstruct

00:08:47.480 --> 00:08:49.200
the mass of the particle.

00:08:49.200 --> 00:08:51.180
And we measure many events.

00:08:51.180 --> 00:08:53.860
For each event, take all final
state particles that belong to

00:08:53.860 --> 00:08:55.040
a particular decay channel.

00:08:55.040 --> 00:08:58.640
We calculate effective mass of
such combination, and plug in

00:08:58.640 --> 00:09:02.400
a histogram of effective mass.

00:09:02.400 --> 00:09:05.610
And eventually we expect to see
a peak in that histogram

00:09:05.610 --> 00:09:08.350
that corresponds to the mass
of the parent particle.

00:09:08.350 --> 00:09:13.360
Of course, there may be other
particles that decay into the

00:09:13.360 --> 00:09:15.390
same final state, and you
will see a spectrum in

00:09:15.390 --> 00:09:17.370
distribution.

00:09:17.370 --> 00:09:19.840
But they should be at peak
corresponding to the mass of

00:09:19.840 --> 00:09:23.310
the particle you search for.

00:09:23.310 --> 00:09:25.710
There will be noise, but if
particle exists, there will be

00:09:25.710 --> 00:09:28.900
a signal simply because
of conservation

00:09:28.900 --> 00:09:30.150
of energy and momentum.

00:09:33.160 --> 00:09:35.810
And on this slide, you can see
how the discovery of the Higgs

00:09:35.810 --> 00:09:38.500
boson unfolded.

00:09:38.500 --> 00:09:40.910
Here, looking at the spectrum
of effective mass of

00:09:40.910 --> 00:09:42.510
[INAUDIBLE]

00:09:42.510 --> 00:09:43.850
detected by ATLAS.

00:09:43.850 --> 00:09:44.800
Decaying to [INAUDIBLE]

00:09:44.800 --> 00:09:47.860
is one of the predicted decay
channels for the Higgs boson.

00:09:47.860 --> 00:09:50.940
And as we were collecting
more and more data,

00:09:50.940 --> 00:09:53.800
the peak at 125 GV--

00:09:53.800 --> 00:09:56.600
and GV is a unit of mass used in
high energy physics-- just

00:09:56.600 --> 00:10:00.530
kept growing, indicating that
there is a new particle, very

00:10:00.530 --> 00:10:05.088
heavy, never seen before, clear
and beautiful peak.

00:10:07.960 --> 00:10:09.680
And of course, you
do search in many

00:10:09.680 --> 00:10:11.550
different decay channels.

00:10:11.550 --> 00:10:14.280
Higgs can decay into anything
that is not forbidden by

00:10:14.280 --> 00:10:15.700
conservation laws.

00:10:15.700 --> 00:10:17.950
In one event, it can decay into
gamma [INAUDIBLE], in

00:10:17.950 --> 00:10:21.420
another into [INAUDIBLE], and so
forth, and some events may

00:10:21.420 --> 00:10:24.060
have no Higgs in there.

00:10:24.060 --> 00:10:27.990
But it's the same Higgs, so in
every decay channel, you

00:10:27.990 --> 00:10:32.180
should expect a peak at the same
place, at the same mass

00:10:32.180 --> 00:10:35.050
in these various
decay channels.

00:10:35.050 --> 00:10:37.270
And you see the clear indication
that there is

00:10:37.270 --> 00:10:41.920
something at 125 GV.

00:10:41.920 --> 00:10:46.860
Now, let's talk about data
challenges of such analysis.

00:10:46.860 --> 00:10:49.130
Particles that we want to
discover and study are rare.

00:10:49.130 --> 00:10:51.580
That's why LHC runs at such
high energy and with such

00:10:51.580 --> 00:10:53.400
intense beams, almost a billion

00:10:53.400 --> 00:10:55.590
interactions per second.

00:10:55.590 --> 00:10:58.065
Even in this condition, the
probability to create Higgs

00:10:58.065 --> 00:10:59.320
boson is tiny.

00:10:59.320 --> 00:11:01.550
You would need to search for
one Higgs in more than a

00:11:01.550 --> 00:11:03.850
trillion events.

00:11:03.850 --> 00:11:06.720
And at high beam densities,
multiple collisions can occur

00:11:06.720 --> 00:11:10.360
in one beam crossing.

00:11:10.360 --> 00:11:14.660
This plot shows how such
event looks like.

00:11:14.660 --> 00:11:16.800
It's quite messy.

00:11:16.800 --> 00:11:19.270
That explains why you need such
a big detector, such high

00:11:19.270 --> 00:11:22.500
granularity, such strong
magnetic fields, so many

00:11:22.500 --> 00:11:26.160
channels of sensors, so many
channels of electronics,

00:11:26.160 --> 00:11:30.960
because you are searching
for something very rare.

00:11:30.960 --> 00:11:36.710
And that's probably for Google
Developers, at a familiar

00:11:36.710 --> 00:11:39.950
problem, your selectivity should
be very, very high,

00:11:39.950 --> 00:11:42.020
like one in several trillions.

00:11:42.020 --> 00:11:44.290
It's like looking for one
person's cells in the world

00:11:44.290 --> 00:11:50.140
population, of one needle
in 20 million haystacks.

00:11:50.140 --> 00:11:54.360
ATLAS is the quintessential
big data experiment.

00:11:54.360 --> 00:11:56.880
ATLAS detector generates
about one petabyte per

00:11:56.880 --> 00:11:59.580
second of raw data.

00:11:59.580 --> 00:12:01.220
No one can store it,
even Google.

00:12:01.220 --> 00:12:04.980
Most is filtered out in real
time by the trigger system.

00:12:04.980 --> 00:12:07.930
Interesting events are
recorded for further

00:12:07.930 --> 00:12:10.130
reconstruction analysis.

00:12:10.130 --> 00:12:13.870
And as of this year, we're
managing about 140 petabytes

00:12:13.870 --> 00:12:17.960
of data worldwide, and that's
distributed over

00:12:17.960 --> 00:12:19.450
100 computing centers.

00:12:19.450 --> 00:12:22.150
And that's actually not only
raw data, but the derived

00:12:22.150 --> 00:12:25.890
formats, the simulation,
about 50% of our data.

00:12:25.890 --> 00:12:29.970
The Monte Carlo simulation of
various properties, simulation

00:12:29.970 --> 00:12:31.760
of how the particle propagates
a detector

00:12:31.760 --> 00:12:34.830
iteration, things like that.

00:12:34.830 --> 00:12:38.030
And we expect that these data
rates will be only growing

00:12:38.030 --> 00:12:43.130
after LHC large shut down,
what's happening now in 2013,

00:12:43.130 --> 00:12:45.090
the data rate will be higher,
the energy of

00:12:45.090 --> 00:12:46.360
collision will be higher.

00:12:46.360 --> 00:12:49.610
We expect the influx of already
filtered data on the

00:12:49.610 --> 00:12:53.430
level of 40 petabytes
per year.

00:12:53.430 --> 00:12:55.760
And we have to deliver this
data to thousands of

00:12:55.760 --> 00:12:57.010
physicists worldwide.

00:12:59.210 --> 00:13:01.830
Now, a little bit about
ATLAS computing.

00:13:01.830 --> 00:13:04.570
ATLAS uses grid computing
paradigm for organization of

00:13:04.570 --> 00:13:06.190
distributed resources.

00:13:06.190 --> 00:13:08.410
Job distribution is
managed by PanDA

00:13:08.410 --> 00:13:09.760
Workload Management System.

00:13:09.760 --> 00:13:11.830
PanDA stands for production
analysis.

00:13:11.830 --> 00:13:14.340
Think of it as grid
metascheduler.

00:13:14.340 --> 00:13:16.810
PanDA was developed by ATLAS,
and now it manages

00:13:16.810 --> 00:13:19.520
distribution of computing jobs
for hundreds of computing

00:13:19.520 --> 00:13:24.060
sites, about 100,000 cores, 100
million jobs per year, and

00:13:24.060 --> 00:13:27.630
serving thousands of users.

00:13:27.630 --> 00:13:30.690
Organizationally, it's a grid
set up in a tiered system,

00:13:30.690 --> 00:13:32.460
highly hierarchical
with the tier zero

00:13:32.460 --> 00:13:34.090
central located at CERN.

00:13:34.090 --> 00:13:39.330
Tier zero center receives the
raw data from the ATLAS

00:13:39.330 --> 00:13:42.530
detectors, performs first pass
analysis, and then distributes

00:13:42.530 --> 00:13:44.730
among other tiers.

00:13:44.730 --> 00:13:48.230
Tier one is typically national
based large centers.

00:13:48.230 --> 00:13:50.880
One of them is the
Brookhaven Lab.

00:13:50.880 --> 00:13:54.260
And each tier one facility then
distributors derived data

00:13:54.260 --> 00:13:56.820
to tier two computing facilities
that provides data

00:13:56.820 --> 00:13:59.720
storage and processing
capabilities for more in depth

00:13:59.720 --> 00:14:00.970
end user analysis.

00:14:04.080 --> 00:14:06.530
This plot shows the distribution
of running jobs,

00:14:06.530 --> 00:14:07.890
both Monte Carlo simulation--

00:14:07.890 --> 00:14:09.560
we call them production jobs--

00:14:09.560 --> 00:14:12.100
on the ATLAS grid and in
analysis jobs on the ATLAS

00:14:12.100 --> 00:14:13.630
grid for the past year.

00:14:13.630 --> 00:14:18.310
We run about 100,000 cores
worldwide, processing 150,000

00:14:18.310 --> 00:14:19.860
jobs per day.

00:14:19.860 --> 00:14:23.520
It's clear that all available
computational resources are

00:14:23.520 --> 00:14:26.370
fully stressed and utilized.

00:14:26.370 --> 00:14:29.560
And on this plot, you see the
distribution of pending jobs,

00:14:29.560 --> 00:14:32.490
submitted but waiting for
execution on ATLAS grid for

00:14:32.490 --> 00:14:33.530
the past year.

00:14:33.530 --> 00:14:35.680
You can see that the job
submission pattern is very

00:14:35.680 --> 00:14:37.210
uneven in time.

00:14:37.210 --> 00:14:39.550
Spikes in demand usually happen
before major physics

00:14:39.550 --> 00:14:42.420
conference or during
data reprocessing.

00:14:42.420 --> 00:14:44.770
And demand can exceed available
computational

00:14:44.770 --> 00:14:47.000
resources by more than an
order of magnitude.

00:14:47.000 --> 00:14:50.480
Lack of resources slows down
the pace of scientific

00:14:50.480 --> 00:14:53.560
discovery, and that's why ATLAS
is interested in cloud

00:14:53.560 --> 00:14:58.760
computing, and, in particular,
in public cloud resources.

00:14:58.760 --> 00:15:01.750
A couple of years ago, ATLAS
set up cloud computing R&amp;D

00:15:01.750 --> 00:15:04.020
project to explore
virtualization and cloud

00:15:04.020 --> 00:15:06.700
computing primarily as a
tool to cope with peak

00:15:06.700 --> 00:15:07.980
loads on the grid.

00:15:07.980 --> 00:15:10.910
We wanted to learn how to use
public and private clouds in

00:15:10.910 --> 00:15:13.810
typical ATLAS computational
scenarios.

00:15:13.810 --> 00:15:16.580
Since then, we gained experience
with many cloud

00:15:16.580 --> 00:15:19.420
platforms, like Amazon,
[INAUDIBLE]

00:15:19.420 --> 00:15:22.120
consortium of European cloud
providers, future grid

00:15:22.120 --> 00:15:25.740
[INAUDIBLE] academic clouds in
US and Canada and many others.

00:15:25.740 --> 00:15:29.510
We also explored private and
hybrid cloud configuration

00:15:29.510 --> 00:15:33.240
based on OpenStack, cloud stack
open nebula, and others.

00:15:33.240 --> 00:15:35.710
And now our latest project was
on Google Compute Engine, and

00:15:35.710 --> 00:15:38.510
I'll talk about it
now in detail.

00:15:38.510 --> 00:15:41.510
We were invited to participate
in Google Compute Engine trial

00:15:41.510 --> 00:15:43.610
period in August 2012.

00:15:43.610 --> 00:15:46.320
And we were immediately
attracted by modern hardware,

00:15:46.320 --> 00:15:49.880
powerful API, and competitive
pricing.

00:15:49.880 --> 00:15:53.310
And this is Google, after all.

00:15:53.310 --> 00:15:55.300
At the beginning, we were
frustrated that none of the

00:15:55.300 --> 00:15:57.695
tools that we had used before
supported Google Compute

00:15:57.695 --> 00:16:00.130
Engine, so initial a lot of
manual labor and image

00:16:00.130 --> 00:16:03.670
building cluster management was
needed since we couldn't

00:16:03.670 --> 00:16:07.000
reuse our standard tools.

00:16:07.000 --> 00:16:09.520
We're glad to see here at Google
I/O that this situation

00:16:09.520 --> 00:16:12.590
is changing and many tools are
supporting Google Compute

00:16:12.590 --> 00:16:14.310
Engine now.

00:16:14.310 --> 00:16:18.480
But Google engineers were very
helpful in helping us with

00:16:18.480 --> 00:16:21.740
initial setup and debugging the
problems and explaining to

00:16:21.740 --> 00:16:24.990
us features of the Google Cloud
platform implementation.

00:16:24.990 --> 00:16:27.780
And also Google was very
gracious in providing more

00:16:27.780 --> 00:16:30.320
resources than the initial
trial quarter so we could

00:16:30.320 --> 00:16:34.650
start working on the larger
scale cluster.

00:16:34.650 --> 00:16:38.110
We wanted to try several ATLAS
computational scenarios, high

00:16:38.110 --> 00:16:40.990
performance analysis clusters
like PROOF.

00:16:40.990 --> 00:16:43.450
We wanted to learn about storage
and data management of

00:16:43.450 --> 00:16:45.970
the cloud, in particular
utilizing XRootD technology

00:16:45.970 --> 00:16:49.450
for storage aggregation,
Ephemeral Storage aggregation,

00:16:49.450 --> 00:16:51.480
and federation.

00:16:51.480 --> 00:16:54.640
We also wanted to try a life
scale Monte Carlo production

00:16:54.640 --> 00:16:57.350
simulation of the cloud using
PanDA Workload Management

00:16:57.350 --> 00:17:01.760
System, as well as some other
smaller projects.

00:17:01.760 --> 00:17:03.570
So let me talk about
PanDA [INAUDIBLE]

00:17:03.570 --> 00:17:05.520
on Google Compute Engine.

00:17:05.520 --> 00:17:08.730
Google agreed to allocate
additional resources for ATLAS

00:17:08.730 --> 00:17:12.280
at the tune of about five
million core hours.

00:17:12.280 --> 00:17:20.130
Resources were organized as
HTCondor PanDA queue, and that

00:17:20.130 --> 00:17:22.569
allows for transparent inclusion
of the cloud

00:17:22.569 --> 00:17:25.000
resource into ATLAS
computational grid.

00:17:25.000 --> 00:17:27.720
Google Compute Engine looks as a
part of the ATLAS grid, just

00:17:27.720 --> 00:17:29.180
like another grid site.

00:17:29.180 --> 00:17:30.770
Very transparent.

00:17:30.770 --> 00:17:32.970
It was intended to run CPU
intensive Monte Carlo

00:17:32.970 --> 00:17:36.280
simulation, and the idea was
to try to have a production

00:17:36.280 --> 00:17:38.360
type of run on Google
Compute Engine.

00:17:38.360 --> 00:17:40.730
And the system was delivered
to ATLAS as a production

00:17:40.730 --> 00:17:44.340
resource, not as R&amp;D platform.

00:17:44.340 --> 00:17:46.690
We ran for about eight weeks.

00:17:46.690 --> 00:17:48.650
Two weeks were planned
for start up.

00:17:48.650 --> 00:17:53.130
And we had very stable
running.

00:17:53.130 --> 00:17:55.020
The Google Compute Engine
was rock solid.

00:17:55.020 --> 00:17:57.240
We had a few problems, and most
of them were on the ATLAS

00:17:57.240 --> 00:18:00.320
side, were on computational
intensive job, not much I/O,

00:18:00.320 --> 00:18:02.670
for this particular workload.

00:18:02.670 --> 00:18:04.430
These were physics [INAUDIBLE]
generators [INAUDIBLE]

00:18:04.430 --> 00:18:07.110
fast detector simulation, full
detector simulation.

00:18:07.110 --> 00:18:09.230
Produced data was automatically
shipped to ATLAS

00:18:09.230 --> 00:18:11.760
grid storage for further
processing and analysis.

00:18:11.760 --> 00:18:14.400
That was really usable data.

00:18:14.400 --> 00:18:20.340
We completed about 450,000 jobs
generated and processed

00:18:20.340 --> 00:18:23.510
about more than 200
million events.

00:18:23.510 --> 00:18:25.970
Very good performance, very
comparable to performance of

00:18:25.970 --> 00:18:28.280
ATLAS grid.

00:18:28.280 --> 00:18:30.680
This plot shows the job
failure rate as

00:18:30.680 --> 00:18:31.910
a function of time.

00:18:31.910 --> 00:18:34.580
Most failures occurred during
start up and scale up period

00:18:34.580 --> 00:18:35.990
as we expected.

00:18:35.990 --> 00:18:38.130
Most problems were actually
on the ATLAS side.

00:18:38.130 --> 00:18:40.680
No failures were due to the
Google Compute Engine.

00:18:40.680 --> 00:18:44.000
Very stable performance
of the platform.

00:18:44.000 --> 00:18:46.960
This plot shows distribution of
finished and failed jobs.

00:18:46.960 --> 00:18:49.420
Green histogram is for finished
jobs, the pink one

00:18:49.420 --> 00:18:50.080
for failed ones.

00:18:50.080 --> 00:18:51.170
Again, very good performance.

00:18:51.170 --> 00:18:53.000
We reached high rates
of production,

00:18:53.000 --> 00:18:55.220
50,000 jobs per day.

00:18:55.220 --> 00:18:56.470
Good number.

00:18:58.230 --> 00:19:01.590
We also tried PROOF clusters.

00:19:01.590 --> 00:19:04.310
And PROOF is implementation of
MapReduce paradigm based on

00:19:04.310 --> 00:19:05.605
the ROOT framework.

00:19:05.605 --> 00:19:09.880
ROOT framework for data analysis
was developed by high

00:19:09.880 --> 00:19:12.630
energy nuclear physics
community, developed and

00:19:12.630 --> 00:19:14.846
supported by the ROOT
team at CERN.

00:19:14.846 --> 00:19:17.370
It's written in C++, free,
open source, very high

00:19:17.370 --> 00:19:18.510
performance.

00:19:18.510 --> 00:19:21.840
And we'll have a slide with the
pointers to the system.

00:19:21.840 --> 00:19:24.330
And PROOF allows for efficient
aggregation and use of

00:19:24.330 --> 00:19:27.700
distributed computing resources
for data intensive

00:19:27.700 --> 00:19:29.940
event based analyses.

00:19:29.940 --> 00:19:32.040
It uses XRootD for clustering,
storage

00:19:32.040 --> 00:19:34.200
aggregation, data discovery.

00:19:34.200 --> 00:19:37.600
Xroot is well suited for
ephemeral storage aggregation

00:19:37.600 --> 00:19:40.390
into one name space.

00:19:40.390 --> 00:19:42.910
And PROOF cluster also
can be federated.

00:19:42.910 --> 00:19:46.430
So on this slide, you can see
typical architecture of the

00:19:46.430 --> 00:19:50.750
PROOF clusters with the super
master, which serves as users'

00:19:50.750 --> 00:19:51.840
single point of entry.

00:19:51.840 --> 00:19:55.680
System complex is completely
hidden from users.

00:19:55.680 --> 00:19:59.500
And it allows, for example,
for interactive analysis,

00:19:59.500 --> 00:20:02.380
where you can send the query and
in real time see how the

00:20:02.380 --> 00:20:05.350
particular histogram just
grows and changes.

00:20:05.350 --> 00:20:07.250
One of the distinctive
feature.

00:20:07.250 --> 00:20:09.820
But it also allows batch
analysis, and you can connect

00:20:09.820 --> 00:20:12.170
to the system, look at the
histogram, and disconnect,

00:20:12.170 --> 00:20:14.610
then connect again, look again,
see how it's going.

00:20:17.380 --> 00:20:19.305
And here's another view of
the typical structure

00:20:19.305 --> 00:20:20.420
of the PROOF cluster.

00:20:20.420 --> 00:20:22.750
PROOF, as I mentioned,
works very well with

00:20:22.750 --> 00:20:24.380
XRootD based storage.

00:20:24.380 --> 00:20:28.260
XRootD provides data discovery
and hails PROOF exploits this

00:20:28.260 --> 00:20:29.510
data locality [INAUDIBLE].

00:20:32.140 --> 00:20:34.530
Access to Google Compute Engine
allowed us to build and

00:20:34.530 --> 00:20:38.520
test large PROOF clusters, up
to 1,000 workers, something

00:20:38.520 --> 00:20:40.410
that is very difficult to
do in the real domain.

00:20:40.410 --> 00:20:44.360
We just don't have resources
to do this kind of test and

00:20:44.360 --> 00:20:45.610
see how it all scales.

00:20:45.610 --> 00:20:48.700
And the figure here shows
scalability test for 500

00:20:48.700 --> 00:20:50.690
worker cluster.

00:20:50.690 --> 00:20:52.700
And it shows very good
performance and pretty good

00:20:52.700 --> 00:20:53.950
scalability.

00:20:57.670 --> 00:21:01.220
We also look at the storage
performance of

00:21:01.220 --> 00:21:02.470
Google Compute Engine.

00:21:02.470 --> 00:21:09.210
And this plot shows the
performance in the typical

00:21:09.210 --> 00:21:14.140
ATLAS analysis scenario of
ephemeral store, persistent

00:21:14.140 --> 00:21:17.890
store, and here compare it to
what happened if you have all

00:21:17.890 --> 00:21:19.970
the data in the memory.

00:21:19.970 --> 00:21:22.340
And know that ephemeral disk
has better single worker

00:21:22.340 --> 00:21:27.430
performance, but the persistent
storage shows

00:21:27.430 --> 00:21:29.750
better scaling and better
peak performance.

00:21:29.750 --> 00:21:32.920
And of course, in this situation
it's clear that the

00:21:32.920 --> 00:21:34.520
RAID is needed for better
performance.

00:21:37.780 --> 00:21:43.030
We also look at the data
transfer capabilities.

00:21:43.030 --> 00:21:47.180
And this plot shows the data
transfer from our own

00:21:47.180 --> 00:21:51.570
Federated ATLAS Xroot to Google
Compute Engine in

00:21:51.570 --> 00:21:54.480
extreme copy mode, which is sort
of similar to bitTorrent.

00:21:54.480 --> 00:21:57.180
If you have several copies,
you can copy them in

00:21:57.180 --> 00:21:59.530
multisource, multistream mode.

00:21:59.530 --> 00:22:03.140
And Google Compute Engine Xroot
cluster using ephemeral

00:22:03.140 --> 00:22:07.120
storage was used for this test,
and average transfer

00:22:07.120 --> 00:22:11.050
rate was about 60 megabyte
per second.

00:22:11.050 --> 00:22:14.190
And this is very good taking
into account that this is over

00:22:14.190 --> 00:22:16.315
completely unmanaged
public network.

00:22:16.315 --> 00:22:18.090
We have no control there.

00:22:18.090 --> 00:22:20.950
But still, this is single
client performance.

00:22:20.950 --> 00:22:24.540
Many of the plots that I'm
showing, they're single client

00:22:24.540 --> 00:22:26.810
because the system that we're
running, they scale very well

00:22:26.810 --> 00:22:28.140
when you add resources.

00:22:28.140 --> 00:22:31.360
So you expect clustering and
scaling up, so what you are

00:22:31.360 --> 00:22:32.390
really interested in is how the

00:22:32.390 --> 00:22:34.710
building blocks are running.

00:22:34.710 --> 00:22:37.660
And here is single client, but
you can run on multiple VMs,

00:22:37.660 --> 00:22:38.875
multiple clients simultaneously

00:22:38.875 --> 00:22:41.720
and stream them over.

00:22:41.720 --> 00:22:44.590
And we're also thinking about
dedicated network peering

00:22:44.590 --> 00:22:46.680
between ATLAS network

00:22:46.680 --> 00:22:48.860
infrastructure and Google storage.

00:22:48.860 --> 00:22:52.400
So that will give us much
higher, 100 gigabyte per

00:22:52.400 --> 00:22:55.750
second performance, than we
would be able to control it

00:22:55.750 --> 00:22:57.660
and do manageable transfers.

00:22:57.660 --> 00:23:00.770
But this is if you just want to
run and bring the data in.

00:23:00.770 --> 00:23:02.020
It's doable.

00:23:04.340 --> 00:23:08.230
And now we'll talk about Xroot,
this clustering and

00:23:08.230 --> 00:23:12.130
storage clustering technology
that Andy was a creator and a

00:23:12.130 --> 00:23:15.550
driving force behind this
project, and still is.

00:23:15.550 --> 00:23:16.760
And Andy, please.

00:23:16.760 --> 00:23:20.826
[APPLAUSE]

00:23:20.826 --> 00:23:23.630
ANDREW HANUSHEVSKY:
[INAUDIBLE].

00:23:23.630 --> 00:23:27.780
So I want to take a quick trip
through a bit of technology we

00:23:27.780 --> 00:23:32.280
developed a while back but wound
up being absolutely an

00:23:32.280 --> 00:23:34.600
ideal match with the Google
Compute Engine.

00:23:34.600 --> 00:23:35.930
And that's XRootD.

00:23:35.930 --> 00:23:37.815
Now you'll say, never
heard of it.

00:23:37.815 --> 00:23:40.710
Well, it's a system for scalable
cluster data access.

00:23:40.710 --> 00:23:41.850
And you say, that's nice.

00:23:41.850 --> 00:23:43.080
What is it really?

00:23:43.080 --> 00:23:45.500
Well, what we have is an
implementation of two

00:23:45.500 --> 00:23:49.780
services, one an XRoot service
that provides access to data.

00:23:49.780 --> 00:23:52.400
So you would take one of these
demons and drop it on every

00:23:52.400 --> 00:23:55.510
node where you have data that
you need to access.

00:23:55.510 --> 00:23:57.380
Now, there's a companion
service.

00:23:57.380 --> 00:23:58.760
It's called CMSD.

00:23:58.760 --> 00:24:01.050
Stands for Cluster Management
Services.

00:24:01.050 --> 00:24:04.540
And that's used for data
discovery as well as routing

00:24:04.540 --> 00:24:08.150
clients to where the data is
and server clustering.

00:24:08.150 --> 00:24:11.880
So these are separate, but we
normally use them together.

00:24:11.880 --> 00:24:14.710
And so for the purposes of this
talk, we'll always talk

00:24:14.710 --> 00:24:16.700
about this particular pair.

00:24:16.700 --> 00:24:19.120
Now I want to emphasize
that this system

00:24:19.120 --> 00:24:21.030
is not a file system.

00:24:21.030 --> 00:24:24.300
People are actually using the
system to cluster existing

00:24:24.300 --> 00:24:25.030
file systems.

00:24:25.030 --> 00:24:30.570
So we have people basically
taking HDFS, GPFS, Lustre, and

00:24:30.570 --> 00:24:34.580
building one big giant cluster
out of that and having uniform

00:24:34.580 --> 00:24:36.530
data access.

00:24:36.530 --> 00:24:40.300
So while it's not a file system,
it's also not just for

00:24:40.300 --> 00:24:41.890
file systems.

00:24:41.890 --> 00:24:45.780
We have an experiment that's
using this as a framework to

00:24:45.780 --> 00:24:51.160
cluster MySQL tables across
hundreds of MySQL servers so

00:24:51.160 --> 00:24:55.400
they could do massively
parallel queries.

00:24:55.400 --> 00:24:59.220
So the idea is that if we don't
have a plug-in for your

00:24:59.220 --> 00:25:02.630
data, and you can write a
plug-in for your data, then

00:25:02.630 --> 00:25:03.850
you can cluster it.

00:25:03.850 --> 00:25:06.560
And so I'd like to show you what
that plug-in architecture

00:25:06.560 --> 00:25:07.620
looks like.

00:25:07.620 --> 00:25:11.150
First, we start off with
a protocol driver.

00:25:11.150 --> 00:25:14.160
You can plug-in any number of
protocols into that driver.

00:25:14.160 --> 00:25:16.580
In our particular case,
we want to do XRoot.

00:25:16.580 --> 00:25:19.410
So let's take a look as we
plug stuff together.

00:25:19.410 --> 00:25:21.650
So plug in your protocol.

00:25:21.650 --> 00:25:24.140
Plug in your authentication
framework.

00:25:24.140 --> 00:25:27.130
Plug in your logical file
system, your authorization

00:25:27.130 --> 00:25:30.190
framework, your storage
system, and then your

00:25:30.190 --> 00:25:31.220
clustering.

00:25:31.220 --> 00:25:34.550
And all of a sudden, you've
built up a clustering system

00:25:34.550 --> 00:25:38.050
for a particular kind
of application.

00:25:38.050 --> 00:25:42.150
So let's get back to what the
data access problem is.

00:25:42.150 --> 00:25:44.140
It's the High Energy
Physics regime.

00:25:44.140 --> 00:25:48.680
Yeah, they do nasty things,
like start up thousands of

00:25:48.680 --> 00:25:50.150
parallel jobs.

00:25:50.150 --> 00:25:53.480
And they all start up pretty
much at the same time.

00:25:53.480 --> 00:25:55.490
And if that weren't bad enough,
each one of those

00:25:55.490 --> 00:25:58.100
opens 10 or more files.

00:25:58.100 --> 00:26:00.200
Pretty much a profile
[INAUDIBLE] denial service

00:26:00.200 --> 00:26:02.320
attack, if you ask me.

00:26:02.320 --> 00:26:05.820
But basically, you have
to handle that.

00:26:05.820 --> 00:26:09.250
To make matters worse, the
particular framework that they

00:26:09.250 --> 00:26:13.320
use is small block sparse random
I/O. What do we mean by

00:26:13.320 --> 00:26:14.430
small block?

00:26:14.430 --> 00:26:16.900
Average read size about 4K.

00:26:16.900 --> 00:26:18.530
What do we mean by sparse?

00:26:18.530 --> 00:26:22.110
Well, they have like a
10-gigabyte file, and you'll

00:26:22.110 --> 00:26:25.780
be lucky if they read
half of it randomly.

00:26:25.780 --> 00:26:27.670
So pretty challenging.

00:26:27.670 --> 00:26:31.100
So we adopted a synergistic
solution to try

00:26:31.100 --> 00:26:33.740
to attack this problem.

00:26:33.740 --> 00:26:37.220
And you'll see what we
mean by synergy here.

00:26:37.220 --> 00:26:39.820
First, we wanted to minimize
the latency.

00:26:39.820 --> 00:26:42.600
And the key elements there
were using a paralyzable

00:26:42.600 --> 00:26:46.700
protocol, file sessions, a
sticky thread model, and

00:26:46.700 --> 00:26:50.970
lockless I/O. Next, we wanted
to minimize hardware

00:26:50.970 --> 00:26:54.360
requirements, so short code
paths, compact data

00:26:54.360 --> 00:26:58.900
structures, members that are
cognizant of what the memory

00:26:58.900 --> 00:27:00.835
cache is so it's friendly
to the memory cache.

00:27:03.340 --> 00:27:06.300
We don't actually move data
around in the server, and we

00:27:06.300 --> 00:27:08.810
don't do crossthread
data sharing.

00:27:08.810 --> 00:27:12.280
So in the end, we wind up with
less than seven microseconds

00:27:12.280 --> 00:27:16.450
overhead on a two gigahertz CPU
per I/O request and less

00:27:16.450 --> 00:27:18.870
than a 100-megabyte
memory footprint.

00:27:18.870 --> 00:27:20.730
So pretty compact.

00:27:20.730 --> 00:27:23.610
Now those two are synergistic
in the sense that if you

00:27:23.610 --> 00:27:27.000
minimize latency, you'll see
opportunities to minimize

00:27:27.000 --> 00:27:28.590
hardware requirements.

00:27:28.590 --> 00:27:32.500
And if you start minimizing
hardware requirements, you see

00:27:32.500 --> 00:27:35.570
opportunities to minimize
latency.

00:27:35.570 --> 00:27:39.570
The next thing we wanted to do
was minimize human cost.

00:27:39.570 --> 00:27:40.980
So what does that mean?

00:27:40.980 --> 00:27:45.110
Well for us that meant a single
configuration file, no

00:27:45.110 --> 00:27:47.430
database requirement.

00:27:47.430 --> 00:27:49.620
You can add and delete
nodes at will.

00:27:49.620 --> 00:27:50.200
We don't care.

00:27:50.200 --> 00:27:51.550
You don't have to restart
anything.

00:27:51.550 --> 00:27:54.220
You just add stuff
and delete stuff.

00:27:54.220 --> 00:27:58.530
And you use your natural file
system administration tools to

00:27:58.530 --> 00:28:02.140
administer this thing because
that's what you know.

00:28:02.140 --> 00:28:06.750
Now, that together, we wanted
to maximize scaling.

00:28:06.750 --> 00:28:09.510
And those two are actually
synergistic.

00:28:09.510 --> 00:28:11.640
You can't maximize scaling
unless you

00:28:11.640 --> 00:28:13.300
minimize the human cost.

00:28:13.300 --> 00:28:15.700
And you see immediately
opportunities between those

00:28:15.700 --> 00:28:18.820
two as you attack both
of those problems.

00:28:18.820 --> 00:28:21.570
So let's talk about scaling.

00:28:21.570 --> 00:28:24.340
We used B64 trees for scaling.

00:28:24.340 --> 00:28:29.060
And we'll basically scale this
using this pair, XRootD CMS.

00:28:29.060 --> 00:28:32.920
So let's start out with
a single node.

00:28:32.920 --> 00:28:35.770
And then what we'll do
is we'll add 64 data

00:28:35.770 --> 00:28:39.120
servers to that node.

00:28:39.120 --> 00:28:42.440
Gee, looks like a really dinky
cluster, doesn't it?

00:28:42.440 --> 00:28:46.310
So what we do, it's a B64 tree,
well, we'll add 64 data

00:28:46.310 --> 00:28:48.940
servers to each one
of those 64 nodes.

00:28:48.940 --> 00:28:52.170
Well now we have a
cluster of 4,096.

00:28:52.170 --> 00:28:54.340
Decent, but not very big.

00:28:54.340 --> 00:28:56.350
We can just repeat this step.

00:28:56.350 --> 00:29:02.010
And now we've accomplished a
cluster of 262,000 servers.

00:29:02.010 --> 00:29:06.640
Well, that looks big, but what
if we iterate one more time?

00:29:06.640 --> 00:29:09.200
Now we've constructed
a cluster of 16

00:29:09.200 --> 00:29:12.240
million data servers.

00:29:12.240 --> 00:29:15.340
And you look at that and
say, hm, that looks

00:29:15.340 --> 00:29:17.860
like a house of cards.

00:29:17.860 --> 00:29:21.130
Well, not really because we can
replicate the head node,

00:29:21.130 --> 00:29:24.530
geographically distribute it,
and now we have quite a bit of

00:29:24.530 --> 00:29:26.660
redundancy in the system.

00:29:26.660 --> 00:29:29.220
So let's add some names
to these things.

00:29:29.220 --> 00:29:30.770
The head node is the manager.

00:29:30.770 --> 00:29:32.800
Intermediate nodes
are supervisors.

00:29:32.800 --> 00:29:35.550
And data servers always
are at the leaf nodes.

00:29:35.550 --> 00:29:38.150
So remember that.

00:29:38.150 --> 00:29:39.850
Now, this is a B tree.

00:29:39.850 --> 00:29:42.300
We can split it up
any way we want.

00:29:42.300 --> 00:29:44.910
And this works great for
basically doing cloud

00:29:44.910 --> 00:29:48.650
deployment because in fact
part of that three can be

00:29:48.650 --> 00:29:52.920
inside the GCE, another part
can be in a private cloud,

00:29:52.920 --> 00:29:55.990
another part in a private
cluster, and we can piece that

00:29:55.990 --> 00:30:00.070
all together to make it look
like one big cluster.

00:30:00.070 --> 00:30:02.140
Now, you look at that
and say great.

00:30:02.140 --> 00:30:04.360
But I have 16 million nodes.

00:30:04.360 --> 00:30:05.660
How do I get to the data?

00:30:05.660 --> 00:30:08.440
I can't keep track of
16 million things.

00:30:08.440 --> 00:30:13.580
Well, in fact, let's take
a look at how we do it.

00:30:13.580 --> 00:30:17.660
So when you've got a big cluster
like this, you really

00:30:17.660 --> 00:30:20.430
have to adopt a brand
new strategy.

00:30:20.430 --> 00:30:22.780
So we have a client.

00:30:22.780 --> 00:30:25.670
He gets an open or doesn't
open to the head node.

00:30:25.670 --> 00:30:29.440
And here we're going to assume
that the head node knows

00:30:29.440 --> 00:30:32.610
nothing about the file
the client needs.

00:30:32.610 --> 00:30:34.160
So what does it have to do?

00:30:34.160 --> 00:30:37.270
It has to find the route
to the file.

00:30:37.270 --> 00:30:39.430
And it'll accomplish that
by just doing a directed

00:30:39.430 --> 00:30:41.370
broadcast parallel query.

00:30:41.370 --> 00:30:44.970
And that'll set up the routing
tables in this tree.

00:30:44.970 --> 00:30:48.880
After that, the head node can
then redirect the client to

00:30:48.880 --> 00:30:50.340
the next subtree.

00:30:50.340 --> 00:30:52.250
That subtree in turns
redirects the

00:30:52.250 --> 00:30:54.970
client to the leaf node.

00:30:54.970 --> 00:30:58.280
Now, this is the only scalable
way of doing it because you

00:30:58.280 --> 00:30:59.940
don't have to keep track
of anything but

00:30:59.940 --> 00:31:01.160
the immediate route.

00:31:01.160 --> 00:31:04.820
Pretty much in how the
internet works.

00:31:04.820 --> 00:31:08.030
So the bottom line here, this
is a simple, flexible, and

00:31:08.030 --> 00:31:09.730
effective system.

00:31:09.730 --> 00:31:13.170
I want to say it's simple, but
the devil's in the details.

00:31:13.170 --> 00:31:16.890
We have a paper we presented
in IPDS that will give you

00:31:16.890 --> 00:31:20.760
some of the algorithms
we have to use.

00:31:20.760 --> 00:31:24.820
And you can actually get the
paper at XRootD.org.

00:31:24.820 --> 00:31:26.440
It's LGPL open-source.

00:31:26.440 --> 00:31:28.680
You can download it, run it.

00:31:28.680 --> 00:31:30.420
It's managed by the XRootD
collaboration.

00:31:30.420 --> 00:31:33.750
That collaboration is
open to new members.

00:31:33.750 --> 00:31:39.030
And I do encourage you
to go to XRootD.org.

00:31:39.030 --> 00:31:41.230
So now Sergey, finish it up.

00:31:45.640 --> 00:31:48.205
DR. SERGEY PANITKIN: And
let me summarize.

00:31:50.820 --> 00:31:52.840
All in all, we had great
experience with

00:31:52.840 --> 00:31:54.450
Google Compute Engine.

00:31:54.450 --> 00:31:57.180
We tested several computational
scenarios on

00:31:57.180 --> 00:32:01.960
that platform, PROOF, XRootD
clusters, PanDA batch

00:32:01.960 --> 00:32:06.990
clusters, and ran large scale
Monte Carlo production.

00:32:06.990 --> 00:32:10.180
We think that Google Compute
Engine is modern cloud

00:32:10.180 --> 00:32:13.480
infrastructure that can serve
as a stable high performance

00:32:13.480 --> 00:32:16.020
platform for scientific
computing.

00:32:16.020 --> 00:32:20.110
And tools developed by the LHC
community may be of some

00:32:20.110 --> 00:32:22.200
interest to the broader
community of developers

00:32:22.200 --> 00:32:24.530
working on Google Compute
Engine and

00:32:24.530 --> 00:32:27.780
other compute engine.

00:32:27.780 --> 00:32:30.122
And thank you very much.

00:32:30.122 --> 00:32:37.840
[APPLAUSE]

00:32:37.840 --> 00:32:39.940
GARRICK EVANS: Thanks a lot.

00:32:39.940 --> 00:32:43.080
So we're done, and we'd be happy
to take any questions

00:32:43.080 --> 00:32:45.470
that you guys have
at the time.

00:32:45.470 --> 00:32:46.870
Please come up to the mics.

00:32:52.040 --> 00:32:53.930
No?

00:32:53.930 --> 00:32:54.950
OK, well, thanks.

00:32:54.950 --> 00:32:56.790
Hope you had a great I/O.

