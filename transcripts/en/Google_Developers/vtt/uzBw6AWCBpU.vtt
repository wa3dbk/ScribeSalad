WEBVTT
Kind: captions
Language: en

00:00:01.510 --> 00:00:02.330
TIM MURRAY: All right.

00:00:02.330 --> 00:00:03.310
Good morning, everyone.

00:00:03.310 --> 00:00:04.350
My name is Tim Murray.

00:00:04.350 --> 00:00:07.370
I'm an engineer at Google on
the RenderScript team.

00:00:07.370 --> 00:00:09.860
And we're here to talk about
writing high performance

00:00:09.860 --> 00:00:13.080
applications with
RenderScript.

00:00:13.080 --> 00:00:17.420
So first, I'd like to talk
about GPUs in general.

00:00:17.420 --> 00:00:21.270
So in the past seven years or
so, GPUs have become useful

00:00:21.270 --> 00:00:23.400
for a lot more applications
than

00:00:23.400 --> 00:00:25.450
just traditional graphics.

00:00:25.450 --> 00:00:28.300
And the reason for this is that
they have a lot of flops

00:00:28.300 --> 00:00:31.540
and a lot of memory bandwidth
verses CPUs.

00:00:31.540 --> 00:00:34.340
And so they're really good at
data parallel tasks that

00:00:34.340 --> 00:00:38.350
happen to look sort
of like graphics.

00:00:38.350 --> 00:00:42.050
And the market where this has
had the most adoption is in

00:00:42.050 --> 00:00:44.930
high performance computing,
supercomputing, oil and gas,

00:00:44.930 --> 00:00:46.170
things like that.

00:00:46.170 --> 00:00:49.670
The current number one machine
on the top 500 is primarily

00:00:49.670 --> 00:00:51.920
GPU based, for example.

00:00:51.920 --> 00:00:54.540
And now in mobile, we're
starting to see these

00:00:54.540 --> 00:00:58.840
programmable GPUs arrive
and become useful.

00:00:58.840 --> 00:01:01.930
But before we go into mobile,
I want to talk a little bit

00:01:01.930 --> 00:01:05.300
about a traditional
desktop, or server

00:01:05.300 --> 00:01:07.760
architecture with a GPU.

00:01:07.760 --> 00:01:10.980
So the first thing you'll notice
is that in terms of raw

00:01:10.980 --> 00:01:14.030
computational throughput,
floating point, memory

00:01:14.030 --> 00:01:18.020
bandwidth, things like that,
the GPU dominates the CPU.

00:01:18.020 --> 00:01:19.880
It's got four times the
memory bandwidth.

00:01:19.880 --> 00:01:24.410
It's got 10 to 15 times the
flop on a high end GPU.

00:01:24.410 --> 00:01:26.560
However, there's a problem,
and that's

00:01:26.560 --> 00:01:28.870
the PCI express bus.

00:01:28.870 --> 00:01:31.500
The PCI express bus is something
that all your data

00:01:31.500 --> 00:01:35.800
has to traverse in order to move
from the GPU to the CPU,

00:01:35.800 --> 00:01:37.650
or vice versa.

00:01:37.650 --> 00:01:41.760
And compared to the speed of
memory bandwidth on either

00:01:41.760 --> 00:01:43.690
device, it's very slow.

00:01:43.690 --> 00:01:46.120
So a lot of the work in porting
an application to the

00:01:46.120 --> 00:01:49.820
GPU and desktop comes from
managing this data transfer

00:01:49.820 --> 00:01:52.360
overhead, and only moving work
to the GPU when you can

00:01:52.360 --> 00:01:56.250
actually amortize the cost of
moving data to the GPU.

00:01:59.220 --> 00:02:01.820
In mobile, things look
very different.

00:02:01.820 --> 00:02:04.165
The first thing you'll notice
is that the GPU and the CPU

00:02:04.165 --> 00:02:06.290
are now in the same package.

00:02:06.290 --> 00:02:09.070
They share a single pool
of physical memory.

00:02:09.070 --> 00:02:11.290
So there's no more
PCI express bus.

00:02:11.290 --> 00:02:13.120
That transfer time is gone.

00:02:13.120 --> 00:02:15.210
However, this does mean that
because they share the same

00:02:15.210 --> 00:02:18.430
pool of physical memory, the
GPU no longer has a number

00:02:18.430 --> 00:02:19.670
bandwidth advantage.

00:02:19.670 --> 00:02:23.330
And to further reduce the GPU's
advantage, its floating

00:02:23.330 --> 00:02:27.190
point performance relative to
the CPU is now only about

00:02:27.190 --> 00:02:29.090
three to four times faster.

00:02:29.090 --> 00:02:32.870
We're not talking 10 to 15
times faster anymore.

00:02:32.870 --> 00:02:35.000
We also may have additional
processors

00:02:35.000 --> 00:02:36.400
available to us in mobile.

00:02:36.400 --> 00:02:40.900
For example, you could have a
camera ISP that can do things

00:02:40.900 --> 00:02:43.800
like color space conversion
or basic convolution, or

00:02:43.800 --> 00:02:45.380
something like that.

00:02:45.380 --> 00:02:47.470
And you may have a
programmable DSP

00:02:47.470 --> 00:02:49.640
on your SoC as well.

00:02:49.640 --> 00:02:53.860
And you may really want to
use these processors when

00:02:53.860 --> 00:02:57.150
possible, because they're
relatively fixed function.

00:02:57.150 --> 00:02:59.530
And they may provide very,
very good perf per watt.

00:03:02.980 --> 00:03:05.070
So in terms of architecture,
mobile

00:03:05.070 --> 00:03:06.510
versus desktop is different.

00:03:06.510 --> 00:03:08.530
And it's different in more
than just the block

00:03:08.530 --> 00:03:10.430
diagram ways, too.

00:03:10.430 --> 00:03:13.530
Mobile has a lot of
architectural diversity.

00:03:13.530 --> 00:03:18.530
In desktop you have two CPU
vendors, three GPU vendors.

00:03:18.530 --> 00:03:21.740
In mobile you have
three CPU ISAs.

00:03:21.740 --> 00:03:25.820
Within ARM, you have a number
of different ARM cores and

00:03:25.820 --> 00:03:27.910
licensed ARM cores.

00:03:27.910 --> 00:03:32.180
You have dramatically more
GPUs and GPU vendors.

00:03:32.180 --> 00:03:36.620
And particularly within the GPU
space, the architecture of

00:03:36.620 --> 00:03:41.210
these GPUs are very different
from one another.

00:03:41.210 --> 00:03:44.040
If you look at a kind of desktop
GPUs, in broad strokes

00:03:44.040 --> 00:03:46.010
they look mostly similar.

00:03:46.010 --> 00:03:47.690
That's not true in mobile.

00:03:47.690 --> 00:03:50.560
Some of them look very CPU like,
and have very small SIMD

00:03:50.560 --> 00:03:52.930
widths, and things like that.

00:03:52.930 --> 00:03:56.550
Others look more like
traditional desktop GPUs, and

00:03:56.550 --> 00:03:59.930
then they have very wide
vector widths.

00:03:59.930 --> 00:04:01.690
Others look sort of
like a pool of

00:04:01.690 --> 00:04:04.610
fixed function hardware.

00:04:04.610 --> 00:04:07.680
And additionally in mobile, and
you have concerns about

00:04:07.680 --> 00:04:09.250
system resources.

00:04:09.250 --> 00:04:13.100
The biggest two there are power
and thermal constraints,

00:04:13.100 --> 00:04:16.630
which generally don't affect
you as much on desktop.

00:04:16.630 --> 00:04:20.220
You also have issues like the
GPU may be busy rendering a

00:04:20.220 --> 00:04:21.410
lot of pixels.

00:04:21.410 --> 00:04:26.540
For example on a Nexux 10, we
have a 2560x1600 display,

00:04:26.540 --> 00:04:29.770
which is equivalent to a 30
inch desktop monitor.

00:04:29.770 --> 00:04:32.900
And we're trying to drive that
on the GPU with 80 gigaflops,

00:04:32.900 --> 00:04:36.050
instead of over a teraflop.

00:04:36.050 --> 00:04:37.870
And you may have additional
processors that you want to

00:04:37.870 --> 00:04:39.700
use as well.

00:04:39.700 --> 00:04:42.990
So our goal here is really to
develop high performance

00:04:42.990 --> 00:04:48.060
applications for these wide
variety of SoCs without

00:04:48.060 --> 00:04:49.720
sacrificing performance
portability.

00:04:49.720 --> 00:04:55.000
Without guaranteeing that the
more you optimize for one SoC,

00:04:55.000 --> 00:04:58.960
the worse you run on
certain other SoCs.

00:04:58.960 --> 00:05:03.190
So our approach to solving that
is called RenderScript.

00:05:03.190 --> 00:05:05.710
And RenderScript is our platform
for high performance

00:05:05.710 --> 00:05:09.300
computing across different
hardware on Android.

00:05:09.300 --> 00:05:12.020
So the first thing you'll notice
about the API is that

00:05:12.020 --> 00:05:13.660
the API is focused
on the system.

00:05:13.660 --> 00:05:15.040
You don't get a list
of devices.

00:05:15.040 --> 00:05:17.680
You don't get a big collection
of device properties and have

00:05:17.680 --> 00:05:19.760
to try to figure out
at run time which

00:05:19.760 --> 00:05:21.000
device you should use.

00:05:21.000 --> 00:05:22.610
The runtime handles
that for you.

00:05:22.610 --> 00:05:26.620
You simply have a computation
that you want to run quickly,

00:05:26.620 --> 00:05:30.010
and we will try to put that on
the best processor we can.

00:05:30.010 --> 00:05:33.080
What this means is that this
gives developers a consistent

00:05:33.080 --> 00:05:36.100
target that will run well
across any SoC.

00:05:36.100 --> 00:05:41.760
We work with the SoC vendors and
get drivers for their GPUs

00:05:41.760 --> 00:05:46.570
and DSPs, ISPs, whatever, and
have those available on

00:05:46.570 --> 00:05:48.440
tablets and phones.

00:05:48.440 --> 00:05:53.980
And this way, the runtime
knows about whatever

00:05:53.980 --> 00:05:56.650
processors and capabilities
it can use.

00:05:56.650 --> 00:06:00.660
It doesn't require the developer
to go in and may

00:06:00.660 --> 00:06:03.860
call these decisions at run time
about an SoC it's never

00:06:03.860 --> 00:06:05.660
seen before.

00:06:05.660 --> 00:06:09.080
We've been influenced by other
data parallel run times,

00:06:09.080 --> 00:06:12.070
obviously, but we've kind of
taken things in a very

00:06:12.070 --> 00:06:13.320
different direction.

00:06:16.260 --> 00:06:20.400
So at a very high level view,
we do look very similar to

00:06:20.400 --> 00:06:23.300
some of these other data
parallel runtimes.

00:06:23.300 --> 00:06:24.995
We write performance
critical kernels

00:06:24.995 --> 00:06:28.550
in a C99 based language.

00:06:28.550 --> 00:06:31.620
These kernels are distributed
with your application as

00:06:31.620 --> 00:06:33.980
architecture independent
bit code.

00:06:33.980 --> 00:06:38.110
We then JIT compile those at
run time to one or more

00:06:38.110 --> 00:06:39.680
processor targets.

00:06:39.680 --> 00:06:45.490
For example, we may compile them
to ARM D7A with Neon, and

00:06:45.490 --> 00:06:49.820
we may compile them to, for
example on the Nexus 10, we

00:06:49.820 --> 00:06:52.470
have GPU support in
the Mali-T604.

00:06:52.470 --> 00:06:57.010
So we may compile to build a
CPU and GPU at run time.

00:06:57.010 --> 00:07:02.720
We also reflect Java classes
for easy integration with

00:07:02.720 --> 00:07:03.800
existing applications.

00:07:03.800 --> 00:07:05.900
And what this means is
essentially that every time

00:07:05.900 --> 00:07:10.180
you have a RenderScript file,
we generate a set of Java

00:07:10.180 --> 00:07:14.340
classes that go with it, so that
you can control execution

00:07:14.340 --> 00:07:17.740
of that without relying on
string based APIs, or having

00:07:17.740 --> 00:07:20.520
to use JNI, or something
like that.

00:07:20.520 --> 00:07:23.000
We also have kind of standard
resource management and

00:07:23.000 --> 00:07:25.510
execution in our Java
API as well.

00:07:25.510 --> 00:07:27.120
One thing I didn't mention
here is we also have a

00:07:27.120 --> 00:07:30.330
collection of, they're called
script intrinsics.

00:07:30.330 --> 00:07:35.720
Essentially these are built in,
very fast operations that

00:07:35.720 --> 00:07:39.860
we can tune to specific
architectures very well.

00:07:39.860 --> 00:07:43.830
And they're things like YUV
conversion, or convolution,

00:07:43.830 --> 00:07:44.930
stuff like that.

00:07:44.930 --> 00:07:49.950
Common operations you may use
very often, where you can get

00:07:49.950 --> 00:07:53.590
a really significant speedup by
tuning very, very closely

00:07:53.590 --> 00:07:55.180
to a particular piece
of hardware.

00:07:58.560 --> 00:08:01.720
So let's go through some basic
RenderScript definitions.

00:08:01.720 --> 00:08:04.830
The first thing to talk
about is the element.

00:08:04.830 --> 00:08:08.100
So an element is essentially
a C type.

00:08:08.100 --> 00:08:10.610
In this case, it's an INT.

00:08:10.610 --> 00:08:12.650
So it can be a scalar type,
like an INT, it could be a

00:08:12.650 --> 00:08:16.050
vector type, like an INT4,
Float 4, whatever.

00:08:16.050 --> 00:08:19.300
We can also supports C
structs as elements.

00:08:19.300 --> 00:08:22.850
So if you declare a struct in
your RenderScript file, we

00:08:22.850 --> 00:08:27.710
will actually reflect an element
class for that, that

00:08:27.710 --> 00:08:29.350
will be available to Java.

00:08:29.350 --> 00:08:33.570
So you can use that from
your Java API directly.

00:08:33.570 --> 00:08:36.429
So an element is not very
useful in and of itself.

00:08:36.429 --> 00:08:39.740
So to actually make use of this,
we have allocations.

00:08:39.740 --> 00:08:42.700
And allocations are collections
of a single type

00:08:42.700 --> 00:08:47.130
of element in some
arrangement.

00:08:47.130 --> 00:08:49.060
It could be 1D or 2D.

00:08:49.060 --> 00:08:52.000
They have an x and y dimension,
and they have a

00:08:52.000 --> 00:08:52.640
backing store.

00:08:52.640 --> 00:08:56.550
And essentially, an allocation
is how you get data from Java

00:08:56.550 --> 00:08:58.500
into RenderScript, so
it can be processed

00:08:58.500 --> 00:09:02.900
by one or more kernels.

00:09:02.900 --> 00:09:06.320
And an allocation has one other
important aspects, and

00:09:06.320 --> 00:09:08.480
that is the type.

00:09:08.480 --> 00:09:12.500
And the type is essentially the
size of the allocation,

00:09:12.500 --> 00:09:17.500
along with the elements used
within that allocation.

00:09:17.500 --> 00:09:19.150
And we use this for
two things.

00:09:19.150 --> 00:09:25.440
First of all, we use it to do
safety checking on copies and

00:09:25.440 --> 00:09:28.050
kernel launches, and things like
that, to make sure that

00:09:28.050 --> 00:09:34.170
you don't try to copy an
allocation of floats onto an

00:09:34.170 --> 00:09:37.260
allocation of chars, because
it won't fit.

00:09:37.260 --> 00:09:43.710
Or a 5x5 allocation of INTs onto
a 3x3 allocation of INTs,

00:09:43.710 --> 00:09:45.245
because it won't fit.

00:09:45.245 --> 00:09:49.770
We also do type checking when
we launch kernels, to make

00:09:49.770 --> 00:09:52.550
sure that the element type
past tense of the kernel

00:09:52.550 --> 00:09:55.230
matches what the
kernel expects.

00:09:55.230 --> 00:09:58.780
The other way we use types is
to control how much parallel

00:09:58.780 --> 00:10:01.440
work actually gets launched
by a kernel.

00:10:01.440 --> 00:10:03.504
And we'll get into this
more in a bit.

00:10:06.280 --> 00:10:10.300
So this is a dot RS file, a
very basic dot RS file.

00:10:10.300 --> 00:10:13.450
And in general, we refer to each
one of these dot RS files

00:10:13.450 --> 00:10:14.690
as a script.

00:10:14.690 --> 00:10:16.990
And a script is essentially
its own little world.

00:10:16.990 --> 00:10:18.930
There's no linkage between
scripts, or

00:10:18.930 --> 00:10:20.960
anything like that.

00:10:20.960 --> 00:10:23.790
So every script is
self-contained.

00:10:23.790 --> 00:10:26.150
And the first thing you'll
notice here is that the script

00:10:26.150 --> 00:10:27.970
starts with two pragmas.

00:10:27.970 --> 00:10:31.030
First we have the RS
language revision.

00:10:31.030 --> 00:10:32.020
For now it's one.

00:10:32.020 --> 00:10:34.550
It will be one for the
foreseeable future.

00:10:34.550 --> 00:10:38.140
After that, we have a
Java package name.

00:10:38.140 --> 00:10:41.410
And this is the package name
that we use for the reflected

00:10:41.410 --> 00:10:44.040
classes from this script.

00:10:44.040 --> 00:10:47.890
After that we have what looks
like a normal C99 global.

00:10:47.890 --> 00:10:49.610
We call that a script
global because it is

00:10:49.610 --> 00:10:51.380
local to the script.

00:10:51.380 --> 00:10:55.230
And this will reflect a method
called set add val, which

00:10:55.230 --> 00:10:57.090
takes an INT to Java.

00:10:57.090 --> 00:11:01.360
So if you want to update this
add val global from Java, you

00:11:01.360 --> 00:11:03.310
can do that.

00:11:03.310 --> 00:11:07.770
After that we have a kernel, and
this kernel looks mostly

00:11:07.770 --> 00:11:11.350
like a standard C99 function,
except it has this decorator

00:11:11.350 --> 00:11:12.760
attribute kernel.

00:11:12.760 --> 00:11:16.400
An attribute kernel is the new
kernel syntax we introduced in

00:11:16.400 --> 00:11:19.560
API 17, and I highly recommend
you use that.

00:11:19.560 --> 00:11:22.990
And essentially what this means
is the kernel has an

00:11:22.990 --> 00:11:25.040
input value and an output.

00:11:25.040 --> 00:11:27.260
Or an input type and
an output type.

00:11:27.260 --> 00:11:32.470
And here the input type is an
INT, so we get an INT in.

00:11:32.470 --> 00:11:37.650
And then we also get two UINT
32Ts, x and y, which are

00:11:37.650 --> 00:11:40.050
coordinates within
the allocation.

00:11:40.050 --> 00:11:43.410
And this function returns an
INT, so essentially what

00:11:43.410 --> 00:11:48.430
happens is when we run this
kernel on a given allocation,

00:11:48.430 --> 00:11:52.400
for every element in the
allocation, this function will

00:11:52.400 --> 00:11:53.370
be executed.

00:11:53.370 --> 00:11:59.180
And so at every particular
coordinate pair in the

00:11:59.180 --> 00:12:03.980
allocation, the value of that
element at that location will

00:12:03.980 --> 00:12:05.490
be passed to this function.

00:12:05.490 --> 00:12:08.040
And then the return value from
this function will be written

00:12:08.040 --> 00:12:10.310
to the output application.

00:12:10.310 --> 00:12:15.560
So because this kernel has both
an in argument and a non

00:12:15.560 --> 00:12:19.040
void return type, it reflects
a method called for each

00:12:19.040 --> 00:12:20.630
underscore kernel.

00:12:20.630 --> 00:12:22.090
Just kernel here because that's

00:12:22.090 --> 00:12:23.620
the name of the function.

00:12:23.620 --> 00:12:28.250
And for each kernel takes two
allocations, an input

00:12:28.250 --> 00:12:30.380
allocation and an output
application.

00:12:30.380 --> 00:12:35.640
If you didn't have INT in and
you simply had x and y, it

00:12:35.640 --> 00:12:37.130
would only take the
output allocation.

00:12:37.130 --> 00:12:41.480
And similarly, if it didn't
return an INT, but simply

00:12:41.480 --> 00:12:44.285
returned void, it would not
need an output allocation.

00:12:48.960 --> 00:12:53.450
So I want to talk a little bit
about a more complicated part

00:12:53.450 --> 00:12:57.600
of the API where you can do
more advanced things.

00:12:57.600 --> 00:12:59.430
And that is called
us script group.

00:12:59.430 --> 00:13:02.000
And a script group allows
a group of kernels to be

00:13:02.000 --> 00:13:05.580
executed as a single
Java call.

00:13:05.580 --> 00:13:08.570
More specifically, this allows
a dag of kernels to be

00:13:08.570 --> 00:13:11.110
executed as a single
function call.

00:13:11.110 --> 00:13:17.060
And by passing this entire
workload to the driver as one

00:13:17.060 --> 00:13:20.890
monolithic entity before we
actually run any part of that

00:13:20.890 --> 00:13:24.000
workload, we can enable
all sorts of different

00:13:24.000 --> 00:13:28.560
optimizations in the run time,
and in the compiler.

00:13:28.560 --> 00:13:31.120
For example, we can actually
enable, like, parallel

00:13:31.120 --> 00:13:34.450
execution across devices, or
tiling, kernel fusion, which

00:13:34.450 --> 00:13:36.630
we'll get into in a minute.

00:13:36.630 --> 00:13:40.200
We actually see some significant
speed improvements

00:13:40.200 --> 00:13:42.250
today by using script
group versus

00:13:42.250 --> 00:13:44.310
using individual scripts.

00:13:44.310 --> 00:13:47.720
And in general, I highly
recommend that you see script

00:13:47.720 --> 00:13:50.110
group as much as possible going
forward, because it is a

00:13:50.110 --> 00:13:52.040
place where we're going
to spend a lot of time

00:13:52.040 --> 00:13:54.130
optimizing.

00:13:54.130 --> 00:13:56.750
So let's go through an example
of script group.

00:13:56.750 --> 00:14:01.210
So here we have five kernels,
and essentially input is

00:14:01.210 --> 00:14:06.330
passed to A, and output comes
from E. And then you have this

00:14:06.330 --> 00:14:08.520
set of dependencies
in between.

00:14:08.520 --> 00:14:12.330
And according to the semantics
of script group, the

00:14:12.330 --> 00:14:15.000
intermediate state is not
visible to the user.

00:14:15.000 --> 00:14:19.990
So all of these connections
between A and B and A and C,

00:14:19.990 --> 00:14:22.960
et cetera, which would
normally be stored in

00:14:22.960 --> 00:14:27.290
programmer managed allocations
are instead simply set up as

00:14:27.290 --> 00:14:32.230
connections between these
kernels and the graph.

00:14:32.230 --> 00:14:35.790
And what this allows us to do
is the runtime can either

00:14:35.790 --> 00:14:39.410
create those allocations if
necessary automatically, or we

00:14:39.410 --> 00:14:43.180
can optimize them away
if possible.

00:14:43.180 --> 00:14:47.080
And so we can do things here
like, because there's no

00:14:47.080 --> 00:14:51.400
dependence between B and C, we
could run B on the CPU and C

00:14:51.400 --> 00:14:54.800
on the GPU if possible, or
something like that.

00:14:54.800 --> 00:14:57.360
If we wanted to get more
advanced, if we have more

00:14:57.360 --> 00:15:00.440
knowledge about the way the
kernels actually run and the

00:15:00.440 --> 00:15:04.840
particular dependency
information between colonels,

00:15:04.840 --> 00:15:09.540
we could potentially do
something like fusing B and D

00:15:09.540 --> 00:15:11.010
into a single kernel.

00:15:11.010 --> 00:15:14.550
And this way you could simply
take the results of some

00:15:14.550 --> 00:15:18.073
portion of B, depending on how
much you actually need to run

00:15:18.073 --> 00:15:22.340
at a time, and immediately start
running that on D. And

00:15:22.340 --> 00:15:25.680
this is potentially really good
for GPUs, and also really

00:15:25.680 --> 00:15:29.740
good for CPUs, because it allows
you to use things like

00:15:29.740 --> 00:15:34.170
local memory on the
GPU, and keep your

00:15:34.170 --> 00:15:35.420
cache hot on the CPU.

00:15:39.580 --> 00:15:43.860
So I'm going to talk about some
features coming up in an

00:15:43.860 --> 00:15:46.560
upcoming release.

00:15:46.560 --> 00:15:50.230
And the first feature we will
talk about this is the

00:15:50.230 --> 00:15:52.460
compatibility library
for gingerbread.

00:15:52.460 --> 00:15:56.020
And I'll go into that in a lot
more detail in a minute.

00:15:56.020 --> 00:15:58.860
Other than that, we've added
rsSetElementAt, which

00:15:58.860 --> 00:16:01.780
essentially is scatter support
for RenderScript.

00:16:01.780 --> 00:16:05.380
You can now write arbitrary
allocations

00:16:05.380 --> 00:16:06.960
from a given kernel.

00:16:06.960 --> 00:16:09.650
We have debug runtime, which
does things like bounce

00:16:09.650 --> 00:16:12.540
checking and prints out errors
to make it easier

00:16:12.540 --> 00:16:14.360
to debug your code.

00:16:14.360 --> 00:16:17.040
We've added more script
intrinsics, things like 3D

00:16:17.040 --> 00:16:21.920
look up tables, and I think
something else.

00:16:21.920 --> 00:16:24.990
We have native support for YUV
allocations, so you can take a

00:16:24.990 --> 00:16:28.610
YUV allocation directly from a
camera, and process that in

00:16:28.610 --> 00:16:29.840
RenderScript.

00:16:29.840 --> 00:16:31.720
We've also improved launch
latency significantly.

00:16:34.820 --> 00:16:36.700
So, the compatibility library.

00:16:36.700 --> 00:16:41.740
The compatibility library
enables API 18 RenderScript on

00:16:41.740 --> 00:16:44.280
devices running Gingerbread
or higher.

00:16:44.280 --> 00:16:48.570
And the way this works is
essentially we can do an

00:16:48.570 --> 00:16:53.880
offline compilation of your
RenderScript bit code.

00:16:53.880 --> 00:16:57.180
When you build your app, you can
actually create a shared

00:16:57.180 --> 00:16:59.400
library that runs on the CPU.

00:16:59.400 --> 00:17:01.570
And we compile this for sort
of the lowest common

00:17:01.570 --> 00:17:04.300
denominator CPU that can
run all the way back to

00:17:04.300 --> 00:17:05.550
Gingerbread.

00:17:08.470 --> 00:17:12.819
When the app is actually built,
then, the compatibility

00:17:12.819 --> 00:17:15.630
library, as well as the shared
libraries for your

00:17:15.630 --> 00:17:18.619
RenderScript kernels are
packaged with the app.

00:17:18.619 --> 00:17:24.369
And we also package the normal
RS bit code as well.

00:17:24.369 --> 00:17:28.329
So on an older device, something
running between

00:17:28.329 --> 00:17:34.680
Gingerbread and Android 4.2, we
can use this shared library

00:17:34.680 --> 00:17:40.080
that we've built into the app
because it doesn't have API18.

00:17:40.080 --> 00:17:43.920
On a suitable device, we can
compile the native bit code

00:17:43.920 --> 00:17:48.960
automatically and use whatever
processors or optimizations

00:17:48.960 --> 00:17:52.370
are available on that device
without the developer having

00:17:52.370 --> 00:17:54.050
to do anything special there.

00:17:54.050 --> 00:17:56.190
So what this means, essentially,
is that you can

00:17:56.190 --> 00:18:02.690
have one APK that runs on the
Nexus 1 running gingerbread.

00:18:02.690 --> 00:18:06.900
And you can take that same APK
and run it on the Nexus 10.

00:18:06.900 --> 00:18:09.980
And on the Nexus 1, it'll run
using the CPU shared library.

00:18:09.980 --> 00:18:15.130
It will run the CPU, and run
as fast as possible there.

00:18:15.130 --> 00:18:20.360
And on the Nexus 10, we'll
compile the bit code for A15.

00:18:20.360 --> 00:18:23.106
Use whatever A15 optimizations
we can.

00:18:23.106 --> 00:18:26.840
We'll use the GPU, and it'll
run a lot faster.

00:18:26.840 --> 00:18:28.890
And so now for the first
time, you can do all

00:18:28.890 --> 00:18:31.760
that from one APK.

00:18:31.760 --> 00:18:35.450
And with that, I'm going to turn
it over to Jason Sams,

00:18:35.450 --> 00:18:37.575
who's going to go through
application.

00:18:43.230 --> 00:18:44.930
JASON SAMS: Thanks, Tim.

00:18:44.930 --> 00:18:46.250
My name is Jason Sams.

00:18:46.250 --> 00:18:48.930
I'm the tech lead for
RenderScript script on

00:18:48.930 --> 00:18:53.360
Android, and I'm going to take
us through a simple example of

00:18:53.360 --> 00:18:55.020
using RenderScript.

00:18:55.020 --> 00:18:56.890
And then we're actually going
to follow up with a more

00:18:56.890 --> 00:18:58.140
complex example.

00:19:00.640 --> 00:19:03.620
So, the examples we're going
to use are going to be a

00:19:03.620 --> 00:19:07.420
Gaussian Blur and a histogram.

00:19:07.420 --> 00:19:10.700
So for anyone who attended the
talk yesterday that Rowan and

00:19:10.700 --> 00:19:12.850
Chet gave, you may have noticed
that they were talking

00:19:12.850 --> 00:19:16.070
about doing drop shadows
using RenderScript.

00:19:16.070 --> 00:19:18.610
And I'm actually going to talk
about the primitive they're

00:19:18.610 --> 00:19:21.280
using to do this.

00:19:21.280 --> 00:19:23.390
RenderScript has been optimized
for doing image

00:19:23.390 --> 00:19:24.720
processing tasks.

00:19:24.720 --> 00:19:27.490
We've been tuning it for
this workload for a

00:19:27.490 --> 00:19:28.210
couple years now.

00:19:28.210 --> 00:19:31.550
It's getting pretty
good at it.

00:19:31.550 --> 00:19:34.730
The intrinsics that Tim
talked about include a

00:19:34.730 --> 00:19:36.060
Gaussian Blur intrinsic.

00:19:36.060 --> 00:19:41.360
And so for blurring images
or applying other simple

00:19:41.360 --> 00:19:45.400
operations to bitmaps, it's
actually extremely easy to do

00:19:45.400 --> 00:19:48.850
that, and that's why we'll walk
through that example.

00:19:48.850 --> 00:19:51.170
Histogram will demonstrate
some of the more advanced

00:19:51.170 --> 00:19:52.730
techniques.

00:19:52.730 --> 00:19:55.020
Things I clipped kernel
launches, which

00:19:55.020 --> 00:19:57.220
is coming up soon.

00:19:57.220 --> 00:20:00.420
The rsSetElementAt, which
Tim was talking about.

00:20:00.420 --> 00:20:02.900
And we'll also demonstrate
multipass

00:20:02.900 --> 00:20:06.230
processing over a workload.

00:20:08.870 --> 00:20:13.270
So general image processing in
RS, and how you go about it.

00:20:13.270 --> 00:20:18.010
The code to do this is actually
pretty simple, and

00:20:18.010 --> 00:20:19.980
the first thing you'll want to
do in an application, if

00:20:19.980 --> 00:20:23.320
you're going to do some use of
RS is you'll want to create a

00:20:23.320 --> 00:20:25.310
RenderScript context.

00:20:25.310 --> 00:20:30.090
And the first line of our code,
we create this context.

00:20:30.090 --> 00:20:34.500
We actually pass the application
context to that.

00:20:34.500 --> 00:20:36.950
Now, coming up soon we'll
actually have a few additional

00:20:36.950 --> 00:20:40.860
flags you can pass here, one
of which is a debug mode,

00:20:40.860 --> 00:20:43.690
which you can pass into your
contacts creation, and it'll

00:20:43.690 --> 00:20:46.590
do things like range check
your access and give you

00:20:46.590 --> 00:20:50.210
additional warnings, or errors
if it detects something that's

00:20:50.210 --> 00:20:53.070
not optimal, or wrong.

00:20:53.070 --> 00:20:56.310
But the next step you'll want
to do is actually create one

00:20:56.310 --> 00:20:58.270
of those RenderScript
allocations.

00:20:58.270 --> 00:21:00.850
And typically if you're doing
image processing, your input's

00:21:00.850 --> 00:21:02.760
probably going to be
a bitmap. map.

00:21:02.760 --> 00:21:06.540
And so we have a create from
bitmap helper function, which

00:21:06.540 --> 00:21:09.580
will actually create an
allocation from a bitmap, and

00:21:09.580 --> 00:21:12.650
it'll automatically set the type
and the height and the

00:21:12.650 --> 00:21:16.680
width of that allocation to that
of the incoming bitmap.

00:21:16.680 --> 00:21:19.140
Now in addition, you can
optionally map that

00:21:19.140 --> 00:21:23.610
allocation, and we have
a few usage flags.

00:21:23.610 --> 00:21:27.650
Usage shared, it's new,
or upcoming soon.

00:21:27.650 --> 00:21:30.350
And it allows us to share the
back end store with the

00:21:30.350 --> 00:21:33.230
bitmap, and that avoids a lot of
the copy overhead that you

00:21:33.230 --> 00:21:37.020
would normally get if you had
a separate bitmap and

00:21:37.020 --> 00:21:40.870
allocation in the older APIs.

00:21:40.870 --> 00:21:42.260
Also, usage graphic texture.

00:21:42.260 --> 00:21:44.820
If you were going to use this
in combination with RS

00:21:44.820 --> 00:21:47.790
sampler, you can set the
flag to enable that.

00:21:47.790 --> 00:21:49.240
And usage script just
means we're going to

00:21:49.240 --> 00:21:50.720
pass it into a kernel.

00:21:50.720 --> 00:21:52.650
Now, this is actually the
default set of flags, so if

00:21:52.650 --> 00:21:54.380
you were to weed the flags
off completely, this is

00:21:54.380 --> 00:21:56.130
what you would get.

00:21:56.130 --> 00:21:58.460
For the output, we're going
to write this to a bitmap.

00:21:58.460 --> 00:22:01.260
So we'll create a second
allocation, and I'm going to

00:22:01.260 --> 00:22:04.170
leave off the graphics texture
usage, because we're not going

00:22:04.170 --> 00:22:07.440
to sample from it.

00:22:07.440 --> 00:22:10.150
So we have a bitmap we're going
to work on, and we're

00:22:10.150 --> 00:22:14.070
going to apply a large
blur to this.

00:22:14.070 --> 00:22:16.910
So after you have your two
allocations, how would you go

00:22:16.910 --> 00:22:19.180
about blurring that image?

00:22:19.180 --> 00:22:21.900
The first thing I would do is
I would create a variable to

00:22:21.900 --> 00:22:25.750
hold the intrinsic script,
that is the blur.

00:22:25.750 --> 00:22:29.130
And then I would create
that blur script.

00:22:29.130 --> 00:22:31.360
In this case, since it's an
intrinsic, you don't actually

00:22:31.360 --> 00:22:32.760
have to provide a dot RS file.

00:22:32.760 --> 00:22:35.530
It's built into the system, But
You still need to tell it

00:22:35.530 --> 00:22:38.170
what type of data you
want it to work on.

00:22:38.170 --> 00:22:41.340
In this case, we're going
to say it's a uchar4.

00:22:41.340 --> 00:22:44.880
That's the element
U8 underscore 4.

00:22:44.880 --> 00:22:48.340
We support just uchar buffers,
if you had, say, an alpha

00:22:48.340 --> 00:22:51.880
channel, or just a
grayscale image.

00:22:51.880 --> 00:22:55.060
And now we'll take that script
that we loaded, and we'll

00:22:55.060 --> 00:22:57.230
configure it to perform
our blur.

00:22:57.230 --> 00:22:58.810
Now, I'm going to first
set of radius.

00:22:58.810 --> 00:22:59.775
In this case, we're
going to set it to

00:22:59.775 --> 00:23:01.220
a nice, large radius.

00:23:01.220 --> 00:23:05.410
20 pixels, so it's going to
be a big blur operation.

00:23:05.410 --> 00:23:08.190
And we'll set on that script
the input allocation we

00:23:08.190 --> 00:23:12.260
created in the previous slide,
that will act as our input.

00:23:12.260 --> 00:23:14.490
And for the blur, we're
going to run it.

00:23:14.490 --> 00:23:16.160
We're just going to call
it the for each.

00:23:16.160 --> 00:23:19.510
Since it's an intrinsic, there
is actually no name provided.

00:23:19.510 --> 00:23:22.720
It's just called for each, and
write it to our output

00:23:22.720 --> 00:23:24.300
allocation.

00:23:24.300 --> 00:23:28.850
And after we have done that,
we need to copy our output

00:23:28.850 --> 00:23:31.520
allocation back into
our output bitmap.

00:23:31.520 --> 00:23:34.400
Now, with usage shared it's
possible this will be a no op

00:23:34.400 --> 00:23:36.553
on some devices, and be
extremely efficient.

00:23:39.460 --> 00:23:42.880
And with that, you get
a nice blurred image.

00:23:42.880 --> 00:23:45.780
This intrinsic is actually
implemented extremely

00:23:45.780 --> 00:23:46.970
efficiently.

00:23:46.970 --> 00:23:49.620
So if you have an ARM device,
you get handed to a Neon.

00:23:49.620 --> 00:23:51.960
If you have an x86 device,
you get SSE that's been

00:23:51.960 --> 00:23:53.670
hand-tuned.

00:23:53.670 --> 00:23:57.030
On some devices you'll get
a hand-tuned GPU kernel.

00:23:57.030 --> 00:23:59.450
So it really is up to each
device how they implement

00:23:59.450 --> 00:24:02.660
this, but the key takeaway is
on each device, it's pretty

00:24:02.660 --> 00:24:04.050
much the fastest
possible way to

00:24:04.050 --> 00:24:05.410
implement it on that device.

00:24:08.260 --> 00:24:09.790
Now for histogram, this
is going to be a

00:24:09.790 --> 00:24:11.850
more complex example.

00:24:11.850 --> 00:24:14.600
We're going to present an
algorithm that does the

00:24:14.600 --> 00:24:16.840
histogram in two passes.

00:24:16.840 --> 00:24:20.100
The first pass we'll use a
large number of workers.

00:24:20.100 --> 00:24:24.540
The second pass will be a much
smaller summation pass.

00:24:24.540 --> 00:24:28.180
We do this because the first
pass, we're breaking the image

00:24:28.180 --> 00:24:32.540
up into chunks to build
intermediate sums.

00:24:32.540 --> 00:24:36.550
And that allows us to do a
lot of work in parallel.

00:24:36.550 --> 00:24:39.290
And we'll for the purpose of
time ignore the actual

00:24:39.290 --> 00:24:42.170
rendering pass into how we draw
it on the screen, because

00:24:42.170 --> 00:24:43.790
there's a lot of different ways
you could theoretically

00:24:43.790 --> 00:24:46.790
render this.

00:24:46.790 --> 00:24:49.550
So how would you write
a histogram script?

00:24:49.550 --> 00:24:51.480
Now, since we don't have an
intrinsic, I'm actually going

00:24:51.480 --> 00:24:53.760
to write a dot RS file.

00:24:53.760 --> 00:24:56.670
The first thing I'm going to
do is declare some globals.

00:24:56.670 --> 00:24:58.060
In this case they're
going to be an RS

00:24:58.060 --> 00:24:59.800
underscore allocation type.

00:24:59.800 --> 00:25:02.350
That is just the script
equivalent of the allocation

00:25:02.350 --> 00:25:05.850
class on the Java side
that will hold our

00:25:05.850 --> 00:25:08.710
input and output data.

00:25:08.710 --> 00:25:11.840
I will also attach, since it's
a multipass, an intermediate

00:25:11.840 --> 00:25:16.130
buffer that'll hold the sums
from the first pass.

00:25:16.130 --> 00:25:20.020
And that buffer will be the
number of steps, meaning how

00:25:20.020 --> 00:25:23.640
many threads I'm going to run to
do the summations, and 256

00:25:23.640 --> 00:25:26.685
units wide, one for each
level of the histogram.

00:25:29.290 --> 00:25:32.130
And we'll also have a final sum
buffer, which will just be

00:25:32.130 --> 00:25:39.200
a one dimensional allocation
of 256 different levels.

00:25:39.200 --> 00:25:42.460
So, additional globals we're
going to pass in will have

00:25:42.460 --> 00:25:45.450
just integers that will
represent the height and width

00:25:45.450 --> 00:25:48.230
of the input image.

00:25:48.230 --> 00:25:51.650
And we will have an integer
which will represent how many

00:25:51.650 --> 00:25:55.740
steps, or how many items is in
each step of the image, and

00:25:55.740 --> 00:25:56.990
how many steps are in total.

00:26:00.140 --> 00:26:03.400
So the kernel for the first
pass, and I'll have a diagram

00:26:03.400 --> 00:26:05.950
of how this works in a moment.

00:26:05.950 --> 00:26:08.720
What this is going to do is
it's going to walk over a

00:26:08.720 --> 00:26:13.070
number of scan lines
in the image.

00:26:13.070 --> 00:26:14.750
However many scan lines
will be determined

00:26:14.750 --> 00:26:16.430
by the steps value.

00:26:16.430 --> 00:26:19.750
Typically you're going to see
values like two or four.

00:26:19.750 --> 00:26:21.930
I tried experimentally, a number
of different steps

00:26:21.930 --> 00:26:23.940
value, and it's very hardware
dependent what the most

00:26:23.940 --> 00:26:26.550
efficient value is.

00:26:26.550 --> 00:26:28.930
But you can actually pick values
that worked across a

00:26:28.930 --> 00:26:32.160
very large range of hardware.

00:26:32.160 --> 00:26:35.190
And we're going to actually
run this kernel.

00:26:35.190 --> 00:26:37.200
So you see it takes an x and
y-coordinate, because it's

00:26:37.200 --> 00:26:39.500
running on a 2D input image.

00:26:39.500 --> 00:26:42.050
But when we watch the kernel
from the Java side, we're

00:26:42.050 --> 00:26:44.440
actually going to clip it so
that it only iterates over x

00:26:44.440 --> 00:26:47.110
equals 0 and walks through the
different y-coordinates.

00:26:47.110 --> 00:26:49.610
I'll go over how we
do that in a bit.

00:26:49.610 --> 00:26:53.290
So the first thing we do for
each of these histogram worker

00:26:53.290 --> 00:26:55.000
threads is we're going to want
to clear our output of

00:26:55.000 --> 00:26:56.460
accumulation buffers.

00:26:56.460 --> 00:27:00.490
And I do that just by walking
through a simple loop that

00:27:00.490 --> 00:27:02.670
sets the sums buffer.

00:27:02.670 --> 00:27:05.600
It sets our particular line, and
then we just loop over the

00:27:05.600 --> 00:27:10.200
256 elements.

00:27:10.200 --> 00:27:12.990
So, continuing our first pass.

00:27:12.990 --> 00:27:15.610
After we've cleared our
accumulation buffer, we'll

00:27:15.610 --> 00:27:18.930
need to iterate over
our image.

00:27:18.930 --> 00:27:22.810
And we do this for 0 to
the number of scan

00:27:22.810 --> 00:27:24.575
lines in our step.

00:27:24.575 --> 00:27:27.230
Then we calculate which
wine we're working on.

00:27:27.230 --> 00:27:29.220
That's the PY variable.

00:27:29.220 --> 00:27:34.140
And if steps is not equally
divisible by the number of

00:27:34.140 --> 00:27:36.610
lines in the image, it's
possible we'll need to check

00:27:36.610 --> 00:27:37.700
for an overrun here.

00:27:37.700 --> 00:27:41.900
So we'll return in that case,
because we'll be done.

00:27:41.900 --> 00:27:44.750
And then we walk through
one scan line.

00:27:44.750 --> 00:27:49.350
So we just iterate in the kernel
from left to right, and

00:27:49.350 --> 00:27:53.670
we'll get an element from our
input image and load that into

00:27:53.670 --> 00:27:56.550
a temporary uchar4 value.

00:27:56.550 --> 00:28:00.050
We then calculate a luminance
value from that.

00:28:00.050 --> 00:28:02.590
In this case, it's just
the integer math.

00:28:02.590 --> 00:28:05.820
Nothing terribly exciting.

00:28:05.820 --> 00:28:08.980
And then after we've calculated
a luminance value,

00:28:08.980 --> 00:28:13.590
we use that is the x-coordinate
index into our

00:28:13.590 --> 00:28:17.290
intermediate buffer, and we
load the existing value,

00:28:17.290 --> 00:28:19.850
increment it by one, and then
you set [INAUDIBLE]

00:28:19.850 --> 00:28:21.350
to write it back.

00:28:21.350 --> 00:28:24.360
And this is how we're constantly
accumulating from

00:28:24.360 --> 00:28:26.530
each work item and
output value.

00:28:29.750 --> 00:28:34.310
So for the second pass, after
the first pass, we have

00:28:34.310 --> 00:28:39.220
basically an image which a
number of temporary buffers

00:28:39.220 --> 00:28:44.000
that indicate however many
values of each level were run

00:28:44.000 --> 00:28:45.340
for that threat.

00:28:45.340 --> 00:28:47.810
Now we need to sum all those
together and get one set of

00:28:47.810 --> 00:28:51.190
results, not n sets
of results.

00:28:51.190 --> 00:28:54.890
This kernel is a 1D kernel, so
it's only going to operate

00:28:54.890 --> 00:28:57.400
over one dimension.

00:28:57.400 --> 00:29:02.510
And we just simply loop over
within that kernel the input

00:29:02.510 --> 00:29:06.930
values, and sum up one vertical
column of levels.

00:29:06.930 --> 00:29:11.100
And we return the sum, and that
writes the output value

00:29:11.100 --> 00:29:12.350
for that level.

00:29:15.390 --> 00:29:18.050
So, rescale.

00:29:18.050 --> 00:29:20.590
This is an example of an
invokable function.

00:29:20.590 --> 00:29:26.410
So after we have our buffer that
has 256 items and tells

00:29:26.410 --> 00:29:30.350
you how many pixels of each
level it saw, what you may

00:29:30.350 --> 00:29:33.750
want to do is actually rescale
that to some range.

00:29:33.750 --> 00:29:35.770
In this case we'll have
an invokable.

00:29:35.770 --> 00:29:38.400
If you notice, there's no
decoration kernel on this.

00:29:38.400 --> 00:29:42.130
And it's going to be called
single threaded.

00:29:42.130 --> 00:29:44.290
This is good for very small
workloads where you don't want

00:29:44.290 --> 00:29:47.230
to necessarily launch a lot
of threads and do a lot of

00:29:47.230 --> 00:29:50.560
overhead for a very small
amount of work.

00:29:50.560 --> 00:29:54.780
So we loop over, we find the
maximum value in terms of any

00:29:54.780 --> 00:29:58.430
one bucket, and then we just
integrate over our buckets,

00:29:58.430 --> 00:30:03.520
and we divide the values there
by the greatest value to

00:30:03.520 --> 00:30:05.060
effectively normalize
the range.

00:30:08.890 --> 00:30:11.160
So what does this look like
on the Java side?

00:30:11.160 --> 00:30:13.440
So you've written this
dot RS file.

00:30:13.440 --> 00:30:15.450
What do you actually do with
it in terms of interfacing

00:30:15.450 --> 00:30:17.870
that with your application?

00:30:17.870 --> 00:30:20.720
Well, the first thing you
need to do is load it.

00:30:20.720 --> 00:30:23.560
The first line here
is how we do that.

00:30:23.560 --> 00:30:26.440
We create a script variable, and
then we can just say new

00:30:26.440 --> 00:30:28.510
script C underscore whatever
the name of

00:30:28.510 --> 00:30:30.540
your dot RS file is.

00:30:30.540 --> 00:30:33.840
And this works because we've
reflected a Java file from

00:30:33.840 --> 00:30:36.620
your dot RS file, and
effectively you now have a

00:30:36.620 --> 00:30:41.180
Java class with all the methods
that you could use to

00:30:41.180 --> 00:30:44.870
actually talk to that
dot RS file.

00:30:44.870 --> 00:30:48.090
And so after it's loaded, things
like the width and the

00:30:48.090 --> 00:30:51.500
height, those globals, you can
just actually call Java

00:30:51.500 --> 00:30:53.545
methods, and it will
set those for you.

00:30:53.545 --> 00:30:57.070
You can set any value you want
at that point, but in this

00:30:57.070 --> 00:30:59.830
case we'll set the width.

00:30:59.830 --> 00:31:00.010
Now

00:31:00.010 --> 00:31:03.430
We, need to create the
allocations to hold our data.

00:31:03.430 --> 00:31:05.770
We demonstrated earlier how you
would create the input and

00:31:05.770 --> 00:31:08.840
output allocations for the
input image and the final

00:31:08.840 --> 00:31:12.420
image, but how do you create
those intermediate buffers?

00:31:12.420 --> 00:31:14.790
In this case, we're going to
have to do it by building a

00:31:14.790 --> 00:31:18.620
custom type, because it's
going to be shaped very

00:31:18.620 --> 00:31:21.800
specific to that intermediate
buffer.

00:31:21.800 --> 00:31:23.250
So we create a type builder.

00:31:23.250 --> 00:31:27.320
We specify an element of INT
32, which is just regular

00:31:27.320 --> 00:31:30.030
integer is our element type.

00:31:30.030 --> 00:31:33.490
And then we set the x and y
dimensions that we want, in

00:31:33.490 --> 00:31:37.560
this case, 256 wide
by steps high.

00:31:37.560 --> 00:31:39.230
We create a type from that.

00:31:39.230 --> 00:31:42.310
That's the dimensions
of the allocation.

00:31:42.310 --> 00:31:46.850
And then we actually allocate
the back end store from that.

00:31:46.850 --> 00:31:50.220
So we'll also need to create
that 1D buffer to hold the

00:31:50.220 --> 00:31:52.840
final histogram.

00:31:52.840 --> 00:31:55.310
And in this case, we'll
again choose INT.

00:31:55.310 --> 00:31:57.540
But we have a helper function so
that you don't necessarily

00:31:57.540 --> 00:32:00.020
have to create a type for every

00:32:00.020 --> 00:32:02.040
allocation you want to create.

00:32:02.040 --> 00:32:04.750
If it's a 1D allocation, we
actually have a helper

00:32:04.750 --> 00:32:06.450
function that will automatically
create

00:32:06.450 --> 00:32:08.210
a 1D type for you.

00:32:08.210 --> 00:32:11.720
In this case, 256 elements.

00:32:11.720 --> 00:32:14.960
And then we set those
allocations we just created to

00:32:14.960 --> 00:32:17.910
the script.

00:32:17.910 --> 00:32:21.100
So, running the first pass.

00:32:21.100 --> 00:32:25.590
Now, launching scripts from
Java, very straightforward.

00:32:25.590 --> 00:32:27.030
But we're going to do something
a little more

00:32:27.030 --> 00:32:30.120
complicated than we talked
about in the blur.

00:32:30.120 --> 00:32:32.360
If you remember when we talked
about the first paths, we said

00:32:32.360 --> 00:32:34.950
we're going to clip this kernel
so that it only ran on

00:32:34.950 --> 00:32:39.610
x equals zero, but actually
iterated over all the y's.

00:32:39.610 --> 00:32:43.270
So coming soon we have on the
Java side, a new option for

00:32:43.270 --> 00:32:46.300
launching kernels, and the
ability to clip the kernel to

00:32:46.300 --> 00:32:48.950
the region of interest
that you want that

00:32:48.950 --> 00:32:49.580
kernel to run on.

00:32:49.580 --> 00:32:53.020
This is very useful if you want
to either do what I'm

00:32:53.020 --> 00:32:57.320
about to do here, or if you're
just editing part of an image.

00:32:57.320 --> 00:33:00.430
We create a launch option
structure, and then

00:33:00.430 --> 00:33:01.700
we set the x range.

00:33:01.700 --> 00:33:04.080
The first value's inclusive, the
second value's exclusive,

00:33:04.080 --> 00:33:06.200
so that's why you see 0 to 1.

00:33:06.200 --> 00:33:10.120
And then we do our for each
pass, and coming soon, not

00:33:10.120 --> 00:33:13.220
only can you specify the input
or output allocation, but we

00:33:13.220 --> 00:33:15.700
actually reflect overloaded
versions of this that will

00:33:15.700 --> 00:33:21.560
take the launch options,
if you so choose.

00:33:21.560 --> 00:33:24.450
And so what does this actually
look like running?

00:33:24.450 --> 00:33:27.030
So we have the allocation
we created.

00:33:27.030 --> 00:33:28.820
The kernel's going
to an each time C

00:33:28.820 --> 00:33:30.410
x-coordinate equals zero.

00:33:30.410 --> 00:33:32.950
And it's going to see a
y-coordinate that is the

00:33:32.950 --> 00:33:35.920
number of steps, as
we drew earlier.

00:33:40.430 --> 00:33:41.830
Now, running the second pass.

00:33:41.830 --> 00:33:46.050
So after you run the first pass,
the output will be in

00:33:46.050 --> 00:33:48.210
that intermediate buffer.

00:33:48.210 --> 00:33:52.160
And you can just immediately
call the for each on pass two,

00:33:52.160 --> 00:33:55.200
and this will sum the first
buffer into the second, and

00:33:55.200 --> 00:33:58.670
now you have a single
allocation, which contains

00:33:58.670 --> 00:34:00.680
your range of values.

00:34:00.680 --> 00:34:04.350
And we can simply call
re-scale by invoke.

00:34:04.350 --> 00:34:05.520
If you notice, this
doesn't take any

00:34:05.520 --> 00:34:07.320
parameters in this example.

00:34:07.320 --> 00:34:10.610
Invoke functions are actually
very useful for

00:34:10.610 --> 00:34:12.370
things like set up.

00:34:12.370 --> 00:34:16.150
So if in the dot RS file, you
put some parameters in the

00:34:16.150 --> 00:34:19.150
prototype for that function,
you can actually pass them

00:34:19.150 --> 00:34:20.469
directly here.

00:34:20.469 --> 00:34:21.969
However, you cannot
return values.

00:34:21.969 --> 00:34:25.719
So the prototype always
must be void.

00:34:25.719 --> 00:34:28.969
And you have your histogram.

00:34:28.969 --> 00:34:32.219
So with that, I'm going to open
it up to questions, if

00:34:32.219 --> 00:34:33.469
Tim would join us again.

00:34:36.179 --> 00:34:37.429
Thanks, everyone.

00:34:44.159 --> 00:34:46.620
One other comment, during office
hours I actually have

00:34:46.620 --> 00:34:51.440
both an ARM and x86 device that
can be seen running this

00:34:51.440 --> 00:34:53.790
example, if anyone's curious.

00:34:53.790 --> 00:34:57.240
But it runs very well
on both devices.

00:34:57.240 --> 00:35:01.730
The good news here is by writing
in the agnostic code,

00:35:01.730 --> 00:35:04.640
we actually generate back end
code that is optimal for all.

00:35:10.220 --> 00:35:11.095
Hello.

00:35:11.095 --> 00:35:13.740
Take the first question?

00:35:13.740 --> 00:35:15.020
AUDIENCE: So you had mentioned
two things.

00:35:15.020 --> 00:35:18.230
You mentioned that the ISP and
DSPs, so that's the mobile

00:35:18.230 --> 00:35:22.480
devices, is there any access to
the fixed function hardware

00:35:22.480 --> 00:35:25.730
like the fast resizing, the fast
color space conversion

00:35:25.730 --> 00:35:27.210
you see on ISPs?

00:35:27.210 --> 00:35:30.850
And then my second question
was are there any plans to

00:35:30.850 --> 00:35:34.150
have RenderScript run on
something other than Android?

00:35:34.150 --> 00:35:38.830
Even if it's Chromebooks, or
just regular old Linux.

00:35:38.830 --> 00:35:40.970
JASON SAMS: OK, I'll go ahead
and answer the first one.

00:35:40.970 --> 00:35:42.620
The answer is yes.

00:35:42.620 --> 00:35:46.970
Part of the reason we have
intrinsics is we can use

00:35:46.970 --> 00:35:50.730
processors that are more fixed
function than you can by

00:35:50.730 --> 00:35:53.930
writing code directly from
a dot RS, or dot FS file.

00:35:53.930 --> 00:35:55.990
And so something like the
Gaussian Blur we used as an

00:35:55.990 --> 00:35:58.090
example, we actually have other
intrinsics like involved

00:35:58.090 --> 00:36:02.450
3x3 and 5x5, those will run very
well on something like a

00:36:02.450 --> 00:36:04.510
fixed function ISP or DSP.

00:36:04.510 --> 00:36:07.850
Another example that's our YUV
to RGB conversion, again, you

00:36:07.850 --> 00:36:09.140
should be able to take advantage
of that fixed

00:36:09.140 --> 00:36:10.490
function hardware, because
it's going to be both

00:36:10.490 --> 00:36:15.120
extremely power efficient,
and usually very fast.

00:36:15.120 --> 00:36:16.210
I'll let you take
the second one.

00:36:16.210 --> 00:36:19.660
TIM MURRAY: So as far as other
platforms, it's possible.

00:36:19.660 --> 00:36:24.600
Basically during my spare time
in the last six months or so,

00:36:24.600 --> 00:36:29.980
I ported this two CUBE
standard Linux.

00:36:29.980 --> 00:36:32.940
It just runs on the CPU, but it
was really straightforward,

00:36:32.940 --> 00:36:37.710
because we make such heavy use
of LLVM and Clang, essentially

00:36:37.710 --> 00:36:41.477
we can port trivially to any
platform that has an LLVM and

00:36:41.477 --> 00:36:44.410
Clang, good LLVM and
CLang support.

00:36:44.410 --> 00:36:47.220
So in an upcoming AOSP release,
you'll see all sorts

00:36:47.220 --> 00:36:51.020
of ifdefs for things
like RS server.

00:36:51.020 --> 00:36:53.800
And if you wanted to construct
your own MIG file with an

00:36:53.800 --> 00:36:55.520
appropriate version of LLVM
and Clang, you could

00:36:55.520 --> 00:36:57.240
certainly do that.

00:36:57.240 --> 00:36:59.200
AUDIENCE: Thanks.

00:36:59.200 --> 00:36:59.925
AUDIENCE: Hi.

00:36:59.925 --> 00:37:04.500
I was wondering when the API
18 RenderScript stuff gets

00:37:04.500 --> 00:37:05.700
into the compatibility
package.

00:37:05.700 --> 00:37:08.060
Will that include the
script intrinsics?

00:37:08.060 --> 00:37:09.183
JASON SAMS: Yes.

00:37:09.183 --> 00:37:09.556
AUDIENCE: OK.

00:37:09.556 --> 00:37:10.420
Easy question.

00:37:10.420 --> 00:37:12.120
Oh, follow up.

00:37:12.120 --> 00:37:13.510
Is there any time
frame for that?

00:37:13.510 --> 00:37:13.765
JASON SAMS: No.

00:37:13.765 --> 00:37:16.210
TIM MURRAY: No.

00:37:16.210 --> 00:37:18.260
JASON SAMS: You can't
trick us.

00:37:18.260 --> 00:37:21.470
AUDIENCE: So just to confirm,
the Gingerbread stuff is

00:37:21.470 --> 00:37:23.530
dependent upon 18 coming out.

00:37:23.530 --> 00:37:24.412
And then the compatibility
library

00:37:24.412 --> 00:37:26.100
will have that, correct?

00:37:26.100 --> 00:37:26.860
TIM MURRAY: Correct.

00:37:26.860 --> 00:37:29.360
AUDIENCE: OK, follow up is what
memory constraints are

00:37:29.360 --> 00:37:32.400
there for the size of the
RenderScript kernels

00:37:32.400 --> 00:37:34.340
that you can build?

00:37:34.340 --> 00:37:37.280
TIM MURRAY: What do you
mean by the size?

00:37:37.280 --> 00:37:41.900
AUDIENCE: Byte code size, like
how big of function or sets of

00:37:41.900 --> 00:37:45.895
functions can you build for
might be hitting memory issues

00:37:45.895 --> 00:37:48.326
in your device dependent,
or things that you

00:37:48.326 --> 00:37:49.670
have seen in the wild?

00:37:49.670 --> 00:37:52.500
JASON SAMS: I've seen an entire
H264 encoder written in

00:37:52.500 --> 00:37:56.240
RenderScript, so the size
limits are pretty high.

00:37:56.240 --> 00:37:57.770
TIM MURRAY: Yeah, I don't think
you'll run into too many

00:37:57.770 --> 00:37:59.100
issues with code size.

00:38:01.610 --> 00:38:02.790
AUDIENCE: Hi, my name is Mark.

00:38:02.790 --> 00:38:05.460
I would like to know, like, more
from the graphical side

00:38:05.460 --> 00:38:10.070
of Android, so they are really
high, dedicated examples of

00:38:10.070 --> 00:38:11.520
doing graphics in
Windows script.

00:38:11.520 --> 00:38:14.910
Like for example, the page
flipping in the ebook reader,

00:38:14.910 --> 00:38:18.960
and in the YouTube app, and I
guess the cover flow and the

00:38:18.960 --> 00:38:22.210
play music is also done
with Windows script.

00:38:22.210 --> 00:38:25.670
Have you thought of building
a website with really high,

00:38:25.670 --> 00:38:27.810
dedicated examples with
Windows script?

00:38:30.650 --> 00:38:33.030
TIM MURRAY: So we are trying to
improve our documentation

00:38:33.030 --> 00:38:34.850
and improve our samples.

00:38:34.850 --> 00:38:39.720
In API [INAUDIBLE] you'll see
a lot of new and kind of

00:38:39.720 --> 00:38:42.030
hopefully clearer
documentation.

00:38:42.030 --> 00:38:45.150
Samples we are working on, in
general, one of hte best

00:38:45.150 --> 00:38:48.420
places to look right now, we
have a test called image

00:38:48.420 --> 00:38:50.050
processing.

00:38:50.050 --> 00:38:51.660
You can find that in ALSP.

00:38:51.660 --> 00:38:54.460
That has a lot of different
kernels, and can basically

00:38:54.460 --> 00:38:56.840
give you a good idea of how
to write a RenderScript

00:38:56.840 --> 00:38:58.250
application.

00:38:58.250 --> 00:39:01.150
AUDIENCE: So, no website
available with great examples

00:39:01.150 --> 00:39:04.342
here, just the examples
in the SDK?

00:39:04.342 --> 00:39:05.480
TIM MURRAY: Yeah, most likely.

00:39:05.480 --> 00:39:09.390
I mean, that's where we're
focusing for now.

00:39:09.390 --> 00:39:12.560
AUDIENCE: You said you're not
going to support anything

00:39:12.560 --> 00:39:14.663
other than the CPU
on Gingerbread.

00:39:14.663 --> 00:39:17.026
Is there a technical reason
you can't do that?

00:39:17.026 --> 00:39:19.440
I mean, there are a lot
of usable phone DSPs.

00:39:19.440 --> 00:39:21.060
TIM MURRAY: Yeah, so essentially
we would need a

00:39:21.060 --> 00:39:28.700
new driver on there, and the way
we have our driver work is

00:39:28.700 --> 00:39:30.340
we're an OS component.

00:39:30.340 --> 00:39:33.090
And the way we can get around
that on the compatibility

00:39:33.090 --> 00:39:37.790
library is by loading these
shared libraries, which,

00:39:37.790 --> 00:39:39.390
they're not OS components.

00:39:39.390 --> 00:39:42.640
So essentially, if we were to
try to support anything more

00:39:42.640 --> 00:39:47.130
than the CPU on Gingerbread,
they would have to update that

00:39:47.130 --> 00:39:49.290
part of their OS.

00:39:49.290 --> 00:39:51.390
AUDIENCE: So is there no way to
get access to, say, like,

00:39:51.390 --> 00:39:53.160
TIDSP or anything like that?

00:39:53.160 --> 00:39:54.375
TIM MURRAY: Not on
Gingerbread, no.

00:39:54.375 --> 00:39:56.250
JASON SAMS: The support
necessarily in the OS simply

00:39:56.250 --> 00:39:59.020
wasn't there in Gingerbread, so
there's really nothing we

00:39:59.020 --> 00:40:02.020
can do without an OS update.

00:40:02.020 --> 00:40:02.760
OK, thanks everyone.

00:40:02.760 --> 00:40:05.600
I think we're out of time, so
will be around for office

00:40:05.600 --> 00:40:07.310
hours if anyone has additional
questions.

