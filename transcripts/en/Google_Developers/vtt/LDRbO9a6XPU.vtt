WEBVTT
Kind: captions
Language: en

00:00:05.486 --> 00:00:06.610
JOSH GORDON: Hey, everyone.

00:00:06.610 --> 00:00:07.709
Welcome back.

00:00:07.709 --> 00:00:10.000
In this episode, we'll write
a decision tree classifier

00:00:10.000 --> 00:00:12.560
from scratch in pure Python.

00:00:12.560 --> 00:00:14.420
Here's an outline
of what we'll cover.

00:00:14.420 --> 00:00:17.330
I'll start by introducing
the data set we'll work with.

00:00:17.330 --> 00:00:19.310
Next, we'll preview
the completed tree.

00:00:19.310 --> 00:00:20.450
And then, we'll build it.

00:00:20.450 --> 00:00:23.030
On the way, we'll cover concepts
like decision tree learning,

00:00:23.030 --> 00:00:24.880
Gini impurity, and
information gain.

00:00:24.880 --> 00:00:26.630
And you can find the
code for this episode

00:00:26.630 --> 00:00:27.560
in the description.

00:00:27.560 --> 00:00:29.600
And it's available
in two formats,

00:00:29.600 --> 00:00:32.990
both as a Jupiter notebook
and as a regular Python file.

00:00:32.990 --> 00:00:34.679
OK, let's get started.

00:00:34.679 --> 00:00:36.387
For this episode, I've
written a toy data

00:00:36.387 --> 00:00:39.590
set that includes both numeric
and categorical attributes.

00:00:39.590 --> 00:00:42.050
And here, our goal will be
to predict the type of fruit,

00:00:42.050 --> 00:00:44.360
like an apple or a
grape, based on features

00:00:44.360 --> 00:00:46.287
like color and size.

00:00:46.287 --> 00:00:47.870
At the end of the
episode, I encourage

00:00:47.870 --> 00:00:50.240
you to swap out this data
set for one of your own

00:00:50.240 --> 00:00:53.250
and build a tree for a
problem you care about.

00:00:53.250 --> 00:00:54.390
Let's look at the format.

00:00:54.390 --> 00:00:56.420
I've re-drawn it
here for clarity.

00:00:56.420 --> 00:00:57.620
Each row is an example.

00:00:57.620 --> 00:01:00.230
And the first two columns
provide features or attributes

00:01:00.230 --> 00:01:02.064
that describe the data.

00:01:02.064 --> 00:01:03.980
The last column gives
the label, or the class,

00:01:03.980 --> 00:01:05.230
we want to predict.

00:01:05.230 --> 00:01:07.310
And if you like, you
can modify this data set

00:01:07.310 --> 00:01:09.740
by adding additional
features or more examples,

00:01:09.740 --> 00:01:12.095
and our program will work
in exactly the same way.

00:01:12.095 --> 00:01:13.970
Now, this data set is
pretty straightforward,

00:01:13.970 --> 00:01:15.470
except for one thing.

00:01:15.470 --> 00:01:18.192
I've written it so it's
not perfectly separable.

00:01:18.192 --> 00:01:20.150
And by that I mean there's
no way to tell apart

00:01:20.150 --> 00:01:22.340
the second and fifth examples.

00:01:22.340 --> 00:01:24.650
They have the same features,
but different labels.

00:01:24.650 --> 00:01:27.555
And this is so we can see how
our tree handles this case.

00:01:27.555 --> 00:01:29.180
Towards the end of
the notebook, you'll

00:01:29.180 --> 00:01:31.040
find testing data
in the same format.

00:01:33.200 --> 00:01:35.700
Now I've written a few utility
functions that make it easier

00:01:35.700 --> 00:01:36.947
to work with this data.

00:01:36.947 --> 00:01:39.030
And below each function,
I've written a small demo

00:01:39.030 --> 00:01:40.351
to show how it works.

00:01:40.351 --> 00:01:42.600
And I've repeated this pattern
for every block of code

00:01:42.600 --> 00:01:45.477
in the notebook.

00:01:45.477 --> 00:01:47.810
Now to build the tree, we use
the decision tree learning

00:01:47.810 --> 00:01:49.299
algorithm called CART.

00:01:49.299 --> 00:01:51.590
And as it happens, there's
a whole family of algorithms

00:01:51.590 --> 00:01:53.404
used to build trees from data.

00:01:53.404 --> 00:01:55.070
At their core, they
give you a procedure

00:01:55.070 --> 00:01:58.430
to decide which questions
to ask and when.

00:01:58.430 --> 00:02:01.190
CART stands for Classification
and Regression Trees.

00:02:01.190 --> 00:02:04.040
And here's a preview
of how it works.

00:02:04.040 --> 00:02:06.500
To begin, we'll add a
root node for the tree.

00:02:06.500 --> 00:02:09.080
And all nodes receive a
list of rows as input.

00:02:09.080 --> 00:02:12.090
And the root will receive
the entire training set.

00:02:12.090 --> 00:02:14.150
Now each node will ask
a true false question

00:02:14.150 --> 00:02:16.010
about one of the features.

00:02:16.010 --> 00:02:17.630
And in response
to this question,

00:02:17.630 --> 00:02:21.300
we split, or partition,
the data into two subsets.

00:02:21.300 --> 00:02:24.080
These subsets then become
the input to two child nodes

00:02:24.080 --> 00:02:25.970
we add to the tree.

00:02:25.970 --> 00:02:28.520
And the goal of the question
is to unmix the labels

00:02:28.520 --> 00:02:30.050
as we proceed down.

00:02:30.050 --> 00:02:32.600
Or in other words, to
produce the purest possible

00:02:32.600 --> 00:02:35.552
distribution of the
labels at each node.

00:02:35.552 --> 00:02:37.010
For example, the
input to this node

00:02:37.010 --> 00:02:39.410
contains only a
single type of label,

00:02:39.410 --> 00:02:41.570
so we'd say it's
perfectly unmixed.

00:02:41.570 --> 00:02:44.630
There's no uncertainty
about the type of label.

00:02:44.630 --> 00:02:47.540
On the other hand, the labels
in this node are still mixed up,

00:02:47.540 --> 00:02:50.510
so we'd ask another question
to further narrow it down.

00:02:50.510 --> 00:02:52.400
And the trick to building
an effective tree

00:02:52.400 --> 00:02:55.590
is to understand which
questions to ask and when.

00:02:55.590 --> 00:02:58.340
And to do that, we need to
quantify how much a question

00:02:58.340 --> 00:03:00.216
helps to unmix the labels.

00:03:00.216 --> 00:03:02.090
And we can quantify the
amount of uncertainty

00:03:02.090 --> 00:03:05.176
at a single node using a
metric called Gini impurity.

00:03:05.176 --> 00:03:06.800
And we can quantify
how much a question

00:03:06.800 --> 00:03:09.140
reduces that uncertainty
using a concept

00:03:09.140 --> 00:03:11.100
called information gain.

00:03:11.100 --> 00:03:12.770
We'll use these
to select the best

00:03:12.770 --> 00:03:14.540
question to ask at each point.

00:03:14.540 --> 00:03:17.150
And given that question, we'll
recursively build the tree

00:03:17.150 --> 00:03:18.840
on each of the new nodes.

00:03:18.840 --> 00:03:20.840
We'll continue dividing
the data until there

00:03:20.840 --> 00:03:23.330
are no further questions
to ask, at which point

00:03:23.330 --> 00:03:24.680
we'll add a leaf.

00:03:24.680 --> 00:03:26.630
To implement this, first
we need to understand

00:03:26.630 --> 00:03:29.120
what type of questions
can we ask about the data.

00:03:29.120 --> 00:03:30.680
And second, we
need to understand

00:03:30.680 --> 00:03:32.770
how to decide which
question to ask when.

00:03:35.520 --> 00:03:37.980
Now each node takes a
list of rows as input.

00:03:37.980 --> 00:03:39.660
And to generate a
list of questions

00:03:39.660 --> 00:03:42.360
we'll iterate over every
value for every feature that

00:03:42.360 --> 00:03:44.166
appears in those rows.

00:03:44.166 --> 00:03:45.540
Each of these
becomes a candidate

00:03:45.540 --> 00:03:48.240
for a threshold we can
use to partition the data.

00:03:48.240 --> 00:03:51.120
And there will often
be many possibilities.

00:03:51.120 --> 00:03:52.950
In code we represent
a question by storing

00:03:52.950 --> 00:03:55.170
a column number
and a column value,

00:03:55.170 --> 00:03:58.035
or the threshold we'll
use to partition the data.

00:03:58.035 --> 00:03:59.910
For example, here's how
we'd write a question

00:03:59.910 --> 00:04:01.850
to test if the color is green.

00:04:01.850 --> 00:04:03.930
And here's an example
for a numeric attribute

00:04:03.930 --> 00:04:07.620
to test if the diameter is
greater than or equal to 3.

00:04:07.620 --> 00:04:10.320
In response to a question, we
divide, or partition, the data

00:04:10.320 --> 00:04:12.060
into two subsets.

00:04:12.060 --> 00:04:14.880
The first contains all the rows
for which the question is true.

00:04:14.880 --> 00:04:17.610
And the second contains
everything else.

00:04:17.610 --> 00:04:19.740
In code, our partition
function takes a question

00:04:19.740 --> 00:04:21.720
and a list of rows as input.

00:04:21.720 --> 00:04:24.060
For example, here's how we
partition the rows based

00:04:24.060 --> 00:04:26.950
on whether the color is red.

00:04:26.950 --> 00:04:29.214
Here, true rows contains
all the red examples.

00:04:29.214 --> 00:04:30.880
And false rows contains
everything else.

00:04:33.886 --> 00:04:36.260
The best question is the one
that reduces our uncertainty

00:04:36.260 --> 00:04:37.220
the most.

00:04:37.220 --> 00:04:39.800
And Gini impurity let's us
quantify how much uncertainty

00:04:39.800 --> 00:04:41.180
there is at a node.

00:04:41.180 --> 00:04:43.190
Information gain will
let us quantify how much

00:04:43.190 --> 00:04:44.960
a question reduces that.

00:04:44.960 --> 00:04:46.880
Let's work on impurity first.

00:04:46.880 --> 00:04:49.490
Now this is a metric that
ranges between 0 and 1

00:04:49.490 --> 00:04:52.190
where lower values indicate
less uncertainty, or mixing,

00:04:52.190 --> 00:04:53.540
at a node.

00:04:53.540 --> 00:04:56.480
It quantifies our chance of
being incorrect if we randomly

00:04:56.480 --> 00:05:00.560
assign a label from a set
to an example in that set.

00:05:00.560 --> 00:05:02.940
Here's an example
to make that clear.

00:05:02.940 --> 00:05:06.240
Imagine we have two bowls
and one contains the examples

00:05:06.240 --> 00:05:08.430
and the other contains labels.

00:05:08.430 --> 00:05:11.760
First, we'll randomly draw an
example from the first bowl.

00:05:11.760 --> 00:05:14.130
Then we'll randomly draw
a label from the second.

00:05:14.130 --> 00:05:17.580
And now, we'll classify the
example as having that label.

00:05:17.580 --> 00:05:21.330
And Gini impurity gives us
our chance of being incorrect.

00:05:21.330 --> 00:05:23.970
In this example, we have
only apples in each bowl.

00:05:23.970 --> 00:05:25.480
There's no way to
make a mistake.

00:05:25.480 --> 00:05:28.335
So we say the impurity is zero.

00:05:28.335 --> 00:05:30.710
On the other hand, given a
bowl with five different types

00:05:30.710 --> 00:05:32.590
of fruit in equal
proportion, we'd

00:05:32.590 --> 00:05:35.120
say it has an impurity of 0.8.

00:05:35.120 --> 00:05:37.790
That's because we have a one out
of five chance of being right

00:05:37.790 --> 00:05:41.650
if we randomly assign
a label to an example.

00:05:41.650 --> 00:05:44.474
In code, this method calculates
the impurity of a data set.

00:05:44.474 --> 00:05:45.890
And I've written
a couple examples

00:05:45.890 --> 00:05:48.110
below that demonstrate
how it works.

00:05:48.110 --> 00:05:50.360
You can see the impurity
for the first set is zero

00:05:50.360 --> 00:05:51.770
because there's no mixing.

00:05:51.770 --> 00:05:57.010
And here, you can see
the impurity is 0.8.

00:05:57.010 --> 00:05:59.890
Now information gain will let us
find the question that reduces

00:05:59.890 --> 00:06:01.319
our uncertainty the most.

00:06:01.319 --> 00:06:02.860
And it's just a
number that describes

00:06:02.860 --> 00:06:06.340
how much a question helps to
unmix the labels at a node.

00:06:06.340 --> 00:06:07.910
Here's the idea.

00:06:07.910 --> 00:06:09.580
We begin by calculating
the uncertainty

00:06:09.580 --> 00:06:11.230
of our starting set.

00:06:11.230 --> 00:06:12.750
Then, for each
question we can ask,

00:06:12.750 --> 00:06:15.160
we'll try partitioning
the data and calculating

00:06:15.160 --> 00:06:18.087
the uncertainty of the
child nodes that result.

00:06:18.087 --> 00:06:20.170
We'll take a weighted
average of their uncertainty

00:06:20.170 --> 00:06:22.870
because we care more about a
large set with low uncertainty

00:06:22.870 --> 00:06:25.667
than a small set with high.

00:06:25.667 --> 00:06:28.000
Then, we'll subtract this
from our starting uncertainty.

00:06:28.000 --> 00:06:29.722
And that's our information gain.

00:06:29.722 --> 00:06:31.680
As we go, we'll keep
track of the question that

00:06:31.680 --> 00:06:33.300
produces the most gain.

00:06:33.300 --> 00:06:36.570
And that will be the best
one to ask at this node.

00:06:36.570 --> 00:06:38.460
Let's see how this
looks in code.

00:06:38.460 --> 00:06:41.409
Here, we'll iterate over
every value for the features.

00:06:41.409 --> 00:06:43.200
We'll generate a question
for that feature,

00:06:43.200 --> 00:06:45.750
then partition the data on it.

00:06:45.750 --> 00:06:49.259
Notice we discard any questions
that fail to produce a split.

00:06:49.259 --> 00:06:51.050
Then, we'll calculate
our information gain.

00:06:51.050 --> 00:06:52.424
And inside this
function, you can

00:06:52.424 --> 00:06:56.107
see we take a weighted average
and the impurity of each set.

00:06:56.107 --> 00:06:57.940
We see how much this
reduces the uncertainty

00:06:57.940 --> 00:06:59.350
from our starting set.

00:06:59.350 --> 00:07:01.850
And we keep track
of the best value.

00:07:01.850 --> 00:07:04.510
I've written a couple
of demos below as well.

00:07:04.510 --> 00:07:07.560
OK, with these concepts in hand,
we're ready to build the tree.

00:07:07.560 --> 00:07:10.060
And to put this all together I
think the most useful thing I

00:07:10.060 --> 00:07:11.890
can do is walk you
through the algorithm

00:07:11.890 --> 00:07:14.200
as it builds a tree
for our training data.

00:07:14.200 --> 00:07:17.770
This uses recursion, so seeing
it in action can be helpful.

00:07:17.770 --> 00:07:21.490
You can find the code for this
inside the Build Tree function.

00:07:21.490 --> 00:07:23.350
When we call build tree
for the first time,

00:07:23.350 --> 00:07:26.050
it receives the entire
training set as input.

00:07:26.050 --> 00:07:27.940
And as output it will
return a reference

00:07:27.940 --> 00:07:30.340
to the root node of our tree.

00:07:30.340 --> 00:07:33.040
I'll draw a placeholder
for the root here in gray.

00:07:33.040 --> 00:07:35.280
And here are the rows we're
considering at this node.

00:07:35.280 --> 00:07:38.380
And to start, that's
the entire training set.

00:07:38.380 --> 00:07:40.960
Now we find the best
question to ask at this node.

00:07:40.960 --> 00:07:44.810
And we do that by iterating
over each of these values.

00:07:44.810 --> 00:07:47.180
We'll split the data and
calculate the information

00:07:47.180 --> 00:07:48.560
gained for each one.

00:07:48.560 --> 00:07:50.690
And as we go, we'll keep
track of the question that

00:07:50.690 --> 00:07:53.790
produces the most gain.

00:07:53.790 --> 00:07:55.980
Now in this case, there's
a useful question to ask,

00:07:55.980 --> 00:07:57.960
so the gain will be
greater than zero.

00:07:57.960 --> 00:08:00.750
And we'll split the data
using that question.

00:08:00.750 --> 00:08:03.780
And now, we'll use recursion
by calling build tree again

00:08:03.780 --> 00:08:06.479
to add a node for
the true branch.

00:08:06.479 --> 00:08:08.520
The rows we're considering
now are the first half

00:08:08.520 --> 00:08:09.570
of the split.

00:08:09.570 --> 00:08:13.170
And again, we'll find the best
question to ask for this data.

00:08:13.170 --> 00:08:15.870
Once more we split and call
the build tree function

00:08:15.870 --> 00:08:17.910
to add the child node.

00:08:17.910 --> 00:08:20.400
Now for this data there are
no further questions to ask.

00:08:20.400 --> 00:08:22.290
So the information
gain will be zero.

00:08:22.290 --> 00:08:24.562
And this node becomes a leaf.

00:08:24.562 --> 00:08:26.270
It will predict that
an example is either

00:08:26.270 --> 00:08:28.820
an apple or a lemon
with 50% confidence

00:08:28.820 --> 00:08:32.460
because that's the ratio
of the labels in the data.

00:08:32.460 --> 00:08:34.640
Now we'll continue by
building the false branch.

00:08:34.640 --> 00:08:36.860
And here, this will
also become a leaf.

00:08:36.860 --> 00:08:40.590
We'll predict apple
with 100% confidence.

00:08:40.590 --> 00:08:42.480
Now the previous call
returns, and this node

00:08:42.480 --> 00:08:44.986
becomes a decision node.

00:08:44.986 --> 00:08:46.860
In code, that just means
it holds a reference

00:08:46.860 --> 00:08:49.975
to the question we asked and
the two child nodes that result.

00:08:49.975 --> 00:08:52.020
And we're nearly done.

00:08:52.020 --> 00:08:55.230
Now we return to the root node
and build the false branch.

00:08:55.230 --> 00:08:58.200
There are no further questions
to ask, so this becomes a leaf.

00:08:58.200 --> 00:09:00.860
And that predicts grape
with 100% confidence.

00:09:00.860 --> 00:09:04.050
And finally, the root node
also becomes a decision node.

00:09:04.050 --> 00:09:06.787
And our call to build tree
returns a reference to it.

00:09:06.787 --> 00:09:08.370
If you scroll down
in the code, you'll

00:09:08.370 --> 00:09:10.740
see that I've added functions
to classify data and print

00:09:10.740 --> 00:09:11.490
the tree.

00:09:11.490 --> 00:09:13.594
And these start with a
reference to the root node,

00:09:13.594 --> 00:09:14.760
so you can see how it works.

00:09:17.610 --> 00:09:18.850
OK, hope that was helpful.

00:09:18.850 --> 00:09:21.340
And you can check out the
code for more details.

00:09:21.340 --> 00:09:23.680
There's a lot more I have
to say about decision trees,

00:09:23.680 --> 00:09:25.955
but there's only so much we
can fit into a short time.

00:09:25.955 --> 00:09:28.330
Here are a couple of topics
that are good to be aware of.

00:09:28.330 --> 00:09:30.413
And you can check out the
books in the description

00:09:30.413 --> 00:09:31.420
to learn more.

00:09:31.420 --> 00:09:33.430
As a next step, I'd
recommend modifying the tree

00:09:33.430 --> 00:09:35.224
to work with your own data set.

00:09:35.224 --> 00:09:36.640
And this can be a
fun way to build

00:09:36.640 --> 00:09:38.830
a simple and interpretable
classifier for use

00:09:38.830 --> 00:09:40.011
in your projects.

00:09:40.011 --> 00:09:41.260
Thanks for watching, everyone.

00:09:41.260 --> 00:09:43.290
And I'll see you next time.

