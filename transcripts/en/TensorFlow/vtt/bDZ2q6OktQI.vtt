WEBVTT
Kind: captions
Language: en

00:42:06.810 --> 00:42:08.810
some examples.

00:42:10.870 --> 00:42:12.870
The first thing you need is 
data.

00:42:13.881 --> 00:42:16.701
You may want to validate results
or test ideas on a common public

00:42:16.702 --> 00:42:18.702
dataset.

00:42:20.742 --> 00:42:21.565
It includes a large and rapidly 
growing collection of datasets 

00:42:21.566 --> 00:42:23.566
you can get 

00:42:25.458 --> 00:42:27.905
started with easily and combined
with tf.data it is simple to 

00:42:27.906 --> 00:42:30.741
wrap your own data too. Here is 
a small sample of the datasets 

00:42:36.247 --> 00:42:39.325
available and all of these and 
many nor are included there. 

00:42:42.803 --> 00:42:44.803
Then with Keras, you can express
the 

00:42:46.455 --> 00:42:48.455
models just like you are used to
thinking about it. 

00:42:48.711 --> 00:42:51.360
Standard package is fit with 
model fit and evaluate as well. 

00:42:55.047 --> 00:42:57.047
Since deep learning models are 
often 

00:42:59.342 --> 00:43:01.342
commutationally expensive you 
way want 

00:43:06.278 --> 00:43:08.278
to try scaling this across more 
than one device. 

00:43:10.805 --> 00:43:13.464
Starting from a pre-trained 
model or component also works 

00:43:13.465 --> 00:43:15.465
well to reduce some 

00:43:16.782 --> 00:43:18.814
of this computational cost. 
TensorFlow helps provide a large

00:43:20.226 --> 00:43:22.226
collection of pretained 
components you 

00:43:27.608 --> 00:43:29.608
can include in your model and 
Feinstein

00:43:31.947 --> 00:43:33.947
-- fine tune for your dataset.

00:43:35.006 --> 00:43:37.250
Keras comes with everything you 
might need for a typical 

00:43:37.251 --> 00:43:39.251
training job.

00:43:41.190 --> 00:43:43.627
Sometimes you need a bit more 
control. For example when you 

00:43:43.628 --> 00:43:45.628
are exploring new kinds of 
algorithms.

00:43:48.711 --> 00:43:50.711
Let's say you wanted to build a 
custom 

00:43:51.972 --> 00:43:53.972
encoder for machine translation,
here is 

00:43:56.851 --> 00:43:58.851
how you could do this by 
subclassing the model.

00:44:01.127 --> 00:44:03.127
You can even customize it 
training loop 

00:44:04.197 --> 00:44:06.197
to get full control over the 
gradients 

00:44:07.262 --> 00:44:09.262
and optimization process.

00:44:11.000 --> 00:44:13.643
While training models, whether 
packaged with Keras or more 

00:44:13.644 --> 00:44:15.644
complex ones, it is often 
valuable to understand the 

00:44:19.141 --> 00:44:21.141
progress and even analyze the 
muddle in detail.

00:44:23.230 --> 00:44:24.855
TensorFlow board provides a lot 
of visualization to help with 

00:44:24.856 --> 00:44:26.856
chis and 

00:44:27.913 --> 00:44:29.913
comes full integration with 
Colab and 

00:44:32.392 --> 00:44:34.392
other Jupyter notebooks allowing
you to 

00:44:35.483 --> 00:44:37.309
see the same visuals. All of 
these features are available in 

00:44:37.310 --> 00:44:39.969
TensorFlow 2.0 and I am really 
excited to announce 

00:44:43.239 --> 00:44:45.239
our alpha release is available 
for you as of today. 

00:44:47.051 --> 00:44:51.993
[Applause] 
Many of you in the room and 

00:44:51.994 --> 00:44:55.446
across the world really helped 
with lots of work to make this 

00:44:55.447 --> 00:44:58.324
possible. I would really like to
take this moment to thank you 

00:44:58.325 --> 00:45:00.384
you all. Please give yourself a 
round of applause.

00:45:04.034 --> 00:45:06.034
We really couldn't have done 
this 

00:45:07.111 --> 00:45:09.111
without you.

00:45:11.985 --> 00:45:14.429
In addition to all the great 
improvements we talked about, 

00:45:14.430 --> 00:45:16.430
this 

00:45:17.878 --> 00:45:19.878
release comes with a converter 
script 

00:45:22.766 --> 00:45:24.766
and compatibility module to give
you 

00:45:25.964 --> 00:45:30.295
access to the 1.X APIs. We are 
working for a full release over 

00:45:30.905 --> 00:45:32.905
the next quarter.

00:45:34.388 --> 00:45:35.613
There is a lot of work going on 
to make TensorFlow 2.0 work well

00:45:35.614 --> 00:45:38.263
for you. You can track the 
progress and provide 

00:45:41.729 --> 00:45:43.729
feedback on the TensorFlow 
GitHub projects page.

00:45:45.589 --> 00:45:47.589
You asked for better 
documentation, and 

00:45:48.677 --> 00:45:50.677
we have worked to streamline our
docs 

00:45:52.780 --> 00:45:54.604
for APIs, guides and tutorials. 
All of this material will be 

00:45:54.605 --> 00:45:56.605
available 

00:45:58.921 --> 00:46:01.171
today on the newly redesigned 
TensorFlow.org website where you

00:46:01.172 --> 00:46:03.172
will find examples, 
documentation and tools to get 

00:46:03.201 --> 00:46:06.658
started. We are very excited 
about these changes and what's 

00:46:06.659 --> 00:46:11.765
to come. To tell you more about 
improvements in TensorFlow for 

00:46:11.766 --> 00:46:13.766
research and production, I would
like to welcome Megan Kacholia 

00:46:15.711 --> 00:46:17.711
on stage.
Thank you.

00:46:19.830 --> 00:46:23.715
Thank you. Thanks Rajat. 
TensorFlow has always been a 

00:46:23.716 --> 00:46:25.768
platform for research to 
production.

00:46:28.843 --> 00:46:31.077
We just saw how TensorFlow 
high-level APIs make it easy at 

00:46:31.078 --> 00:46:33.078
a get started and build models 
and now let's talk about 

00:46:35.548 --> 00:46:37.372
how it improves experimentation 
for research and let's you take 

00:46:37.373 --> 00:46:39.373
models from 

00:46:41.256 --> 00:46:43.741
research and production all the 
way through. We can see this in 

00:46:43.742 --> 00:46:45.742
paper publications 

00:46:47.422 --> 00:46:51.349
which are shown over the past 
few years in this chart. 

00:46:51.350 --> 00:46:53.350
Powerful experimentation begins 
and 

00:46:54.800 --> 00:46:57.252
really needs flexibility and 
this begins with eager execution

00:46:57.253 --> 00:46:59.294
and TensorFlow 2.0 every Python 
command is immediately executed.

00:46:59.495 --> 00:47:01.495
This means you can write your 
code in 

00:47:05.038 --> 00:47:05.648
the style you are used it 
without having to use session 

00:47:05.649 --> 00:47:08.386
realm. This makes a big 
difference in the realm of 

00:47:08.387 --> 00:47:10.387
debugging.

00:47:11.454 --> 00:47:13.454
As you iterate through, you will
want 

00:47:14.716 --> 00:47:16.716
to distribute your code on to 
GPUs and 

00:47:17.776 --> 00:47:19.823
TPUs and we have provided tf.
function turning your eager code

00:47:19.824 --> 00:47:23.298
into a graph 
function-by-function. You get 

00:47:23.299 --> 00:47:25.582
Python control flow, asserts and
even print but can convert to a 

00:47:27.815 --> 00:47:29.639
graph any time you need to, 
including when you are ready to 

00:47:29.640 --> 00:47:32.731
move your model into production.
Even with this, you will 

00:47:32.732 --> 00:47:36.262
continue to get great debugging.

00:47:39.935 --> 00:47:41.935
Debugability is great not just 
in eager 

00:47:43.627 --> 00:47:46.884
but we have made improves in tf.
function and graph. Because of 

00:47:46.885 --> 00:47:48.885
the mismatch inputs you get an 
error.

00:47:52.572 --> 00:47:54.572
As you can see, we give 
information to 

00:47:55.649 --> 00:47:57.064
user about the file and line 
number where the error occurs. 

00:48:01.362 --> 00:48:02.614
We have made the error messages 
concise, easy to understand and 

00:48:02.615 --> 00:48:07.295
actionable. We hope you enjoy 
the changes and they make it 

00:48:07.296 --> 00:48:09.585
easier to progress with the 
models. Performance is another 

00:48:09.586 --> 00:48:11.821
area we know researchers as well
as all users for that matter 

00:48:11.822 --> 00:48:15.282
care about. We have continued 
improving core performance in 

00:48:15.283 --> 00:48:18.276
TensorFlow. Since last year, we 
have sped up 

00:48:25.032 --> 00:48:27.032
training on eight Nvidia 
TeslaV100s by LLGS double.

00:48:30.307 --> 00:48:31.737
With Intel and MKL acceleration 
we have gotten inference speed 

00:48:31.738 --> 00:48:33.780
up by almost three times. 
Performance will continue to be 

00:48:33.781 --> 00:48:35.781
a focus of TensorFlow 2.

00:48:38.066 --> 00:48:40.066
0 and a core part of our 
progress to final release.

00:48:43.149 --> 00:48:45.149
TensorFlow also provides 
flexibility 

00:48:47.055 --> 00:48:49.055
and many add on libraries that 
expand and extend TensorFlow.

00:48:50.929 --> 00:48:52.929
Some are extensions to make 
certain 

00:48:54.632 --> 00:48:56.632
problems easier like tf.text 
with Unicode.

00:49:00.378 --> 00:49:02.632
It helps us explore how we can 
make machine learning model 

00:49:02.633 --> 00:49:04.868
safer by a tf.privacy. You will 
hear new announcements on 

00:49:05.883 --> 00:49:07.883
reinforcement learning and 
tomorrow we 

00:49:10.101 --> 00:49:12.211
will discuss the new tf.
federated library.

00:49:15.471 --> 00:49:17.471
It is being applied to real 
world applications as well.

00:49:18.143 --> 00:49:21.006
Here are a few examples from 
researchers at Google where we 

00:49:21.007 --> 00:49:23.249
see it applied to areas like 
data centers and making them 

00:49:25.293 --> 00:49:27.784
more efficient. Our apps like 
Google Maps, the one in 

00:49:31.680 --> 00:49:33.680
the middle, which has a new 
feature 

00:49:36.992 --> 00:49:38.992
called global localization and 
combines street service.

00:49:41.262 --> 00:49:43.528
And devices like the Google 
Pixel that use machine learning 

00:49:43.529 --> 00:49:47.748
to improve depth estimation to 
create better portrait mode 

00:49:47.749 --> 00:49:49.388
photos like the ones shown here.
In order to make these real 

00:49:49.389 --> 00:49:51.460
world applications a reality, 
you must be able 

00:49:54.918 --> 00:49:56.165
to take models from research and
prototyping to launch and 

00:49:56.166 --> 00:49:58.403
production. This has been a core
strength and focus for 

00:49:58.404 --> 00:50:03.304
TensorFlow. Using TensorFlow you
can deploy models on a number of

00:50:03.305 --> 00:50:05.775
platforms shown here and models 
end up in a lot of places so we 

00:50:08.622 --> 00:50:11.484
want to make sure TensorFlow 
works across all these servers, 

00:50:11.485 --> 00:50:13.485
Cloud, mobile, 

00:50:19.067 --> 00:50:21.067
edge devices and Java and number
of platforms.

00:50:23.569 --> 00:50:25.569
We have products for these.

00:50:31.054 --> 00:50:33.054
TensorFlow Extended is the end 
the end platform.

00:50:36.197 --> 00:50:38.197
In orange, shown here, you can 
see the 

00:50:39.454 --> 00:50:42.300
libraries we have Open 
SourceSourced so far. We are 

00:50:42.301 --> 00:50:43.518
taking a step further and 
providing components built from 

00:50:43.519 --> 00:50:48.454
these libraries that make up an 
end-to-end platform. These are 

00:50:48.455 --> 00:50:51.098
the same components used 
internally in thousands of 

00:50:51.099 --> 00:50:53.099
production systems powering 
Google's most important 

00:50:53.140 --> 00:50:55.170
products. Components are only 
part of there story.

00:50:58.421 --> 00:51:00.450
2019 is the year we are putting 
it together and providing you 

00:51:00.451 --> 00:51:02.451
with an integrated end-to-end 
platform. 

00:51:03.712 --> 00:51:05.712
You can bring your own 
orchestrator.

00:51:09.242 --> 00:51:11.242
Here is airflow or raw 
Kubernetes even.

00:51:17.333 --> 00:51:19.333
Not matter what orchestrate you

00:51:20.676 --> 00:51:22.676
chose, the items integrate with 
the metadata store.

00:51:24.537 --> 00:51:26.169
This enables experiments, 
experimentation, experiment 

00:51:26.170 --> 00:51:31.361
tracking and model comparison 
and things I am sure you will be

00:51:31.362 --> 00:51:36.057
excited about and will help you 
as you iterate through. We have 

00:51:36.058 --> 00:51:38.478
an end-to-end talk coming up 
from Clemens and his team and 

00:51:38.479 --> 00:51:40.479
they will 

00:51:42.784 --> 00:51:44.784
take you on a complete tour of 

00:51:45.851 --> 00:51:48.289
TensorFlow Extended to solve a 
real problem. TensorFlow Lite is

00:51:48.290 --> 00:51:50.290
our solution for 

00:51:53.552 --> 00:51:55.552
running models on a mobile and 
IOt hardware.

00:51:59.306 --> 00:52:01.306
On device models can be 

00:52:03.408 --> 00:52:05.408
reore responsive and keep users 
on device for privacy.

00:52:10.309 --> 00:52:12.309
Google and partners like

00:52:15.664 --> 00:52:18.094
iqiyi provide all sorts of 
things. TensorFlow Lite is about

00:52:18.095 --> 00:52:20.095
performance.

00:52:26.072 --> 00:52:28.072
You can deploy models to CPU, 
GPU, and even EdgeTPU.

00:52:30.560 --> 00:52:33.201
By using the latest techniques 
and adding support for OpenGL 

00:52:33.202 --> 00:52:35.202
and metal on 

00:52:36.461 --> 00:52:39.286
GPUs and tuning performance on 
EdgeTPUs we are constantly 

00:52:39.287 --> 00:52:42.767
pushing the limits of what is 
possible. You should expect 

00:52:42.768 --> 00:52:44.768
greater enhancements in the year
ahead.

00:52:47.872 --> 00:52:49.872
We will hear details from Raziel
and colleaguess coming up later.

00:52:50.940 --> 00:52:52.940
JavaScript is the number one 

00:52:54.429 --> 00:52:56.429
programming laj  language in the
world 

00:52:58.110 --> 00:53:00.110
and until recently hasn't 
benefited from 

00:53:01.176 --> 00:53:03.210
all the machine learning tools. 
We released TensorFlow.js last 

00:53:03.211 --> 00:53:05.211
year.

00:53:06.264 --> 00:53:08.109
Since then we have seen huge 
adoption in the JavaScript 

00:53:08.110 --> 00:53:10.110
community with more 

00:53:12.592 --> 00:53:15.441
than 300,000 downloads and a 100
contributors. We are just at the

00:53:15.442 --> 00:53:17.442
beginning given how 

00:53:18.671 --> 00:53:21.173
big the JavaScript ecosystem is.

00:53:24.435 --> 00:53:26.435
Today we are excited to announce
TensorFlow.js 1.0.

00:53:29.167 --> 00:53:31.796
We have a library of off the 
shell models for common ML 

00:53:31.797 --> 00:53:33.797
problems and we 

00:53:36.107 --> 00:53:38.107
have adding support for more 
platforms 

00:53:40.432 --> 00:53:42.260
where jafs runs and a huge focus
in TensorFlow.js is on 

00:53:42.261 --> 00:53:44.907
performance improvements. 
Compared to last year, mobile 

00:53:44.908 --> 00:53:46.908
net 

00:53:47.952 --> 00:53:50.610
inference and browser is now 
nine times faster. You will 

00:53:50.611 --> 00:53:52.855
learn more about these advances 
in our talk later in the day. 

00:53:55.857 --> 00:53:57.956
Another language we are excited 
about is Swift.

00:54:02.648 --> 00:54:03.663
It is reexamining what it means 
for performance and usability.

00:54:07.309 --> 00:54:08.347
With a new programming model, it
intends to bring further 

00:54:08.348 --> 00:54:11.823
usability. We are announcing 
Swift for TensorFlow is now at 

00:54:11.824 --> 00:54:13.824
0.2 and ready for you to 
experiment with, 

00:54:17.097 --> 00:54:17.713
try out and we are really 
excited to be bringing this to 

00:54:17.714 --> 00:54:20.802
come community.
In addition to telling you about

00:54:20.803 --> 00:54:22.803
version 0.

00:54:25.077 --> 00:54:28.335
2 we are excited to announce 
Jeremy Howard of fast.ai is 

00:54:28.336 --> 00:54:31.178
writing a new course and Chris 
and Brennan will tell you a lot 

00:54:31.179 --> 00:54:33.179
more about this later today.

00:54:35.249 --> 00:54:37.249
To recap everything we have 
shown you, 

00:54:38.755 --> 00:54:40.755
TensorFlow has grown to a full 
Eco 

00:54:41.807 --> 00:54:43.025
system from research to 
production from server to 

00:54:43.026 --> 00:54:47.366
mobile. This growth has been 
fuelled by there community and 

00:54:47.367 --> 00:54:49.367
wouldn't be possible without the
community. 

00:54:51.881 --> 00:54:54.722
To talk about what we are 
planning for you and with you in

00:54:54.723 --> 00:54:57.167
2019, I will hand it over to 
Kemal. 

00:55:00.013 --> 00:55:02.013
[Applause] 
Thank you, Megan.

00:55:10.259 --> 00:55:13.518
Hi, may name is Kemal and I am 
the product director. We are 

00:55:13.519 --> 00:55:15.169
celebrating the most important 
part of what we are building and

00:55:15.170 --> 00:55:17.170
that is the community.

00:55:19.250 --> 00:55:21.250
I love building developer 
platforms.

00:55:26.182 --> 00:55:28.182
I used to be a developer and now
I 

00:55:31.037 --> 00:55:33.785
enable developers by building a 
better platplatform. We turned 

00:55:33.786 --> 00:55:35.786
to the community and consulted 
with all of you on important 

00:55:37.041 --> 00:55:39.041
product decisions.

00:55:41.531 --> 00:55:43.770
We received valuable feedback 
and couldn't have built 2.0 

00:55:43.771 --> 00:55:45.771
without you.

00:55:47.468 --> 00:55:49.468
We created special interest 
groups, or 

00:55:50.954 --> 00:55:52.954
SIGs, like networking and 
TensorBoard to name a few.

00:55:55.660 --> 00:55:56.670
SIGs are a great way to build 
the pieces of TensorFlow they 

00:55:56.671 --> 00:55:58.702
care the most about. We wanted 
to hear more about what you 

00:56:02.152 --> 00:56:04.152
were building and launched the 
power by TensorFlow campaign.

00:56:07.043 --> 00:56:09.043
We were amazed by the creativity
of the 

00:56:11.556 --> 00:56:18.297
products. After three years, our
community is really thriving. 

00:56:18.298 --> 00:56:20.531
There are 70 machine learningGD 
Es 

00:56:25.612 --> 00:56:27.235
around the world, 1800 
contributors and countless more 

00:56:27.236 --> 00:56:29.236
doing amazing work to help make 
TensorFlow successful.

00:56:31.958 --> 00:56:33.958
On behalf of the whole 
TensorFlow team 

00:56:35.825 --> 00:56:39.290
we want to say a huge thank you.
[Applause] 

00:56:40.747 --> 00:56:43.598
We have big plans for 2019 and I
would like to make a few 

00:56:43.599 --> 00:56:45.599
announcements.

00:56:46.675 --> 00:56:48.939
First, as our community grows, 
we welcome people who are new to

00:56:48.940 --> 00:56:50.940
machine 

00:56:52.215 --> 00:56:54.443
learning and it is really 
important to provide them with 

00:56:54.444 --> 00:56:57.105
the best educational material. 
We are excited to announce two 

00:56:57.106 --> 00:57:02.809
new online courses. One is with 
deeplearning.ai and published in

00:57:02.810 --> 00:57:04.810
the Coursera 

00:57:07.734 --> 00:57:10.183
platform and the other is with 
Udacity. The first batch of the 

00:57:10.184 --> 00:57:14.495
lessons is available right now 
and provide an awesome 

00:57:14.496 --> 00:57:15.929
introduction to TensorFlow for 
developers. They require no 

00:57:15.930 --> 00:57:17.930
prior knowledge to machine 
learning.

00:57:19.643 --> 00:57:21.643
I highly encourage you to check 
them out.

00:57:22.102 --> 00:57:24.747
Next, if you are a student for 
the very first time, you can 

00:57:24.748 --> 00:57:27.611
apply to the Google summer of 
code program and get to work 

00:57:29.666 --> 00:57:31.666
with the TensorFlow engineering 
team to 

00:57:32.935 --> 00:57:34.935
help build a part of TensorFlow.

00:57:38.022 --> 00:57:40.022
I also talked about the power by
TensorFlow campaign.

00:57:42.918 --> 00:57:45.815
We are excited and decided to 
launch a 2.0 hackathon on 

00:57:45.816 --> 00:57:47.816
DevPost to let you share 

00:57:48.877 --> 00:57:51.140
your latest and greatest and win
cool prizes. We are really 

00:57:51.141 --> 00:57:53.141
excited to see what you are 
going to build.

00:57:56.224 --> 00:57:58.454
Finally, as our ecosystem grows,
we are having a second day of 

00:57:58.455 --> 00:58:02.734
the summit, but we really wanted
to do something more. We wanted 

00:58:02.735 --> 00:58:04.389
a place where you can share what
you have been building on 

00:58:04.390 --> 00:58:06.390
TensorFlow.

00:58:09.485 --> 00:58:11.485
So we are excited to announce 
TensorFlow World.

00:58:13.591 --> 00:58:14.601
A week long conference dedicated
to open source collaboration. 

00:58:17.893 --> 00:58:19.533
This conference will be 
co-presented by O'Rielly media 

00:58:19.534 --> 00:58:24.414
and TensorFlow and held in Santa
Clara at the end of October. Our

00:58:24.415 --> 00:58:25.855
vision is to bring together the 
awesome TensorFlow World and 

00:58:25.856 --> 00:58:28.833
give a place for folks to 
connect with each other.

00:58:33.518 --> 00:58:34.966
I would like to invite on stage 
Gina Blaber to say a few words 

00:58:34.967 --> 00:58:36.967
about the conference. 

00:58:37.972 --> 00:58:39.972
[Applause] 

00:58:42.099 --> 00:58:44.099
Thank you, Kemal.

00:58:45.766 --> 00:58:47.602
O'Rielly is a learning company 
with a focus on technology and 

00:58:47.603 --> 00:58:51.703
business. We have strong ties 
with the open source community 

00:58:51.704 --> 00:58:53.704
as many of you know, 

00:58:55.209 --> 00:58:57.209
and we have a history of 
bringing big ideas to life. 

00:58:58.510 --> 00:58:59.529
That's why we are excited about 
partnering with TensorFlow to 

00:58:59.530 --> 00:59:01.574
create this new event that 
brings machine 

00:59:04.635 --> 00:59:06.635
learning and AI to the 
community.

00:59:07.921 --> 00:59:10.806
The event is TensorFlow 
happening October 28th-31 in 

00:59:10.807 --> 00:59:13.254
Santa Clara. When I say 
community, I mean everyone. 

00:59:16.310 --> 00:59:19.160
We want to bring together the 
entire TensorFlow community of 

00:59:19.161 --> 00:59:23.122
individuals and teams and 
enterprises. This is it place 

00:59:23.123 --> 00:59:26.620
where you will meet experts from
around the world, the team that 

00:59:26.621 --> 00:59:28.868
actually creates TensorFlow, and
the companies and enterprises 

00:59:28.869 --> 00:59:30.951
that will help you deploy it.

00:59:35.853 --> 00:59:38.303
We have an open CFP on the 
TensorFlow World site. I invite 

00:59:38.304 --> 00:59:40.774
you all to check it out and send
in your proposals soon so your 

00:59:42.015 --> 00:59:44.260
voice is heard at that event. We
look forward to seeing you at 

00:59:45.478 --> 00:59:48.522
TensorFlow World in October. 
Thank you. 

00:59:50.562 --> 00:59:52.603
[Applause] 
Thank you, Gina. This is going 

00:59:52.604 --> 00:59:54.604
to be great.

00:59:55.696 --> 00:59:57.696
Are you guys excited?

00:59:59.766 --> 01:00:01.766
So we have a few calls to action
for you.

01:00:06.695 --> 01:00:08.949
Take a course, submit a talk to 
TF World. The grand prize for 

01:00:08.950 --> 01:00:11.612
hackathon on DevPost will 
include free tickets to 

01:00:12.021 --> 01:00:16.910
TensorFlow World. You know, one 
thing I love is to hear about 

01:00:16.911 --> 01:00:18.967
these amazing stories of people 
building awesome stuff on top of

01:00:18.968 --> 01:00:20.968
TensorFlow.

01:00:22.276 --> 01:00:24.312
As a team, we really believe 
that AI advances faster when 

01:00:24.313 --> 01:00:26.367
people have access to our tools 
and can then apply them to 

01:00:29.631 --> 01:00:32.072
the problems they care about in 
ways that we never really 

01:00:32.073 --> 01:00:36.148
dreamed of. And when people can 
really do that, some special 

01:00:36.149 --> 01:00:40.265
things happen. I would like to 
share when you something really 

01:00:40.266 --> 01:00:42.266
special.

01:01:15.617 --> 01:01:18.069
Looking at this book, page by 
page, and 

01:01:22.998 --> 01:01:25.035
trying to decipher, read and 
transcribe whatever is there 

01:01:25.036 --> 01:01:27.036
takes an enormous amount of 
time.

01:01:30.285 --> 01:01:33.985
It would require an army army 
polyographer. 

01:01:35.808 --> 01:01:37.808
Machine learning enabled us to 
solve 

01:01:39.472 --> 01:01:41.472
problems that up to 10-15 years 
ago we 

01:01:43.352 --> 01:01:45.352
thought were unsolvable.

01:02:04.839 --> 01:02:10.156
You have thousands of images of 
dogs and cats in the internet 

01:02:10.157 --> 01:02:12.157
but very little 

01:02:15.500 --> 01:02:18.341
images of ancient manuscripts. 
We involved high school students

01:02:18.342 --> 01:02:21.415
to collect the data. I didn't 
know much about machine 

01:02:24.675 --> 01:02:26.506
learning in general, but I found
it easy to create a TensorFlow 

01:02:26.507 --> 01:02:29.148
environment.
When we were trying to figure 

01:02:29.149 --> 01:02:31.795
out which model worked best for 
us, Keras was the best solution.

01:02:35.911 --> 01:02:37.911
The production model runs on 
TensorFlow 

01:02:39.606 --> 01:02:41.606
layers and estimator interface. 
We experimented with binary 

01:02:42.679 --> 01:02:44.679
classification with fully 
connected 

01:02:48.404 --> 01:02:50.404
networks and finally we moved to

01:02:51.876 --> 01:02:53.876
convolutional neural networks 
and classification. 

01:02:55.773 --> 01:02:57.773
When it comes to recognizing 
single 

01:03:00.709 --> 01:03:02.709
characters, we can get 95% 
average accuracy.

01:03:15.088 --> 01:03:17.088
This will have an enormous 
impact.

01:03:19.445 --> 01:03:20.262
We will have a massive quantity 
of historic information 

01:03:20.263 --> 01:03:22.359
available. 
I think solving problems is fun.

01:03:28.087 --> 01:03:30.087
It is a game against myself and 
how good I can do.

01:03:40.473 --> 01:03:44.184
That is -- 
[Applause] 

01:03:46.032 --> 01:03:48.032
This is such a great story.

01:03:49.291 --> 01:03:51.291
I think about the scholars who 
wrote 

01:03:57.800 --> 01:03:59.800
these manuscripts, they cannot 
have

01:04:00.917 --> 01:04:04.247
imagined centuries later people 
bringing their work to life.

01:04:09.905 --> 01:04:11.905
We are thrilled to have

01:04:16.927 --> 01:04:22.271
Elena. The team and I will be 
around. Come and say hi. With 

01:04:22.272 --> 01:04:24.272
that, I am going to hand it over

01:04:25.967 --> 01:04:30.652
to Martin who will talk about 
TensorFlow 2.0. Thank you. 

01:04:32.693 --> 01:04:34.693
Thank you, Kemal. I am Martin 
Wicke.

01:04:37.394 --> 01:04:39.394
I am the engineer lead for 
TensorFlow 2.

01:04:40.447 --> 01:04:43.530
0 and I will unsurprisingly talk
about TensorFlow 2.0. TensorFlow

01:04:43.531 --> 01:04:45.979
has been extremly successful and
inside of Google and outside of 

01:04:47.852 --> 01:04:50.084
Google it has grown into a 
vibrant community that when we 

01:04:50.085 --> 01:04:52.085
started it we were not able to 
imagine.

01:04:56.719 --> 01:04:58.719
Our users are building these 
engenous, 

01:05:00.989 --> 01:05:02.989
clever, elegant things every day
from 

01:05:06.086 --> 01:05:07.326
art to music to medieval 
manuscripts, science and 

01:05:07.327 --> 01:05:09.399
medicine.
The things people created were 

01:05:10.008 --> 01:05:12.008
unexpected and beautiful. We 
learned a lot from that.

01:05:15.311 --> 01:05:17.311
We learned a lot from how people
did it.

01:05:18.793 --> 01:05:20.793
TensorFlow has enabled all this 

01:05:22.312 --> 01:05:24.312
creativity and jump started this
whole 

01:05:26.621 --> 01:05:28.284
AI democraization which is 
great. 

01:05:28.285 --> 01:05:30.557
It has been hard to use 
sometimes. Sometimes a little 

01:05:30.558 --> 01:05:32.558
bit painful.

01:05:33.619 --> 01:05:35.619
So, you know, using sessions 
wasn't the 

01:05:36.875 --> 01:05:40.948
most natural thing to do when 
coming from regular Python. The 

01:05:40.949 --> 01:05:43.640
TensorFlow API has grown over 
time. It got a little bit 

01:05:43.641 --> 01:05:45.700
cluttered and confusing. In the 
end, you can do everything with 

01:05:48.380 --> 01:05:50.033
TensorFlow, but it wasn't maybe 
always clear what was the best 

01:05:50.034 --> 01:05:52.034
way.

01:05:53.533 --> 01:05:55.837
We have learned a lot since we 
started this and we reallyized 

01:05:55.838 --> 01:05:59.947
that you need rapid prototyping 
and easier debugging. There is a

01:05:59.948 --> 01:06:04.837
lot of clutter. So, we are 
fixing this, addressing these 

01:06:04.838 --> 01:06:06.838
issues with a new version of 
TensorFlow.

01:06:07.893 --> 01:06:10.557
Today, we are releasing an alpha
of TensorFlow 2.0. Many of you 

01:06:10.558 --> 01:06:12.558
have participated in the 

01:06:14.628 --> 01:06:16.462
design reviews that went into 
this for all the features we 

01:06:16.463 --> 01:06:20.532
have blended. Those of you 
living on the bleeding edge of 

01:06:20.533 --> 01:06:23.409
development have played it with,
installed it, tested it, found 

01:06:23.410 --> 01:06:26.293
issues and fixed issues. Thank 
you very much for your help.

01:06:29.560 --> 01:06:31.560
You can now install the alpha 
release of TensorFlow 2.

01:06:34.451 --> 01:06:35.665
0 using just pip install 
pre-TensorFlow and you can go 

01:06:35.666 --> 01:06:38.319
and play with it. What has 
changed?

01:06:44.592 --> 01:06:46.592
When we created TensorFlow 2.

01:06:49.358 --> 01:06:51.387
0, the biggest flow was 
flexibility. We have integrated 

01:06:51.388 --> 01:06:54.235
Keras with TensorFlow. It gives 
you a path of how to develop 

01:06:57.330 --> 01:06:59.160
and deploy models and has a well
established API that makes 

01:06:59.161 --> 01:07:02.676
people productive and we know it
gets you started quickly. 

01:07:04.303 --> 01:07:07.179
TensorFlow also includes a 
complete implementation of 

01:07:07.180 --> 01:07:10.099
Keras, of course, but we went 
further. We have extended Keras 

01:07:10.100 --> 01:07:12.100
so that you can 

01:07:13.991 --> 01:07:15.991
use all of the advanced features
in 

01:07:18.069 --> 01:07:20.069
TensorFlow directly from 
tf.Keras.

01:07:22.561 --> 01:07:24.561
The other change is eager 
execution.

01:07:26.035 --> 01:07:26.840
It used declarative style and 
was distance from running 

01:07:26.841 --> 01:07:28.841
Python. In TensorFlow 2.

01:07:32.320 --> 01:07:34.320
0, TensorFlow 

01:07:44.604 --> 01:07:46.604
behaves just like

01:07:47.738 --> 01:07:49.774
the Python style. All of that 
stuff is not going away.

01:07:53.661 --> 01:07:57.125
It is just getting easier. 
TensorFlow 2.0 is a major 

01:07:57.126 --> 01:07:59.126
release which means we have 

01:08:00.812 --> 01:08:04.121
the ability to clean it up and 
we did, like a lot. We had a lot

01:08:04.122 --> 01:08:06.122
of duplicate functionality we 
have removed. 

01:08:10.659 --> 01:08:14.176
We have consolidated the API and
organized the API. This is not 

01:08:14.177 --> 01:08:16.177
only with TensorFlow itself 

01:08:19.442 --> 01:08:21.300
but the whole ecosystem of tools
that has grown around 

01:08:21.301 --> 01:08:25.612
TensorFlow. We have spent a lot 
of effort to align these 

01:08:25.613 --> 01:08:27.613
interfaces and defined exchange 

01:08:29.900 --> 01:08:32.363
formats that will allow you to 
move all throughout this 

01:08:32.364 --> 01:08:38.063
ecosystem without hitting any 
barriers. We have removed a lot 

01:08:38.064 --> 01:08:40.764
of stuff from TensorFlow but it 
doesn't mean -- it 

01:08:44.621 --> 01:08:48.940
means that it has gotten more 
flexible not less. It retains 

01:08:48.941 --> 01:08:51.199
and expands on the flexibility 
and we have created a more 

01:08:52.836 --> 01:08:54.836
complete low level API that 
exposes all 

01:08:57.372 --> 01:08:59.372
of the internal used ops to the 
user.

01:09:01.452 --> 01:09:03.452
We provide inheritable 
interfaces for 

01:09:04.499 --> 01:09:06.499
the crucial concepts like 
variables and checkpoints.

01:09:07.961 --> 01:09:09.961
This allows framework authors to
build 

01:09:11.655 --> 01:09:13.655
on top of TensorFlow and 
maintain 

01:09:16.149 --> 01:09:18.149
interoperability with the rest 
of the ecosystem.

01:09:19.399 --> 01:09:21.399
There will be a talk about tf.

01:09:23.893 --> 01:09:26.125
agents later and Sonnet by 
DeepMind tomorrow. You will see 

01:09:26.126 --> 01:09:28.183
how this works in practice. The 
real question is of course what 

01:09:28.184 --> 01:09:30.184
do 

01:09:33.073 --> 01:09:35.128
I have to do be apart of this 
great new era of TensorFlow and 

01:09:35.129 --> 01:09:37.129
we know it is 

01:09:39.197 --> 01:09:44.333
always hard to upgrade to 
anything and that is especially 

01:09:44.334 --> 01:09:45.801
true for major up grades. We 
will start the process of 

01:09:45.802 --> 01:09:48.849
converting one of the largest 
codebases in the world to 

01:09:48.850 --> 01:09:51.097
TensorFlow 2.0 and we will be 
writing a lot of migration 

01:09:51.098 --> 01:09:53.569
guides. We have started already 
and some are 

01:09:57.390 --> 01:10:00.748
online and will provide a lot of
best practices for you. If we 

01:10:00.749 --> 01:10:04.236
can do it at Google, you can 
probably do it too. We are 

01:10:04.237 --> 01:10:06.237
giving you, and us, a lot of 

01:10:09.325 --> 01:10:11.325
tools to make this transition 
easier.

01:10:16.291 --> 01:10:18.820
We are shipping a module called 
tf.contrib which contains all of

01:10:18.821 --> 01:10:22.093
the complex API. If you rely on 
a specific function you 

01:10:25.374 --> 01:10:26.590
can find it there except for tf.
contrib which isn't going to be 

01:10:26.591 --> 01:10:31.305
included but we have created a 
community supported alternative 

01:10:31.306 --> 01:10:34.210
that support your use case if 
you are relying on something in 

01:10:34.211 --> 01:10:36.211
there. 

01:10:37.506 --> 01:10:39.746
We are also publishing a script 
which automatically updates your

01:10:39.747 --> 01:10:44.095
code so it runs on TensorFlow 
2.0. Let me show you how that 

01:10:44.096 --> 01:10:46.096
works. Let's say I want to 
convert this 

01:10:50.217 --> 01:10:50.824
program which is a simple 
language model chain of 

01:10:50.825 --> 01:10:52.825
Shakespeare.

01:10:56.916 --> 01:11:02.290
What where do is run  TF upgrade
2. There is just a handful of 

01:11:02.291 --> 01:11:05.565
things it changed in this case. 
You can see, if you look at what

01:11:05.566 --> 01:11:07.566
it changed, there is some 
functions it renamed.

01:11:14.191 --> 01:11:16.191
A lot of the reorganization of 
the API.

01:11:22.292 --> 01:11:24.292
It renamed multi nomial to 
random

01:11:26.013 --> 01:11:29.090
categorical. A lot had functions
changed or added and deleted. 

01:11:32.970 --> 01:11:34.970
The script will make those 

01:11:36.066 --> 01:11:38.066
changes.

01:11:39.943 --> 01:11:41.973
If all else fails, there is not 
an equivalent and we will use 

01:11:41.974 --> 01:11:44.249
the compatibility module I 
talked about earlier. 

01:11:47.108 --> 01:11:48.731
If there is no perfect 
replacement, we will use that to

01:11:48.732 --> 01:11:50.576
make sure your code still works 
as expected after the 

01:11:50.577 --> 01:11:52.577
conversion. It is pretty 
conservative.

01:11:57.110 --> 01:11:58.751
For instance, the atom optimizer
is a subtle behavior change that

01:11:58.752 --> 01:12:03.863
will mostly not affect you but 
we will convert that just in 

01:12:03.864 --> 01:12:05.864
case it might. Once the 
conversion is complete, and 

01:12:09.570 --> 01:12:11.253
there are no errors, then you 
can run the new program against 

01:12:11.254 --> 01:12:15.576
TensorFlow 2.0 and it will work.
That's the idea. It should be 

01:12:15.577 --> 01:12:20.318
pretty easy for you. Hopefully 
you won't get trouble. Note that

01:12:20.319 --> 01:12:22.319
this automatic conversion 

01:12:23.991 --> 01:12:26.043
will fix it so it WOSH works but
won't fix the style to the new 

01:12:26.044 --> 01:12:28.044
TensorFlow 2.0 style.

01:12:30.307 --> 01:12:32.307
That can not be done 
automatically and 

01:12:35.566 --> 01:12:37.566
there is a lot more to know.

01:12:41.519 --> 01:12:43.519
If you want to know more, check 
out the 

01:12:44.736 --> 01:12:49.941
talk at 10:30 tomorrow. Let me 
give you an idea about the 

01:12:51.383 --> 01:12:55.035
timeline of all this. We have 
cut 2.0 alpha today or 

01:12:55.036 --> 01:12:57.036
yesterday.

01:12:58.159 --> 01:13:00.197
Took some time building it. We 
are now working on implementing 

01:13:00.198 --> 01:13:04.551
some missing features that we 
know about, converting 

01:13:04.552 --> 01:13:06.552
libraries, converting Google.

01:13:07.609 --> 01:13:10.485
There is lots of testing and 
optimization that will happen 

01:13:10.486 --> 01:13:13.574
and we are targeting to have a 
release candidate in spring.

01:13:20.957 --> 01:13:22.957
Spring is a flexible concept but
in spring.

01:13:24.010 --> 01:13:25.632
After we have this release 
candidate, it still has to go 

01:13:25.633 --> 01:13:27.698
through release testing and 
integration testing.

01:13:31.422 --> 01:13:34.087
I expect that to take longer 
than regular releases but that 

01:13:34.088 --> 01:13:38.564
is one you can see an RC and 
eventually a final. So, if you 

01:13:38.565 --> 01:13:40.565
want to follow along with the 
process, please, do so.

01:13:47.059 --> 01:13:49.840
All of this is tracked online on
the GitHub TensorFlow 2.0 

01:13:49.841 --> 01:13:52.481
tracker. 
If you file any bugs, those are 

01:13:52.482 --> 01:13:54.482
up there 

01:13:56.565 --> 01:13:57.986
as well so everybody knows what 
is going on. 

01:13:57.987 --> 01:14:01.471
TensorFlow 2.0 really wouldn't 
be possible without you. That 

01:14:01.472 --> 01:14:04.744
has been the case already but in
the future even more so. We will

01:14:04.745 --> 01:14:06.745
need you to test this, we will 

01:14:10.339 --> 01:14:11.967
need you to tell us what works 
and doesn't. Please install the 

01:14:11.968 --> 01:14:15.222
alpha today. We are extremely 
excited to see what you can 

01:14:15.223 --> 01:14:18.084
create. Check out the docs.

01:14:21.974 --> 01:14:23.801
They are at tensorflow.org/r2.0.
With that, I think you should 

01:14:23.802 --> 01:14:28.314
hear about what it is like to 
work with TensorFlow 2.0. We 

01:14:28.315 --> 01:14:30.768
will have plenty of content over
the next couple days to tell you

01:14:30.769 --> 01:14:33.649
exactly about all the different 
aspects of it 

01:14:36.936 --> 01:14:39.779
but we will start with Karmel 
who will speak about high-level 

01:14:39.780 --> 01:14:41.780
APIs in 

01:14:42.785 --> 01:14:46.704
TensorFlow 
2.0.thank you. 

01:14:53.032 --> 01:14:55.032
Hi,  my name is Karmel Allison. 

01:14:56.930 --> 01:14:59.603
Martin just told you about 
TensorFlow 2.0 and how to get 

01:14:59.604 --> 01:15:02.052
there and I will tell you a 
little bit more about the 

01:15:02.453 --> 01:15:04.453
high-level APIs.

01:15:09.398 --> 01:15:11.440
What do I mean when I say 
high-level APIs? With our 

01:15:11.441 --> 01:15:13.441
high-level APIs, we bring you 

01:15:16.154 --> 01:15:18.154
tools to help you easily build 
your models and scale them.

01:15:20.435 --> 01:15:22.045
As Martin mentioned, a couple 
years ago, TensorFlow adopted 

01:15:22.046 --> 01:15:25.925
Keras as one of the high level 
APIs we offered. Keras is a 

01:15:25.926 --> 01:15:28.168
specification for model bidding.
It works with multiple machine 

01:15:28.169 --> 01:15:30.270
learning frameworks and it 
provides a shared 

01:15:35.570 --> 01:15:37.570
language for defining layers, 
models, 

01:15:38.778 --> 01:15:41.313
losses, optimizers and so on.

01:15:44.318 --> 01:15:46.318
We

01:15:49.874 --> 01:15:55.408
implemented tf.Keras. Raise your
hand if you are used this? Keras

01:15:55.409 --> 01:15:57.409
is just that. Simple.

01:15:58.458 --> 01:16:00.458
It was built from the ground up 
to be 

01:16:01.927 --> 01:16:03.568
Pythonic and easy to learn and 
has been instrumental in 

01:16:03.569 --> 01:16:06.417
inviting people into the machine
learningworld.

01:16:09.685 --> 01:16:11.521
It is easy for you to design and
modify your model architectures 

01:16:11.522 --> 01:16:13.522
without needing 

01:16:14.987 --> 01:16:17.630
to write tons of boilerplate. At
the same time, by replying on 

01:16:20.262 --> 01:16:22.262
inheritance and interfaces, 
Keras is 

01:16:28.021 --> 01:16:30.021
extremely flexible and 
customizeable.

01:16:31.695 --> 01:16:33.532
I can define architectures, 
modify what happens and even 

01:16:33.533 --> 01:16:35.533
change the whole training loop, 
if I want to.

01:16:38.848 --> 01:16:40.848
Which is to say Keras is simple 
and effective.

01:16:42.346 --> 01:16:44.346
Anyone can figure out how to use
it but we had a problem. Tf.

01:16:50.318 --> 01:16:51.740
Keras was built for smaller 
problems and many machine 

01:16:51.741 --> 01:16:53.770
learning need to operate at a 
larger scale. We have 

01:16:53.771 --> 01:16:55.771
estimators.

01:16:57.243 --> 01:16:59.243
They were built from the ground 
up to 

01:17:00.364 --> 01:17:02.001
distribute and scale with fault 
tolerance across hundreds of 

01:17:02.002 --> 01:17:04.002
machines no questioned asked.

01:17:06.310 --> 01:17:08.310
This is the much love wide and 
deep model.

01:17:11.457 --> 01:17:13.457
A work horse that took mean 
years to 

01:17:14.936 --> 01:17:16.936
define but is a built-in for 
training and deployment. 

01:17:19.659 --> 01:17:22.319
Estimators are powerful machines
but you told us there is a steep

01:17:22.320 --> 01:17:25.807
learning curve and it is not 
always easy to figure out which 

01:17:25.808 --> 01:17:29.055
parts to connect where. We have 
learned a lot building estimator

01:17:29.056 --> 01:17:31.294
and tf.Keras and reached the 
point we don't 

01:17:34.789 --> 01:17:35.800
think you should have to chose 
between a simple API and 

01:17:35.801 --> 01:17:37.801
scalable API.

01:17:40.329 --> 01:17:42.783
We want a higher level API 
taking you from endness to plan 

01:17:42.784 --> 01:17:47.039
and scale, no questions asked. 
In TensorFlow 2.0, we are 

01:17:47.040 --> 01:17:49.040
standardizing on the Keras API 

01:17:50.563 --> 01:17:52.563
for building layers and models 
but 

01:17:55.043 --> 01:17:57.411
bringing the power estimators 
into tf.Keras so you can move 

01:17:57.412 --> 01:18:01.677
from prototype, distributes 
training and production serving 

01:18:01.678 --> 01:18:03.678
in one go.

01:18:05.345 --> 01:18:09.625
OK. How are we doing this? This 
is a tf.

01:18:15.116 --> 01:18:17.116
Keras model definition in 
TensorFlow 1.13.

01:18:20.623 --> 01:18:22.623
This is the same model 
definition in 2.0. 

01:18:27.203 --> 01:18:30.686
The code is the same but what is
different? In 1.13 this built a 

01:18:30.687 --> 01:18:34.161
graph-base model that ran a 
session under the hood. In 2.

01:18:37.208 --> 01:18:38.631
0, the same model runs in eager 
mode without any modification. 

01:18:38.632 --> 01:18:42.713
This let's you take advantage of
everything eager can do for us. 

01:18:47.977 --> 01:18:49.977
We see our dataset pipeline

01:18:54.095 --> 01:18:56.095
behaves like a num

01:18:58.465 --> 01:19:03.346
py array. We are able to achieve
this performance with datasets 

01:19:03.347 --> 01:19:05.793
and Keras by taking advantage of
graphs even in an eager context.

01:19:05.991 --> 01:19:08.237
Eager makes debugging and 
prototyping 

01:19:11.297 --> 01:19:12.106
easy and all the while Keras 
builds an eager-friendly 

01:19:12.107 --> 01:19:14.140
function under the hood that 
tracks your model and the 

01:19:18.682 --> 01:19:20.306
TensorFlow Runtime takes care of
optimizing for performance and 

01:19:20.307 --> 01:19:25.827
scaling. You can explicitly run 
your model in eager mode with 

01:19:25.828 --> 01:19:27.828
the flag run eagerly even though
for this example it might be 

01:19:30.530 --> 01:19:32.983
overkill, you can see run 
eagerly allows you to use Python

01:19:32.984 --> 01:19:34.984
control flow for 

01:19:36.402 --> 01:19:38.693
debugging while prototyping your
model.

01:19:40.127 --> 01:19:42.127
Another big change in TensorFlow
2.

01:19:44.626 --> 01:19:46.626
0 is we have consolidated many 
headings 

01:19:49.164 --> 01:19:51.164
and maing it easier to know what
you should use and when.

01:19:54.471 --> 01:19:56.471
We have one set of optimizers 
that work 

01:19:57.526 --> 01:19:59.526
across TensorFlow in and out of 
eager 

01:20:01.201 --> 01:20:03.201
mode on one machine or 
distributed.

01:20:09.118 --> 01:20:11.118
You can set opt amam -- 
optimizers.

01:20:15.127 --> 01:20:17.127
We have one set of metrics that 
encompasses all the former tf.

01:20:20.029 --> 01:20:22.029
metrics and Keras metrics and 
allow for 

01:20:23.483 --> 01:20:25.483
easy subclassing if you want 
even more.

01:20:27.180 --> 01:20:29.270
Losses, similarly, have been 
consolidated into a single set 

01:20:29.271 --> 01:20:34.170
with a number of frequency used 
built-ins and an easily 

01:20:34.171 --> 01:20:36.425
customizeable interface if you 
chose. We have one set of layers

01:20:36.426 --> 01:20:39.077
built in the style of Keras 
layers which means they 

01:20:43.105 --> 01:20:45.105
are class based and

01:20:49.835 --> 01:20:51.547
fully configureconfiguable. One 
set of layers that we took 

01:20:51.548 --> 01:20:53.548
particularly care with were the 
RNN layers in TensorFlow.

01:20:55.680 --> 01:20:57.680
Raise your hand if you are used 
RNN layers in TensorFlow?

01:21:00.166 --> 01:21:02.817
These slides are for you. In 
TensorFlow version 1 we had 

01:21:02.818 --> 01:21:04.818
several 

01:21:06.022 --> 01:21:08.022
versions of 

01:21:09.746 --> 01:21:13.869
LSTMs and GRUs. In 2.0, there is
one version of the LSTM 

01:21:20.163 --> 01:21:22.163
layer and one version of the

01:21:24.068 --> 01:21:26.961
gru level. 
This runs the kernel but falls 

01:21:26.962 --> 01:21:28.962
back to 

01:21:30.044 --> 01:21:31.669
CPU ops if you don't have GPUs 
available. In addition to all of

01:21:31.670 --> 01:21:33.670
those, if you 

01:21:34.731 --> 01:21:36.731
need layers that are not 
included in the 

01:21:40.656 --> 01:21:42.656
built in set, tf.

01:21:43.968 --> 01:21:47.848
Keras provides a subclass that 
is easy to customize. This is 

01:21:47.849 --> 01:21:49.849
how TensorFlow addons operate 

01:21:50.918 --> 01:21:53.774
providing specialized and 
complex layers, metrics, and so 

01:21:53.775 --> 01:21:55.775
on by building 

01:21:57.422 --> 01:21:59.422
on top of the Keras base 
classes.

01:22:02.931 --> 01:22:04.931
We streamlined APIs. That is a 
start.

01:22:09.447 --> 01:22:11.447
The next thing we did was 
integrate.

01:22:13.300 --> 01:22:15.300
We found easily configurable 
data parsing. In TensorFlow 2.

01:22:18.270 --> 01:22:20.514
0, you can use Feature Columns 
to parse data and feed directly 

01:22:20.515 --> 01:22:22.515
to downstream layers.

01:22:23.563 --> 01:22:25.593
This works with Keras and 
estimators so you can mix and 

01:22:25.594 --> 01:22:29.287
match to create reusable data 
input pipelines. OK. You have 

01:22:29.288 --> 01:22:31.288
data in your model and you are 
ready to train.

01:22:36.050 --> 01:22:38.485
One of the most loved tools we 
have is TensorBoard and I am 

01:22:38.486 --> 01:22:40.486
pleased to see TensorBoard 
integration with Keras is as 

01:22:40.769 --> 01:22:42.792
simple as one line. 
We have the TensorBoard callback

01:22:42.793 --> 01:22:47.291
to our model when training here 
and this gets us both our 

01:22:47.292 --> 01:22:49.292
training progress, here we 

01:22:51.970 --> 01:22:54.409
see accuracy and loss, and a 
conceptual graph representing 

01:22:54.410 --> 01:22:56.410
the model we built layer by 
layer.

01:23:02.153 --> 01:23:03.569
As an add in bonus, it includes 
full callback so you can better 

01:23:03.570 --> 01:23:05.570
understand 

01:23:07.884 --> 01:23:10.343
the placement and find ways to 
minimize bottlenecks. Speaking 

01:23:10.344 --> 01:23:12.344
of performance, one of the 

01:23:17.428 --> 01:23:20.164
exciting pieces we are bringing 
TensorFlow 2.

01:23:24.837 --> 01:23:26.837
0 is the -- these APIs have been

01:23:28.297 --> 01:23:33.619
designed to be easy to use, out 
of the box performance and be 

01:23:33.620 --> 01:23:35.620
versetle. How do they work?

01:23:38.321 --> 01:23:40.354
With Keras, you can add and 
switch distribution patterns 

01:23:40.355 --> 01:23:42.355
with a to a lines of code.

01:23:46.063 --> 01:23:48.063
We are distributing this across 
multiple GPUs.

01:23:50.536 --> 01:23:51.346
Because we have integrated 
distribution strategies 

01:23:51.347 --> 01:23:53.347
throughout Keras and TensorFlow 
you get a lot with these few 

01:23:54.602 --> 01:23:56.602
lines of code.

01:23:59.706 --> 01:24:02.146
Data is prefetched batch by 
batch and mirrored in sync using

01:24:02.147 --> 01:24:04.791
all reduce and we see greater 
than 90% scaling 

01:24:08.091 --> 01:24:09.512
efficiencies meaning you can 
scale your model up without 

01:24:09.513 --> 01:24:12.188
changinging code and without 
loosing any of the conveniences 

01:24:12.388 --> 01:24:17.084
of Keras. Once you have trained 
your model, you are likely going

01:24:17.085 --> 01:24:19.734
to want to package it for use 
with production systems, mobile 

01:24:24.390 --> 01:24:26.753
phones or other programming laj 
-- language. 

01:24:30.011 --> 01:24:31.837
This functionality is 
experimental but today and going

01:24:31.838 --> 01:24:36.124
into 2.0 you can easily export 
your models for use with tf.

01:24:36.732 --> 01:24:38.732
serving, tf.lite and more. 

01:24:40.393 --> 01:24:42.393
You can reload the models back 
into that 

01:24:43.865 --> 01:24:45.491
you be Python to continue 
training and using in your 

01:24:45.492 --> 01:24:47.731
normal workflow. That is where 
we are today in the alpha.

01:24:49.565 --> 01:24:52.204
We are obviously not done yet. 
Let me tell you a little bit 

01:24:52.205 --> 01:24:54.272
about what is coming up in the 
next few months.

01:24:57.768 --> 01:24:59.204
We talked about distributing 
over multiple GPU but that is 

01:24:59.205 --> 01:25:01.205
just the start.

01:25:02.251 --> 01:25:04.251
We can swap out strategy and 
scale up to multiple nodes.

01:25:06.547 --> 01:25:08.378
Here we cake the same Keras 
model and distribute it across 

01:25:08.379 --> 01:25:10.379
multiple machines 

01:25:12.670 --> 01:25:14.670
working in sync using collective
ops.

01:25:19.054 --> 01:25:21.778
The multi-worker mirror strategy
supports Ring and allows great 

01:25:21.779 --> 01:25:25.235
performance. 
You can try it out today. If you

01:25:25.236 --> 01:25:27.236
are interested in a TensorFlow 

01:25:29.135 --> 01:25:32.567
native solution to synchronous 
multi working training. We are 

01:25:32.568 --> 01:25:35.026
excited to say using Keras and 
distribute strategies you will 

01:25:35.027 --> 01:25:37.027
be able 

01:25:39.760 --> 01:25:42.792
to use the same code to 
distribute models on TPUs. When 

01:25:42.793 --> 01:25:44.793
released, this strategy will 
work 

01:25:46.836 --> 01:25:48.836
on Google Cloud

01:25:50.747 --> 01:25:53.387
TPUs and Colab which can access 
TPUs directly. As we approach 

01:25:53.388 --> 01:25:57.726
the final 2.0 release we will 
continue to bring scalability 

01:25:57.727 --> 01:25:59.773
into Keras and allow parameter 
service strategy, the multi 

01:26:04.273 --> 01:26:06.273
node asynchronous training that 
estimator does today.

01:26:09.796 --> 01:26:12.656
For those that want a higher 
level API, we will be adding 

01:26:12.657 --> 01:26:14.657
support for partition variables 
across many machines for some 

01:26:17.344 --> 01:26:19.344
of the largest scale models like
those we run at Google.

01:26:22.646 --> 01:26:24.886
And that's just a sneak peek so 
please check out the 2.

01:26:27.948 --> 01:26:29.948
0 preview and check out Keras if
you haven't already. 

01:26:31.199 --> 01:26:33.230
You will hear more about Keras 
and high-level APIs throughout 

01:26:33.231 --> 01:26:35.231
the day and 

01:26:36.722 --> 01:26:38.722
we have a workshop tomorrow that
Martin mentioned. 

01:26:39.988 --> 01:26:41.988
With that, let me introduce 
Raziel who 

01:26:46.245 --> 01:26:51.179
will tell you more about TFLite.
Thanks, Karmel. Hi, everyone.

01:26:55.099 --> 01:26:57.099
My name is Raziel and I am an 
engineering lead.

01:26:59.537 --> 01:27:01.679
Today I will be covering at  -- 
a lot of the work the team has 

01:27:01.680 --> 01:27:03.680
done 

01:27:06.400 --> 01:27:09.296
over the past year that you can 
take advantage of right away. I 

01:27:09.297 --> 01:27:11.549
am cover a lot of work we have 
for this year. Before we jump 

01:27:11.550 --> 01:27:13.550
into that, let me give 

01:27:15.476 --> 01:27:17.476
you a brief introduction of what
TensorFlow Lite is.

01:27:19.756 --> 01:27:21.585
TensorFlow Lite is our 
infrastructure framework to run 

01:27:21.586 --> 01:27:23.586
machine learning on device.

01:27:26.912 --> 01:27:27.723
The reason why we decided to 
built TensorFlow Lite was 

01:27:27.724 --> 01:27:31.794
because machine learning you 
think of it as running on the 

01:27:31.795 --> 01:27:33.795
server, but more and more it is 

01:27:36.285 --> 01:27:38.285
moving to edge devices like 
mobile 

01:27:39.947 --> 01:27:41.207
devices, cars, Wearables, you 
name it. There is more machine 

01:27:41.208 --> 01:27:44.088
learning moving to different 
kinds of devices. There is good 

01:27:44.089 --> 01:27:46.089
reason for that.

01:27:47.570 --> 01:27:49.570
One, you have access to a lot 
more data 

01:27:50.628 --> 01:27:52.051
because you have access to the 
algorithm, camera, and don't 

01:27:52.052 --> 01:27:54.052
have to 

01:27:55.098 --> 01:27:57.551
stream it all the way to the 
server so you can do a lot of 

01:27:57.552 --> 01:27:59.552
stuff.

01:28:01.227 --> 01:28:03.870
It means you can build faster 
and close-knit interactions. 

01:28:03.871 --> 01:28:05.871
Another advantage of running 
machine 

01:28:08.802 --> 01:28:10.238
learning on devices is it has a 
strong privacy preserving 

01:28:10.239 --> 01:28:12.239
component because the data stays
on the device and doesn't 

01:28:15.549 --> 01:28:18.189
have to go to the server. 
However, that means basically by

01:28:21.239 --> 01:28:23.475
building machine learning on 
devices we can build products 

01:28:23.476 --> 01:28:25.476
that otherwise 

01:28:27.558 --> 01:28:29.558
wouldn't be able to really work 
if you 

01:28:30.823 --> 01:28:33.665
really rely only on server-side 
execution. However, you know, it

01:28:33.666 --> 01:28:38.551
is hard. Perhaps one obvious 
thing is you don't have the same

01:28:38.552 --> 01:28:42.435
compute power you have on the 
server. If you have a new shiny 

01:28:42.436 --> 01:28:47.142
machine learning model that 
consumes too much compute power,

01:28:47.143 --> 01:28:49.143
it is not like in the server 
where you can throw perhaps more

01:28:50.402 --> 01:28:52.646
servers at it and be done with 
it. You have to be mindful of 

01:28:52.647 --> 01:28:54.687
the resources that you have on 
the device.

01:28:57.954 --> 01:29:00.189
Another of those resources is 
the memory. You have restricted 

01:29:00.190 --> 01:29:02.629
memory so you have to be careful
about how you craft your 

01:29:05.902 --> 01:29:07.902
model, perhaps try some 
compression techniques. 

01:29:09.368 --> 01:29:10.989
Last but not least, many of 
these devices are battery 

01:29:10.990 --> 01:29:13.457
powered and even if you have a 
model that fits in memory and 

01:29:16.753 --> 01:29:19.592
can execute, well, you don't 
want to consume all the battery 

01:29:19.593 --> 01:29:21.593
in a battery powered device.

01:29:24.719 --> 01:29:27.196
For all these reasons, we built 
TensorFlow Lite and we want to 

01:29:27.197 --> 01:29:31.066
make deploying machine learning 
on edge devices as easily as 

01:29:31.067 --> 01:29:35.555
possible. What can you do with 
it right now? You can do a lot 

01:29:35.556 --> 01:29:41.053
of things. These are different 
tasks that are supported by 

01:29:41.054 --> 01:29:43.282
TensorFlow Lite already. If you 
look at it when you hear machine

01:29:46.354 --> 01:29:50.027
learning you typically fall into
one of these tasks. You know, 

01:29:50.028 --> 01:29:52.028
text-to-speech, image 

01:29:53.493 --> 01:29:55.493
processing, audio processing, a 
lot of 

01:29:56.540 --> 01:29:58.166
content generation so really, 
like, everything is already 

01:29:58.167 --> 01:30:00.167
there for you to 

01:30:02.444 --> 01:30:03.882
take advantage of and because we
are already able to support 

01:30:03.883 --> 01:30:05.883
these tasks, it 

01:30:06.945 --> 01:30:09.205
means it has a lot of usage 
already. TensorFlow Lite is 

01:30:09.206 --> 01:30:11.206
currently deployed 

01:30:12.885 --> 01:30:14.885
in over 2 billion devices in 
production. 

01:30:15.951 --> 01:30:18.639
This not only means Google 
properties, some of which are 

01:30:18.640 --> 01:30:20.640
core to Google like 

01:30:24.575 --> 01:30:26.004
the assistant and Google photos 
but come from other large 

01:30:26.005 --> 01:30:28.005
companies and those are 
frameworks.

01:30:29.887 --> 01:30:31.887
A lot of our infrastructure is 
power 

01:30:33.763 --> 01:30:35.763
and frameworks.

01:30:37.021 --> 01:30:39.284
I can tell you many reasons why 
it is a good idea to use our 

01:30:39.285 --> 01:30:41.729
infrastructure but we thought it
would be best if we bring 

01:30:47.087 --> 01:30:49.087
two of our component clients to 
tell you 

01:30:50.566 --> 01:30:52.566
how they use TensorFlow Lite.

01:30:54.045 --> 01:30:56.567
Our first presenter is Alex 
coming from the Google Assistant

01:30:56.568 --> 01:31:00.667
which has a wide variety of 
devices it is running. Let's 

01:31:00.668 --> 01:31:02.668
hear why they are using 

01:31:03.970 --> 01:31:05.970
TensorFlow Lite and how they are
doing. 

01:31:07.018 --> 01:31:11.130
[Applause] 
Thanks, Raziel. I am Alex. I am 

01:31:11.131 --> 01:31:13.975
an engineering lead on our 
embedded speech recognition team

01:31:13.976 --> 01:31:16.417
at Google. One of the key 
applications for speech 

01:31:18.681 --> 01:31:20.681
recognition at Google is the 
Google Assistant.

01:31:22.160 --> 01:31:24.399
Google recently announced the 
Google Assistant is on about a 

01:31:24.400 --> 01:31:26.644
billion devices ranging from 
phones, speakers, smart 

01:31:31.532 --> 01:31:33.532
displays, cars, TVs, laptops, 
Wearables, 

01:31:36.216 --> 01:31:39.879
you name it we are trying to put
the assistant into it. That 

01:31:39.880 --> 01:31:41.880
means high end devices, low end 

01:31:43.766 --> 01:31:45.766
devices, battery powered, 
plugged in, 

01:31:47.704 --> 01:31:49.154
and a large range of operating 
systems. The neural networks I 

01:31:49.155 --> 01:31:51.155
build have to be able to run 
anywhere.

01:31:53.024 --> 01:31:55.464
For assistant, we have a couple 
key speech on-device 

01:31:55.465 --> 01:31:57.465
capabilities. The first is the 
hot word.

01:32:01.940 --> 01:32:03.940
You need to be able to say

01:32:10.157 --> 01:32:12.221
hey, Google to activate the 
device and we need to make sure 

01:32:12.222 --> 01:32:14.222
it is your voice. 

01:32:16.097 --> 01:32:18.097
We have to have a tiny memory 
and 

01:32:19.199 --> 01:32:23.261
computational input and we are 
extremely latency sensitive.

01:32:27.122 --> 01:32:29.363
Our other major application is 
on device speech recognition. We

01:32:29.364 --> 01:32:31.818
use this when you are offline or
have a bad network connection. 

01:32:35.693 --> 01:32:37.121
Instead of using the server, we 
run speech recognition on the 

01:32:37.122 --> 01:32:39.122
device.

01:32:41.614 --> 01:32:44.258
Here we are trying to run large 
models as fast as possible to 

01:32:44.259 --> 01:32:47.517
get the best probably accuracy. 
It is an a high load returning 

01:32:47.518 --> 01:32:50.613
into short burst typically. Over
the years, we have been working 

01:32:50.614 --> 01:32:52.614
on 

01:32:53.935 --> 01:32:55.564
this for a long time, before 
there was TensorFlow Lite, 

01:32:55.565 --> 01:32:57.565
before there was TensorFlow, so 
we have been investing a 

01:33:01.700 --> 01:33:03.346
lot for a long time in building 
our own on-device neural net 

01:33:03.347 --> 01:33:06.207
libraries and shipping them to 
production in Google product for

01:33:06.208 --> 01:33:08.208
a long time. 

01:33:10.752 --> 01:33:12.752
As a team, we have invested in 
keeping 

01:33:15.270 --> 01:33:16.704
the code size tiny, memory 
small, and optimizing for all 

01:33:16.705 --> 01:33:19.387
different platform and all of 
our code after many years 

01:33:21.655 --> 01:33:22.876
production use is hardened and 
as bug-free as you can get it, 

01:33:22.877 --> 01:33:24.877
hopefully.

01:33:28.396 --> 01:33:30.396
Over the last year, we decided 
to 

01:33:31.479 --> 01:33:33.479
migrate to TFLite.

01:33:34.737 --> 01:33:36.737
This wasn't an easy decision for
us and 

01:33:39.985 --> 01:33:41.985
as we switched over we checked

01:33:47.189 --> 01:33:49.189
carefully to make sure TFLite 
would 

01:33:51.255 --> 01:33:52.295
beat our existing models and we 
have moved all over in the last 

01:33:52.296 --> 01:33:54.320
year. We are excited because 
this lays the 

01:33:57.823 --> 01:33:59.861
ground work for us moving to a 
new standard that is being 

01:33:59.862 --> 01:34:01.862
widely adopted 

01:34:02.940 --> 01:34:04.940
and we think TFLite will help us

01:34:07.644 --> 01:34:09.868
accelerate models on GPUs and 
EdgeTPUs and new accelerators 

01:34:09.869 --> 01:34:11.869
coming. Thanks.

01:34:14.379 --> 01:34:15.387
I will turn it back over to 
Raziel. 

01:34:15.388 --> 01:34:20.718
[Applause] 
Thanks, Alex. So, Alex analyzed 

01:34:20.719 --> 01:34:22.719
the key goals for TensorFlow 
Lite.

01:34:25.459 --> 01:34:27.105
We want to be able to support 
your current-use-cases, but 

01:34:27.106 --> 01:34:29.106
perhaps equally important or 
more important, is we want 

01:34:32.789 --> 01:34:34.429
to cater to your needs that you 
will have 1-2 years in the 

01:34:34.430 --> 01:34:36.430
future.

01:34:42.452 --> 01:34:43.691
Our next presenter is Huiije 
from a China-based company that 

01:34:43.692 --> 01:34:48.581
has several applications and 
hundreds of millions of users. 

01:34:48.582 --> 01:34:50.582
Let's welcome him.

01:34:52.074 --> 01:34:54.074
Thanks, Raziel.

01:35:01.441 --> 01:35:03.441
I am Huiije from NetEase in 
Youdao.

01:35:07.032 --> 01:35:08.249
We have built a lot of 
dictionary translation and note 

01:35:08.250 --> 01:35:10.903
taking applications in China.

01:35:15.430 --> 01:35:17.430
We have over 800 million users 
in total.

01:35:20.319 --> 01:35:22.319
We also have 22 million daily 
active users.

01:35:25.842 --> 01:35:27.842
In the past years, we have built

01:35:29.715 --> 01:35:33.168
several dictionary translation 
and note taking applications. We

01:35:33.169 --> 01:35:35.169
have the most popular dictionary

01:35:36.657 --> 01:35:38.657
application in China, for 
example, which 

01:35:42.968 --> 01:35:44.968
is Youdao dictionaries and we 
have the 

01:35:48.114 --> 01:35:50.562
most popular translator in China
and the most popular dictionary 

01:35:50.563 --> 01:35:52.563
and language 

01:35:56.083 --> 01:35:58.083
learning application in India 
which is Youdao dictionary.

01:36:01.603 --> 01:36:03.235
We provide features for users in
these applications so they can 

01:36:03.236 --> 01:36:05.888
conveniently look up words in 
the scenario such as, 

01:36:10.789 --> 01:36:12.789
for example, you can use your 
camera to 

01:36:15.320 --> 01:36:17.320
recognize the words from the 
images and 

01:36:20.024 --> 01:36:22.024
do the OCR translation on your 
devices.

01:36:26.387 --> 01:36:28.387
We use the TensorFlow Lite to 
use 

01:36:30.501 --> 01:36:32.959
theOCR and translation here.
We have provided the photo 

01:36:32.960 --> 01:36:34.960
translation in your 
applications.

01:36:39.726 --> 01:36:41.726
For example, you can use camera 
to take 

01:36:44.198 --> 01:36:46.198
a photo from the many scenario 
and it 

01:36:48.094 --> 01:36:50.094
will do the whole image of OCR 
and 

01:36:53.195 --> 01:36:55.195
translate the text into another 
language.

01:36:58.336 --> 01:37:00.336
Then it will erase the text on 
the 

01:37:03.478 --> 01:37:06.109
image and replace the original 
text to the translated text to 

01:37:06.110 --> 01:37:08.110
present users with the 
translation.

01:37:10.760 --> 01:37:12.760
In this scenario, we also use 
the 

01:37:14.265 --> 01:37:16.265
TensorFlow Lite to accelerate 
our OCR 

01:37:17.570 --> 01:37:19.570
and translation services here.

01:37:22.503 --> 01:37:24.960
The OCR and translation is very 
sensitive to the binary size and

01:37:24.961 --> 01:37:26.961
also 

01:37:30.476 --> 01:37:32.476
computational resources and 
responding time. 

01:37:34.183 --> 01:37:36.183
So we chose the TensorFlow Lite 
to 

01:37:39.314 --> 01:37:41.558
accelerate our abilities on 
device to provide more efficient

01:37:41.559 --> 01:37:44.837
on device inference here. That 
is why we chose the TensorFlow 

01:37:46.676 --> 01:37:48.716
Lite to do this. Thanks. 

01:37:51.918 --> 01:37:56.071
[Applause] 
Really exciting to see how our 

01:37:59.343 --> 01:38:01.343
&gt;&gt; INSTRUCTOR: -- infrastructure
is used 

01:38:03.893 --> 01:38:06.556
by hundreds of users around the 
world. TensorFlow Lite is geared

01:38:06.557 --> 01:38:10.855
not only towards Google but 
everybody should be able to take

01:38:10.856 --> 01:38:12.856
advantage of it. 

01:38:14.325 --> 01:38:15.155
For the rest of the 
presentation, I will cover what 

01:38:15.156 --> 01:38:17.156
I said at the start.

01:38:18.569 --> 01:38:20.569
A lot

01:38:22.075 --> 01:38:24.725
of features that are available 
over the past year and roadmap 

01:38:24.726 --> 01:38:26.726
for this year.

01:38:27.792 --> 01:38:29.792
We organized our engineering in 
four things. 

01:38:30.655 --> 01:38:33.132
First is usability, overall it 
means we want to be able to get 

01:38:33.133 --> 01:38:35.157
you up and running as easily and
fast as possible.

01:38:38.439 --> 01:38:41.275
Now once you have your model 
basically we want to get you 

01:38:41.276 --> 01:38:44.744
executing that model as fast and
possible in the hardware that 

01:38:44.745 --> 01:38:46.745
you care about.

01:38:49.263 --> 01:38:50.903
Then with optimization we look 
at certain things we can do to 

01:38:50.904 --> 01:38:55.395
make it faster and smaller. 
Finally, with our documentation 

01:38:55.396 --> 01:38:57.396
efforts, 

01:39:00.739 --> 01:39:03.194
we want to give you all the 
resources you need. Let's start 

01:39:03.195 --> 01:39:08.283
with usability. There is 
different ways of getting up and

01:39:08.284 --> 01:39:10.284
running in TensorFlow Lite but 
at 

01:39:12.768 --> 01:39:14.768
the end of the day you will most
likely 

01:39:15.872 --> 01:39:17.720
train the model in TensorFlow 
and convert it to TensorFlow 

01:39:17.721 --> 01:39:19.771
Lite. The flow is very simple.

01:39:24.048 --> 01:39:26.048
You train in TensorFlow, you get
a 

01:39:27.504 --> 01:39:29.939
model and use the TensorFlow 
Lite convertert and get a model 

01:39:29.940 --> 01:39:31.940
you can 

01:39:33.961 --> 01:39:35.961
execute on different kinds of 
devices.

01:39:38.159 --> 01:39:40.599
However, there is points of 
failure. TensorFlow Lite made 

01:39:40.600 --> 01:39:42.600
the decision to 

01:39:44.474 --> 01:39:47.359
take a subset of TensorFlow and 
optimize those. That means 

01:39:47.360 --> 01:39:50.221
basically we don't have all the 
same hundreds of operations that

01:39:50.416 --> 01:39:52.416
TensorFlow has.

01:39:54.544 --> 01:39:56.544
The other reason is some 
semantics.

01:39:58.209 --> 01:40:00.209
We don't support jet on 
TensorFlow like 

01:40:01.666 --> 01:40:03.117
control flow which is typically 
used for reoccurring neural 

01:40:03.118 --> 01:40:05.118
networks.

01:40:07.003 --> 01:40:09.003
We have made already like 
strides 

01:40:10.313 --> 01:40:12.313
trying to basically make this 
easier and better for you.

01:40:14.403 --> 01:40:16.403
Last year we launched TensorFlow
select 

01:40:18.285 --> 01:40:19.500
and this basically means you can
execute TensorFlow ops in 

01:40:19.501 --> 01:40:22.366
TensorFlow Lite. In the 
pipeline, we are making 

01:40:24.613 --> 01:40:26.268
improvements to TensorFlow 
Select by enabling selective 

01:40:26.269 --> 01:40:28.269
registration and I will explain 
what all this means in the 

01:40:28.513 --> 01:40:30.513
following slides.

01:40:32.639 --> 01:40:34.639
We are working on contraflow 
support as well.

01:40:35.910 --> 01:40:36.723
Select, like I said, means you 
can execute TensorFlow and 

01:40:36.724 --> 01:40:42.223
TensorFlow Lite. It comes at the
cost of some binary size 

01:40:42.224 --> 01:40:43.656
increase because right now it is
basically all or nothing, but 

01:40:43.657 --> 01:40:46.713
that is why we are building 
selective registration. 

01:40:48.771 --> 01:40:51.024
Selective registration is 
something you can take advantage

01:40:51.025 --> 01:40:53.025
of already on 

01:40:54.484 --> 01:40:56.484
TensorFlow Lite for our building
ops.

01:41:00.054 --> 01:41:03.738
You include in the binary only 
the ops you are using. We are 

01:41:03.739 --> 01:41:05.739
bringing that to TensorFlow 
Select.

01:41:08.411 --> 01:41:10.245
We are trying to blur the line 
of what the TensorFlow and 

01:41:10.246 --> 01:41:12.246
TensorFlow Lite ops 

01:41:13.309 --> 01:41:15.309
are and achieve one of the key 
points is 

01:41:17.984 --> 01:41:19.984
blur the performance between one
and the other.

01:41:22.651 --> 01:41:24.651
Now, contraflow is something we 
are working on. 

01:41:27.945 --> 01:41:29.945
This is one of fully lighter 
pain points 

01:41:32.429 --> 01:41:34.871
we are trying to convert 
occurring neural networks to 

01:41:34.872 --> 01:41:37.738
TensorFlow Lite. We are adding 
supports and loops and 

01:41:37.739 --> 01:41:39.739
conditionals.

01:41:40.995 --> 01:41:42.995
All of this is part of a bigger 
effort 

01:41:44.468 --> 01:41:47.305
to revamp our converter and we 
took your feedback very 

01:41:47.306 --> 01:41:50.212
seriously and we wanted this 
converter to answer these three 

01:41:50.213 --> 01:41:52.213
questions. 

01:41:53.433 --> 01:41:55.433
Some

01:41:56.535 --> 01:41:57.546
Something fails and what went 
wrong and what can you do to fix

01:41:57.547 --> 01:42:00.598
it. OK. Jumping to the next 
theme is performance.

01:42:03.654 --> 01:42:05.654
We want your models to execute 
as fast 

01:42:07.125 --> 01:42:09.763
as possible in the hardware you 
have available. If you saw in 

01:42:09.764 --> 01:42:11.764
Megan's slides, we have made 
huge slides in these areas.

01:42:14.659 --> 01:42:16.659
This is an example of a model.

01:42:19.183 --> 01:42:21.183
The execution of the CPU, 
floating 

01:42:23.666 --> 01:42:26.533
point compared to the fast 
version in CPU. With the 

01:42:26.534 --> 01:42:28.766
recently developing preview we 
launched, we get huge gains. 

01:42:32.256 --> 01:42:34.256
Like over 7X improvements.

01:42:36.568 --> 01:42:38.568
By using the HTP we get crazy 
speed ups. 62X.

01:42:40.902 --> 01:42:42.902
Again, this is an area we are 
working really hard. 

01:42:44.401 --> 01:42:46.401
Again, some things that are 
already available.

01:42:48.894 --> 01:42:50.894
We have a very nice benchmark 
and 

01:42:52.354 --> 01:42:54.826
profiling tool and a few 
delegates. For those not 

01:42:54.827 --> 01:42:56.827
familiar with the concept 

01:42:58.961 --> 01:43:01.189
of TensorFlow delegates, that is
our abstraction layer that 

01:43:01.190 --> 01:43:03.190
allows executing 

01:43:06.251 --> 01:43:08.251
models on the CPU.

01:43:11.018 --> 01:43:13.055
In the pipeline we are working 
on CPU improvements because we 

01:43:13.056 --> 01:43:15.699
know CPU is important. I am not 
listing here in the pipeline 

01:43:18.145 --> 01:43:20.787
anything around newer delegates 
because it tends to be a 

01:43:20.788 --> 01:43:23.034
partnership with other companies
and I don't have anything to 

01:43:27.917 --> 01:43:29.917
announce yet, but we are working
on that.

01:43:33.060 --> 01:43:35.303
Our benchmarking tool is 
basically a profiler. We added 

01:43:35.304 --> 01:43:38.363
some features. Personal 
threading. 

01:43:40.592 --> 01:43:42.825
You can see statistics for 
opposite execution and it 

01:43:42.826 --> 01:43:45.072
supports the neural networks 
API. If you execute the profiler

01:43:45.073 --> 01:43:47.932
you get a lot of information. 
This is just a snippet of all 

01:43:47.933 --> 01:43:49.933
the stuff that you get.

01:43:53.018 --> 01:43:55.669
In this case, we are showing you
here information for each op in 

01:43:55.670 --> 01:44:00.346
your graph and how long it took 
to execute. Then you also get a 

01:44:00.347 --> 01:44:02.347
nice summary of the 

01:44:03.428 --> 01:44:04.651
archives in your graph and many 
useful statistics so you can 

01:44:04.652 --> 01:44:06.652
decide maybe there 

01:44:08.833 --> 01:44:11.475
is a way that you can, you know,
change one for another type that

01:44:11.476 --> 01:44:13.476
is more efficient or tweak your 
graph in some way.

01:44:18.558 --> 01:44:20.558
Now let's talk about the

01:44:22.532 --> 01:44:24.532
delegates which is an abtraction
layer for extracting graphs.

01:44:27.677 --> 01:44:30.125
The way it works is very nice. 
In the TensorFlow Lite you 

01:44:30.126 --> 01:44:33.172
prepare a model, right? You are 
not necessarily saying this 

01:44:34.398 --> 01:44:36.398
model is for this hardware. You 
just prepare your model.

01:44:39.472 --> 01:44:44.005
Then inpreter will take it and 
for those delegates you raised 

01:44:44.006 --> 01:44:46.006
it will go one by 

01:44:47.873 --> 01:44:51.140
one and it will say how much of 
these graph can you execute. 

01:44:51.141 --> 01:44:53.141
What the delegate cannot 
execute, it 

01:44:54.194 --> 01:44:56.194
will be executed in CPU.

01:44:57.255 --> 01:44:59.295
So, by doing this basically you 
will always be able to execute, 

01:44:59.296 --> 01:45:01.296
right?

01:45:03.196 --> 01:45:05.862
And if anything, you will get 
better performance if the 

01:45:05.863 --> 01:45:07.863
delegate can consume more of 
your graph.

01:45:12.535 --> 01:45:14.535
We have already an

01:45:17.286 --> 01:45:19.731
edge-TPU delegate. It has a 
small power footprint and 

01:45:23.200 --> 01:45:25.200
later on we will have more 
information 

01:45:28.501 --> 01:45:30.501
about the edge-TPU and a very 
cool demo. 

01:45:31.959 --> 01:45:34.825
The next delegate is the GPU 
delegate. These are available 

01:45:34.826 --> 01:45:36.826
right now.

01:45:38.929 --> 01:45:40.929
We see very nice performance 

01:45:43.024 --> 01:45:44.051
improvements at the cost of 
relatively small binary size 

01:45:44.052 --> 01:45:46.052
increase.

01:45:47.328 --> 01:45:49.328
How do you enable the GPU 
delegate?

01:45:51.832 --> 01:45:54.465
Just like any other delegate, 
this is an example of the 

01:45:54.466 --> 01:45:56.466
interpreter and executing one 
inference.

01:46:00.251 --> 01:46:02.251
The only thing you need to do is
share 

01:46:04.339 --> 01:46:07.202
and pass to interpreter and that
is it. So, stuff that we are 

01:46:07.203 --> 01:46:09.203
working on for 

01:46:10.212 --> 01:46:12.212
the

01:46:17.145 --> 01:46:19.145
GPU deldelegate, we want to add 
more ops.

01:46:21.538 --> 01:46:23.538
We want to improve performance 
even further.

01:46:27.198 --> 01:46:29.198
We want to finalize APIs so this
is generally available.

01:46:31.772 --> 01:46:32.998
The final delegate I want to 
mention is the neural network 

01:46:32.999 --> 01:46:34.999
API.

01:46:37.915 --> 01:46:39.915
This will allow you to execute 
any 

01:46:41.201 --> 01:46:43.201
other hardware that the API 
supports.

01:46:44.888 --> 01:46:46.888
It should be just transparent 
for you.

01:46:49.641 --> 01:46:51.641
Finally, CPU is still very 
important for many use-cases.

01:46:54.180 --> 01:46:56.180
Most devices have some form of a
CPU.

01:46:58.687 --> 01:47:00.687
We are targeting further 
improvements 

01:47:02.756 --> 01:47:05.750
on the arm and X86 
architectures.

01:47:09.585 --> 01:47:11.585
The

01:47:12.685 --> 01:47:14.685
next theme is optimization.

01:47:16.470 --> 01:47:19.727
Maybe we can do 2X model to make
it faster and smaller. We 

01:47:19.728 --> 01:47:21.728
already did some work here.

01:47:26.202 --> 01:47:28.572
Last year we released this 
quantization tool that gives 

01:47:28.573 --> 01:47:31.190
nice performance on CPU. 
This is the first launch that we

01:47:31.191 --> 01:47:33.191
have 

01:47:37.909 --> 01:47:39.909
while we are calling model 
optimization toolkit.

01:47:41.009 --> 01:47:43.009
It will be available for use for
everybody.

01:47:47.125 --> 01:47:49.125
In the pipeline some of these 
new tech  

01:47:50.732 --> 01:47:52.836
techniques we will put in the 
toolkit is more quantization 

01:47:52.837 --> 01:47:54.837
work.

01:47:56.105 --> 01:47:57.761
It gives further improvement on 
CPU and allows you to execute 

01:47:57.762 --> 01:47:59.762
more hardware.

01:48:00.824 --> 01:48:02.824
A lot of the hardware like 
neural 

01:48:04.364 --> 01:48:06.364
processing units are fixed point
base 

01:48:07.675 --> 01:48:09.675
with no floating base support.

01:48:14.606 --> 01:48:18.097
We are working on a technique 
called production pruning. So 

01:48:18.098 --> 01:48:20.978
quantization means changing the 
precision of your model and how 

01:48:20.979 --> 01:48:22.979
it is executed on the device.

01:48:26.308 --> 01:48:28.308
We launch post training 
quantization 

01:48:31.168 --> 01:48:33.168
tool where you take your save 
pod  model 

01:48:37.694 --> 01:48:39.694
and when you convert to 
TensorFlow Lite 

01:48:41.348 --> 01:48:43.348
you turn on the flag and it 
updates.

01:48:47.263 --> 01:48:50.663
Trying to get the most 
performance and small Keras 

01:48:47.263 --> 01:48:49.263
loss.

01:48:50.308 --> 01:48:52.308
We see nice improvements this 
way.

01:48:54.983 --> 01:48:57.012
If the model is is fully 
quantized and you get 4X. 

01:49:02.150 --> 01:49:04.789
For convolutional, we see 10-15%
performance improvements but for

01:49:04.790 --> 01:49:06.790
fully 

01:49:11.544 --> 01:49:15.630
connected RNNs we see up to 3X. 
This is simple to try. If you 

01:49:15.631 --> 01:49:17.959
are using TensorFlow Lite or 
plan to use it, give this a try.

01:49:21.766 --> 01:49:23.766
You have to validate how the 
accuracy is 

01:49:24.843 --> 01:49:25.875
affected in your particular 
model but so far we have seen 

01:49:25.876 --> 01:49:28.941
very good numbers. 
Many of the two billion devices 

01:49:28.942 --> 01:49:30.942
we 

01:49:32.007 --> 01:49:34.007
mentioned before are running 
this way.

01:49:35.507 --> 01:49:37.507
In the pipeline, again, we are 
making 

01:49:38.800 --> 01:49:39.413
further improvements in the 
quantization approaches that we 

01:49:39.414 --> 01:49:41.414
have.

01:49:43.957 --> 01:49:45.957
We are working on this for 
quantization.

01:49:47.899 --> 01:49:50.401
We want to make it powerful by 
enabling quanterize straining 

01:49:50.402 --> 01:49:53.451
and easy to use. We are bringing
more post-training quantization 

01:49:53.452 --> 01:49:56.136
support. This is an example of 
Keras model.

01:50:00.006 --> 01:50:02.006
It is very easy with two dense 
layers.

01:50:02.492 --> 01:50:04.492
Let's say you want to train with

01:50:06.362 --> 01:50:08.844
quantization those two dense 
layers. Well, soon enough, you 

01:50:08.845 --> 01:50:10.845
will be able to 

01:50:13.510 --> 01:50:15.510
just import your API and say 
quantatize those two layers.

01:50:20.563 --> 01:50:22.852
The post-training quantization 
for fixed point will work very 

01:50:22.853 --> 01:50:26.733
much like the one we already 
support where you have the 

01:50:26.734 --> 01:50:28.734
conversion path and the only 
thing 

01:50:31.967 --> 01:50:33.967
you need to parse extra is some 

01:50:35.220 --> 01:50:36.660
calibration data which is the 
typical model your sees as 

01:50:36.661 --> 01:50:41.942
input. If you have an image 
processing model, you just need 

01:50:41.943 --> 01:50:43.943
to pass some images.

01:50:45.860 --> 01:50:48.744
You don't really need to pass a 
ton of images. In preliminary 

01:50:48.745 --> 01:50:50.745
numbers, model Nets, we 

01:50:54.031 --> 01:50:55.855
see with 25 images we do well. 
These are preliminary numbers we

01:50:55.856 --> 01:50:57.856
see with this tool.

01:51:00.780 --> 01:51:02.780
We have the float baseline and 
then if 

01:51:05.102 --> 01:51:07.676
you do quantization training you
get almost the same accuracy 

01:51:07.677 --> 01:51:09.677
with all the benefits.

01:51:11.534 --> 01:51:14.367
Then even further if you really,
for some reason don't want to 

01:51:14.368 --> 01:51:17.032
invest in this training or you 
have a model, you know, 

01:51:20.975 --> 01:51:22.975
that you got somewhere, you can 
still 

01:51:24.637 --> 01:51:26.637
quantize it with post training 
and you 

01:51:28.310 --> 01:51:30.310
see the accuracy is almost the 
same.

01:51:34.003 --> 01:51:36.003
The other technique we are 
heavily 

01:51:37.089 --> 01:51:39.163
investing in is connection 
pruning meaning we prune 

01:51:39.164 --> 01:51:41.164
connections in the 

01:51:42.218 --> 01:51:44.218
network and those connections 
become 

01:51:45.738 --> 01:51:47.738
Keras and that creates a sparse 
tensor.

01:51:50.791 --> 01:51:55.974
The benefits of a parse tensor 
is you can compress them and 

01:51:55.975 --> 01:51:57.975
create potentially faster models
if 

01:51:59.248 --> 01:52:01.248
you have less computations to 
do.

01:52:04.497 --> 01:52:07.356
Coming very soon, we are going 
to add a training pruning as a 

01:52:07.357 --> 01:52:09.357
Keras-base API and 

01:52:10.412 --> 01:52:12.412
in the pipeline we are working 
on first 

01:52:13.510 --> 01:52:15.510
class support of this parse 
execution in TensorFlow Lite.

01:52:17.370 --> 01:52:19.370
Again, in our example from Keras
you 

01:52:21.343 --> 01:52:23.801
have two dense layers, want to 
prune connection from those 

01:52:23.802 --> 01:52:25.802
layers, and the 

01:52:27.263 --> 01:52:29.711
only thing you need to do is say
prune. It is very, very simple.

01:52:33.366 --> 01:52:35.366
When you convert the model to 

01:52:36.625 --> 01:52:38.625
TensorFlow Lite format, 
basically by 

01:52:39.684 --> 01:52:41.684
just even seeding the file you 
will get nice compressions.

01:52:44.182 --> 01:52:47.265
These are some, again, very 
preliminary numbers. A mobile 

01:52:47.266 --> 01:52:49.723
net which is pretty hard to 
prune compared to other perhaps 

01:52:49.724 --> 01:52:51.724
larger 

01:52:54.585 --> 01:52:56.585
models, we see various models of
50% and even 75%.

01:52:59.904 --> 01:53:02.366
Finally, documentation, again, 
we want to give you all the 

01:53:02.367 --> 01:53:05.207
resources you need to get the 
most out of TensorFlow Lite.

01:53:11.105 --> 01:53:12.931
We have already done pretty nice
improvements. 

01:53:12.932 --> 01:53:14.932
We have a site with more 
content.

01:53:16.617 --> 01:53:18.617
We currently have a model 

01:53:20.842 --> 01:53:22.842
representation with

01:53:24.177 --> 01:53:28.263
five models and representations 
you can try. We are working on 

01:53:28.264 --> 01:53:30.264
new models and looking to expand
our repository.

01:53:35.254 --> 01:53:35.333
The goal is not only that it is 
pretty but that it answers your 

01:53:35.334 --> 01:53:37.597
questions. Please give us more 
feedback if there 

01:53:41.722 --> 01:53:43.722
is things that are still lacking
there.

01:53:44.796 --> 01:53:46.796
Yeah, we revamp the 
documentation and 

01:53:47.796 --> 01:53:49.859
very important, we also have the
roadmap for the foreseeable 

01:53:49.860 --> 01:53:51.860
future. A lot of the stuff I am 
talking about 

01:53:54.569 --> 01:53:56.569
now you can see there in more 
detail perhaps. 

01:53:57.215 --> 01:53:59.669
And then, again, we have our 
repository with all these 

01:53:59.670 --> 01:54:04.196
applications and tutorials and 
we are expanding it. We want to 

01:54:04.197 --> 01:54:06.197
make the repository into 

01:54:07.688 --> 01:54:10.129
something that, you know, if you
don't want to go, or before you 

01:54:10.130 --> 01:54:12.401
start trying to train something,
perhaps you have a 

01:54:16.265 --> 01:54:18.265
model you can reuse.

01:54:22.305 --> 01:54:24.305
By the way, TensorFlow mobile is

01:54:25.350 --> 01:54:28.209
aggregated especially using 
TensorFlow select allowing you 

01:54:28.210 --> 01:54:30.210
to use TensorFlow 

01:54:31.474 --> 01:54:34.144
Lite and there is no need for 
TensorFlow mobile except if you 

01:54:34.145 --> 01:54:36.999
are doing training on the 
device. That is the reason we 

01:54:37.000 --> 01:54:39.498
are keeping that there. Yeah, 
training.

01:54:44.811 --> 01:54:46.434
So, if you see we are building a
lot of the basic infrastructure 

01:54:46.435 --> 01:54:48.690
that you will need when you are 
trying to prepare a 

01:54:52.398 --> 01:54:55.302
training pipeline to execute on 
device. I don't have anything to

01:54:55.303 --> 01:54:57.303
announce now but I just wanted 
to let you know that 

01:55:01.023 --> 01:55:03.285
we are working on it and 
thinking a lot about it. 

01:55:05.925 --> 01:55:07.925
What about TensorFlow 2.0?

01:55:11.046 --> 01:55:13.046
Well, it should work.

01:55:17.169 --> 01:55:18.598
You should be able to convert it
to TensorFlow Lite. Before we go

01:55:18.599 --> 01:55:21.095
two last things. One is there 
was a lot of content in this 

01:55:21.096 --> 01:55:23.096
talk. 

01:55:25.994 --> 01:55:28.865
Tomorrow we have breakout 
sessions and will cover 

01:55:28.866 --> 01:55:30.866
TensorFlow Lite in general and 
optimization in particular.

01:55:34.148 --> 01:55:36.148
And I want to introduce Pete 
Warden who 

01:55:37.851 --> 01:55:39.893
is going to talk about some very
cool products. 

01:55:41.772 --> 01:55:43.772
[Applause] 
Awesome. 

01:55:45.652 --> 01:55:47.652
Thanks so much, Raziel.

01:55:48.760 --> 01:55:50.760
I am really excited to be here 
to talk 

01:55:53.891 --> 01:55:56.546
about a new project that I think
is pretty cool. So, TensorFlow 

01:55:56.547 --> 01:56:01.820
Lite for microcontrollers. What 
is that all about? This all 

01:56:01.821 --> 01:56:03.821
comes back to when I actually 

01:56:06.354 --> 01:56:08.607
first joined Google back in 
2014. There were a whole bunch 

01:56:08.608 --> 01:56:10.608
of internal projects I didn't 
know about as a member 

01:56:13.143 --> 01:56:15.143
of the public that sort of blew 
my mind 

01:56:16.202 --> 01:56:18.466
but one in particular came about
when I actually spoke to Raziel 

01:56:18.467 --> 01:56:20.467
for the first time.

01:56:21.719 --> 01:56:23.142
He was on the speech team at the
time working with Alex who you 

01:56:23.143 --> 01:56:26.024
just saw. He explained that they
use neural 

01:56:29.901 --> 01:56:32.894
networks models of only 13 
kilobytes in size. At that time,

01:56:32.895 --> 01:56:38.055
I only really had experience 
with image networks and the very

01:56:38.056 --> 01:56:40.014
smallest of them was still like 
multiple megabytes so this idea 

01:56:40.015 --> 01:56:42.015
of 

01:56:43.854 --> 01:56:45.854
having a 13 KB model was amazing
for me.

01:56:49.438 --> 01:56:51.308
What amazed me even more was 
when when he told me why they 

01:56:51.309 --> 01:56:53.309
had to be so small. 

01:56:56.014 --> 01:56:58.473
They needed to run them on dsp 
and other embedded chips in 

01:56:58.474 --> 01:57:00.474
smartphones so Android 

01:57:01.547 --> 01:57:03.575
could listen for wake words like
hey Google while the main CPU 

01:57:03.576 --> 01:57:05.576
was powered off to save the 
battery.

01:57:09.721 --> 01:57:11.721
These microcontrollers often 
only had 10 

01:57:13.212 --> 01:57:15.212
kilobytes of RAM and flash 
storage.

01:57:16.510 --> 01:57:18.510
They couldn't rely on Cloud 

01:57:19.795 --> 01:57:21.795
connectivity because the amount 
of power 

01:57:22.889 --> 01:57:24.889
that would be drained to send 
data over 

01:57:25.950 --> 01:57:30.436
would have been prohibitive. So,
that really struck me. That 

01:57:30.437 --> 01:57:32.437
conversation and the continued 

01:57:34.288 --> 01:57:36.139
work that we did with the speech
team because they had so much 

01:57:36.140 --> 01:57:39.198
experience doing all sorts of 
different approaches with 

01:57:39.199 --> 01:57:41.846
speech. They had spent a lot of 
time and a lot 

01:57:45.374 --> 01:57:47.423
of energy experimenting and even
within the tough constraints of 

01:57:47.424 --> 01:57:52.510
embedded devices, neural 
networks were better than any of

01:57:52.511 --> 01:57:54.341
the traditional method they 
used. I was left wondering if 

01:57:54.342 --> 01:57:56.342
they would be 

01:57:58.197 --> 01:57:59.833
really useful for other embedded
sensor applications as well and 

01:57:59.834 --> 01:58:04.970
it left me really wanting to see
if we could actually build 

01:58:04.971 --> 01:58:06.971
support for these kind of 

01:58:09.928 --> 01:58:12.366
devices into TensorFlow itself, 
so that more people could 

01:58:12.367 --> 01:58:15.003
actually get access to it. At 
the time, only people in the 

01:58:15.004 --> 01:58:17.004
speech community really knew 
about the 

01:58:19.500 --> 01:58:21.500
groundbreaking work that was 
being done 

01:58:25.328 --> 01:58:27.961
and I wanted to share it a lot 
more widely. So, today I am 

01:58:27.962 --> 01:58:30.218
pleased to announce we are 
releasing the first experimental

01:58:33.521 --> 01:58:35.521
support for embedded platforms 
in TensorFlow Lite.

01:58:37.826 --> 01:58:39.826
To show you what I mean, here is
a 

01:58:40.909 --> 01:58:42.909
demonstration board that I 
actually have in my pocket.

01:58:45.628 --> 01:58:47.628
This is a prototype of a 
development 

01:58:49.315 --> 01:58:51.315
board built by SparkFun and it 
has a 

01:58:53.845 --> 01:58:55.845
cortex M4 processor with 384 
kilobytes 

01:59:00.608 --> 01:59:02.608
of RAM and a whole MB of flash 
storage 

01:59:07.360 --> 01:59:08.999
and it was built by  to be 
extremely low power -- Ambiq.

01:59:12.093 --> 01:59:13.756
It is able to run on a single 
core battery like this for many 

01:59:13.757 --> 01:59:15.757
days potentially.

01:59:16.808 --> 01:59:18.808
I am going to take my life in my
hands 

01:59:20.903 --> 01:59:22.903
now by trying a live demo.

01:59:29.068 --> 01:59:31.068
Let us see if this is actually

01:59:32.183 --> 01:59:34.183
-- it's going to be extremely 
hard to 

01:59:36.082 --> 01:59:38.082
see unless we dim the lights. 
There we go.

01:59:41.376 --> 01:59:43.376
What I am going to be doing here
is by 

01:59:44.424 --> 01:59:47.280
saying a particular word and see
if it actually lights up the 

01:59:47.281 --> 01:59:51.701
little yellow light. You can see
the blue LED is flashing and 

01:59:51.702 --> 01:59:53.702
that is telling me it is running
inference.

01:59:57.580 --> 01:59:59.580
If I try saying

02:00:02.248 --> 02:00:04.923
"yessaying "yes", "yes".

02:00:11.453 --> 02:00:13.469
I knew I was taking my own life 
into my own hands. There we go.

02:00:17.204 --> 02:00:19.885
As you can see, it still is far 
from perfect but it is managing 

02:00:19.886 --> 02:00:22.363
to do a job of recognizing when 
I say the word and 

02:00:27.156 --> 02:00:29.605
not lighting up when there is 
unrelated conversations. So why 

02:00:29.606 --> 02:00:32.451
is this useful? Well, first this
is running entirely 

02:00:36.960 --> 02:00:38.960
locally on the embedded chip, so
we 

02:00:40.223 --> 02:00:42.223
don't need to have any internet 
connection.

02:00:43.707 --> 02:00:45.150
It is a good useful first 
component of a voice interface 

02:00:45.151 --> 02:00:47.151
system.

02:00:48.254 --> 02:00:50.913
The model isn't quite 13 
kilobytes but it is down to 20 

02:00:50.914 --> 02:00:52.914
kilobytes so it only 

02:00:53.981 --> 02:00:55.981
takes up 20 kilobytes of flash 
storage on this device.

02:00:58.946 --> 02:01:00.946
The footprint of the TensorFlow 
Lite 

02:01:04.314 --> 02:01:06.139
for microcontrollers is only 
another 25 kilobytes and it only

02:01:06.140 --> 02:01:09.857
needs about 30 kilobytes of RAM 
to operate. It is within the 

02:01:09.858 --> 02:01:11.858
capabilities of a lot 

02:01:13.521 --> 02:01:15.521
of different embedded devices. 

02:01:20.717 --> 02:01:23.375
Secondly, this is all open 
source, so you can actually grab

02:01:23.376 --> 02:01:25.376
the code yourself 

02:01:26.635 --> 02:01:28.635
and build it yourself, and you 
can modify it.

02:01:30.351 --> 02:01:32.199
I am showing you here on this 
particular platform but it 

02:01:32.200 --> 02:01:34.200
actually works on a 

02:01:35.466 --> 02:01:37.466
whole bunch of different embeded
chips.

02:01:39.155 --> 02:01:41.407
We really want to see lots more 
supports so we are keen to work 

02:01:41.408 --> 02:01:43.408
with the 

02:01:44.872 --> 02:01:49.559
community on collaboratoring to 
get more devices supported. -- 

02:01:49.560 --> 02:01:51.560
collaborating.

02:01:54.290 --> 02:01:56.561
You can also train our own 
model. Just something that 

02:01:56.562 --> 02:02:01.256
recognizes yes isn't all that 
useful, but the key thing is 

02:02:01.257 --> 02:02:03.532
that this comes with a toil you 
can use to actually train your 

02:02:03.533 --> 02:02:05.533
own models. 

02:02:08.648 --> 02:02:10.648
It always comes with a dataset 
of 

02:02:13.296 --> 02:02:15.296
100,000 utterances of about 
20,000 common words.

02:02:19.180 --> 02:02:22.042
The first link there, if you 
could go to that link, and 

02:02:22.043 --> 02:02:24.043
contribute your voice 

02:02:26.551 --> 02:02:28.603
to the open dataset, it should 
actually increase the size and 

02:02:28.604 --> 02:02:32.932
the quality of the dataset that 
we can actually make available. 

02:02:32.933 --> 02:02:34.933
That would be awesome. You can 
actually use the same approach 

02:02:37.434 --> 02:02:39.071
to do a lot of different audio 
recognition to recognize 

02:02:39.072 --> 02:02:41.072
different kinds 

02:02:42.143 --> 02:02:44.240
of sounds and even start to use 
it for similar signal processing

02:02:44.241 --> 02:02:46.241
problems like, 

02:02:48.946 --> 02:02:51.589
you know, things like predictive
maintenance. So, how can you try

02:02:51.590 --> 02:02:53.590
this out for yourself?

02:02:54.846 --> 02:02:57.715
If you are in the audience here,
at the end of today, you will 

02:02:57.716 --> 02:02:59.716
find that you get 

02:03:02.257 --> 02:03:04.257
a gift box, and you actually 
have one of these in there. 

02:03:06.991 --> 02:03:09.253
[Applause] 

02:03:12.539 --> 02:03:14.970
All you should need to do is 
remove the little tab between 

02:03:14.971 --> 02:03:16.971
the battery and it 

02:03:19.790 --> 02:03:21.790
should automatically boot up 
pre-flash with this yes example.

02:03:24.071 --> 02:03:29.192
You can try it out for yourself 
and let me know how it goes. Say

02:03:29.193 --> 02:03:31.193
yes to TensorFlow Lite. We also 
include all the cables so you 

02:03:32.925 --> 02:03:34.148
should be able to program it 
yourself through there serial 

02:03:34.149 --> 02:03:36.149
port.

02:03:37.870 --> 02:03:40.514
These are the first 700 boards 
ever built so there is a wiring 

02:03:40.515 --> 02:03:42.958
issue. It will drain the 
battery.

02:03:47.191 --> 02:03:49.842
It will last more like hours 
than days but that will 

02:03:49.843 --> 02:03:52.779
actually, knock on wood, be 
fixed in the final product that 

02:03:52.780 --> 02:03:54.780
is shipping.

02:03:55.810 --> 02:03:56.839
And you should be able to 
develop with these in the exact 

02:03:56.840 --> 02:03:59.149
same way you will with the final
shipping product.

02:04:03.042 --> 02:04:05.711
If you are watching at home, you
can pre-order one of these from 

02:04:05.712 --> 02:04:07.712
SparkFun 

02:04:09.382 --> 02:04:11.382
right now for, I think, it is 
$15.

02:04:14.108 --> 02:04:16.108
And you will also find lots of 
other 

02:04:17.197 --> 02:04:19.197
instructions for other platforms
in the documentation.

02:04:21.160 --> 02:04:23.417
We are trying to support all of 
the -- or as many of the modern 

02:04:24.434 --> 02:04:26.434
microcontrollers that are out 
there that 

02:04:27.718 --> 02:04:29.718
people are using as possible.

02:04:31.176 --> 02:04:32.813
And we welcome collaboration 
with everybody across the 

02:04:32.814 --> 02:04:37.324
community to help unlock all of 
the creativity that I know is 

02:04:37.325 --> 02:04:42.419
out there. I am really hoping I 
am going to be spending a lot of

02:04:42.420 --> 02:04:44.420
time over the next few 

02:04:46.211 --> 02:04:52.095
months reviewing pull requests. 
And finally, this was my first 

02:04:52.096 --> 02:04:54.096
hardware 

02:04:55.585 --> 02:04:57.408
project so I needed a lot of 
help from a lot of people 

02:04:57.409 --> 02:04:59.409
including to TensorFlow 

02:05:03.770 --> 02:05:05.770
Lite team especially Raziel, 
Tim, and 

02:05:10.348 --> 02:05:12.390
Andy, Allisster, Nathan, owen 
and Jim at SparkFun were life 

02:05:12.391 --> 02:05:14.391
savers.

02:05:20.311 --> 02:05:22.311
We got these in our hands in the
middle of the day yesterday.

02:05:24.816 --> 02:05:26.029
And also those at Ambiq who 
actually designed this process 

02:05:26.030 --> 02:05:28.030
and helped us get the software 
going.

02:05:30.925 --> 02:05:32.925
A lot of people are arms as well

02:05:38.100 --> 02:05:40.606
including a big shout out to 
Neal and Zach. This is a very 

02:05:40.607 --> 02:05:42.035
early experiment but I can't 
wait to see what people build 

02:05:42.036 --> 02:05:44.111
with this. One final note, I 
will be around to 

02:05:49.858 --> 02:05:52.735
talk about  M CUs with anyone 
interested on day two. I am 

02:05:52.736 --> 02:05:55.811
really looking forward to 
chatting with everyone. Thank 

02:05:55.812 --> 02:05:57.812
you. 
[Applause] 

02:05:57.873 --> 02:06:00.522
Thanks, Pete. We really hope 
that you try this.

02:06:05.387 --> 02:06:07.332
It is the early stages but you 
see a huge help to make this 

02:06:07.333 --> 02:06:10.317
happen.
We think it will be really 

02:06:10.318 --> 02:06:12.389
impactful for everybody. Now, 
before we go again, and I 

02:06:12.390 --> 02:06:14.390
promise 

02:06:16.939 --> 02:06:18.939
this is the last thing you hear 
from me, 

02:06:20.194 --> 02:06:22.194
I want to welcome June, who is 
going to 

02:06:26.291 --> 02:06:28.291
talk about how by use

02:06:30.321 --> 02:06:32.321
TensorFlow Lite with the 
edge-TPU 

02:06:36.766 --> 02:06:38.766
delegate are able to train these
usable machines. 

02:06:38.769 --> 02:06:42.929
[Applause] 
Thanks, Raziel. Hi, my name is 

02:06:42.930 --> 02:06:45.800
June Tate-Gans and I am one of 
the lead software engineers 

02:06:47.440 --> 02:06:49.440
inside Google's new Coral group 
and I 

02:06:51.128 --> 02:06:53.128
have been asked to give a talk 
about the 

02:06:54.402 --> 02:06:56.402
edge-TPU based teachable demo.

02:06:59.291 --> 02:07:02.005
Coral is a 
platformplatformplatform  for 

02:07:02.006 --> 02:07:04.006
products with on demand 
learning.

02:07:07.530 --> 02:07:09.530
Our first product is a single 
board 

02:07:12.441 --> 02:07:14.475
computer and a USB stick. What 
is the edge-TPU?

02:07:20.606 --> 02:07:24.139
It is a Google designed asic. It
doesn't require a network 

02:07:24.140 --> 02:07:26.171
connection to run and allows for
a whole new range 

02:07:29.797 --> 02:07:31.797
of applications and machine 
learning.

02:07:35.971 --> 02:07:37.971
The first product we built is 
the Coral Dev Board.

02:07:40.821 --> 02:07:44.106
This is a single board computer 
with a removable som. It runs 

02:07:44.107 --> 02:07:46.107
Linux and Android and the som 

02:07:47.520 --> 02:07:49.520
has a 

02:07:54.223 --> 02:07:56.754
gigabyte of ram, Wi-Fi and 
Bluetooth and the EdgeTPU.

02:08:01.411 --> 02:08:04.307
The second is the corilatored 
board connected to whatever 

02:08:07.802 --> 02:08:09.802
you need be it Raspberry Pi or a
Linux work station.

02:08:10.823 --> 02:08:12.823
This teachable machine shows off
a form of edge training.

02:08:15.894 --> 02:08:17.894
Traditionally, there is three 
ways to do this.

02:08:20.213 --> 02:08:21.438
There is K-nearest neighbors, 
weight imprinting and last layer

02:08:21.439 --> 02:08:23.439
retraining.

02:08:26.197 --> 02:08:28.197
For this demo, we are using the 
K-nearest neighbors approach.

02:08:30.596 --> 02:08:33.466
In this animated GIF, you can 
see the TPU enables very high 

02:08:33.467 --> 02:08:35.467
classification rates. The frame 
rate is actually the rate at 

02:08:37.147 --> 02:08:40.244
which the TPU is classifying the
images that I am showing it. In 

02:08:40.245 --> 02:08:43.198
this case, you can see we are 
getting about 30 frames per 

02:08:43.199 --> 02:08:45.199
second.

02:08:46.327 --> 02:08:48.327
It is essentially real-time 
classification.

02:08:50.407 --> 02:08:52.706
With that, I actually have one 
of our teachable machine demos 

02:08:52.707 --> 02:08:54.707
here.

02:08:55.778 --> 02:08:58.442
If we can turn this on. There we
go. OK.

02:09:04.779 --> 02:09:06.758
So, on this board we have our 
EdgeTPU development board 

02:09:06.759 --> 02:09:10.308
assembled with a camera and a 
series of buttons. Each button 

02:09:10.309 --> 02:09:12.335
corresponds with a class and 
lights up when the model 

02:09:12.336 --> 02:09:14.336
identifies 

02:09:15.404 --> 02:09:17.404
an object from the camera.

02:09:18.659 --> 02:09:20.659
First we have to plug this in.

02:09:25.677 --> 02:09:27.677
Every time I take a picture, by 
pressing one of these buttons it

02:09:27.701 --> 02:09:29.587
associates that picture with 
that particular class and 

02:09:29.588 --> 02:09:31.588
because it is 

02:09:32.598 --> 02:09:34.598
running 

02:09:35.782 --> 02:09:37.411
inference on the EdgeTPU it 
lights up immediately. The first

02:09:37.412 --> 02:09:40.455
thing I have to do is train it 
on the background. I will press 

02:09:40.456 --> 02:09:41.878
this blue button and where you 
can see it immediately turns on 

02:09:41.879 --> 02:09:43.879
and 

02:09:44.997 --> 02:09:47.915
this is because it is doing 
inference in realtime.

02:09:53.906 --> 02:09:55.906
Now, if I train one of the other
buzz 

02:10:01.441 --> 02:10:04.378
buttons using something like a 
tangerine, you can see it can 

02:10:04.379 --> 02:10:06.379
classify 

02:10:09.494 --> 02:10:12.360
between this tangerine and the 
background. I can grab other 

02:10:12.361 --> 02:10:15.632
objects, such as this TensorFlow
Lite sticker. It looks similar. 

02:10:15.633 --> 02:10:18.718
It is the same color. Let's see 
what was the class I used? 

02:10:18.719 --> 02:10:24.463
Yellow. OK. Sorry.
Now, even though it is a similar

02:10:24.464 --> 02:10:26.464
color, 

02:10:29.434 --> 02:10:31.434
it can still discern the 
TensorFlow Lite 

02:10:34.089 --> 02:10:36.089
logo from the tangerine. 

02:10:38.983 --> 02:10:40.983
[Applause]

02:10:44.171 --> 02:10:46.656
you can imagine in a 
manufacturing content, your 

02:10:46.657 --> 02:10:48.112
operators are no knowledge of 
machine learning or training, 

02:10:48.113 --> 02:10:51.573
can adapt your system easily and
quickly using this exact 

02:10:51.574 --> 02:10:53.574
technique.

02:10:55.473 --> 02:10:58.148
That is about it for the demo 
but before I go, I should grab 

02:10:58.149 --> 02:11:00.149
the clicker 

02:11:01.660 --> 02:11:03.660
and also I should say, we are 
also 

02:11:05.996 --> 02:11:07.714
giving away some EdgeTPU 
accelerators. 

02:11:07.715 --> 02:11:09.341
For those here today, we will 
have one available and for those

02:11:09.342 --> 02:11:12.340
on the livestream

02:11:19.622 --> 02:11:22.484
you can purchase one at 
Coral.Google.com. We also have 

02:11:22.485 --> 02:11:24.485
links for you up here. Please 
take a look at the TensorFlow 

02:11:27.420 --> 02:11:30.077
Lite and our new hardware and 
thank you very much. 

02:11:33.093 --> 02:11:37.259
[Applause] 
Thank you, June. That was 

02:11:37.260 --> 02:11:39.260
awesome.

02:11:40.340 --> 02:11:42.340
So, that is the end of our first
block of talks.

02:11:44.385 --> 02:11:46.385
We are now going to have a break
that's about 45 minutes.

02:11:48.766 --> 02:11:52.021
We want to be back here by noon.
Sorry we ran a little long. 

02:11:52.022 --> 02:11:54.670
Thank you for coming up. After 
the break, we will be talking 

02:11:56.763 --> 02:11:59.429
about building models, Swift, 
and the community. It is going 

02:11:59.430 --> 02:12:01.430
to be a lot of great talks.

02:12:03.550 --> 02:12:05.550
Go grab something to eat and 
come on back here by noon.

02:12:07.880 --> 02:12:09.880
Thank you very much.

02:28:16.001 --> 02:28:18.001
Tf.

02:28:27.098 --> 02:28:29.098
tf.tf.tf.tf.

02:28:30.910 --> 02:28:32.910
TF an

02:28:37.901 --> 02:28:39.901
TensorFlow 2.

02:28:40.904 --> 02:28:42.904
0

02:28:44.071 --> 02:28:46.071
flight 370

02:28:51.799 --> 02:28:53.799
TensorFlow Lite

02:29:39.063 --> 02:29:41.063
TensorFlow Lite

02:29:50.324 --> 02:29:52.324
Swift Keras

02:30:27.874 --> 02:30:29.874
tf.

02:30:30.876 --> 02:30:32.876
touchdown

02:30:54.961 --> 02:30:56.961
megabytes

02:31:01.532 --> 02:31:03.532
CPU GPU TPU

02:33:30.988 --> 02:33:32.988
SIGs

02:41:19.777 --> 02:41:21.777
tf.

02:41:25.632 --> 02:41:27.632
function function

02:41:29.664 --> 02:41:31.664
function TUNGZ

02:41:46.445 --> 02:41:48.445
nction

02:44:33.082 --> 02:44:35.082
function function tf.function 
tf.

02:44:36.909 --> 02:44:38.909
function

02:44:43.828 --> 02:44:45.828
autograph Alexandre Passos

02:45:33.550 --> 02:45:35.550
SIGs SIGs Edd

02:45:39.124 --> 02:45:41.124
Swift TensorFlow 2.

02:45:42.142 --> 02:45:44.142
0

02:49:07.336 --> 02:49:09.336
FASZ

02:50:34.707 --> 02:50:36.707
test test test test

02:51:37.259 --> 02:51:39.259
Hello, everyone.

02:52:33.895 --> 02:52:35.895
Typhl TIFL TIFL

02:52:39.207 --> 02:52:41.207
te

02:53:06.853 --> 02:53:08.853
TFLite TFLite

02:53:14.283 --> 02:53:16.283
K-nearest

02:55:26.355 --> 02:55:28.355
Youdao

02:55:32.631 --> 02:55:34.631
Youdao Youdao Youdao

02:55:37.341 --> 02:55:39.341
Youdao Youdao

02:57:23.790 --> 02:57:25.790
test test test test test

02:59:07.764 --> 02:59:09.764
Hi, everyone.

02:59:14.672 --> 02:59:19.150
My name is Gal Oshri and I am 
here to talk about TensorBoard. 

02:59:19.151 --> 02:59:21.393
How many have used TensorBoard 
before? It helps you visualize a

02:59:21.394 --> 02:59:23.394
lot of data coming out of 
TensorFlow. 

02:59:25.875 --> 02:59:27.875
It has a wide variety of 
features but 

02:59:32.484 --> 02:59:34.484
today we will talk about a 
couple new additions.

02:59:37.155 --> 02:59:39.155
We will actually switch over to 
rademo.

02:59:40.972 --> 02:59:42.972
This is Colab

02:59:46.695 --> 02:59:48.695
.

02:59:50.461 --> 02:59:52.461
I made sure we installed 
TensorFlow 2.

02:59:55.415 --> 02:59:57.047
0 alpha and a couple other sets 
so we don't have to install it 

02:59:57.048 --> 03:00:00.546
now.
We are going to use the dataset 

03:00:00.547 --> 03:00:02.547
and 

03:00:07.618 --> 03:00:09.618
train a really simple keras 
sequintial model.

03:00:15.940 --> 03:00:16.971
We will train it with the fit 
API and give it the TensorFlow 

03:00:16.972 --> 03:00:19.621
callback. If you have used Colab
before, you will 

03:00:23.696 --> 03:00:25.696
know we will need to download 
the logs 

03:00:27.580 --> 03:00:29.816
to our local machine, make sure 
TensorBoard and set-up and point

03:00:29.817 --> 03:00:31.817
at it and go back to Colab.

03:00:34.581 --> 03:00:35.207
We are enabled showing 
TensorBoard directly within 

03:00:35.208 --> 03:00:37.662
Colab.
You will notice the way we start

03:00:39.309 --> 03:00:41.309
TensorBoard here is exactly the 
same as in the command line.

03:00:43.580 --> 03:00:46.231
It is the same command just as 
the magic cell and magic 

03:00:46.232 --> 03:00:48.232
function in front of it.

03:00:50.513 --> 03:00:52.513
The same thing will also work in
Jupyter notebooks. 

03:00:53.377 --> 03:00:57.064
Let's see what is different. 
When using the keras callback we

03:00:57.065 --> 03:01:02.012
have trained validation showing 
up on the same chart to make it 

03:01:02.013 --> 03:01:03.839
easier to compare them in 
accuracy, loss and other 

03:01:03.840 --> 03:01:09.331
metrics. This makes it easier to
detect things like overfitting. 

03:01:09.332 --> 03:01:11.380
In the graph's dashboard, while 
seeing 

03:01:14.834 --> 03:01:16.834
the ops and exillerary 
information is 

03:01:18.524 --> 03:01:21.204
useful for many scenario, 
sometimes you want to see the 

03:01:21.205 --> 03:01:23.246
model you created in keras. So 
you can switch to the keras tag 

03:01:23.247 --> 03:01:25.247
and just few that model.

03:01:28.962 --> 03:01:30.962
We can expand this to see the 
actual layers we added. 

03:01:32.850 --> 03:01:34.874
There is several other APIs for 
using TensorBoard within 

03:01:34.875 --> 03:01:38.991
notebooks that let you change 
the height of the cell as well 

03:01:38.992 --> 03:01:41.838
as list the active instances 
within your Colab notebook but 

03:01:41.839 --> 03:01:43.839
we are not going 

03:01:46.919 --> 03:01:48.919
to look at that because I want 
to switch 

03:01:51.432 --> 03:01:54.287
gears and talk about 
hyperparameter training. We 

03:01:54.288 --> 03:01:56.935
picked the dropout rate, number 
of units in the dense layer and 

03:01:56.936 --> 03:01:58.936
the optimizer. 

03:02:02.249 --> 03:02:04.481
If we really care about the 
modeled's performance we will 

03:02:04.482 --> 03:02:08.577
want to try out different values
and experiment. The way this 

03:02:08.578 --> 03:02:09.397
looks in TensorBoard is you 
might include all that 

03:02:09.398 --> 03:02:12.084
information about what the 
values are in the run names.

03:02:17.541 --> 03:02:19.541
As you can see here

03:02:22.425 --> 03:02:27.064
in the bottlebottle -- bottom 
left. It is not the best 

03:02:27.065 --> 03:02:29.065
experience so let's 

03:02:30.166 --> 03:02:32.166
see if we can do something 
better. 

03:02:32.397 --> 03:02:34.050
What I'm going to show next is 
something that is going to 

03:02:34.051 --> 03:02:36.051
change in terms of the 

03:02:38.335 --> 03:02:40.335
API and the UI but it is 
available in 

03:02:42.645 --> 03:02:48.567
the TF 2.alpha today. We will do
several different imports and 

03:02:48.568 --> 03:02:49.998
define which values of the 
hyperparameters we want to try. 

03:02:49.999 --> 03:02:53.495
We will start out with just a 
few so we don't take up too much

03:02:53.496 --> 03:02:56.380
time during the demo. We are 
going to log a summary that 

03:02:58.621 --> 03:03:00.280
tells TensorBoard what were the 
hyperparameters we care about 

03:03:00.281 --> 03:03:02.281
and what 

03:03:03.543 --> 03:03:08.854
were the metrics. We then wrap 
the existing training code that 

03:03:08.855 --> 03:03:10.680
we had, just to make sure we 
logged the accuracy at the end 

03:03:10.681 --> 03:03:13.357
on the validation set, and also 
tell 

03:03:16.637 --> 03:03:18.637
TensorBoard that the experiment 
has started and finished.

03:03:21.156 --> 03:03:22.371
This time we are going to start 
TensorBoard before doing our 

03:03:22.372 --> 03:03:27.553
training because in most cases 
your training will take longer 

03:03:27.554 --> 03:03:29.199
than one minute and you want to 
view the TensorBoard while your 

03:03:29.200 --> 03:03:31.200
model 

03:03:32.699 --> 03:03:34.699
is training to understand its 
progress. We started it.

03:03:36.816 --> 03:03:38.816
It has no data but once a couple
epox 

03:03:39.891 --> 03:03:42.131
of training have finished we can
refresh it and start to see 

03:03:42.132 --> 03:03:44.132
something.

03:03:46.876 --> 03:03:49.109
You will notice in the top we 
have the h-parameters dashboard 

03:03:49.110 --> 03:03:51.110
which show a 

03:03:52.599 --> 03:03:54.440
table where each run is 
represented by a row and we have

03:03:54.441 --> 03:03:56.441
columns.

03:03:58.738 --> 03:04:03.027
As the runs finish, the table 
becomes populated with them. On 

03:04:03.028 --> 03:04:06.295
the left, we have ability to 
filter and sort. We can say we 

03:04:06.296 --> 03:04:08.296
don't actually care about 

03:04:12.819 --> 03:04:15.300
the number of units or only want
to see experiments where the 

03:04:15.301 --> 03:04:18.566
accuracy is at least 85. Before 
we proceed further, I want to 

03:04:22.304 --> 03:04:23.934
actually cheat a little bit and 
log and access some completed 

03:04:23.935 --> 03:04:29.260
experiments where we have run a 
wider range of combinations of 

03:04:29.261 --> 03:04:31.702
values for the hyperparameters. 
Now, while this is loading, I 

03:04:31.703 --> 03:04:33.703
want to 

03:04:34.758 --> 03:04:36.758
point out that I am pointing 
TensorBoard 

03:04:39.671 --> 03:04:41.924
directly at a folder in my 
Google Drive. I had all my 

03:04:41.925 --> 03:04:43.925
TensorFlow logs on another 

03:04:45.876 --> 03:04:47.503
machine, uploaded them to my 
Google Drive and I can access 

03:04:47.504 --> 03:04:51.385
them directly within my Colab 
notebook. This takes a moment to

03:04:51.386 --> 03:04:53.411
load but, hopefully, when I 
refresh we can see it.

03:05:00.182 --> 03:05:02.238
I can switch over to the 
H-parameters dashboard and see 

03:05:02.239 --> 03:05:05.185
the experience. I can switch 
over to this view that 

03:05:08.283 --> 03:05:09.912
shows the visualization of 
access for each hyperparameter 

03:05:09.913 --> 03:05:12.773
and each metric. 
Each run is represented by a 

03:05:12.774 --> 03:05:14.774
line that 

03:05:18.069 --> 03:05:20.930
passes through all these axis at
the points corresponding to 

03:05:20.931 --> 03:05:24.420
hyperparameters and values. I 
can click and grab to select a 

03:05:24.421 --> 03:05:28.907
range. I have selected the 
experiments with the relatively 

03:05:28.908 --> 03:05:31.140
high accuracy and they become 
highlighted in this 

03:05:31.141 --> 03:05:33.141
visualization.

03:05:34.194 --> 03:05:36.840
I can immediately see that all 
these experiments used the 

03:05:36.841 --> 03:05:38.841
add-in optimizer as 

03:05:41.780 --> 03:05:43.606
asupposed to SGD and had a 
relatively high number of units 

03:05:43.607 --> 03:05:45.607
in the dense layer.

03:05:47.094 --> 03:05:48.159
This gives great ideas about 
what we can experiment with 

03:05:48.160 --> 03:05:52.224
next. I can skew the scatter 
plot view which shows the 

03:05:52.225 --> 03:05:54.225
correlation between the 

03:05:55.292 --> 03:05:57.292
different hyperparameters and 
metrics.

03:05:58.781 --> 03:06:00.781
I can, again, select a region 
here to 

03:06:02.675 --> 03:06:04.675
view those points across the 
other charts.

03:06:07.414 --> 03:06:11.519
Just to summarize, we have 
looked at TensorBoard in Colab. 

03:06:11.520 --> 03:06:14.431
An easier way to compare the 
trend validation runs, 

03:06:14.432 --> 03:06:18.952
visualizing the keras conceptual
graph, and better hype parameter

03:06:18.953 --> 03:06:20.953
tuning with the H-program 
dashboard.

03:06:23.525 --> 03:06:25.761
All this information is 
available as documentation at 

03:06:25.762 --> 03:06:27.762
tensorflow.org/TensorBoard.

03:06:30.660 --> 03:06:33.519
We also have a demo station 
upstairs and would love to hear 

03:06:33.520 --> 03:06:35.520
from you.

03:06:36.544 --> 03:06:38.544
Next up is Alex who will talk 
about tf.function and autograph.

03:06:40.226 --> 03:06:42.226
Thank you very much.

03:06:43.485 --> 03:06:47.014
Hello. I am Alex. I am here to 
tell you about how you are 

03:06:50.496 --> 03:06:53.378
going to build graphs in 
TensorFlow 2.0. This might make 

03:06:53.379 --> 03:06:56.715
you a little uncomfortable 
because we already spent time 

03:06:56.716 --> 03:06:58.800
early telling you in TensorFlow 
2.0 we use eager execution by 

03:06:58.801 --> 03:07:01.637
default. So why am I taking that
away from you?

03:07:04.707 --> 03:07:07.730
And I am not. Graphs are useful 
for quite a few things. 

03:07:11.286 --> 03:07:13.286
The two ones I care the most 
about 

03:07:14.948 --> 03:07:16.948
personally are that some 
hardware like 

03:07:18.434 --> 03:07:20.434
our TPUs really benefit from the
full 

03:07:22.353 --> 03:07:25.226
program optimization if we have 
graphs and if you have graphs 

03:07:25.227 --> 03:07:27.880
you can take your model and 
deploy it on mobile and server 

03:07:31.164 --> 03:07:32.181
devices and make it available to
as many people as you can think 

03:07:32.182 --> 03:07:34.182
of.

03:07:36.692 --> 03:07:40.141
At this point you are probably 
-- I remember TensorFlow 1.0 and

03:07:40.142 --> 03:07:42.142
the kind of code I had to write 
to use graphs.

03:07:47.522 --> 03:07:50.791
Where -- is he going to tell me 
I have to keep doing that? No, 

03:07:50.792 --> 03:07:53.243
with TensorFlow 2.0 we are 
changing the programming model 

03:07:54.663 --> 03:07:56.663
which builds the graphs in 
TensorFlow.

03:07:57.717 --> 03:07:59.717
We are removing that model where
you 

03:08:00.817 --> 03:08:03.269
add a bunch of nodes and rely on
session.run to prune things out 

03:08:03.270 --> 03:08:05.270
and figure out 

03:08:06.322 --> 03:08:08.322
the precise things you want to 
run and 

03:08:10.217 --> 03:08:11.011
replacing it with a simpler 
model based on the idea of the 

03:08:11.012 --> 03:08:16.364
function. We are calling it tf.
function because that is the 

03:08:16.365 --> 03:08:20.067
main API entry point. With tf.
function, many things you are 

03:08:20.068 --> 03:08:22.068
used to 

03:08:23.585 --> 03:08:25.585
are going to go away and I 
dearly hope you won't miss them.

03:08:27.486 --> 03:08:30.332
The first thing going away is 
you will never have to use 

03:08:30.333 --> 03:08:35.469
session.run anymore. 
[Applause]

03:08:37.132 --> 03:08:39.591
If you have used TensorFlow 
execution, you know how it work.

03:08:43.223 --> 03:08:45.223
You have 

03:08:47.408 --> 03:08:49.039
tensors and operations.
Tf.function is just like an 

03:08:49.040 --> 03:08:51.040
operation 

03:08:52.287 --> 03:08:54.935
except one that you get to 
define using a composition of 

03:08:54.936 --> 03:08:58.595
the other operations in 
TensorFlow however you wish. 

03:08:58.596 --> 03:09:00.596
Once you have your tf.function, 
you can call it inside another 

03:09:02.096 --> 03:09:04.978
function, you can take its 
gradient, you can run it on the 

03:09:04.979 --> 03:09:06.979
GPU, on the TPU, on 

03:09:09.076 --> 03:09:11.076
the CPU, on distributed things 
just like 

03:09:14.591 --> 03:09:16.591
you would do with any other 
TensorFlow operation.

03:09:17.871 --> 03:09:20.710
We are letting you define 
oprations in Python and making 

03:09:20.711 --> 03:09:23.357
it as easy as possible and 
trying to preserve as many of 

03:09:23.358 --> 03:09:27.062
the semantics of the Python 
programming language you know 

03:09:27.063 --> 03:09:29.063
and love and when you execute 
these functions in a graph.

03:09:31.342 --> 03:09:33.168
So, obviously the first thing 
you would think is that is it 

03:09:33.169 --> 03:09:35.169
actually faster?

03:09:36.454 --> 03:09:38.454
If you look at models that are 
large 

03:09:39.468 --> 03:09:41.468
conconvolutions it is not any 
faster 

03:09:44.287 --> 03:09:46.934
because they execute plenty 
fast. Res models get small and 

03:09:46.935 --> 03:09:48.935
you can measure the difference 
in performance.

03:09:53.102 --> 03:09:55.767
I show for this tiny LSTM cell 
there is a 10-fold speed up if 

03:09:55.768 --> 03:09:58.198
you use tf.function verse if you
don't use tf.function to execute

03:09:58.199 --> 03:10:00.199
it.

03:10:01.277 --> 03:10:04.168
As I was saying, we really try 
to preserve as much as the 

03:10:04.169 --> 03:10:06.634
Python semantics as we can to 
make this code easy at a use.

03:10:09.724 --> 03:10:11.724
If you have seen TensorFlow 
graphs you 

03:10:14.762 --> 03:10:16.762
know they are not polymorphic
polymorphic.

03:10:20.955 --> 03:10:22.955
Python code tends to be free in 
the things it accepts. With tf.

03:10:23.810 --> 03:10:25.810
function we do the same.

03:10:27.536 --> 03:10:29.536
Under the hood, when you call a 
tf.

03:10:31.159 --> 03:10:33.159
function, we look at the

03:10:35.739 --> 03:10:37.987
tensors you are passing and see 
if you made a graph that is 

03:10:37.988 --> 03:10:42.260
compatible, if not we make a new
one. We hide them so you can use

03:10:42.261 --> 03:10:44.739
your tf.function as you would 
use a normal TensorFlow 

03:10:44.740 --> 03:10:47.585
operation and eventually you 
will get all the graphs you need

03:10:47.586 --> 03:10:49.586
built 

03:10:51.719 --> 03:10:54.568
up and your code will run BLAGZ 
blazingly fast. If you want 

03:10:54.569 --> 03:10:57.416
accease the graphs you are 
generating you can get them. We 

03:10:57.417 --> 03:10:59.417
expose them to you so you have 
need 

03:11:02.497 --> 03:11:03.705
to manipulate the grap or do 
weird things I don't approve you

03:11:03.706 --> 03:11:08.886
can still do it. The main reason
we changed this model, really, 

03:11:08.887 --> 03:11:11.537
is not to replace session.run 
with tf.function but it is by 

03:11:11.538 --> 03:11:16.631
changing the promise for what we
do to your code, we can do so 

03:11:16.632 --> 03:11:18.632
much more for you than we could 
do before.

03:11:20.948 --> 03:11:23.441
With the model where you add 
nodes to a graph and prune them 

03:11:23.442 --> 03:11:26.747
it is hard for it TensorFlow 
Runtime to know what order you 

03:11:26.748 --> 03:11:28.748
want the operations to be 
executed in. 

03:11:29.666 --> 03:11:31.666
Almost every TensorFlow 
operation is 

03:11:34.354 --> 03:11:36.603
stateless so it doesn't matter 
but for where it does you 

03:11:36.604 --> 03:11:39.083
probably had to use control 
dependencies other things to 

03:11:39.470 --> 03:11:41.968
make it work. I am here to tell 
you you will never 

03:11:46.249 --> 03:11:49.124
have to use control dependencies
again if you are using 

03:11:49.125 --> 03:11:52.799
tf.function. The promise behind 
tf.function is you write code 

03:11:52.800 --> 03:11:55.898
you would like to run eagerly, 
we take it and make it run fast.

03:11:56.310 --> 03:12:00.590
As we trace your Python code to 
generate a graph, we look at the

03:12:00.591 --> 03:12:02.591
operations you 

03:12:04.063 --> 03:12:06.063
run and every time we see a 
stateful 

03:12:07.351 --> 03:12:09.994
operation, we add the minimal 
necessary set of control 

03:12:09.995 --> 03:12:12.846
dependencies to ensure the 
resources accessed are accessed 

03:12:12.847 --> 03:12:16.104
in the order you want them to 
be. If you have two variables 

03:12:16.105 --> 03:12:20.413
you are updating we will do that
in parallel. If you have one 

03:12:20.414 --> 03:12:22.414
variable updating many times we 
will order the updates so you 

03:12:24.094 --> 03:12:25.729
are not surprised by them 
happening out of order or 

03:12:25.730 --> 03:12:27.730
something like that.

03:12:30.006 --> 03:12:32.460
There is really no crazy 
surprises and weird identifying 

03:12:32.461 --> 03:12:34.461
behavior.

03:12:36.005 --> 03:12:38.043
You should never need to 
explicitly add control 

03:12:38.044 --> 03:12:40.490
dependencies to your code but 
you will still get the ability 

03:12:40.491 --> 03:12:42.536
of knowing what order things 
execute.

03:12:45.670 --> 03:12:47.106
If you want to check something 
before something else, just put 

03:12:47.107 --> 03:12:49.107
that line of 

03:12:51.678 --> 03:12:53.678
code before that line of code 
like how 

03:12:54.971 --> 03:12:56.971
you do normal programming. 

03:13:00.071 --> 03:13:02.509
We can simplify how you use 
variables also. You know 

03:13:02.510 --> 03:13:05.424
variables are useful and let you
persist and checkpoint and all 

03:13:05.425 --> 03:13:07.425
those 

03:13:09.471 --> 03:13:16.673
things but they can be a little 
finically.Thifinicy. Another 

03:13:16.674 --> 03:13:18.674
thing we are removing from 
TensorFlow is the need to manual

03:13:21.767 --> 03:13:23.767
initialize variables yourself.

03:13:25.019 --> 03:13:26.851
The story for variables is a 
little complicated because as we

03:13:26.852 --> 03:13:28.852
try to make 

03:13:30.338 --> 03:13:33.219
code compatible with eager 
execution and graph semantics 

03:13:33.220 --> 03:13:36.488
you will find examples where it 
is unclear what we should do. My

03:13:36.489 --> 03:13:38.489
favorite one is this one.

03:13:40.578 --> 03:13:41.990
If you run this in TensorFlow 1.
X and session run repeatedly you

03:13:41.991 --> 03:13:43.991
will get a series of numbers 
that goes up.

03:13:47.916 --> 03:13:49.916
But if you run the code eagerly,
every 

03:13:52.245 --> 03:13:55.326
time you run it you will get the
same number back. If I want to 

03:13:55.327 --> 03:13:57.327
wrap this code of tf.function, 
which one should it do?

03:14:00.467 --> 03:14:02.715
Follow the 1.X behavior or the 
eager behavior? 

03:14:05.759 --> 03:14:08.021
I think if we took a poll I 
don't agree with myself. This is

03:14:08.022 --> 03:14:10.022
an area.

03:14:11.705 --> 03:14:14.151
Non ambiguously created 
variables is perfectly OK. You 

03:14:14.152 --> 03:14:16.833
can create a variable and 
capture it by closure in a 

03:14:16.834 --> 03:14:18.834
function.

03:14:22.597 --> 03:14:24.421
That is a way of a lot of 
TensorFlow is written. You can 

03:14:24.422 --> 03:14:26.487
write the function so it only 
creates variables the first time

03:14:26.488 --> 03:14:30.174
it is called. This is what most 
libraries in TensorFlow do under

03:14:30.175 --> 03:14:32.175
the hood.

03:14:33.287 --> 03:14:35.528
This is how keras is 
implemented, sonnet, and all 

03:14:35.529 --> 03:14:37.529
sorts of other libraries that 
use TensorFlow variables. 

03:14:38.790 --> 03:14:40.013
They try to take care to not 
create variables every time they

03:14:40.014 --> 03:14:44.120
are called otherwise you are 
creating too many variables and 

03:14:44.121 --> 03:14:46.991
not training anything. Code that
behaves well gets thrown into 

03:14:48.009 --> 03:14:50.104
function and it is fine. If you 
have seen this, I didn't need to

03:14:52.968 --> 03:14:55.605
call the initializer for this 
variable I am creating. It is 

03:14:55.606 --> 03:14:58.268
even better. I can make the 
initializer depend on 

03:15:02.405 --> 03:15:04.405
the value of there argument of 
the 

03:15:06.905 --> 03:15:08.905
FUNGSZFUNGSZ -- function in 
arbitarily 

03:15:10.154 --> 03:15:11.783
ways and we add the dependencies
to make sure the state updates 

03:15:11.784 --> 03:15:14.443
happen in the way we want, so 
there is no need to worry about 

03:15:14.444 --> 03:15:16.444
this.

03:15:18.337 --> 03:15:20.605
You can create variables like 
how you would use in a normal 

03:15:20.606 --> 03:15:22.606
programming 

03:15:25.705 --> 03:15:27.705
language and things behave the 
way you want. 

03:15:29.197 --> 03:15:31.223
I am really happy about the 
autograph integration as well. 

03:15:34.278 --> 03:15:36.278
If anyone has used TensorFlow 
with 

03:15:38.954 --> 03:15:40.954
autograph it can be awkward but 
we are 

03:15:42.820 --> 03:15:44.820
finally breaking up with the 
prior forms 

03:15:45.893 --> 03:15:47.893
and you can write code that 
looks like this.

03:15:48.896 --> 03:15:51.627
I have a wild loop here and this
is 

03:15:58.343 --> 03:16:00.872
probably the worst way to make a
tensor. If you put this in a tf.

03:16:02.900 --> 03:16:04.347
function it will put it in a 
graph and execute it. This is 

03:16:04.348 --> 03:16:06.403
nice and great. How does it 
work?

03:16:12.104 --> 03:16:14.104
Under the hood things like tf.

03:16:15.450 --> 03:16:17.450
file are still there but we have
tf.

03:16:19.573 --> 03:16:22.025
autograph that rewrites control 
expressions into something that 

03:16:22.026 --> 03:16:27.994
looks like this which is not how
you would want to write code. 

03:16:27.995 --> 03:16:30.480
This then can be taken by 
TensorFlow and turned into fast 

03:16:30.481 --> 03:16:36.255
dynamic graph code. 
How does this work? To explain 

03:16:36.256 --> 03:16:38.490
that I would like to take a step
back and think about how does 

03:16:39.104 --> 03:16:41.104
anything in TensorFlow work.

03:16:42.165 --> 03:16:44.165
You can have a tensor and do 
tensor 

03:16:46.275 --> 03:16:50.398
plus tensor times other tensors.
Just use a tensor like you would

03:16:50.399 --> 03:16:54.268
a floating point number. Python 
has a thing called operator 

03:16:56.315 --> 03:16:57.563
overloading that let's us change
the behavior of standard Python 

03:16:57.564 --> 03:16:59.564
operators 

03:17:02.288 --> 03:17:04.288
when applied on our custom data 
types 

03:17:05.805 --> 03:17:08.864
like tensors. 
We can add these underscores and

03:17:08.865 --> 03:17:10.865
change 

03:17:13.371 --> 03:17:15.371
how TensorFlow does addition and
subtraction.

03:17:18.891 --> 03:17:20.519
Python doesn't let us write 
underscore underscore. It makes 

03:17:20.520 --> 03:17:22.520
me very sad.

03:17:24.796 --> 03:17:27.239
If you this can  -- if you think
about this for a few second, you

03:17:27.240 --> 03:17:29.240
can come up with rewrite rules 
that will let us 

03:17:34.555 --> 03:17:36.276
lower to byte code with 
underscore-underscore if that is

03:17:36.277 --> 03:17:38.277
under writeable.

03:17:40.971 --> 03:17:43.644
If code looks like this, you can
write this as condition.if a and

03:17:43.645 --> 03:17:45.645
B.

03:17:47.308 --> 03:17:49.134
You need to do fiddling of the 
scopes because I am sure you 

03:17:49.135 --> 03:17:53.631
know Python's lexical scoping is
not as lexical as you think. 

03:17:53.632 --> 03:17:55.632
Names can leak out of scopes and
it is 

03:17:57.106 --> 03:17:59.106
a little messy but that is also 
a mechanical transformation.

03:18:03.682 --> 03:18:04.696
If this is potentially a 
mechanical transformation let's 

03:18:04.697 --> 03:18:06.697
do this.

03:18:07.797 --> 03:18:09.429
We wrote this compiler called 
autograph that takes your Python

03:18:09.430 --> 03:18:11.430
code and rewrites 

03:18:13.986 --> 03:18:15.986
it in a form that let's us call 

03:18:17.255 --> 03:18:19.896
underscore underscore if and etc
on tensors. This is all does but

03:18:19.897 --> 03:18:21.897
this unlocks a lot 

03:18:26.043 --> 03:18:28.043
of the power of native Python 
control into your tensor graphs.

03:18:30.108 --> 03:18:32.108
On this function, I have two 
loops.

03:18:33.586 --> 03:18:35.034
One it is a static Python loop 
because where write four I in 

03:18:35.035 --> 03:18:39.955
range. I is an integer. 
Autograph sees this and leaves 

03:18:39.956 --> 03:18:41.956
it untouched.

03:18:43.624 --> 03:18:45.862
You still get to use Python 
control flow to chose how many 

03:18:45.863 --> 03:18:47.863
layers your 

03:18:49.536 --> 03:18:50.990
network will have or iterate 
over a sequential but when your 

03:18:50.991 --> 03:18:53.644
control flow does depend on the 
properties of tensors 

03:18:57.114 --> 03:18:58.534
like in the second loop for 
intf.range then autograph sees 

03:18:58.535 --> 03:19:00.535
it and turns 

03:19:03.871 --> 03:19:06.133
it into a dynamic tf.wildloop. 
This means you can implement 

03:19:06.134 --> 03:19:08.134
this in 10 lines of code just 
like how you would 

03:19:11.447 --> 03:19:14.121
use in a normal language which 
is pretty nice. And anything 

03:19:14.122 --> 03:19:16.558
that you can do in a TensorFlow 
graph you can make happen 

03:19:16.559 --> 03:19:18.559
dynamically.

03:19:21.049 --> 03:19:24.838
You can make prints and 
assertions happen. Notice here 

03:19:24.839 --> 03:19:26.887
that I don't need to add control
dependencies to make sure they 

03:19:27.905 --> 03:19:29.778
happen in the right order 
because of the thing we were 

03:19:29.779 --> 03:19:33.458
talking about earlier. 
We already do. We control the 

03:19:33.459 --> 03:19:35.459
dependencies automatically for 
you to try to really 

03:19:40.347 --> 03:19:42.347
make your code look and

03:19:43.506 --> 03:19:45.506
behave the same as Python code.

03:19:46.774 --> 03:19:48.885
The TensorFlow Runtime is not 
powerful enough to support 

03:19:48.886 --> 03:19:52.169
everything that Python can do. 
So, for example, if you are 

03:19:55.638 --> 03:19:57.467
manipulating list of tenors at 
run-time you should still use a 

03:19:57.468 --> 03:19:59.468
tensor array.

03:20:03.647 --> 03:20:05.693
It works very well and compiles 
to eefficient TensorFlow code.

03:20:08.757 --> 03:20:10.183
You no longer need to write the 
boilerplate code associated with

03:20:10.184 --> 03:20:13.106
it. This is how you stack a 
bunch together in a loop. 

03:20:17.126 --> 03:20:19.126
Wrapping up, I think we have

03:20:21.115 --> 03:20:23.115
changed a lot in TF 2.

03:20:24.165 --> 03:20:26.430
I hope you agree the changes are
worth it. I will quickly walk 

03:20:26.431 --> 03:20:29.710
through you a dif of what your 
code will look look before and 

03:20:29.711 --> 03:20:31.711
after. 
Session run goes away.

03:20:35.610 --> 03:20:37.610
Control dependencies go away.

03:20:40.370 --> 03:20:42.370
Variable initialization goes 
away.

03:20:44.427 --> 03:20:46.427
Cond and

03:20:49.014 --> 03:20:50.839
wildloop go away and you just 
use functions like a normal 

03:20:50.840 --> 03:20:52.840
language.

03:20:55.147 --> 03:20:57.147
Thank you and welcome to TF 2.

03:20:58.661 --> 03:21:00.661
All the examples are on 
tensorflow.

03:21:01.708 --> 03:21:03.708
org/alpha and dig you will find 
a Colab 

03:21:05.364 --> 03:21:07.364
notebook with these and more and
play 

03:21:10.259 --> 03:21:12.259
around with the tf.function for 
auto. 

03:21:13.342 --> 03:21:15.342
Now I would like to introduce 
Ryan who 

03:21:17.202 --> 03:21:19.229
is going to tell you about all 
the ways we have of feeding data

03:21:19.230 --> 03:21:21.230
into TensorFlow. 

03:21:24.163 --> 03:21:26.163
I am here to tell you about 
TensorFlow datasets.

03:21:28.278 --> 03:21:30.278
It is on GitHub and pipi. First 
a little set-up.

03:21:34.774 --> 03:21:39.526
This is the partnership care 
about. TensorFlow 2.0 makes it 

03:21:39.527 --> 03:21:41.527
look nice.

03:21:42.785 --> 03:21:44.785
You get predictions, your loss, 

03:21:47.062 --> 03:21:48.699
gradient, update, it all looks 
fantastic. And they are good 

03:21:48.700 --> 03:21:53.416
together. Data and models. 
Really, I just want to focus in 

03:21:53.417 --> 03:21:55.417
on this one piece. This one 
line.

03:21:59.388 --> 03:22:01.388
Iterating through your training 
dataset.

03:22:03.877 --> 03:22:05.877
In TF 2, with the training data 
API, 

03:22:06.941 --> 03:22:08.941
you have this expressive simple 
way of 

03:22:11.732 --> 03:22:13.732
expressing high performance but 
flexible in put pipelines.

03:22:17.222 --> 03:22:19.222
We might start with a TF dataset
and we 

03:22:20.504 --> 03:22:22.504
can shuffle and repeat and build
complex 

03:22:24.332 --> 03:22:26.332
pipelines and we know this will 
be performance.

03:22:29.012 --> 03:22:31.797
This map call might be doing 
image preprocessing on each 

03:22:31.798 --> 03:22:33.003
record in the dataset. It is 
happening in parallel you can 

03:22:33.004 --> 03:22:39.184
see here. TensorFlow will be 
spinning up a bunch of threads 

03:22:39.185 --> 03:22:41.880
so the paping is high 
performance. At the end, you 

03:22:41.881 --> 03:22:43.910
have the ability to pre-fetch. 
As your tight model training 

03:22:43.911 --> 03:22:45.911
loop is 

03:22:46.983 --> 03:22:49.646
going, you can be preparing 
batches ready to feed into the 

03:22:49.647 --> 03:22:51.647
iteration.

03:22:54.347 --> 03:22:56.347
This is fantastic bit but 
unfortunately 

03:22:58.044 --> 03:23:00.044
the source data is a little out 
of step.

03:23:02.192 --> 03:23:06.278
We want this tight iteration 
with four iputs and targets. 

03:23:06.279 --> 03:23:08.928
What is hidden is how did you 
get the data into a format that 

03:23:08.929 --> 03:23:11.598
you can feed into the model in 
the first place? 

03:23:18.544 --> 03:23:21.811
Every source dataset when is all
a little bit different. This 

03:23:21.812 --> 03:23:23.662
makes sense when you distribute 
data at rest there is some 

03:23:23.663 --> 03:23:27.366
natural format you would like to
distribute it in. We are not 

03:23:27.367 --> 03:23:29.809
interest in data at rest. We are
interested in having data in a 

03:23:35.111 --> 03:23:37.111
format that is ready to move 
into a TF data pipeline.

03:23:39.009 --> 03:23:41.009
What we really want is something
that is good together.

03:23:45.582 --> 03:23:47.582
We want these two pups, data and
model, 

03:23:50.504 --> 03:23:53.160
to really be simpatico. It is 
about getting data into a format

03:23:55.048 --> 03:23:57.540
that is ready to move into a tf.
data pipeline. Here is 

03:23:57.541 --> 03:23:59.541
TensorFlow datasets.

03:24:02.237 --> 03:24:04.237
You import TensorFlow datasets 
and this 

03:24:06.469 --> 03:24:08.469
load API for

03:24:10.580 --> 03:24:12.222
mnist, and we are grabbing the 
training split of the dataset. 

03:24:17.519 --> 03:24:19.519
We are asking for supervised 
tuples.

03:24:21.953 --> 03:24:25.149
We will get input and target 
tuples. We get a tf.

03:24:28.210 --> 03:24:30.050
data dataset and we can build a 
complex pipeline. We are adding 

03:24:30.051 --> 03:24:32.051
shuffling and batching in this 
case.

03:24:33.945 --> 03:24:36.631
We are off and we can iterate 
through our inputs and do all 

03:24:36.632 --> 03:24:39.686
the modeling things we need. 
Load hides a lot of the 

03:24:39.687 --> 03:24:44.821
complexity. What is happening 
here with load is that mnist is 

03:24:44.822 --> 03:24:46.854
being fetched from the source, 
it is being pre-processed, put 

03:24:49.696 --> 03:24:51.121
into a standard format, and it 
is being documented with 

03:24:51.122 --> 03:24:55.014
statistics and everything which 
I will show you in a second, and

03:24:55.015 --> 03:24:57.252
then you are building the sort 
of front end where you have 

03:24:59.289 --> 03:25:01.289
preprocessed data on disks ready
to move.

03:25:02.359 --> 03:25:05.420
Typically you would only do this
one and tf.ds will tash the 

03:25:05.421 --> 03:25:10.775
results for you and you are 
getting a tf.data pipeline out 

03:25:10.776 --> 03:25:12.776
of it.

03:25:13.826 --> 03:25:16.683
We don't just for mnist. We have
imagenet.

03:25:20.173 --> 03:25:22.003
The first step is running this 
batch script, muck with the 

03:25:22.004 --> 03:25:26.157
files over here, make sure you 
filter out these records and 

03:25:26.158 --> 03:25:28.158
things like that and of course 
this 

03:25:30.317 --> 03:25:32.317
is all taken care of for you.

03:25:36.417 --> 03:25:38.417
ImageNet is gated with a 
password and 

03:25:39.932 --> 03:25:42.165
username and so currently 
TensorFlow datasets ask you to 

03:25:42.166 --> 03:25:45.216
download the data and put it in 
a certain place and TensorFlow 

03:25:45.217 --> 03:25:47.217
datasets takes it from there.

03:25:51.260 --> 03:25:53.260
We are working with cagal to get

03:25:56.267 --> 03:25:58.267
ImageNet to be downloaded 
without any user interaction.

03:26:01.406 --> 03:26:03.238
We are imdb reviews in this 
case. 

03:26:03.239 --> 03:26:05.239
Note the API doesn't change. 

03:26:08.771 --> 03:26:10.771
This is a tfts data call.

03:26:12.640 --> 03:26:15.084
Of course, text is a notoriously
difficult format to work with 

03:26:15.085 --> 03:26:17.741
when you are interested in doing
machine learning on it you have 

03:26:17.742 --> 03:26:20.193
to do encoding. We are shipping 
with three really 

03:26:23.401 --> 03:26:25.401
powerful 

03:26:26.541 --> 03:26:28.165
encodings one is byte level, 
character loading encoding and 

03:26:28.166 --> 03:26:30.166
another is subwords.

03:26:35.645 --> 03:26:37.645
We have a great subword text 
encoder.

03:26:40.023 --> 03:26:42.023
We will get imdb reviews already

03:26:43.477 --> 03:26:45.477
encoded using a vocabulary with 
about 8,000 tokens.

03:26:48.071 --> 03:26:49.490
Each of these datasets is 
packaged together as a dataset 

03:26:49.491 --> 03:26:52.166
builder. 
If you want to add a dataset to 

03:26:54.807 --> 03:26:56.634
TensorFlow datasets, which I 
hope every single one of you do,

03:26:56.635 --> 03:26:58.635
there should be 

03:27:00.788 --> 03:27:02.830
about a thousand new datasets in
a week, dataset builder is 

03:27:02.831 --> 03:27:04.831
simple.

03:27:05.895 --> 03:27:11.077
It does those exact three things
lat load hiding. It has a method

03:27:11.078 --> 03:27:13.078
called download and prepare 
taking source data on the local 

03:27:16.407 --> 03:27:19.467
web or local directory and 
produces preprocessed files. It 

03:27:19.468 --> 03:27:21.112
takes data at rest and put it 
into a format of data ready to 

03:27:21.113 --> 03:27:23.571
move. 
The second is as dataset and 

03:27:23.572 --> 03:27:26.420
that takes the preprocessed 
files on disk and 

03:27:31.662 --> 03:27:33.662
produces a tf.data.dataset.

03:27:38.031 --> 03:27:41.597
It third is metadata about the 
dataset. 

03:27:41.598 --> 03:27:45.943
Dataset.info has the features in
the dataset and each feature 

03:27:45.944 --> 03:27:47.944
documents the name, shape and 
type.

03:27:50.421 --> 03:27:52.421
So in this case, the image for 
mnist is 

03:27:56.786 --> 03:27:59.027
28, 28 by 1, type int 8. The 
number of classes is documented 

03:27:59.028 --> 03:28:01.028
in the class label feature and 
you get 

03:28:05.139 --> 03:28:07.375
statistics on the dataset and 
what are the splits exposed, 

03:28:07.376 --> 03:28:10.262
train and set, and how many 
records in each one.

03:28:16.194 --> 03:28:19.859
Mnist has 60,000 in the training
and 10,000 for it testing. If 

03:28:19.860 --> 03:28:22.087
you want to grab a dataset from 
TensorFlow datasets and filter 

03:28:22.088 --> 03:28:24.088
for all 

03:28:27.608 --> 03:28:29.643
the ones that are trivially 
supervised datasets, you can 

03:28:29.644 --> 03:28:33.317
look for supervised keys and 
pick up the features you need. 

03:28:33.318 --> 03:28:35.318
For text, again, one of these 
things 

03:28:37.393 --> 03:28:39.393
that is a bit annoying to work 
with 

03:28:42.677 --> 03:28:44.677
often, so this is imdB reviews. 

03:28:46.583 --> 03:28:48.583
The text feature contains the 
encoder.

03:28:51.708 --> 03:28:54.154
We see it is a texting encoder 
with a vocabulary size of 8, 

03:28:54.155 --> 03:28:56.155
185.

03:28:59.001 --> 03:29:02.716
We support numpy usage as well. 
We want to make TensorFlow 

03:29:02.717 --> 03:29:04.717
datasets really portable.

03:29:13.338 --> 03:29:15.338
You can call this tfts as 

03:29:16.627 --> 03:29:21.530
umpy arrays. Coming back to our 
initial example, you know, we 

03:29:21.531 --> 03:29:23.531
have this beautiful, you know, 

03:29:26.005 --> 03:29:27.910
model and data, but of course 
trained dataset -- where did 

03:29:27.911 --> 03:29:31.593
this come from and how did you 
get the data? With TensorFlow 

03:29:31.594 --> 03:29:33.594
datasets a simple change and you
have something that works 

03:29:36.954 --> 03:29:38.579
end-to-end, out of the box, 
which is something we are really

03:29:38.580 --> 03:29:40.580
happy about and hope you are 
too.

03:29:49.390 --> 03:29:51.390
You can come find us on pypi and
GitHub.

03:29:54.098 --> 03:29:56.948
The community on GitHub has 
gotten surprisingly active 

03:29:56.949 --> 03:30:00.865
surprisingly quickly. Come join 
us. We could use your data if 

03:30:00.866 --> 03:30:02.866
you are 

03:30:04.733 --> 03:30:06.733
distributing a dataset and would
like to 

03:30:10.740 --> 03:30:12.364
make your dataset famous, please
add it. Come help us on GitHub.

03:30:16.026 --> 03:30:18.026
There are dataset requests and 
you can 

03:30:20.581 --> 03:30:22.411
help implement a dataset that 
has been requested or help us 

03:30:22.412 --> 03:30:25.699
develop. We develop out in the 
open and develop straight out on

03:30:25.700 --> 03:30:27.700
GitHub.
Thanks very much.

03:30:32.631 --> 03:30:34.631
Hopefully you guys find this 
useful and 

03:30:36.293 --> 03:30:38.293
come help us out. Thank you.

03:30:42.010 --> 03:30:44.010
Next is something that I think 
is really great.

03:30:46.449 --> 03:30:48.449
Chris Lattner

03:30:54.274 --> 03:30:55.291
and Brendon telling you about 
Swift for TensorFlowTensorFlow. 

03:30:55.292 --> 03:30:59.786
Thanks, Brian. Fantastic. Hi, I 
am Chris and this is Brennan and

03:31:01.815 --> 03:31:02.843
we are super excited to tell you
about a new approach to machine 

03:31:02.844 --> 03:31:07.348
learning. Here in the TensorFlow
team, it is our jobs to push the

03:31:07.349 --> 03:31:09.995
state-of-the-art of machine 
learning forward. We have 

03:31:09.996 --> 03:31:13.243
learned a lot over the last few 
years with deep learning. We 

03:31:13.244 --> 03:31:15.244
have incorporated most of that 
all into TensorFlow 2.

03:31:17.960 --> 03:31:19.960
0 and we are really excited 
about it. 

03:31:21.641 --> 03:31:23.641
Here we are looking further 
beyond than TensorFlow 2.0. 

03:31:25.111 --> 03:31:27.629
Eager mode makes it easy to 
train dynamic model, but 

03:31:27.630 --> 03:31:29.661
deploying it still requires you 
take that and write a bunch 

03:31:32.719 --> 03:31:36.801
of C++ code to help drive it. 
That could be better. Some 

03:31:36.802 --> 03:31:38.218
researchers are interested in 
taking machine learning models 

03:31:38.219 --> 03:31:41.088
and integrating them into larger
applications.

03:31:46.446 --> 03:31:48.446
That often requires writing C++ 
code also. 

03:31:50.717 --> 03:31:52.717
We want more expressive and 
differential 

03:31:54.202 --> 03:31:55.824
mechanisms and we are excited to
define reusable types that can 

03:31:55.825 --> 03:31:57.825
be put into new 

03:32:01.103 --> 03:32:05.093
places and used with automatic 
differentiate. We want to make 

03:32:05.094 --> 03:32:07.969
your more productive by taking 
errors in your code and 

03:32:07.970 --> 03:32:11.856
improving your iteration time. 
Now, what we are really trying 

03:32:11.857 --> 03:32:16.570
to do here is lift TensorFlow to
entirely new heights. We need to

03:32:16.571 --> 03:32:18.192
innovate at all levels of the 
stack and this includes the 

03:32:18.193 --> 03:32:21.469
compiler and the language. That 
is what's Swift for TensorFlow 

03:32:21.470 --> 03:32:23.470
is about all.

03:32:26.009 --> 03:32:27.430
We think that applying new 
solutions to old problems with 

03:32:27.431 --> 03:32:31.980
help push machine learning even 
further than before. Let's jump 

03:32:31.981 --> 03:32:34.857
into some code. First, what is 
Swift? 

03:32:37.950 --> 03:32:40.015
Swift is a modern and class 
platform language designed to be

03:32:40.016 --> 03:32:43.066
easy to learn and use. Swift 
uses types which are great 

03:32:46.755 --> 03:32:49.239
because they can help you catch 
errors earlier and encourage 

03:32:49.240 --> 03:32:51.240
good API design.

03:32:52.303 --> 03:32:54.145
Swift uses type inference so it 
is easy to use and elegant but 

03:32:54.146 --> 03:32:56.146
it is also open 

03:32:57.294 --> 03:32:59.294
source and has an open evolution

03:33:02.215 --> 03:33:04.468
language processing allowing us 
to change the language and make 

03:33:04.469 --> 03:33:06.469
it better.

03:33:07.532 --> 03:33:10.395
This is how you define a model 
in Swift for TensorFlow. We are 

03:33:10.396 --> 03:33:13.281
laying out our layers here and 
we can define a forward function

03:33:13.282 --> 03:33:15.282
which 

03:33:17.573 --> 03:33:19.573
com poses them together in a 
linear sequence.

03:33:24.641 --> 03:33:26.641
You probably noticed this looks

03:33:28.051 --> 03:33:30.051
like keras and that is no error.

03:33:32.141 --> 03:33:34.141
All we have to do to train is 

03:33:35.392 --> 03:33:37.249
substantiate the model, pick an 
optimizer and random data and a 

03:33:37.250 --> 03:33:43.192
training loop. We will write it 
by hand because that gives you 

03:33:43.193 --> 03:33:45.193
the maximum flexibility to play 
with constructs and you can do 

03:33:45.231 --> 03:33:47.231
whatever you want.

03:33:49.146 --> 03:33:50.363
Some of the major advantages of 
Swift for TensorFlow is the 

03:33:50.364 --> 03:33:52.364
workflow. 

03:33:54.450 --> 03:33:56.450
Instead of telling you about it,
what do you think Brian?

03:34:02.047 --> 03:34:04.047
-- 
Brennan. 

03:34:04.487 --> 03:34:06.487
What could be easier than 
opening up a browser tab?

03:34:10.443 --> 03:34:11.878
This is Google Colab hosted 
Jupyter notebooks and comes with

03:34:11.879 --> 03:34:13.879
Swift for TensorFlow built right
in. 

03:34:14.168 --> 03:34:16.168
Let's see it in action.

03:34:17.227 --> 03:34:18.874
Here is the layer model or the 
model that Chris just showed you

03:34:18.875 --> 03:34:20.875
a couple slides ago.

03:34:23.382 --> 03:34:25.382
We are going to run it using 
some 

03:34:29.067 --> 03:34:32.184
random training data right here 
in the browser. We will

03:34:35.724 --> 03:34:37.743
instantiate the data and trained
it using Swift for TensorFlow in

03:34:37.744 --> 03:34:42.056
our browser on training data 
right here. We can see the 

03:34:42.057 --> 03:34:44.898
training losses decreasing over 
time so that is great, 

03:34:48.169 --> 03:34:49.193
but if you are ever like me when
I try to use machine learning in

03:34:49.194 --> 03:34:52.049
any application, I start with 
the simple model and I have to 

03:34:52.050 --> 03:34:57.359
iterate. I have to tweak the 
model to make it fit better to 

03:34:57.360 --> 03:35:00.001
the task at hand. So, since we 
are trying to show you the 

03:35:03.689 --> 03:35:05.717
workflow, let's edit this model,
let's make it more accurate. 

03:35:05.718 --> 03:35:07.718
Here we are. Now, let's think a 
little for a moment.

03:35:11.467 --> 03:35:13.467
What changes do we want to make 
to our model?

03:35:17.332 --> 03:35:19.332
This is deep learning 

03:35:20.690 --> 03:35:22.690
after all so the answer is 
always to go deeper. 

03:35:24.829 --> 03:35:26.249
Not just sequential layer but 
skip connections are a good idea

03:35:26.250 --> 03:35:28.075
to make sure your model 
continues to train effectively. 

03:35:31.607 --> 03:35:33.607
Let's go through and actually 
add an 

03:35:34.653 --> 03:35:36.653
extra layer to our model, let's 
add skip 

03:35:38.930 --> 03:35:40.774
connections, and we will do it 
all right now in under 90 

03:35:40.775 --> 03:35:44.892
seconds. Are you ready?
Here we go. We need to define 

03:35:44.893 --> 03:35:46.893
our additional layer first.

03:35:48.582 --> 03:35:50.582
We will fill in this dense 
layer. 

03:35:51.652 --> 03:35:53.739
One thing you can see is that we
are using tab auto complete to 

03:35:53.740 --> 03:35:56.795
help fill in code as we are 
trying to develop and modify our

03:35:56.796 --> 03:35:58.796
model.

03:36:00.475 --> 03:36:03.356
Now, we are going to fix up the 
shapes right here really quick 

03:36:03.357 --> 03:36:09.056
so that the residual connections
will all work. If I can type 

03:36:09.057 --> 03:36:11.057
properly that would go better. 
Great.

03:36:13.322 --> 03:36:16.592
We have now defined our model 
with the additional layers. All 

03:36:16.593 --> 03:36:18.593
we need to do is modify the 
forward 

03:36:20.065 --> 03:36:23.170
pass so that we add those skip 
connections. Here we go. The 

03:36:23.171 --> 03:36:25.171
first thing we need to do is 
store 

03:36:27.069 --> 03:36:29.950
in a temporary variable the 
output of the flattened layer.

03:36:33.017 --> 03:36:35.661
Then we will feed that output to
our first dense layer.

03:36:39.778 --> 03:36:41.778
Dense.

03:36:42.825 --> 03:36:44.825
applied to tmp in context.

03:36:50.397 --> 03:36:52.397
For the cuda graphs here is our 
dense applied.

03:36:56.200 --> 03:36:58.200
Tmp plus tmp2 in context. Run 
that.

03:37:00.458 --> 03:37:05.680
Yes, it worksism -- works. Let's
see how it does.

03:37:10.172 --> 03:37:12.172
We will reinstantiate our model 
and run the training loop.

03:37:16.531 --> 03:37:18.531
This loss is now substantially 
lower.

03:37:19.770 --> 03:37:22.662
This is an example of what it is
like to use Swift for TensorFlow

03:37:22.663 --> 03:37:24.663
as you apply 

03:37:25.939 --> 03:37:26.553
and develop models for 
applications and challenges. 

03:37:26.554 --> 03:37:28.554
[Applause] 
Thank you.

03:37:31.499 --> 03:37:34.422
But Swift for TensorFlow was 
designed for researchers and 

03:37:34.423 --> 03:37:36.423
researchers often 

03:37:38.916 --> 03:37:40.916
need to do more than just change
models 

03:37:41.968 --> 03:37:43.968
and change the way architecture 
fits together.

03:37:45.431 --> 03:37:48.123
They need to define entirely new
abstractions and layers. Let's 

03:37:48.124 --> 03:37:50.124
see that live right now.

03:37:51.818 --> 03:37:53.818
Let's define a new custom layer.

03:37:54.919 --> 03:37:56.757
Let's say we had the brilliant 
idea we wanted to modify the 

03:37:56.758 --> 03:37:59.214
standard dense layer that takes 
weights and biases and 

03:38:02.673 --> 03:38:04.673
add an additional bias set of 
parameters.

03:38:06.364 --> 03:38:10.418
We will define this double bias 
dense layer right here. I am 

03:38:10.419 --> 03:38:14.317
going to type this quickly. 
Stand by. 15 seconds. Here we 

03:38:14.318 --> 03:38:17.674
go!
[Laughter]

03:38:18.272 --> 03:38:22.150
That was great. Let's walk 
through the code so you can see 

03:38:22.151 --> 03:38:25.842
what is going on. The first 
thing that we have is we define 

03:38:25.843 --> 03:38:27.843
our parameters.

03:38:29.924 --> 03:38:31.924
These are w, like our weights 
for our 

03:38:33.247 --> 03:38:35.247
neurons and B1 and B2.

03:38:36.744 --> 03:38:38.998
We define an initializer that 
takes an input and output size 

03:38:38.999 --> 03:38:40.999
just like dense 

03:38:42.050 --> 03:38:44.506
does and we use that to 
initialize our parameters. The 

03:38:44.507 --> 03:38:46.507
forward pass is very simple to 
write.

03:38:49.797 --> 03:38:52.053
Here is just aplied to and we 
take the matrix multiplication 

03:38:52.054 --> 03:38:54.054
of input by our 

03:38:58.597 --> 03:39:01.603
weights and add in our bias 
terms. We have just defined a 

03:39:01.604 --> 03:39:05.573
custom layer in Colab in just a 
few lines of code. Here is model

03:39:05.574 --> 03:39:07.574
two and we will use our 

03:39:09.488 --> 03:39:11.488
double bias dense layer and we 
are going 

03:39:15.159 --> 03:39:17.159
to instantiate and use our

03:39:19.126 --> 03:39:20.990
custom handwritten training 
loop. Because TensorFlow can 

03:39:20.991 --> 03:39:26.710
statically analyze your code, it
can be really helpful to you. I 

03:39:26.711 --> 03:39:28.711
don't know about you but I 
regularly 

03:39:29.778 --> 03:39:31.415
put typos in my code and Swift 
for TensorFlow is helping you 

03:39:31.416 --> 03:39:33.416
out.

03:39:40.043 --> 03:39:43.342
It is saying look, you mistyped 
softcross-in.

03:39:45.170 --> 03:39:47.608
This is not the right idea but 
this is an example of how easy 

03:39:47.609 --> 03:39:50.068
it is for researchers to 
experiment with new ideas 

03:39:54.385 --> 03:39:59.109
really easily in Swift for 
TensorFlow but let's go deeper. 

03:39:59.110 --> 03:40:01.775
Swift for TensorFlow is, again, 
designed for researchers and 

03:40:01.776 --> 03:40:06.265
researchers need to be able to 
customize everything. That is 

03:40:06.266 --> 03:40:08.266
the whole point of research.

03:40:09.934 --> 03:40:10.944
So, let's show an example of how
to customize something other 

03:40:10.945 --> 03:40:12.945
than just a model or layer.

03:40:15.317 --> 03:40:17.317
You may have heard that large 
GPU 

03:40:18.376 --> 03:40:20.825
clusters or TPU super pods are, 
like, delivering massive 

03:40:20.826 --> 03:40:22.826
breakthroughs in 

03:40:23.900 --> 03:40:24.741
research and advancing the 
state-of-the-art in certain 

03:40:24.742 --> 03:40:26.742
applications and domains.

03:40:31.091 --> 03:40:32.933
You may have always heard as you
scale up you need to increase 

03:40:32.934 --> 03:40:34.802
your batch size. 
Let's say you are a researcher 

03:40:34.803 --> 03:40:37.259
and you want to try and figure 
out what are the 

03:40:41.117 --> 03:40:43.117
best ways to train deep neural 
networks 

03:40:44.824 --> 03:40:46.436
at larger batch sizesment if you
are a researcher, you probably 

03:40:46.437 --> 03:40:49.348
can't guy a whole GPU cluster or
went a whole TPU 

03:40:53.665 --> 03:40:55.499
super pod all the time for your 
experiments but you often have a

03:40:55.500 --> 03:40:57.500
GPU under your desk.

03:41:01.449 --> 03:41:03.449
Let's see how we can simulate 
that on a single machine.

03:41:06.377 --> 03:41:09.455
We will do it all in a few lines
of code here. Here is our custom

03:41:09.456 --> 03:41:12.733
training loop and the standard 
part. This is our 1-10 training 

03:41:12.734 --> 03:41:14.982
epics. What we will do is 
instead of just 

03:41:18.251 --> 03:41:20.522
applying our model forward once,
we have an additional interloop,

03:41:20.523 --> 03:41:22.523
right?

03:41:24.184 --> 03:41:26.264
We will run our forward pass, we
will run our model four times, 

03:41:26.265 --> 03:41:28.265
and we are 

03:41:29.763 --> 03:41:31.763
going to take the gradients for 
each 

03:41:34.243 --> 03:41:36.243
step and aggregate them in this 
grads variable.

03:41:37.328 --> 03:41:39.365
This simulates running on four 
independent accelerates in a 

03:41:39.366 --> 03:41:41.599
data parallel fashion on a batch
that is four 

03:41:44.681 --> 03:41:46.681
times as large as what we 
actually run. 

03:41:49.907 --> 03:41:51.907
We will then use our 

03:41:54.928 --> 03:41:57.769
optimizer. 
That is all there is to it. We 

03:41:57.770 --> 03:41:59.397
are really excited by this 
flexibility and capabilities 

03:41:59.398 --> 03:42:02.454
that Swift for TensorFlow brings
to researchers. Back over to 

03:42:02.455 --> 03:42:06.743
you, Chris. 
Thanks, Brennan. 

03:42:09.593 --> 03:42:12.072
[Applause] 
So I think the focus on catching

03:42:12.073 --> 03:42:16.146
errors early and also 
productivity enhancements like 

03:42:16.147 --> 03:42:19.465
code completion can help you in 
a lot of ways. It's not just 

03:42:19.466 --> 03:42:21.466
about automating typing 

03:42:23.175 --> 03:42:25.462
of code, but it can be about 
discovery of APIs. So another 

03:42:25.463 --> 03:42:27.463
thing that is really cool 

03:42:31.144 --> 03:42:33.571
about Swift as a language is it 
has good interoperability with C

03:42:33.572 --> 03:42:35.572
code. 

03:42:37.018 --> 03:42:38.468
You can import header files and 
call directly from C without 

03:42:38.469 --> 03:42:40.117
wrappers or boilerplate or 
anything involved. It just 

03:42:40.118 --> 03:42:43.980
works. We have taken this 
approach in the TensorFlow and 

03:42:43.981 --> 03:42:46.247
brought it to the world of 
Python. One of the cool things 

03:42:46.248 --> 03:42:50.992
about this is that allows you to
combine the power of Swift for 

03:42:50.993 --> 03:42:53.638
TensorFlow with all the 
advantages of the Python Eco 

03:42:53.639 --> 03:42:58.356
system. How about we take a 
look? 

03:42:58.960 --> 03:43:00.960
Thanks, Chris.

03:43:03.653 --> 03:43:04.465
The Python data science 
ecosystem is incredibly powerful

03:43:04.466 --> 03:43:06.466
and vibrant.

03:43:09.978 --> 03:43:11.978
We wanted to make sure you 
didn't miss 

03:43:14.088 --> 03:43:16.088
your favorite libraries and 
utilitiesio  

03:43:18.421 --> 03:43:20.421
you are  -- 
utilities you were used to. 

03:43:25.497 --> 03:43:27.497
Let's see how this works in

03:43:29.379 --> 03:43:31.379
context to

03:43:33.562 --> 03:43:35.562
numpy.

03:43:38.288 --> 03:43:40.288
We import py plot from the 
library and 

03:43:41.406 --> 03:43:43.406
numpy and assign it to np.

03:43:44.892 --> 03:43:46.986
After that we can use np just as
if we were in Python.

03:43:50.449 --> 03:43:52.449
We will call sine and cosine and
pass them to py plot.

03:43:56.171 --> 03:43:58.171
When we run the cell, it just 
works 

03:43:59.530 --> 03:44:01.159
exactly as you would expect. 
[Applause] 

03:44:01.160 --> 03:44:03.160
Thank you. Now, this sort of 
kind of looks like 

03:44:06.868 --> 03:44:08.287
the Python code you are used to 
writing but this is actually 

03:44:08.288 --> 03:44:11.759
pure Swift. It just works 
seamlessly. This is maybe a bit 

03:44:11.760 --> 03:44:13.760
of a toy example. 

03:44:17.248 --> 03:44:19.248
Let's see this a little bit more
more in context.

03:44:22.768 --> 03:44:25.004
OpenAI has done a lot of work in
the area of reinforcement 

03:44:25.005 --> 03:44:27.005
learning and 

03:44:28.076 --> 03:44:30.747
developed a Python library 
called OpenAI gym which contains

03:44:30.748 --> 03:44:34.620
a collection of environments 
that are very useful when you 

03:44:34.621 --> 03:44:36.858
are trying to train a 
reinforcement agent across a 

03:44:36.859 --> 03:44:38.859
variety of different challenges.

03:44:42.455 --> 03:44:44.075
Let's use OpenAI gym to train a 
reinforcement agent in 

03:44:44.076 --> 03:44:47.952
TensorFlow right in our 
browsers. The first thing we 

03:44:47.953 --> 03:44:49.953
need to do is import 

03:44:51.991 --> 03:44:53.456
gym and define a few 
hyperparameters and now define 

03:44:53.457 --> 03:44:55.933
our neural networks. 
We will pick a simple two layer 

03:44:55.934 --> 03:45:00.635
dense network in this case and 
it is just a sequential model. 

03:45:00.636 --> 03:45:02.672
After that we have helper code 
to filter 

03:45:06.160 --> 03:45:08.388
out bad or short episodes and 
whatnot. Here is the real meat 

03:45:08.389 --> 03:45:10.389
of it.

03:45:15.763 --> 03:45:17.800
We will use gym to instantiate 
the cart pole environment and we

03:45:17.801 --> 03:45:21.089
will instantiate our network in 
the optimizer and here is our 

03:45:21.090 --> 03:45:23.383
training loop. We are going to 
get a bunch of 

03:45:27.465 --> 03:45:28.283
episodes, we are going to run 
our model, get the 

03:45:28.284 --> 03:45:30.943
gradientgradients and apply 
those to the optimizer and 

03:45:30.944 --> 03:45:35.447
record the mean rewards as we 
train. Very simple, 

03:45:35.448 --> 03:45:37.691
straightforward Swift. 
And here you go see us training 

03:45:37.692 --> 03:45:39.692
a Swift 

03:45:40.751 --> 03:45:42.998
for TensorFlow in an OpenAI gym 
environment using the Python 

03:45:42.999 --> 03:45:48.701
bridge. Totally seamless.
Afterwards, you can keep track 

03:45:48.702 --> 03:45:51.979
of the parameters of the 
rewards. In this case we will 

03:45:51.980 --> 03:45:53.980
plot the mean 

03:45:55.662 --> 03:45:58.146
rewards as the model trained 
using Python Numpy. Totally 

03:45:58.147 --> 03:46:00.998
seamless. You can get started 
using Swift for TensorFlow using

03:46:00.999 --> 03:46:03.847
all the libraries you know and 
love and take advantage of what 

03:46:05.493 --> 03:46:07.160
Swift for TensorFlow brings to 
the table. 

03:46:07.161 --> 03:46:10.040
Back over to you, Chris. 
Thanks, Brennan.

03:46:13.559 --> 03:46:15.582
One thing I love about this is 
it is not just about leveraging 

03:46:15.583 --> 03:46:17.583
big important libraries.

03:46:18.849 --> 03:46:21.322
We are working on the ability to
integrate Swift for TensorFlow 

03:46:21.323 --> 03:46:25.612
and Swift for Python together 
which we think will provide a 

03:46:25.613 --> 03:46:27.613
nice path to help you 

03:46:29.519 --> 03:46:31.519
incrementally move code from one
worth to the other.

03:46:34.215 --> 03:46:36.215
I think it is fair to say 
calculus is 

03:46:37.681 --> 03:46:40.531
an intergral part of machine 
learning. We think 

03:46:40.532 --> 03:46:43.188
differentiable programming is so
important we built it right into

03:46:43.189 --> 03:46:45.189
the 

03:46:47.517 --> 03:46:50.398
language enabling more flexible 
and custom work with 

03:46:50.399 --> 03:46:54.054
derivatives. We think this is 
really cool. I would like to 

03:46:54.055 --> 03:46:56.055
take a look. 

03:46:59.840 --> 03:47:01.462
We have been using Swift for 
TensorFlow's differential 

03:47:01.463 --> 03:47:03.463
programming 

03:47:04.955 --> 03:47:06.955
throughout the demo but let's 
really break dedown. 

03:47:08.651 --> 03:47:11.292
Here we define my function that 
takes two doubles and returns a 

03:47:11.293 --> 03:47:13.293
double based 

03:47:15.970 --> 03:47:18.832
on some products and sums and 
quotients. If we want Swift for 

03:47:18.833 --> 03:47:20.833
TensorFlow to automatically 
compute the derivative for 

03:47:25.491 --> 03:47:27.491
us, I would annotate it at 
differential. 

03:47:27.932 --> 03:47:30.384
Swift for TensorFlow will then 
derive the derivative for this 

03:47:30.385 --> 03:47:32.620
function right when we run the 
cell.

03:47:38.756 --> 03:47:40.611
To use this auto generated 
derivative, use gradient which 

03:47:40.612 --> 03:47:44.939
takes a closure to evaluate and 
a point you want to evaluate 

03:47:44.940 --> 03:47:48.649
your closure at. Here we go. 
This is what it is to take the 

03:47:50.922 --> 03:47:52.819
derivative of a function at a 
particular point. We can change 

03:47:52.820 --> 03:47:57.508
this around. This one is my 
favorite. Tasty number. That 

03:47:57.509 --> 03:47:59.509
works nicely.

03:48:01.201 --> 03:48:03.036
One thing to note is we have 
been taking the partial 

03:48:03.037 --> 03:48:06.299
derivatives of my function with 
respect to A. Of course you can 

03:48:06.300 --> 03:48:08.300
take the partial 

03:48:10.591 --> 03:48:11.615
derivative and get a full 
gradient of my function like so.

03:48:11.616 --> 03:48:13.616
Often with neural networks, 
however, you 

03:48:19.000 --> 03:48:21.433
want to get not just the 
gragradients, you often want 

03:48:21.434 --> 03:48:23.667
what the network predicted?
This is useful to compute 

03:48:23.668 --> 03:48:25.668
accuracy or other debugging sort
of information.

03:48:29.029 --> 03:48:31.029
For that, you can use value with
gradient. 

03:48:32.724 --> 03:48:34.971
That returns a tuple containing 
the value and gradient 

03:48:34.972 --> 03:48:36.972
shockingly enough.

03:48:41.741 --> 03:48:42.767
One thing to note, in Swift 
tuples can have named 

03:48:42.768 --> 03:48:46.031
parameters. You can see it 
prints out nicely and you can 

03:48:46.032 --> 03:48:50.770
access values. We think this is 
another nice little thing that 

03:48:50.771 --> 03:48:52.771
helps make writing and debugging
code and more importantly 

03:48:56.882 --> 03:49:00.381
reading it and understanding it 
later a little bit easier. The 

03:49:00.382 --> 03:49:01.798
one thing I want to call out is 
throughout this we have been 

03:49:01.799 --> 03:49:03.799
using normal types.

03:49:05.067 --> 03:49:06.881
These are not tensor of 
something. It is just plain old 

03:49:06.882 --> 03:49:08.882
double.

03:49:11.586 --> 03:49:12.598
This is because automatic 
differentiateation is built 

03:49:12.599 --> 03:49:14.599
right in and 

03:49:16.074 --> 03:49:18.917
it makes it easy to express your
thoughts very clearly. But even 

03:49:18.918 --> 03:49:20.918
though it is built into the 

03:49:22.987 --> 03:49:24.987
language, we have worked very 
hard to 

03:49:32.043 --> 03:49:32.652
make sure automatic differiation
is customizeable. Let's show 

03:49:32.653 --> 03:49:35.109
you. 
Let's say you want to define an 

03:49:35.110 --> 03:49:37.110
algebra in 2d space.

03:49:38.977 --> 03:49:40.977
You are certainly going to need 
a point data type.

03:49:42.848 --> 03:49:44.848
Here we define a point instruct 
with X 

03:49:45.964 --> 03:49:48.228
and Y and mark it 
differentiable. We can define 

03:49:48.229 --> 03:49:53.561
helper functions on it like dot 
or other helper functions. Swift

03:49:53.562 --> 03:49:55.987
for TensorFlow when you try and 
use your code it will often 

03:49:57.629 --> 03:49:59.629
automatically infer when you 
need 

03:50:02.383 --> 03:50:03.598
gradients to be automatically 
computed for you by the 

03:50:03.599 --> 03:50:08.502
compiler. It is a good idea to 
document your intentions so you 

03:50:08.503 --> 03:50:11.355
can annotate your helping 
functions as at differentiable.

03:50:14.768 --> 03:50:18.574
The  other reason we recommend 
doing this is because it helps 

03:50:18.575 --> 03:50:21.686
catch errors. 
Swift for TensorFlow is telling 

03:50:21.687 --> 03:50:26.358
you that hey, you can only 
differentiate functions that 

03:50:26.359 --> 03:50:28.359
return values that 

03:50:29.846 --> 03:50:30.876
conform to differentiable but 
int doesn't conform to 

03:50:30.877 --> 03:50:32.877
differentiable.

03:50:33.971 --> 03:50:35.971
This is telling you my helper 
function 

03:50:39.497 --> 03:50:42.146
returns an int and int is about 
taking small steps and integers 

03:50:42.147 --> 03:50:44.147
are very discreet. 

03:50:47.268 --> 03:50:48.896
Swift for TensorFlow is helping 
to catch errors right when you 

03:50:48.897 --> 03:50:50.897
write the code and tell you what
is going on.

03:50:52.990 --> 03:50:54.990
The solution, of course, is to 
just not 

03:50:56.695 --> 03:51:00.360
mark it as a differentiable and 
the cell runs just fine. Let's 

03:51:00.361 --> 03:51:05.076
say we also wanted to go beyond 
just defining the dot product. 

03:51:05.077 --> 03:51:10.386
Let's say we wanted to define 
the magnitude helper function. 

03:51:10.387 --> 03:51:12.387
That is the magnitude of the 
vector 

03:51:14.261 --> 03:51:16.261
defined by the origin to the 
point in question.

03:51:18.571 --> 03:51:20.822
To do that, we can use the 
distance formula and we can 

03:51:20.823 --> 03:51:22.823
define the extension 

03:51:23.878 --> 03:51:25.878
on point that does this.

03:51:27.333 --> 03:51:29.400
We will pretend for a moment 
that Swift doesn't include a 

03:51:29.401 --> 03:51:31.916
square root function because I 
want a good excuse for you to 

03:51:36.145 --> 03:51:38.145
see the interopera

03:51:41.697 --> 03:51:43.697
interoperability with C.

03:51:44.956 --> 03:51:47.836
We will use C's square root 
function. We can define the 

03:51:47.837 --> 03:51:49.837
magnitude and it -- no, it 
doesn't quite work.

03:51:52.385 --> 03:51:54.448
Let's see what is going on. We 
wanted magnitude to be 

03:51:56.683 --> 03:51:57.906
differentiable and it is saying 
you can't differentiate the 

03:51:57.907 --> 03:51:59.907
square root 

03:52:02.000 --> 03:52:04.035
function because this is an 
external function that hasn't 

03:52:04.036 --> 03:52:06.036
been marked as differentiable.

03:52:07.921 --> 03:52:10.417
The square root is a C function 
compiled by the C compiler and 

03:52:10.418 --> 03:52:12.418
that 

03:52:15.473 --> 03:52:18.611
can't automatically compute 
derivatives for you. This is 

03:52:18.612 --> 03:52:20.612
saying it will not work and 

03:52:22.917 --> 03:52:25.162
this is excellent because it 
gives me a great excuse to show 

03:52:25.163 --> 03:52:27.402
you how to write custom 
gradientss.

03:52:31.111 --> 03:52:32.575
We define a wrapper function 
here that calls to the C square 

03:52:32.576 --> 03:52:35.019
root function. The backwards 
pass, we take our double, 

03:52:39.617 --> 03:52:42.309
and we return to a tuple of two 
values. Rather the first element

03:52:42.310 --> 03:52:44.947
in the tuple is the normal value
and the forward pass.

03:52:48.427 --> 03:52:51.321
The second a pullback closure 
and this is where you define the

03:52:51.322 --> 03:52:53.791
backwards pass capturing 
whatever values you need from 

03:52:54.398 --> 03:52:57.047
the forward pass. 
We are going to run that.

03:53:00.109 --> 03:53:02.109
We are going to go back up to 
the 

03:53:03.591 --> 03:53:05.591
definition of magnitude and 
change it 

03:53:06.656 --> 03:53:07.870
from square root to my square 
root, rerun the cell and it 

03:53:07.871 --> 03:53:09.871
works.

03:53:12.094 --> 03:53:17.468
We defined point, dot and 
magnitude. Here I defined the 

03:53:17.469 --> 03:53:21.167
silly function and marked it as 
differentiable. We are going to 

03:53:21.168 --> 03:53:23.619
take two points, we are also 
going to take a double, right? 

03:53:23.620 --> 03:53:25.656
You can mix and match 
differentiable 

03:53:29.387 --> 03:53:31.387
data types totally fluidly.

03:53:33.708 --> 03:53:35.191
We will do magnitude and dot 
products. It is a silly 

03:53:35.192 --> 03:53:37.192
function.

03:53:38.274 --> 03:53:40.274
We can use it, compute the 
gradient of 

03:53:44.014 --> 03:53:46.703
this function at arbitrary data 
points. You can get the value of

03:53:46.704 --> 03:53:48.704
there function 

03:53:50.571 --> 03:53:52.602
and full gradients in partial 
derivatives with respect to 

03:53:52.603 --> 03:53:54.603
individual values.

03:53:56.551 --> 03:53:58.551
That has been a quick run 
through of 

03:54:01.259 --> 03:54:03.259
custom gradients J custom data 
types 

03:54:04.732 --> 03:54:06.969
with the differentiate built in.
Let's put all this together and 

03:54:06.970 --> 03:54:09.038
show how you can write your own 
debuggers as 

03:54:12.921 --> 03:54:14.942
an example of how this power is 
all in your hands.

03:54:17.220 --> 03:54:19.988
Often when you are developing 
models, you want to -- or 

03:54:19.989 --> 03:54:22.640
debugging models, you want to be
to see the gradients at 

03:54:24.274 --> 03:54:26.274
different points within your 
model.

03:54:27.337 --> 03:54:29.399
We can define in regular Swift 
code a gradient debugger. 

03:54:31.443 --> 03:54:33.272
It will take as input, a double,
and it is going to return it 

03:54:33.273 --> 03:54:35.273
just like normal for the forward
pass, right?

03:54:39.835 --> 03:54:41.491
It is an identity function. We 
will get the gradient, print it 

03:54:41.492 --> 03:54:44.407
and return it. We are just 
passing it through printing it 

03:54:44.408 --> 03:54:49.985
out. We have defined this 
gradient debugger and we can use

03:54:49.986 --> 03:54:51.986
it in our silly function 

03:54:54.234 --> 03:54:57.746
to see what is going on as we 
take derivatives. 

03:54:59.430 --> 03:55:01.430
Gradientdebugger, here we go. We
can rerun that.

03:55:04.356 --> 03:55:06.808
And when we take the gradients, 
we can see that for that point 

03:55:06.809 --> 03:55:08.809
in the silly function, of a.

03:55:12.126 --> 03:55:14.126
b, the gradient is 3.80.

03:55:20.323 --> 03:55:22.323
That's been a brief through of 
how 

03:55:24.586 --> 03:55:26.586
automatic differiation works and
how you 

03:55:30.307 --> 03:55:33.373
can extract the power to build 
whatever systems you need. Back 

03:55:33.374 --> 03:55:35.374
over to you, Chris. 

03:55:40.356 --> 03:55:42.356
The algorithms we are defining 
were built in the 1970s.

03:55:46.275 --> 03:55:48.275
There is a tremendous amount of 
depth 

03:55:50.999 --> 03:55:52.999
and I am excited to see what you
all can do it about.

03:55:59.693 --> 03:56:01.765
If you are interested in 
learning more, we are detail 

03:56:01.766 --> 03:56:03.766
documents online.

03:56:05.647 --> 03:56:07.672
The language of Swift has low 
level performperformance and 

03:56:07.673 --> 03:56:10.167
there is no gill to get in the 
way of concurrency.

03:56:13.832 --> 03:56:16.519
They have advanced compiler 
techniques to identify and 

03:56:16.520 --> 03:56:18.989
extract graphs for you. The 
consequence of this together is 

03:56:18.990 --> 03:56:20.990
we 

03:56:23.470 --> 03:56:27.173
think Swift has the world's most
advanced eager mode. You may 

03:56:27.174 --> 03:56:29.174
wonder why do we care about this
stuff?

03:56:30.834 --> 03:56:32.834
We are seeing trends in the 
industry 

03:56:36.362 --> 03:56:38.396
where people are defining neural
networks. This requires you to 

03:56:38.397 --> 03:56:41.109
export graphs and write a bunch 
of C++ code to load and 

03:56:42.548 --> 03:56:44.548
orchestrate them in various 
ways.

03:56:46.444 --> 03:56:48.444
Let's look at an example of 
this.

03:56:51.087 --> 03:56:53.087
Alpha go-

03:56:55.619 --> 03:56:57.619
zero is amazing work.

03:57:01.730 --> 03:57:05.792
You have deep learning and it 
drives through it Monte Carlo. 

03:57:05.793 --> 03:57:07.793
It is the combination of all 
three 

03:57:09.684 --> 03:57:11.314
things that make AlphaGo Zero 
possible.

03:57:11.315 --> 03:57:13.315
This is possible today.

03:57:16.054 --> 03:57:20.562
If you are an advanced team like
DeepMind you can do this. We 

03:57:20.563 --> 03:57:23.404
think breaking down barriers 
like this can lead to new 

03:57:23.405 --> 03:57:27.070
breakthroughs in science and 
drive progress forward. Instead 

03:57:27.071 --> 03:57:31.555
of talking about it, again, 
let's take a look. 

03:57:35.917 --> 03:57:37.917
MiniGo is an Open Source Go 
player 

03:57:44.160 --> 03:57:46.160
inspired by DeepMind's AlphaGo 
Zero.

03:57:48.274 --> 03:57:49.709
The miniGo project were writing 
everything in normal TensorFlow 

03:57:49.710 --> 03:57:53.784
and it was working great until 
tay started trying to run at 

03:57:53.785 --> 03:57:55.785
scale on large clusters of TPUs.

03:57:59.468 --> 03:58:01.468
They ran into performance 
problems and 

03:58:03.759 --> 03:58:06.189
had to rewrite things into C++ 
to effectively utilize the 

03:58:06.190 --> 03:58:08.190
modern accelerators.

03:58:11.480 --> 03:58:13.480
We have reimplemented Monte 
Carlo 

03:58:19.477 --> 03:58:23.971
research and the rest of MiniGo 
player in Swift. We define a 

03:58:23.972 --> 03:58:25.972
helper function, these are 

03:58:27.224 --> 03:58:28.670
our white and black players, we 
are going to run basically play 

03:58:28.671 --> 03:58:33.592
the game until we have a winner 
or looser. So, let's actually 

03:58:33.593 --> 03:58:35.593
run this.

03:58:37.132 --> 03:58:39.132
We define a game configuration 
and we 

03:58:42.521 --> 03:58:44.989
will play between a Monte Carlo 
tree search versus a random 

03:58:44.990 --> 03:58:46.990
player to see how 

03:58:48.287 --> 03:58:50.944
easy it is to flip back and 
forth or mix and match between 

03:58:50.945 --> 03:58:52.945
deep learning and other 
arbitrary machine learning 

03:58:53.798 --> 03:58:55.798
algorithms right here in Swift. 
Here you go.

03:58:58.478 --> 03:59:00.922
You can see them playing white, 
black, playing different moves 

03:59:00.923 --> 03:59:02.923
back and forth and it just goes.

03:59:06.448 --> 03:59:08.515
We think that Swift for 
TensorFlow is going to unlock 

03:59:08.516 --> 03:59:10.516
whole new classes of algorithms 
and research because of how 

03:59:14.624 --> 03:59:16.664
easy it is to do everything in 
one language with no barriers 

03:59:16.665 --> 03:59:20.341
and not rewriting everything 
into the C++. Back to you, 

03:59:20.342 --> 03:59:23.265
Brennan. 
The cool thing about this is you

03:59:23.266 --> 03:59:25.266
can do 

03:59:26.901 --> 03:59:29.763
something like this in a work
workbook. We have seen families 

03:59:29.764 --> 03:59:34.536
of techniques that can be bound 
together and fused and bringing 

03:59:34.537 --> 03:59:35.772
this to more people we think 
will lead to new kinds of 

03:59:35.773 --> 03:59:40.750
research. Our work on usability 
and design is not just about 

03:59:40.751 --> 03:59:42.751
high-end researchers.

03:59:44.638 --> 03:59:46.638
We love them but Swift is widely
used 

03:59:50.599 --> 03:59:52.599
to teach new programmers to 
learn how to code.

03:59:54.916 --> 03:59:56.916
I am excited to announce a new 
journey 

03:59:58.807 --> 04:00:00.467
we are embarking on with Jeremy 
Howard.

04:00:00.468 --> 04:00:04.144
At fast.ai we are always looking
to push the boundaries of what 

04:00:04.145 --> 04:00:06.396
is possible especially pushing 
to make recent 

04:00:10.612 --> 04:00:13.160
advances more accessibility. We 
have been involved with this and

04:00:16.492 --> 04:00:18.492
building the world's best 
document classifier.

04:00:21.166 --> 04:00:23.166
Hundreds of thousands have 
become deep 

04:00:24.670 --> 04:00:26.670
lun  learners.

04:00:27.877 --> 04:00:29.877
We

04:00:33.244 --> 04:00:35.244
think with Swift for TensorFlow 
we can go even further.

04:00:38.359 --> 04:00:40.608
We are announcing a new program 
caught by someone that knows 

04:00:40.609 --> 04:00:44.914
Swift well. 
Chris, I think he means you. 

04:00:44.915 --> 04:00:47.389
I am super excited to help teach
the 

04:00:50.472 --> 04:00:52.472
next generation of learners and 
excited 

04:00:53.948 --> 04:00:55.948
Jeremy is bringing his expertise
in 

04:00:57.077 --> 04:00:59.941
Swift and helping us shift the 
higher level APIs. The most 

04:00:59.942 --> 04:01:01.783
important part is Swift for 
TensorFlow is really TensorFlow 

04:01:01.784 --> 04:01:03.784
at its core and we think this is
super important.

04:01:07.270 --> 04:01:08.915
We have worked hard to make sure
it integrates with all things 

04:01:08.916 --> 04:01:12.042
going on in the big TensorFlow 
family and we are very excited 

04:01:12.043 --> 04:01:14.278
about that. 
You may be wondering where you 

04:01:14.279 --> 04:01:18.824
can get this. Swift for 
TensorFlow is Open Source. You 

04:01:18.825 --> 04:01:20.825
can find it on GitHub now and 
you can join our community.

04:01:22.920 --> 04:01:27.613
It also works great in Colab as 
you have seen today. We have 

04:01:27.614 --> 04:01:29.457
tutorials and examples and the 
demos you saw today are 

04:01:29.458 --> 04:01:33.992
available in Colab.
We have released our 0.2 release

04:01:33.993 --> 04:01:35.821
including all the basic 
infrastructure and underlying 

04:01:35.822 --> 04:01:41.162
technology to power these demos 
and examples and we are actively

04:01:41.163 --> 04:01:43.163
working on high-level APIs right
now.

04:01:44.672 --> 04:01:46.529
This is not ready for production
yet as you can guess but we are 

04:01:46.530 --> 04:01:48.530
excited about 

04:01:50.185 --> 04:01:52.018
shaping the future, building 
this out, exploring this new 

04:01:52.019 --> 04:01:54.019
program model, and 

04:01:58.976 --> 04:02:01.041
this is a great example for 
advanced researchers to get 

04:02:01.042 --> 04:02:04.900
involved and help shape the 
platform. We would love for you 

04:02:04.901 --> 04:02:07.167
to try it out and let us know 
what you think.

04:02:11.121 --> 04:02:13.121
Next up, I would like to invite 
Edd to 

04:02:14.609 --> 04:02:17.285
talk about the TensorFlow 
community.

04:02:17.919 --> 04:02:19.919
Thanks, Chris.

04:02:24.670 --> 04:02:28.546
 Hey, everybody. I work with 
TensorFlow helping build 

04:02:31.436 --> 04:02:33.077
community and collaboration 
around the open source project. 

04:02:33.078 --> 04:02:36.555
Usually I put the thank you 
slide at the end for listening 

04:02:36.556 --> 04:02:38.556
to me but actually I want to 
thank you for contributing and 

04:02:40.461 --> 04:02:42.939
being part of the TensorFlow 
project. Whether you are here in

04:02:42.940 --> 04:02:47.056
this room or on the livestream, 
there has been some amazing talk

04:02:47.057 --> 04:02:49.294
on YouTube from people in China,
and India, and Japan, and all 

04:02:52.391 --> 04:02:53.813
over the world joining us 
virtually today. Thank you for 

04:02:53.814 --> 04:02:55.814
your contributions.

04:02:57.529 --> 04:02:59.529
The project is where it is today
because of you.

04:03:02.218 --> 04:03:04.047
Of course, in core TensorFlow 
alone we have had this many 

04:03:04.048 --> 04:03:06.048
commits.

04:03:07.704 --> 04:03:10.343
This figures out over 50,000 
from over 18,000 contributors 

04:03:10.344 --> 04:03:12.344
and much more than just code 
commits.

04:03:14.669 --> 04:03:16.510
39,000 stackoverflow questions 
about TensorFlow, we have 66 

04:03:16.511 --> 04:03:18.511
machine learning 

04:03:21.830 --> 04:03:23.274
Google developer experts, many 
of whom who are here with us 

04:03:23.275 --> 04:03:25.275
today.

04:03:26.751 --> 04:03:28.374
Welcome and thank you guys. It 
is really great to have you with

04:03:28.375 --> 04:03:32.464
us and thank you for everything 
you do helping teach people 

04:03:32.465 --> 04:03:34.920
about TensorFlow. We have had 14
guest posts to the 

04:03:38.213 --> 04:03:40.213
TensorFlow blog and that keeps 
going up.

04:03:43.356 --> 04:03:45.356
There are so many ways people 
are contributing. Thank you.

04:03:48.470 --> 04:03:51.736
You are really helping build out
the TensorFlow ecosystem. I want

04:03:51.737 --> 04:03:53.800
to discuss how we are growing 
the Eco system and report back 

04:03:53.801 --> 04:03:57.582
on some of the changes we have 
made over the last year. I am 

04:03:57.583 --> 04:03:59.583
going to cover how we are making

04:04:00.679 --> 04:04:03.324
it easier to get involved in 
TensorFlow and how we are trying

04:04:03.325 --> 04:04:05.325
to consult betwer 

04:04:06.380 --> 04:04:06.987
the users in the community and 
be more transparent about our 

04:04:06.988 --> 04:04:10.917
development. Going to cover how 
we are empowering everybody to 

04:04:10.918 --> 04:04:12.918
get involved and to do more 

04:04:18.855 --> 04:04:20.855
and increasing in the number of 
contact points.

04:04:25.667 --> 04:04:28.129
I will go a bit more into depth 
about the conference announced 

04:04:28.130 --> 04:04:32.201
today. 
Let's talk about how we are 

04:04:32.202 --> 04:04:34.202
making 

04:04:36.733 --> 04:04:38.733
contributions to TensorFlow 
easier.

04:04:40.455 --> 04:04:42.455
One of there most important 
thing is 

04:04:43.764 --> 04:04:45.764
increasing its modularity.

04:04:48.656 --> 04:04:50.656
We are trying to make it less of
a mono lift.

04:04:53.488 --> 04:04:55.488
When you come and want to

04:04:57.408 --> 04:04:57.812
contribute to an open source 
project it helps to know where 

04:04:57.813 --> 04:04:59.813
to go.

04:05:00.884 --> 04:05:02.730
We are creating more surface 
area. Our special interest 

04:05:02.731 --> 04:05:04.731
groups play a big 

04:05:06.646 --> 04:05:08.917
part in this and I will talk a 
bit more about them later. It is

04:05:08.918 --> 04:05:10.918
not just code.

04:05:11.992 --> 04:05:13.992
There is so many more places to 

04:05:16.499 --> 04:05:18.726
contribute this year compared to
last year. I will talk about the

04:05:18.727 --> 04:05:22.625
documentation groups, groups 
involved in testing, groups who 

04:05:22.626 --> 04:05:24.626
are blogging and on YouTube and 
more.

04:05:28.963 --> 04:05:30.963
I was super excited to see we 
published 

04:05:32.472 --> 04:05:35.130
TensorFlow tutorial in Korean. 
That isn't something we did on 

04:05:35.131 --> 04:05:37.589
our team. That is part of the 
community.

04:05:44.373 --> 04:05:46.373
We were similarly able to 
publish it in 

04:05:48.451 --> 04:05:50.451
Russian thank you to Andrew 
Stepin.

04:05:52.354 --> 04:05:55.016
I am also really excited about 
the TensorFlow 2.0 testing group

04:05:55.017 --> 04:05:57.706
led my Paige Bailey. This is a 
bunch of contributors and 

04:06:01.205 --> 04:06:03.450
Google developer experts who are
working to get TensorFlow 2.0 a 

04:06:03.451 --> 04:06:05.563
test. You see an example of 
friction load and 

04:06:11.101 --> 04:06:13.957
folks are going through ML loads
and documenting what they find 

04:06:13.958 --> 04:06:16.464
awesome and also things that 
could be a little better.

04:06:19.729 --> 04:06:21.729
This group meets weekly and 
often has 

04:06:22.850 --> 04:06:24.694
guest talks from maintainers and
SIGs leaders and it helping 

04:06:24.695 --> 04:06:26.931
bring TensorFlow 2.0 from 
cutting edge to something 

04:06:30.595 --> 04:06:32.595
thoroughly tested and ready to 
use.

04:06:34.670 --> 04:06:36.544
We mentioned we have over 14 
posts from guests on the 

04:06:36.545 --> 04:06:38.545
TensorFlow blog.

04:06:41.313 --> 04:06:43.552
This is from a great post about 
real-time segmentation with 

04:06:43.553 --> 04:06:47.041
TensorFlow.js that comes from a 
grad student and researcher at 

04:06:47.042 --> 04:06:52.176
itp. Whether it is testing, 
whether it is documentation, 

04:06:52.177 --> 04:06:57.092
whether it is blogs or 
conference talks, thank you. 

04:06:57.093 --> 04:06:59.093
Now, I want to talk a little bit
about TensorFlow RFCs.

04:07:02.065 --> 04:07:04.963
As you probably know, RFC means 
request for comments. This time 

04:07:04.964 --> 04:07:07.214
last year, we were not that 
organized about how we evolved 

04:07:09.506 --> 04:07:11.506
TensorFlow's design in terms of 

04:07:13.602 --> 04:07:16.070
communicating and I stood on 
this stage and told you about 

04:07:16.071 --> 04:07:18.071
launching the RFC process.

04:07:19.755 --> 04:07:22.421
We have accepted 21 of them over
the period of the last year. 

04:07:22.422 --> 04:07:27.311
This is our way to communicate 
design. We post an RFC about a 

04:07:27.312 --> 04:07:29.312
design and consult widely.

04:07:34.072 --> 04:07:36.551
This isn't just about code 
coming outwards, it can be 

04:07:36.552 --> 04:07:38.552
created on and commented on by 
anyone.

04:07:44.340 --> 04:07:45.941
We have several RFCs brought by 
the broader community and I 

04:07:45.942 --> 04:07:48.617
expect to see several more. One 
of the things I am most proud 

04:07:48.618 --> 04:07:50.618
about 

04:07:53.086 --> 04:07:54.506
is how the RFC process is 
underpinning the 2.0 transition.

04:07:58.197 --> 04:07:59.826
All the major changes have been 
proposed and consulted with in 

04:07:59.827 --> 04:08:04.161
RFCs. This isn't just a great 
way of consulting and getting 

04:08:04.162 --> 04:08:06.162
feedback, going 

04:08:08.032 --> 04:08:09.860
forward you have a big 
repository of documentation 

04:08:09.861 --> 04:08:11.861
about why design choices were 
made a certain way in TensorFlow

04:08:13.156 --> 04:08:14.787
and great educational results 
for people who are coming on and

04:08:14.788 --> 04:08:19.501
want to get involved in 
contributing to the project. I 

04:08:19.502 --> 04:08:21.502
really want to give a big thanks
to 

04:08:22.996 --> 04:08:24.996
anyone who authored or reviewed 
an RFC.

04:08:26.479 --> 04:08:30.588
You played a vital role in 
making TensorFlow better. Let's 

04:08:30.589 --> 04:08:33.039
talk a bit about the social 
structure of TensorFlow. You 

04:08:33.040 --> 04:08:35.281
know, last year I talked about 
how coming to a large project 

04:08:35.282 --> 04:08:38.773
can be a little bit daunting. 
You don't know where people are,

04:08:41.322 --> 04:08:43.974
whether people have your 
interest in common. We created 

04:08:43.975 --> 04:08:46.624
the special interest groups or 
SIGs as a way of organizing our 

04:08:46.625 --> 04:08:50.433
work. There are so many uses of 
TensorFlow, so many environments

04:08:50.434 --> 04:08:52.434
and architectures.

04:08:54.709 --> 04:08:56.709
Many of them are outside of the 
scope 

04:08:57.949 --> 04:08:59.985
of the core team and we want to 
enable TensorFlow to grow and be

04:08:59.986 --> 04:09:01.986
more 

04:09:04.324 --> 04:09:05.747
sustainable by creating a way 
for like-minded people to 

04:09:05.748 --> 04:09:07.748
collaborate.

04:09:09.031 --> 04:09:11.031
This is why SIGs exist. They are
groups of people working 

04:09:12.478 --> 04:09:14.478
together for a define project 
focus.

04:09:15.949 --> 04:09:17.949
We started last year and we have
six up and running now.

04:09:20.429 --> 04:09:22.429
I am going to give you a quick 
state of 

04:09:23.508 --> 04:09:24.537
the SIGs and many, in fact most 
of the leaders, are here with 

04:09:24.538 --> 04:09:27.851
us. I will give a shout out to 
them and hopefully you will be 

04:09:27.852 --> 04:09:29.852
able to talk to 

04:09:31.565 --> 04:09:33.565
them in the launch and tomorrow.

04:09:38.104 --> 04:09:40.104
SIG addons first. 

04:09:42.412 --> 04:09:44.412
Martin mentioned at the 
beginning of the 

04:09:50.149 --> 04:09:52.149
day that TF contrib is no longer
a part 

04:09:54.780 --> 04:09:57.132
and SIG addons are where that is
going.

04:10:00.812 --> 04:10:02.812
They conform to these well 
defined APIs.

04:10:06.315 --> 04:10:08.315
Most  there is already an RFC 
posted 

04:10:10.193 --> 04:10:12.193
about where you can find things 
that you 

04:10:15.096 --> 04:10:17.933
used to find in contrib. How can
you get involved if you have a 

04:10:19.984 --> 04:10:21.414
favorite and want to step up and
be a maintainer for everybody 

04:10:21.415 --> 04:10:23.415
and how you can join in the 
project.

04:10:26.758 --> 04:10:29.602
I encourage you to take a look 
at that. SIG build is where 

04:10:29.603 --> 04:10:32.470
TensorFlow meets the outside 
world. It is not always the most

04:10:32.471 --> 04:10:34.471
glamorous 

04:10:35.568 --> 04:10:37.568
piece of work but building it 
and 

04:10:39.010 --> 04:10:41.010
packaging and distributing it is
tough.

04:10:42.447 --> 04:10:44.447
Thank you to

04:10:47.996 --> 04:10:49.996
Jason and Austin who lead that 
SIG.

04:10:51.078 --> 04:10:53.926
It is home for third party 
builds for architectures we 

04:10:53.927 --> 04:10:55.927
don't ship.

04:11:03.368 --> 04:11:05.408
IBM power and SIG helps us be a 
better neighbor in the Python 

04:11:05.409 --> 04:11:07.409
ecosystem.

04:11:09.111 --> 04:11:11.347
Machine learning generates a lot
of extreme situations and 

04:11:11.348 --> 04:11:13.348
changes in the 

04:11:15.636 --> 04:11:17.636
way we package and distribute 
software.

04:11:21.330 --> 04:11:23.791
SIG-IO helps connect other 
systems and your data NLT real 

04:11:23.792 --> 04:11:26.662
world exists using systems in 
other formats.

04:11:33.538 --> 04:11:37.475
This group is led my yang tan 
and antoine. If you are using 

04:11:37.476 --> 04:11:39.476
any of these in the 

04:11:43.601 --> 04:11:45.601
Apache Eco system or the 
environments 

04:11:47.624 --> 04:11:50.565
you can use this addaddon. This 
group has already dropped four 

04:11:52.223 --> 04:11:54.223
releases and shipped our 
integration 

04:11:55.270 --> 04:11:57.270
with their module too.

04:12:07.599 --> 04:12:09.599
SIG networking is where a lot 
are going to.

04:12:14.716 --> 04:12:16.986
If you are using gdr verbs this 
is where you can find this.

04:12:20.915 --> 04:12:22.915
If you are interested in this, 
or any 

04:12:24.605 --> 04:12:27.071
of the other SIGs, please talk 
to the leaders. They want help 

04:12:27.072 --> 04:12:29.072
and are up and running 

04:12:30.126 --> 04:12:32.126
and in a great place to bring 
people on. 

04:12:33.200 --> 04:12:35.236
If you are looking for a way to 
get involved in TensorFlow, this

04:12:35.237 --> 04:12:37.237
is an ideal one.

04:12:41.954 --> 04:12:43.954
Let me talk about SIG 
TensorBoard.

04:12:45.875 --> 04:12:47.875
This is a great time to be 
involved as 

04:12:48.961 --> 04:12:51.663
the TensorBoard team is trying 
to figure out how we can best 

04:12:51.664 --> 04:12:53.763
enable people using TensorBoard 
for creating plugins or at 

04:12:53.764 --> 04:12:56.639
scale. If you go to the demo 
area above and go 

04:13:00.323 --> 04:13:02.323
to the TensorBoard stand you 
will find 

04:13:03.662 --> 04:13:06.142
Gal and Moni there who would be 
happy to talk to you. This is 

04:13:06.143 --> 04:13:08.591
the URL for anything you want to
do joining the TensorFlow 

04:13:08.592 --> 04:13:12.666
community from docs to testing 
to SIGs and all the other ways 

04:13:12.667 --> 04:13:14.720
to be involved. The developer 
mailing list. Please head there.

04:13:14.910 --> 04:13:19.412
If you are here with us, rather 
than on the livestream, we are 

04:13:19.413 --> 04:13:21.413
doing a 

04:13:24.058 --> 04:13:26.058
contributor 

04:13:27.426 --> 04:13:29.426
luncheon tomorrow where many of 
the core 

04:13:31.120 --> 04:13:33.549
team and SIGs will be there to 
talk to you. Finally, let's move

04:13:33.550 --> 04:13:35.676
on and talk about TensorFlow 
World. I am so excited about 

04:13:35.677 --> 04:13:40.196
this. It is our vision to bring 
together the amazing people who 

04:13:40.197 --> 04:13:42.871
are part of our community and 
give space for everyone to 

04:13:44.711 --> 04:13:46.754
connect with each other. There 
is so much we can learn from how

04:13:49.209 --> 04:13:51.423
we are all working with 
TensorFlow. Working with 

04:13:51.424 --> 04:13:53.424
O'Rielly media, we will have 
this event at the end of October

04:13:56.520 --> 04:13:58.520
here in Santa Clara.

04:13:59.593 --> 04:14:01.217
It will be four days that really
celebrate the TensorFlow 

04:14:01.218 --> 04:14:03.218
ecosystem.

04:14:04.549 --> 04:14:06.574
We will have content, talks, 
tutorials, there will be an expo

04:14:06.575 --> 04:14:09.194
and place for vendors to 
present. We understand 

04:14:09.195 --> 04:14:11.874
TensorFlow gets out into the 
real world there is a large 

04:14:15.335 --> 04:14:18.201
ecosystem beyond folks in this 
room. We are really excited and 

04:14:18.202 --> 04:14:21.268
that means we can bring everyone
together. The main point of 

04:14:21.269 --> 04:14:23.736
doing something like this is to 
connect all the amazing users 

04:14:27.259 --> 04:14:29.522
and everyone with experience to 
share. As you heard the call for

04:14:29.523 --> 04:14:31.523
proposals is now open.

04:14:33.591 --> 04:14:36.064
If you have anything to share 
with your work, your product, 

04:14:36.065 --> 04:14:41.623
your company, head to TensorFlow
World and put in a proposal. You

04:14:41.624 --> 04:14:43.711
have about 4-5 weeks while the 
call for proposals is open and 

04:14:43.712 --> 04:14:45.712
we will be selecting talks a few
weeks after that.

04:14:49.760 --> 04:14:51.760
I really hope you will

04:14:53.551 --> 04:14:55.580
lend your voice to this event 
and I am so excited to see you 

04:14:55.581 --> 04:14:58.850
in October. Once again, thank 
you. We appreciate how much you 

04:14:58.851 --> 04:15:00.851
are part of our community.

04:15:02.595 --> 04:15:04.595
In my job, the best thing every 
day is 

04:15:05.860 --> 04:15:08.712
when I get to talk to folks 
developing or using TensorFlow. 

04:15:08.713 --> 04:15:10.713
It is so exciting to work with 
everybody here.

04:15:12.848 --> 04:15:14.848
This is the way you can meet me.
Please do.

04:15:18.577 --> 04:15:21.841
If you have any issue or devire 
to contribute, reach out. We are

04:15:21.842 --> 04:15:23.468
really happy to talk to you. 
Thank you very much for your 

04:15:23.469 --> 04:15:32.832
attention. -- desire. 
[Applause] 

04:15:33.850 --> 04:15:38.115
Hey, everybody. Thank you, Edd. 
That was wonderful. We are at 

04:15:38.116 --> 04:15:43.405
another break. Come back here at
2:25. We will be talking about 

04:15:43.406 --> 04:15:45.847
TensorFlow in production. Then 
the next block of talks is about

04:15:46.879 --> 04:15:48.912
probability and so forth. Don't 
forget to go upstairs.

04:15:55.257 --> 04:15:57.543
We have deem demos of all kinds 
of stuff and Googlers galore.

04:16:00.671 --> 04:16:03.524
Check it out. Thanks a lot.

04:17:03.762 --> 04:17:05.783
. 
&gt;&gt;PAIGE BAILEY: Eager execution 

04:17:05.784 --> 04:17:07.784
and all 

04:17:08.853 --> 04:17:10.892
of the things that we have loved
to come to know and love through

04:17:10.893 --> 04:17:12.893
the last several iterations of 
TensorFlow development.

04:17:16.254 --> 04:17:18.485
We have a lot of tutorials and 
documentation that have been 

04:17:18.486 --> 04:17:20.486
recently released, it is about 
making you 

04:17:23.930 --> 04:17:25.930
productive as quick

04:17:28.188 --> 04:17:30.188
as possible, and giving you 
scalable, 

04:17:31.626 --> 04:17:33.626
reproducible learn

04:17:43.937 --> 04:17:44.949
ing. 
&gt;&gt;LAURENCE MORONEY: Yes, and 

04:17:44.950 --> 04:17:46.950
with it being open source, of 
course, is the 

04:17:52.708 --> 04:17:54.169
community is just a huge part of
what we do. 

04:17:54.170 --> 04:17:56.170
&gt;&gt;PAIGE BAILEY: We could not do 
it without the community. 

04:17:57.033 --> 04:17:58.686
&gt;&gt;LAURENCE MORONEY: And that is 
why we are doing the live 

04:17:58.687 --> 04:18:00.687
stream. 

04:18:02.773 --> 04:18:04.615
&gt;&gt;PAIGE BAILEY: If you have 
questions, #askTensorFlow is the

04:18:04.616 --> 04:18:06.695
hashtag. 
&gt;&gt;LAURENCE MORONEY: And one 

04:18:06.696 --> 04:18:08.696
question 

04:18:10.629 --> 04:18:12.629
that we are going to cover, you 
will get 

04:18:14.360 --> 04:18:16.360
today, it is great to do my 
training for 

04:18:18.652 --> 04:18:21.561
a fixed number of epochs, but 
when I reached the desired 

04:18:21.562 --> 04:18:23.562
accuracy metric, how do I 
continue training? 

04:18:24.207 --> 04:18:25.632
&gt;&gt;PAIGE BAILEY: It is not easy 
to compute if you have gotten to

04:18:25.633 --> 04:18:28.901
a point where your boss says, 
okay, cool, 99 percent accuracy,

04:18:28.902 --> 04:18:30.325
that is fine for us today. 
&gt;&gt;LAURENCE MORONEY: Should we 

04:18:30.326 --> 04:18:34.181
take a look at how we are doing 
that? 

04:18:34.789 --> 04:18:36.004
&gt;&gt;PAIGE BAILEY: Absolutely. 
&gt;&gt;LAURENCE MORONEY: I opened up 

04:18:36.005 --> 04:18:38.005
a codelab where you can see 
where I'm 

04:18:40.731 --> 04:18:42.167
causing call backs, and call 
backs are the ways that you 

04:18:42.168 --> 04:18:44.448
would achieve this.  And at the 
top, you can see the class, the 

04:18:44.449 --> 04:18:46.449
call back.

04:18:47.773 --> 04:18:49.605
And then when an epoch ends in 
training, I can look at the 

04:18:49.606 --> 04:18:51.606
logs.

04:18:53.293 --> 04:18:55.293
If the accuracy log is greater 
than 60 

04:18:56.570 --> 04:18:58.243
percent, he is really happy that
I'm on the training, 60 percent 

04:18:58.244 --> 04:19:00.244
accuracy, then I would cancel 
the training.

04:19:03.180 --> 04:19:05.180
And then to be able to set it 
up, I 

04:19:06.835 --> 04:19:08.835
would create an object, call 
backs, an 

04:19:10.940 --> 04:19:12.940
instance of the class, and I 
love the 

04:19:14.647 --> 04:19:16.277
click-in, the little things that
I love, and call backs equal 

04:19:16.278 --> 04:19:18.278
call backs, and 

04:19:20.172 --> 04:19:22.172
when I do the training, I will 
show off 

04:19:24.283 --> 04:19:27.549
and make my run time tied to the
GPU, so it is really fast. So 

04:19:27.550 --> 04:19:29.583
let's do a little bit of 
training with this one, so I'm 

04:19:29.584 --> 04:19:31.584
doing this live 

04:19:32.633 --> 04:19:34.898
and I'm connecting to the VM, it
is training, getting ready to --

04:19:34.899 --> 04:19:37.375
&gt;&gt;PAIGE BAILEY: The RAM and disk
utilization. 

04:19:38.443 --> 04:19:41.103
&gt;&gt;LAURENCE MORONEY: We are on 
the first epoch, that is 

04:19:41.104 --> 04:19:43.104
progressing away, 60,000 

04:19:44.755 --> 04:19:46.755
images being trained

04:19:50.129 --> 04:19:53.191
, and I hit accuracy of 83, 
which is really good.  But it 

04:19:53.192 --> 04:19:55.418
reached 60 percent accuracy, so 
I canceled the training.

04:20:00.180 --> 04:20:02.180
Call backs are helpful, before I

04:20:03.263 --> 04:20:05.263
learned about call backs, it is 
like, I 

04:20:07.550 --> 04:20:10.198
would set something up to train 
for 100 epochs, go to sleep, 

04:20:10.199 --> 04:20:15.575
wait up the next morning, and 
after three epochs, it didn't do

04:20:15.576 --> 04:20:18.214
its job and I wasted the time.  
&gt;&gt;PAIGE BAILEY: Yes, call backs 

04:20:18.215 --> 04:20:20.895
are incredibly invaluable, it 
does not just apply to accuracy 

04:20:20.896 --> 04:20:25.590
either.  There's a bunch of 
additional metrics that can be 

04:20:25.591 --> 04:20:27.591
useful for your particular 
workflow, and this code works in

04:20:28.914 --> 04:20:30.344
TensorFlow 2.0. 
&gt;&gt;LAURENCE MORONEY: Absolutely. 

04:20:35.059 --> 04:20:37.059
&gt;&gt;PAIGE BAILEY: So it is Caras, 
I love Caras. 

04:20:37.523 --> 04:20:40.376
&gt;&gt;LAURENCE MORONEY: It is a love
affair, and one neat thing about

04:20:40.377 --> 04:20:42.377
Caras that you 

04:20:43.468 --> 04:20:45.909
might not realize, is the same 
code for TensorFlow 1.x is the 

04:20:45.910 --> 04:20:50.170
same for 2.0, so what is going 
on behind the scenes is it is 

04:20:50.171 --> 04:20:55.105
equally in 2.0 instead of a 
graph mode.  So this codelab, 1.

04:20:57.960 --> 04:21:00.801
13, this code will run in 2.0 
without you modifying the code. 

04:21:00.802 --> 04:21:02.025
&gt;&gt;PAIGE BAILEY: Absolutely. 
&gt;&gt;LAURENCE MORONEY: Should we 

04:21:02.026 --> 04:21:05.950
take the next question? 
&gt;&gt;PAIGE BAILEY: Cool.  So the 

04:21:05.951 --> 04:21:08.383
next question is from Twitter it
looks like.  And what about all 

04:21:08.384 --> 04:21:11.049
the web developers who are new 
to AI ?

04:21:11.654 --> 04:21:14.319
Does the version 2.0 help them 
get started? 

04:21:20.091 --> 04:21:22.091
&gt;&gt;LAURENCE MORONEY: Oh web 
developers. 

04:21:24.628 --> 04:21:26.478
&gt;&gt;PAIGE BAILEY: I just finished 
talking to two folks from the 

04:21:26.479 --> 04:21:28.753
TensorFlow JS team about the 
cool new things they have seen 

04:21:31.599 --> 04:21:34.025
from the community, a vibrant 
ecosystem of artists and 

04:21:34.026 --> 04:21:36.873
creators that are using 
browser-based and server-based 

04:21:36.874 --> 04:21:40.136
tools to create these machine 
learning models, training and 

04:21:40.137 --> 04:21:42.199
running them. 
&gt;&gt;LAURENCE MORONEY: Yep.  And 

04:21:42.200 --> 04:21:44.200
for web developers, there's a 

04:21:45.275 --> 04:21:47.759
whole bunch of ways you can get 
started with this.  So you 

04:21:47.760 --> 04:21:49.760
mentioned TensorFlow JS, we will
start with that, but it is a 

04:21:49.791 --> 04:21:51.791
JavaScript library.

04:21:53.249 --> 04:21:55.277
And this will allow you to train
models in the browser, as well 

04:21:55.278 --> 04:21:57.923
as executing them.  And that 
blew my mind. 

04:21:59.746 --> 04:22:02.390
&gt;&gt;PAIGE BAILEY: And the node 
bindings, being able to use the 

04:22:02.391 --> 04:22:04.391
GPU inside your 

04:22:05.663 --> 04:22:07.886
laptop with, you know, Google 
Chrome or your favorite flavor 

04:22:07.887 --> 04:22:09.887
of browser, to 

04:22:11.499 --> 04:22:13.499
train a model

04:22:16.856 --> 04:22:17.052
.  That is absurd. 
&gt;&gt;LAURENCE MORONEY: And node 

04:22:17.053 --> 04:22:20.971
bindings as well, with node.JS. 
It is not only in JavaScript, 

04:22:20.972 --> 04:22:26.452
but server-side JavaScript with 
node.  And am I supposed to say 

04:22:26.453 --> 04:22:29.321
Node.js? I will say node.  And, 
of course, by the fact that it 

04:22:29.322 --> 04:22:33.823
is a node, one of my personal 
favorites, are you familiar with

04:22:33.824 --> 04:22:37.781
Cloud Functions for Firebase? 
&gt;&gt;PAIGE BAILEY: I'm not, I'm 

04:22:37.782 --> 04:22:39.224
intrigued. 
&gt;&gt;LAURENCE MORONEY: I used to 

04:22:39.225 --> 04:22:42.072
work on the Firebase team, so 
shout out to my friends on 

04:22:42.073 --> 04:22:43.490
Firebase. 
&gt;&gt;PAIGE BAILEY: I have heard so 

04:22:43.491 --> 04:22:45.953
many good things about Firebase.
&gt;&gt;LAURENCE MORONEY: It is for 

04:22:45.954 --> 04:22:47.954
mobile and web developers, and 
one of the things 

04:22:50.763 --> 04:22:52.763
that Firebase gives you are 
these things 

04:22:53.776 --> 04:22:55.776
called Cloud Functions for

04:22:58.544 --> 04:23:01.386
Firebase, I called up the web 
page, and you can execute the 

04:23:01.387 --> 04:23:03.863
functions on the back end 
without maintaining a server 

04:23:05.280 --> 04:23:07.127
infrastructure and execute them 
in response to a trigger, such 

04:23:07.128 --> 04:23:09.128
as an 

04:23:10.377 --> 04:23:12.018
analytic event, or a sign-in 
event. 

04:23:12.019 --> 04:23:14.019
&gt;&gt;PAIGE BAILEY: Or you get new 
data and 

04:23:15.644 --> 04:23:17.565
you have to process it.  
&gt;&gt;LAURENCE MORONEY: There you 

04:23:17.566 --> 04:23:20.208
go. And the fact that they run 
JavaScript code on the back end,

04:23:20.209 --> 04:23:22.462
and now you can train models in 
a cloud function. 

04:23:24.275 --> 04:23:25.705
&gt;&gt;PAIGE BAILEY: That's amazing. 
&gt;&gt;LAURENCE MORONEY: So web 

04:23:25.706 --> 04:23:27.706
developers, a lot of great 
options for you.

04:23:31.456 --> 04:23:33.456
However it is you want to do it,
in 

04:23:35.766 --> 04:23:37.766
the browser, the back end, you 
can get started with it. 

04:23:38.239 --> 04:23:40.464
&gt;&gt;PAIGE BAILEY: And TensorFlow 
2.0, if it gives additional 

04:23:40.465 --> 04:23:42.465
tools for application 
developers, I think it would 

04:23:45.159 --> 04:23:46.372
mostly be in terms of those 
codes and tutorial that we are 

04:23:46.373 --> 04:23:48.426
mentioning before.  And we also 
released some courses. 

04:23:49.227 --> 04:23:50.254
&gt;&gt;LAURENCE MORONEY: Yep. 
&gt;&gt;PAIGE BAILEY: So it is easier 

04:23:50.255 --> 04:23:52.255
than ever to get started.

04:23:57.384 --> 04:23:59.384
And, um, the models that you 
create can 

04:24:01.915 --> 04:24:02.321
be deployed to TS lite. 
&gt;&gt;LAURENCE MORONEY: And we have 

04:24:02.322 --> 04:24:05.975
been talking about Caras, and 
the layers are supported in 

04:24:05.976 --> 04:24:10.915
TensorFlow.JS.  It is not just 
for Python developers, if you 

04:24:10.916 --> 04:24:12.916
are a JS developer, you can 
define your layers. 

04:24:14.172 --> 04:24:16.172
&gt;&gt;PAIGE BAILEY: And an R 
developer, they 

04:24:19.689 --> 04:24:21.689
have Caras for R, which was 
created by 

04:24:25.386 --> 04:24:28.307
JJ Layer and Francois Chalet. 
&gt;&gt;LAURENCE MORONEY: A lot of 

04:24:28.308 --> 04:24:30.540
options for developers.  And 
next question, from Twitter, are

04:24:34.195 --> 04:24:36.453
there any JS learning examples 
from object detection? 

04:24:36.863 --> 04:24:39.099
&gt;&gt;PAIGE BAILEY: Node.JS is 
popular, we have learned. 

04:24:40.521 --> 04:24:41.940
&gt;&gt;LAURENCE MORONEY: So object 
detection.

04:24:41.941 --> 04:24:43.988
How do we answer this? So I'm --
it depends on what you mean 

04:24:47.674 --> 04:24:49.304
by object detection because, in 
Google, we talk about object 

04:24:49.305 --> 04:24:51.744
detection and use that term for,
in an image, you have a 

04:24:55.580 --> 04:24:57.580
lot of objects and bounding 
boxes for them, there are 

04:24:58.932 --> 04:25:01.568
samples for that. 
&gt;&gt;PAIGE BAILEY: And a lovely 

04:25:01.569 --> 04:25:03.569
thing is 

04:25:05.046 --> 04:25:07.046
the community is adept at 
creating TensorFlow.

04:25:13.244 --> 04:25:15.244
JS examples, for example, code 
pens, and

04:25:17.343 --> 04:25:19.343
Victor Dibia, a

04:25:20.895 --> 04:25:23.324
machine learning expert, for 
tracking hand movements.  So if 

04:25:23.325 --> 04:25:25.325
you want to create -- 
&gt;&gt;LAURENCE MORONEY: You have an 

04:25:25.558 --> 04:25:27.558
incentive. 

04:25:29.998 --> 04:25:31.998
&gt;&gt;PAIGE BAILEY: I have a limited

04:25:34.007 --> 04:25:36.102
edition collection of TensorFlow
socks, and I would love to send 

04:25:36.103 --> 04:25:38.103
you a bright, 

04:25:42.849 --> 04:25:44.063
shiny pair of limited edition 
socks. 

04:25:44.064 --> 04:25:45.885
&gt;&gt;LAURENCE MORONEY: They are 
classics, they have the old 

04:25:45.886 --> 04:25:47.773
logo. 
&gt;&gt;PAIGE BAILEY: Not the new 

04:25:47.774 --> 04:25:49.774
logo, but the old box style 
ones. 

04:25:50.299 --> 04:25:52.299
&gt;&gt;LAURENCE MORONEY: I love them 
both.

04:25:54.475 --> 04:25:56.475
And, like the question was about

04:25:57.716 --> 04:25:59.716
transfer learning, and we did 
not have a demo of object 

04:26:01.327 --> 04:26:03.975
detection, I would love to show 
Transfer Learning and being able

04:26:03.976 --> 04:26:06.421
to detect a single item in the 
frame.  To do that, we call 

04:26:06.422 --> 04:26:09.288
image classification.  So can I 
roll the demo? 

04:26:09.907 --> 04:26:12.747
&gt;&gt;PAIGE BAILEY: Please do.  Are 
you going to do your favorite --

04:26:12.748 --> 04:26:18.028
&gt;&gt;LAURENCE MORONEY: I am a child
of the '80s, I love Pac-Man.  So

04:26:18.029 --> 04:26:20.082
you will notice that it says 
loading mobile net now, so it 

04:26:20.083 --> 04:26:22.083
downloaded 

04:26:23.704 --> 04:26:26.631
the mobile net model.  So I'm 
going to add some new classes to

04:26:26.632 --> 04:26:28.632
the mobile net model and then 
use transfer -- 

04:26:29.473 --> 04:26:31.473
&gt;&gt;PAIGE BAILEY: There it goes.  
No, no, go for it.  

04:26:35.452 --> 04:26:37.306
&gt;&gt;LAURENCE MORONEY: So Pac-Man, 
old arcade game, you try to move

04:26:37.307 --> 04:26:39.307
away from the ghosts.

04:26:42.270 --> 04:26:44.546
I will train it to move up when 
I'm pointing up, I will gather 

04:26:44.547 --> 04:26:49.234
samples, about 50, when I go 
right like this, and turning 

04:26:49.235 --> 04:26:52.940
left is going to be hard.  I 
didn't think it through.  Bear 

04:26:52.941 --> 04:26:55.786
with me, so maybe I will do left
like this, get my head out of 

04:26:55.787 --> 04:27:00.078
the way, a few samples like 
that, and then down will look 

04:27:00.079 --> 04:27:02.932
like this, and hopefully these 
aren't weird gestures in some 

04:27:02.933 --> 04:27:04.933
country.  And something like 
that, okay.

04:27:08.453 --> 04:27:11.099
So I have now picked 50 samples 
of these, I will retrain these 

04:27:11.100 --> 04:27:13.100
in the browser with transfer 
learning.

04:27:16.034 --> 04:27:17.871
So if I look at the rate, I will
start training and I will see --

04:27:17.872 --> 04:27:19.872
it starts going quickly, you can
see the loss 

04:27:23.526 --> 04:27:25.826
started at 4, it went town down 
down down, all to zero, and 

04:27:28.315 --> 04:27:30.315
probably a digit beyond the 6 
digits, 

04:27:32.414 --> 04:27:34.414
never at zero, it is a very low 
loss, now we will give it a try.

04:27:36.111 --> 04:27:37.566
I will stop playing the game, 
move left, you can see the 

04:27:37.567 --> 04:27:39.567
bounding box 

04:27:42.629 --> 04:27:44.629
around it, kind of shows that 
I'm

04:27:48.858 --> 04:27:50.858
up, right, watching the screen.

04:27:52.770 --> 04:27:54.195
And now you can see that I have 
trained it, going right this 

04:27:54.196 --> 04:27:56.196
time.  Right, right, and up.

04:27:58.681 --> 04:28:00.681
And it thinks it is down, up, 
and right.

04:28:05.635 --> 04:28:07.261
And I'm not good at the game. 
&gt;&gt;PAIGE BAILEY: You are using it

04:28:07.262 --> 04:28:10.150
as an excuse to play Pac-Man all
day. 

04:28:13.745 --> 04:28:16.323
&gt;&gt;LAURENCE MORONEY: It is a 
great example of transfer 

04:28:16.324 --> 04:28:18.324
learning, and you can see in 
JavaScript how easy it is to 

04:28:19.571 --> 04:28:21.806
extract the features from mobile
net and then retrain it, and it 

04:28:21.807 --> 04:28:24.678
is moving as I'm talking.

04:28:30.127 --> 04:28:32.127
And so enough fun with Pac-Man

04:28:35.291 --> 04:28:36.512
-- 
&gt;&gt;PAIGE BAILEY: Transfer 

04:28:36.513 --> 04:28:41.413
learning can be used for a 
variety of cases.  So be sure to

04:28:41.414 --> 04:28:47.318
look at the examples listed on 
the website.

04:28:50.362 --> 04:28:53.623
And this is on Twitter, which is
very popular.  Hi, dear ask 

04:28:53.624 --> 04:28:54.636
TensorFlow.  
&gt;&gt;LAURENCE MORONEY: We have to 

04:28:54.637 --> 04:28:57.943
respond to that, they said dear.
&gt;&gt;PAIGE BAILEY: Yes, are you 

04:28:57.944 --> 04:28:59.944
going to publish the updated 
version of 

04:29:03.069 --> 04:29:05.069
TensorFlow, the poet's tutorial 
from Pete Warden on TS2.

04:29:09.258 --> 04:29:11.258
0, TS lite, and a lot of other 
shenanigans. 

04:29:11.942 --> 04:29:13.942
&gt;&gt;LAURENCE MORONEY: And the 
network API. 

04:29:16.253 --> 04:29:18.715
&gt;&gt;PAIGE BAILEY: I love Pete 
Warden's codelab on TensorFlow 

04:29:18.716 --> 04:29:21.140
for poets, and he had a great 
talk today. 

04:29:24.008 --> 04:29:26.008
&gt;&gt;LAURENCE MORONEY: I didn't get
to see it, I was preparing. 

04:29:27.289 --> 04:29:29.289
&gt;&gt;PAIGE BAILEY: It is about 
creating 

04:29:31.970 --> 04:29:34.820
tiny models and putting them on 
computers on mobile and embedded

04:29:34.821 --> 04:29:35.856
devices. 
&gt;&gt;LAURENCE MORONEY: So at some 

04:29:35.857 --> 04:29:39.526
point, we will update it.  I 
don't think there is an updated 

04:29:42.001 --> 04:29:44.249
version available, but one of 
the things that I liked about 

04:29:44.250 --> 04:29:46.250
the TensorFlow for 

04:29:49.354 --> 04:29:50.570
poets codelab is it got me 
building a model quickly for the

04:29:50.571 --> 04:29:52.571
mobile device.  And the draw 
back of that is there are 

04:29:56.263 --> 04:29:58.263
a bunch of scripts that I ran, I
didn't 

04:29:59.525 --> 04:30:01.525
know what I was going on with 
them.

04:30:04.193 --> 04:30:06.193
And I decided a bunch of 
TensorFlow 

04:30:07.869 --> 04:30:10.124
lite examples, image 
classification, object 

04:30:10.125 --> 04:30:12.125
detection, image recognition, 
and speech detection.

04:30:13.616 --> 04:30:15.616
They are all open source, 
Android and 

04:30:16.883 --> 04:30:19.156
iOS, and they have full 
instructions on how to do it 

04:30:19.157 --> 04:30:21.157
yourself.  The image 
classification is fun, and I 

04:30:25.572 --> 04:30:27.572
will try and run that in my, um,
Android 

04:30:28.841 --> 04:30:29.251
Emulator, so we can see it 
running in an emulated 

04:30:29.252 --> 04:30:33.140
environment.  So let me get that
started.  You can see it being 

04:30:33.141 --> 04:30:35.141
cached.

04:30:36.410 --> 04:30:38.479
For example, here, it is doing a
classification of what is in the

04:30:40.527 --> 04:30:43.187
background, if I hold up a water
bottle, this way, it detects it 

04:30:43.188 --> 04:30:45.229
is a water bottle.  This is 
running in the Android Emulator,

04:30:45.230 --> 04:30:50.564
okay? And this is using 
TensorFlow lite, and this is the

04:30:50.565 --> 04:30:51.987
sample on there that does the 
same thing that you would seen 

04:30:51.988 --> 04:30:56.452
in TensorFlow for poets, it is 
using mobile net and building an

04:30:56.453 --> 04:30:58.907
application around mobile net.  
If you look, running in the 

04:30:58.908 --> 04:31:00.908
emulator, 

04:31:02.176 --> 04:31:04.259
I'm getting inference times in 
the 100 and 170 milliseconds. 

04:31:04.260 --> 04:31:07.749
&gt;&gt;PAIGE BAILEY: The ability to 
take large scale models and sort

04:31:07.750 --> 04:31:12.260
of pull them done to a 
manageable side on a mobile, or 

04:31:12.261 --> 04:31:14.080
an embedded device, is huge, and
I'm excited to see what 

04:31:14.081 --> 04:31:16.322
TensorFlow lite does this year. 
&gt;&gt;LAURENCE MORONEY: So we are 

04:31:16.323 --> 04:31:19.988
working on a bunch of tutorials,
both samples are out there, if 

04:31:19.989 --> 04:31:22.428
you look at the GitHub page, you
will see that there are -- 

04:31:27.115 --> 04:31:29.115
there are details on how it is 
built.  Let me go back on here.

04:31:32.636 --> 04:31:34.275
And there are details how it is 
built, how you can put it all 

04:31:34.276 --> 04:31:36.276
together and compile it.

04:31:38.516 --> 04:31:40.516
You don't need to use 

04:31:44.078 --> 04:31:45.926
build TensorFlow to use 
TensorFlow lite, so you can go 

04:31:45.927 --> 04:31:48.365
and start kicking the tires on 
these applications for yourself.

04:31:48.366 --> 04:31:54.060
&gt;&gt;PAIGE BAILEY: Excellent. 
&gt;&gt;LAURENCE MORONEY: All right.  

04:31:54.061 --> 04:31:56.061
We will have more codelabs, I 
would 

04:31:57.322 --> 04:31:59.322
love to get Pete's TensorFlow 
for poets 

04:32:01.966 --> 04:32:04.707
updated, hopefully for I/O. 
&gt;&gt;PAIGE BAILEY: And watch Pete's

04:32:04.708 --> 04:32:07.162
talk.  It was so good, people 
were talking 

04:32:12.457 --> 04:32:14.478
about putting models on 
comadors.  I was nerding out a 

04:32:14.479 --> 04:32:16.134
little bit. 
&gt;&gt;LAURENCE MORONEY: And that's 

04:32:16.135 --> 04:32:20.604
why we're here.  Cool!
So -- do we have time for one 

04:32:20.605 --> 04:32:23.726
more question before break? So 
we are heading to break now, so 

04:32:25.575 --> 04:32:26.628
Paige and I will be back after 
the break with the rest of your 

04:32:26.629 --> 04:32:30.505
questions.  Please keep asking 
them on the live stream and on 

04:32:30.506 --> 04:32:33.580
social media.  Like I said, 
anything that we cannot get to, 

04:32:33.581 --> 04:32:35.415
we will try to get to offline 
and we will reach out back to 

04:32:35.416 --> 04:32:38.496
you. 
&gt;&gt;PAIGE BAILEY: Absolutely.  So 

04:32:38.497 --> 04:32:40.497
super excited to have you here, 

04:32:42.577 --> 04:32:42.781
super excited to be at the 
TensorFlow Developer Summit.  

04:32:42.782 --> 04:32:46.253
&gt;&gt;LAURENCE MORONEY: And we will 
see you back here in a few 

04:32:46.254 --> 04:32:48.254
minutes.  
&gt;&gt;PAIGE BAILEY: Yep.

04:32:48.501 --> 04:32:50.501
[Music].

04:41:01.210 --> 04:41:03.210
PAIGE BAILEY: Welcome back.  I'm
Paige Bailey. 

04:41:09.444 --> 04:41:11.444
LAWRENCE MORONEY: I'm Laurence 
Moroney. 

04:41:12.085 --> 04:41:14.938
PAIGE BAILEY: We're here to 
answer your TensorFlow questions

04:41:14.939 --> 04:41:16.366
with #askTensorFlow. 
LAWRENCE MORONEY: And we will 

04:41:16.367 --> 04:41:19.650
try to get back to you today, if
not, we will reach out to you 

04:41:19.651 --> 04:41:21.651
later to answer them. 

04:41:24.404 --> 04:41:25.814
PAIGE BAILEY: We will go to the 
first question. 

04:41:25.815 --> 04:41:27.815
LAWRENCE MORONEY: This is on 
Twitter, 

04:41:30.356 --> 04:41:32.403
once I installed TensorFlow GPU,
it did not work at first, failed

04:41:32.404 --> 04:41:34.404
to run native TensorFlow run 
time.

04:41:36.688 --> 04:41:38.553
I remember seeing this one, it 
was on YouTube in response to a 

04:41:38.554 --> 04:41:40.554
pip install 

04:41:43.699 --> 04:41:45.699
TensorFlow GPU in codelab, 
because once 

04:41:47.161 --> 04:41:48.581
upon a time, you had to pip 
install TensorFlow GPU to use 

04:41:48.582 --> 04:41:53.489
it, and now you are running into
issues, and the reason is good, 

04:41:53.490 --> 04:41:55.490
it is good news, you don't have 
to run it anymore.

04:41:58.371 --> 04:42:00.828
So if we switched to colab, this
is a notebook that I was showing

04:42:00.829 --> 04:42:02.829
you earlier on.

04:42:04.093 --> 04:42:06.358
If you wanted to use GPU in 
colab, pick the run time type, 

04:42:06.359 --> 04:42:08.396
pick GPU as the hardware 
accelerator and now you don't 

04:42:11.260 --> 04:42:13.260
need to pip install TensorFlow 
GPU, it 

04:42:14.335 --> 04:42:16.369
does it under the hood behind 
the scenes, it is really cool 

04:42:16.370 --> 04:42:18.370
and earlier 

04:42:19.685 --> 04:42:21.724
why I trained this so quickly, I
used the GPU and there is no pip

04:42:21.725 --> 04:42:26.029
install GPU on here. 
PAIGE BAILEY: When we were 

04:42:26.030 --> 04:42:28.680
installing TensorFlow 2.0, we 
had an issue with the GPU 

04:42:28.681 --> 04:42:30.681
install, 

04:42:34.969 --> 04:42:36.969
and you needed specific

04:42:39.174 --> 04:42:41.174
Kuta drivers.

04:42:44.321 --> 04:42:46.321
Kolob is a great tip if you -- 
LAWRENCE MORONEY: 

04:42:46.813 --> 04:42:48.854
&gt;&gt;AND:  GPU stuff, this is what 
I ran into a number of times 

04:42:48.855 --> 04:42:53.578
when using the GPU, you have to 
carefully take a look at the 

04:42:53.579 --> 04:42:55.448
version of Kuda and see the DNN 
that you are using, because I 

04:42:55.449 --> 04:42:59.951
made the mistake that I just 
went to the vendor's website, I 

04:42:59.952 --> 04:43:03.219
downloaded the latest version, I
installed them, and then I saw 

04:43:03.220 --> 04:43:04.235
TensorFlow was actually 
supporting a slightly earlier 

04:43:04.236 --> 04:43:06.236
version.

04:43:07.720 --> 04:43:09.751
So if you do get an error when 
you are using GPU, take a look 

04:43:09.752 --> 04:43:11.752
at the version of the driver it 
is looking to support and 

04:43:15.659 --> 04:43:17.557
then from the VEPD er vendor's 
website, download that version. 

04:43:17.558 --> 04:43:19.026
PAIGE BAILEY: Driver issues, 
they are always a treat. 

04:43:19.027 --> 04:43:21.027
LAWRENCE MORONEY: Exactly.  And 
it is like -- it is one of the 

04:43:22.348 --> 04:43:23.360
things that makes our job 
interesting. 

04:43:23.361 --> 04:43:24.989
PAIGE BAILEY: Absolutely.  
LAWRENCE MORONEY: Haha, all 

04:43:24.990 --> 04:43:26.990
right.  Should we look at the 
next one? 

04:43:30.092 --> 04:43:32.092
PAIGE BAILEY: Oh, let's go.

04:43:36.073 --> 04:43:37.336
Adi Kumar had at least eight 
excellent questions on Twitter. 

04:43:37.337 --> 04:43:40.038
LAWRENCE MORONEY: He wins the 
volume award. 

04:43:40.849 --> 04:43:42.849
PAIGE BAILEY: He absolutely 
does, we 

04:43:44.328 --> 04:43:45.947
will get to all of them, not in 
this Ask TensorFlow segment, but

04:43:45.948 --> 04:43:47.948
we will focus on one for today 
and then answer the rest 

04:43:48.451 --> 04:43:49.464
offline. 
LAWRENCE MORONEY: And I think a 

04:43:49.465 --> 04:43:52.747
lot of them were about file 
formats and how do I use 

04:43:52.748 --> 04:43:55.228
different file formats.  Should 
I drill into that? 

04:43:55.420 --> 04:43:57.420
PAIGE BAILEY: Absolutely.

04:43:58.682 --> 04:44:00.941
The question is, which is the 
preferred format for saving the 

04:44:00.942 --> 04:44:03.399
model going forward, saved 
model, or something else?

04:44:08.359 --> 04:44:11.208
If we look at the laptop, we can
take a gander at one of the 

04:44:11.209 --> 04:44:13.209
slides from the 

04:44:14.619 --> 04:44:16.619
keynote this

04:44:21.207 --> 04:44:25.473
morning, that Keras is a driver 
of 2.0 and this is at the heart 

04:44:21.207 --> 04:44:23.207
of the deployment.

04:44:24.880 --> 04:44:27.319
You can see it for TensorFlow 
serving, lite, and JS, and a lot

04:44:27.320 --> 04:44:29.776
of other language bindings.

04:44:33.078 --> 04:44:34.084
We are pushing for the saved 
model. 

04:44:34.085 --> 04:44:35.932
LAWRENCE MORONEY: And you cannot
go wrong. 

04:44:35.933 --> 04:44:37.762
PAIGE BAILEY: It is easier to 
use than the other deployment 

04:44:37.763 --> 04:44:39.763
options we have seen before. 
LAWRENCE MORONEY: So I think the

04:44:40.828 --> 04:44:42.253
guidance is in the 
recommendation, not just for AD,

04:44:42.254 --> 04:44:44.312
but for everybody else.  When 
you are thinking about saving 

04:44:47.178 --> 04:44:48.598
your models, take a look at save
model, consider using it, and 

04:44:48.599 --> 04:44:50.599
not only is the 

04:44:52.831 --> 04:44:54.613
advantage of the file format, 
but just how it is supported 

04:44:54.614 --> 04:44:56.450
across all of these. 
PAIGE BAILEY: And an excellent 

04:44:56.451 --> 04:44:58.451
point of TensorFlow 2.

04:45:01.783 --> 04:45:03.783
0, I will keep selling it, we 
have a 

04:45:06.669 --> 04:45:08.294
number of code samples and 
tutorials today on saved model. 

04:45:08.295 --> 04:45:10.295
LAWRENCE MORONEY: I was playing 
with the 

04:45:15.846 --> 04:45:17.490
lite stuff, saving as a saved 
model, and the TensorFlow lite 

04:45:17.491 --> 04:45:20.764
process was a lot easier for me.
So it is really refined, we are 

04:45:22.585 --> 04:45:24.007
iterating on that, and I think 
it looks really cool. 

04:45:24.008 --> 04:45:26.450
PAIGE BAILEY: Yep!  Excellent. 
LAWRENCE MORONEY: So thank you 

04:45:26.451 --> 04:45:28.451
for all 

04:45:31.160 --> 04:45:32.673
of those questions, Adi, we will
try to answer the rest of them, 

04:45:32.674 --> 04:45:37.196
most of them were focused around
the file format and hopefully 

04:45:37.197 --> 04:45:39.028
saved model will help you. 
PAIGE BAILEY: Let's go to the 

04:45:39.029 --> 04:45:41.029
next one. 

04:45:42.330 --> 04:45:44.330
LAWRENCE MORONEY: This is from 
Eli 

04:45:45.609 --> 04:45:48.945
Gaguba, is it possible to run 
TensorBoard on codelabs? 

04:45:50.363 --> 04:45:52.363
PAIGE BAILEY: You are going to 
be so delighted. 

04:45:52.629 --> 04:45:54.877
LAWRENCE MORONEY: Because before
if was running on codelabs, we 

04:45:54.878 --> 04:45:57.521
were talking about, we really 
want to run codelabs. 

04:45:58.337 --> 04:46:00.337
PAIGE BAILEY: It is so painful!

04:46:03.206 --> 04:46:05.206
If you wanted to get it working 
in a

04:46:08.829 --> 04:46:10.664
collab notebook, it was not 
really approved by our bosses or

04:46:10.665 --> 04:46:13.128
in general.  But, yes, so the 
good news is that you 

04:46:18.439 --> 04:46:19.871
can run TensorBoard in codelab. 
LAWRENCE MORONEY: And before it 

04:46:19.872 --> 04:46:24.721
was publicly announced, when it 
was announced

04:46:28.020 --> 04:46:30.020
internally, we got an email from
Paige with a lot of smily faces.

04:46:32.097 --> 04:46:34.560
So thank you for your question, 
it made Paige's day. 

04:46:36.608 --> 04:46:38.026
PAIGE BAILEY: And I'm 
downloading files in hopes to 

04:46:38.027 --> 04:46:41.939
play with it a little bit, but 
here you can see it working.  

04:46:41.940 --> 04:46:43.940
You should be able to do 
different 

04:46:46.172 --> 04:46:48.172
operations like smoothing

04:46:52.645 --> 04:46:54.645
, and changing the values and 
using the 

04:46:57.930 --> 04:46:59.930
embedding visualizer from the 
Collab 

04:47:01.056 --> 04:47:03.495
notebook to improve your 
accuracy and model debugging.  

04:47:03.496 --> 04:47:05.496
Another thing that we are 
working 

04:47:06.992 --> 04:47:10.725
very, very hard on is you don't 
have to specify ports.  You 

04:47:10.726 --> 04:47:12.350
don't need to remember if you 
have multiple TensorBoard 

04:47:12.351 --> 04:47:14.351
instances running, 

04:47:15.359 --> 04:47:18.104
that you are using 6006, or 
whatever, for another.

04:47:19.919 --> 04:47:21.768
It automatically selects one 
that's a good candidate, and it 

04:47:21.769 --> 04:47:25.036
would create a good fit for you.
So the team is phenomenal, if 

04:47:25.037 --> 04:47:27.037
you have 

04:47:30.998 --> 04:47:33.990
any interest whatsoever in 
TensorBoard at all, I recommend 

04:47:33.991 --> 04:47:36.228
stalking the POs, like I do.  
That's how I found out that 

04:47:40.907 --> 04:47:45.191
TensorBoard was added to Jupiter
notebooks and also to codelab.  

04:47:45.192 --> 04:47:47.639
So it is so exciting, we will 
have this link in the 

04:47:47.640 --> 04:47:49.459
documentation as well, the 
little notes underneath for you 

04:47:49.460 --> 04:47:51.103
to play it. 
LAWRENCE MORONEY: It is so great

04:47:51.104 --> 04:47:53.561
to have a PR stalker [ laughter 
]. 

04:47:57.874 --> 04:47:59.300
PAIGE BAILEY: I get push 
notifications in my phone, it is

04:47:59.301 --> 04:48:01.553
a problem.  But they are doing 
such great work. 

04:48:03.626 --> 04:48:05.251
LAWRENCE MORONEY: So yes, 
TensorBoard is working in 

04:48:05.252 --> 04:48:07.252
Collab. 

04:48:10.219 --> 04:48:12.663
PAIGE BAILEY: And also project 
Jupyter notebooks!  TensorBoard 

04:48:12.664 --> 04:48:14.664
everywhere. 

04:48:15.690 --> 04:48:18.704
LAWRENCE MORONEY: And we all 
love it.  I have not played with

04:48:18.705 --> 04:48:20.705
it that much, do the plugins 
work? 

04:48:22.998 --> 04:48:25.024
PAIGE BAILEY: So TensorBoard is 
a collection of different 

04:48:25.025 --> 04:48:29.315
visualization, you can see 
scalers, accuracy, histograms, 

04:48:29.316 --> 04:48:31.316
you can see embedding 

04:48:32.366 --> 04:48:34.366
visualizers which allows you to 
do a 

04:48:36.486 --> 04:48:38.486
great Mnist example from 
TensorFlow a 

04:48:41.223 --> 04:48:42.864
couple years ago, it is 
beautiful, and beholder, created

04:48:42.865 --> 04:48:44.865
by a community member.

04:48:46.329 --> 04:48:48.567
And the plugins, a couple of the
Google Summer of Code projects 

04:48:48.568 --> 04:48:52.460
this summer are focused on 
getting additional visualization

04:48:52.461 --> 04:48:55.112
plugins added to TensorBoard.  
And so in addition to the what 

04:48:55.113 --> 04:48:57.113
if tool, and in addition to 
beholder, you 

04:49:00.424 --> 04:49:02.424
can make your own TensorFlow 
plugin. 

04:49:03.627 --> 04:49:05.627
LAWRENCE MORONEY: And what

04:49:06.992 --> 04:49:08.446
while I'm geeking out about it 
all day, I'm sure we need to 

04:49:08.447 --> 04:49:11.343
move to another question.  So 
the next question that came in: 

04:49:11.344 --> 04:49:13.344
How 

04:49:15.900 --> 04:49:17.744
would you use future columns 
with Keras? 

04:49:17.745 --> 04:49:18.948
PAIGE BAILEY: I know you know 
the answer to this question. 

04:49:18.949 --> 04:49:23.460
LAWRENCE MORONEY: And it is a 
passion of yours.  Feature 

04:49:23.461 --> 04:49:25.461
columns are a part of estimators
and are a way of getting your 

04:49:27.201 --> 04:49:29.437
data efficiently into 
estimators, and with people 

04:49:29.438 --> 04:49:31.264
saying, like, hey, it is all 
there in estimates, what about 

04:49:31.265 --> 04:49:33.265
Keras?

04:49:34.585 --> 04:49:36.585
You have been working on great 
stuff around that. 

04:49:38.249 --> 04:49:40.249
PAIGE BAILEY: And I know that 
the 

04:49:43.605 --> 04:49:44.633
YouTube channel has a series 
from Karmel. 

04:49:44.634 --> 04:49:46.318
LAWRENCE MORONEY: Karmel is the 
engineering director for 

04:49:46.319 --> 04:49:48.319
high-level 

04:49:51.830 --> 04:49:54.074
APIs, she has a great series 
around high level APIs for 

04:49:54.075 --> 04:49:56.095
TensorFlow, and is teaching 
about that and her and her team 

04:50:00.418 --> 04:50:04.112
are working on parity for 
columns on TensorFlow 2.  I have

04:50:04.113 --> 04:50:06.153
not checked the alpha yet, but 
it is on the way, if it is not 

04:50:06.154 --> 04:50:08.649
there already.  So you should be
able to use them in Keras. 

04:50:09.460 --> 04:50:11.460
PAIGE BAILEY: Yes, we are in the
process 

04:50:13.186 --> 04:50:16.062
of building out, if you wanted 
to migrate your model, using 

04:50:16.063 --> 04:50:18.063
estimators to be more of a 
TensorFlow 2.

04:50:20.762 --> 04:50:24.035
0 with Keras, I'm in the process
of building a migration guide.  

04:50:24.036 --> 04:50:25.870
If you have any interest around 
that, please feel free to reach 

04:50:25.871 --> 04:50:28.747
out and we're excited to get 
that released pretty soon. 

04:50:29.563 --> 04:50:31.228
LAWRENCE MORONEY: I'm excited to
see that, personally, because 

04:50:31.229 --> 04:50:33.229
the very first 

04:50:34.924 --> 04:50:36.363
stuff I did in TensorFlow was 
with estimateers before I 

04:50:36.364 --> 04:50:39.618
learned Keras, and I wanted to 
change them to use Keras without

04:50:39.619 --> 04:50:42.268
re-writing them. 
PAIGE BAILEY: Absolutely, Keras 

04:50:42.269 --> 04:50:44.269
is just friendlier to work with,
I feel. 

04:50:46.982 --> 04:50:49.659
LAWRENCE MORONEY: They are both 
greatest estimators, and Keras 

04:50:49.660 --> 04:50:52.696
is great for beginners, so 
hopefully we will get the best 

04:50:52.697 --> 04:50:54.697
of both worlds.

04:50:57.006 --> 04:50:59.678
So next question, Jeff Thomas, 
looking for data sets for 

04:50:59.679 --> 04:51:01.679
testing and comparing different 
training methods.

04:51:05.664 --> 04:51:07.664
Looking for new data sets, like 
MNIST 

04:51:08.948 --> 04:51:11.594
is great, but after a while, 
people want something new.  So 

04:51:11.595 --> 04:51:13.005
what could you tell Jeff? 
PAIGE BAILEY: We can tell him 

04:51:13.006 --> 04:51:15.081
about TensorFlow data sets!  

04:51:18.772 --> 04:51:20.772
LAWRENCE MORONEY: And there's a 
blog post about it. 

04:51:21.013 --> 04:51:23.707
PAIGE BAILEY: It is about 
creating the data ingestion 

04:51:23.708 --> 04:51:25.708
pipelines for you to be able to 
easily use a variety of data 

04:51:27.981 --> 04:51:30.231
sets with all of your deep 
learning and machine learning 

04:51:30.232 --> 04:51:32.232
models with just a few lines of 
code.

04:51:34.079 --> 04:51:36.079
So if you are familiar with 
skikit 

04:51:37.147 --> 04:51:38.775
learn, and all of this, it is 
nifty data ingestion practices, 

04:51:38.776 --> 04:51:41.207
this feels very similar.  It is 
easy to do, training and 

04:51:42.639 --> 04:51:44.639
testing, and verifications.

04:51:47.804 --> 04:51:50.032
And we have an LOT of data sets 
readily available for you right 

04:51:50.033 --> 04:51:52.033
now to go ahead and explore.  
And another thing that I would 

04:51:54.173 --> 04:51:57.048
especially like to call out is, 
um, a member of our community --

04:51:57.049 --> 04:52:01.158
so anybody can make your data 
famous, right? If you have a 

04:52:01.159 --> 04:52:03.188
data set that you are using on 
your research lab, if you have, 

04:52:08.125 --> 04:52:10.148
like, a bright and shiny CSV, 
that you think would be a cool 

04:52:10.149 --> 04:52:13.012
add to this section. 
LAWRENCE MORONEY: I have never 

04:52:13.013 --> 04:52:15.013
used a 

04:52:17.243 --> 04:52:19.559
CSV called bright and shiny. 
PAIGE BAILEY: Everybody uses 

04:52:19.560 --> 04:52:21.560
them, a 

04:52:24.926 --> 04:52:27.797
community manager, Andrew, an 
undergraduate researcher at 

04:52:27.798 --> 04:52:29.798
Stanford, 

04:52:31.889 --> 04:52:32.703
added 200,000 of these to the 
Stanford ML group, in less than 

04:52:32.704 --> 04:52:34.704
a day.

04:52:35.748 --> 04:52:37.579
It is as simple as take the 
template format for images or 

04:52:37.580 --> 04:52:39.580
for audio or 

04:52:41.678 --> 04:52:44.561
whatever you are using, add some
additional metadata for the data

04:52:44.562 --> 04:52:49.081
set, change the types for the 
features you are using with 

04:52:49.082 --> 04:52:50.331
that, and voila. 
LAWRENCE MORONEY: You are off to

04:52:50.332 --> 04:52:51.991
the races. 
PAIGE BAILEY: And you can make 

04:52:51.992 --> 04:52:53.831
your data famous. 
LAWRENCE MORONEY: A wonderful 

04:52:53.832 --> 04:52:55.832
thing about it, if you are 
getting started or 

04:53:00.039 --> 04:53:02.496
if you are learning, if you look
at the samples, pre-TensorFlow 

04:53:02.497 --> 04:53:06.768
data sets, you get a lot of code
about downloading the data, 

04:53:06.769 --> 04:53:09.640
unzip it here, relabel it like 
this, put the files in these 

04:53:09.641 --> 04:53:14.965
folders, take a CSV and make 
these features, but it is one or

04:53:14.966 --> 04:53:17.414
two lines of code and the 
training is set into data and 

04:53:17.415 --> 04:53:19.415
test sets for you, that type of 
thing.

04:53:21.929 --> 04:53:23.370
So for learners, I found it 
exciting because I didn't have 

04:53:23.371 --> 04:53:25.371
to go through 100 lines of code 
before I get to the neural 

04:53:25.609 --> 04:53:27.433
network. 
PAIGE BAILEY: And also data 

04:53:27.434 --> 04:53:29.434
sciency 

04:53:31.335 --> 04:53:33.805
people, creating and training a 
model is understanding the 

04:53:33.806 --> 04:53:34.818
shape, format, and the 
statistical distributions, and 

04:53:34.819 --> 04:53:37.267
this is really helpful. 
LAWRENCE MORONEY: Yep.  So 

04:53:37.268 --> 04:53:38.719
that's all we have time for.  
We're going to take a break, we 

04:53:38.720 --> 04:53:41.586
will be taking some more of your
questions during the break.

04:53:43.219 --> 04:53:45.075
And then we will be back in just
a few minutes to start answering

04:53:45.076 --> 04:53:47.950
them.  Thanks very much.
[Music].

04:58:53.349 --> 04:58:54.996
. 
LAWRENCE MORONEY: Welcome back 

04:58:54.997 --> 04:58:59.698
everybody to the live stream 
where Paige and I are answering 

04:58:59.699 --> 04:59:01.699
questions that you posted on 

04:59:03.614 --> 04:59:06.312
social media, on the live chat, 
Stack Overflow, and anywhere 

04:59:06.313 --> 04:59:08.313
else you can find them.

04:59:11.207 --> 04:59:13.207
If you have questions, please 
hashtag #AskTensorFlow. 

04:59:14.139 --> 04:59:16.412
PAIGE BAILEY: And if we do not 
answer your questions today, we 

04:59:16.413 --> 04:59:19.090
will in further sessions, so 
keep asking, even if we 

04:59:25.225 --> 04:59:27.225
don't necessarily get to them. 

04:59:28.966 --> 04:59:31.206
LAWRENCE MORONEY: There are so 
many that we cannot get to them 

04:59:31.207 --> 04:59:33.259
there today, a great problem to 
have.  We have a show, Ask 

04:59:33.260 --> 04:59:35.314
TensorFlow.  So we will put 
these questions into 

04:59:39.182 --> 04:59:41.182
the show, so please asking and 
this is 

04:59:42.867 --> 04:59:44.867
what keeps the community 
engaged.

04:59:50.509 --> 04:59:52.728
So the first question, the 
pre-built binary for GPU on 16? 

04:59:52.729 --> 04:59:54.729
PAIGE BAILEY: That is very 
specific. 

04:59:54.805 --> 04:59:56.805
LAWRENCE MORONEY: I like 
specific 

04:59:58.320 --> 04:59:59.748
questions, even if I can't 
answer them.  So the pre-built 

04:59:59.749 --> 05:00:01.749
binaries for 

05:00:03.419 --> 05:00:05.419
TensorFlow tend to be associated
with a 

05:00:08.102 --> 05:00:10.102
specific driver for

05:00:11.443 --> 05:00:16.171
NVIDIA.  So if you are looking 
at any of the pre-built 

05:00:16.172 --> 05:00:18.172
binaries, look at what version 

05:00:20.077 --> 05:00:22.077
of the driver you have 
supported.

05:00:24.553 --> 05:00:26.594
I'm not an expert on NVIDIA 
cards, and if you go over here, 

05:00:26.595 --> 05:00:29.246
like on my laptop, I have -- I 
have called up, like, some 

05:00:32.558 --> 05:00:34.558
of the what NVIDIAs say about 
the 

05:00:36.058 --> 05:00:38.058
TensorFlow system requirements, 
and the 

05:00:39.745 --> 05:00:41.745
specific versions of the drivers
they support.

05:00:43.428 --> 05:00:45.886
And one gotcha that I found in 
working with GPUs is it is 

05:00:45.887 --> 05:00:47.887
easier to go to the driver 
vendor and download the latest 

05:00:48.555 --> 05:00:50.398
version, but that is not one 
that TensorFlow is built for 

05:00:50.399 --> 05:00:52.399
that it supports.

05:00:54.118 --> 05:00:56.591
So make sure that they actually 
match each other, you should be 

05:00:56.592 --> 05:00:59.440
good to go even with that 
particular card. 

05:01:02.274 --> 05:01:04.571
PAIGE BAILEY: If you have warm 
feelings and excitement for 

05:01:04.572 --> 05:01:06.572
builds for 

05:01:09.473 --> 05:01:12.340
TensorFlow, we have an interest 
group, Sync build.  So please go

05:01:12.341 --> 05:01:14.341
to the community section 

05:01:15.826 --> 05:01:18.535
of our GitHub and check up the 
listserv and joining it in and 

05:01:18.536 --> 05:01:22.411
joining our weekly stand-up. 
LAWRENCE MORONEY: Right.  So 

05:01:22.412 --> 05:01:24.412
thanks, Arthur, for that 
question.

05:01:25.476 --> 05:01:28.181
And then the next question is a 
really funny one.  How many 

05:01:28.182 --> 05:01:30.182
times have you been asked this 
today? 

05:01:30.450 --> 05:01:33.099
PAIGE BAILEY: At least 12.  At 
least, and then the other flavor

05:01:33.100 --> 05:01:35.100
of 

05:01:36.197 --> 05:01:37.837
it is, is this symbol that I use
all the time, is this supported 

05:01:37.838 --> 05:01:39.880
in TensorFlow 2.0, and if not, 
what has changed? 

05:01:40.073 --> 05:01:42.073
LAWRENCE MORONEY: Yep.

05:01:44.181 --> 05:01:46.215
And I -- I mean, people have 
invested so much time in 1.

05:01:49.285 --> 05:01:51.285
x, they don't want it to be 
deprecated or go away.

05:01:56.024 --> 05:01:57.446
So do scripts for 1 work with 2?
PAIGE BAILEY: The sad fact is 

05:01:57.447 --> 05:02:01.170
probably not.  They would not 
work with TensorFlow 2.0 out of 

05:02:01.171 --> 05:02:03.171
the box.

05:02:07.650 --> 05:02:10.574
But, we have created an upgrade 
utility to use, it is 

05:02:10.575 --> 05:02:14.038
automatically downloaded with 
TensorFlow 2.0 when you download

05:02:14.039 --> 05:02:16.134
it, and you can check out what 
it is doing in this 

05:02:19.814 --> 05:02:22.499
Medium blog post that I and my 
colleague, Ana, created, as well

05:02:22.500 --> 05:02:26.152
as this TensorFlow 2.0 video, 
and the gifs, the best 

05:02:28.421 --> 05:02:31.275
communication medium possible, 
shows you how you can use the 

05:02:31.276 --> 05:02:33.276
upgrade script on an N file.

05:02:34.949 --> 05:02:36.949
So any, sort of, arbitrary 
Python file 

05:02:39.221 --> 05:02:40.868
or Jupyter notebooks, a machine 
learning IDE extension that 

05:02:40.869 --> 05:02:42.869
allows you to do that as well.

05:02:45.184 --> 05:02:47.184
And it will give you an export.

05:02:49.828 --> 05:02:51.828
text file that shows you the 
symbol

05:02:54.943 --> 05:02:57.041
renames and the manual changes 
if you have to add them.  And 

05:02:57.042 --> 05:02:59.135
usually you do not.  So to see 
it in action, you can take a 

05:03:01.385 --> 05:03:03.385
look at this particular text 
generation 

05:03:05.691 --> 05:03:07.691
example that we have running a 

05:03:09.360 --> 05:03:11.792
Shakespeare, well, take all of 
the corpus of Shakespeare text, 

05:03:11.793 --> 05:03:13.793
trains it 

05:03:15.463 --> 05:03:17.497
against the Shakespeare text, 
and generates something that he 

05:03:17.498 --> 05:03:20.401
could have potentially written, 
should he have had 

05:03:23.506 --> 05:03:24.962
access to deep learning 
resources. 

05:03:24.963 --> 05:03:26.963
LAWRENCE MORONEY: I know you 
all, and I 

05:03:28.989 --> 05:03:31.325
will up hold the wildest humor 
of your idle dream. 

05:03:33.589 --> 05:03:35.589
PAIGE BAILEY: I didn't know you 
knew Shakespeare. 

05:03:38.687 --> 05:03:40.687
LAWRENCE MORONEY: I played Henry
the IV. 

05:03:42.190 --> 05:03:44.020
PAIGE BAILEY: I was in much ado 
about nothing. 

05:03:44.021 --> 05:03:46.021
LAWRENCE MORONEY: So while we 
are on 

05:03:48.735 --> 05:03:50.735
much ado about nothing, the 
Jupyter 

05:03:51.739 --> 05:03:54.464
notebook. 
PAIGE BAILEY: Export the Python 

05:03:54.465 --> 05:03:57.923
file, and to reupgrade it. 
LAWRENCE MORONEY: You have to 

05:03:57.924 --> 05:03:59.924
reconnect 

05:04:01.148 --> 05:04:04.686
the run. 
PAIGE BAILEY: So starting it.  

05:04:04.687 --> 05:04:06.687
And the requirements, we can 
check 

05:04:07.943 --> 05:04:09.584
that we are using TensorFlow 
alpha, and like I mentioned 

05:04:09.585 --> 05:04:11.585
before, all you have to 

05:04:13.238 --> 05:04:15.488
do is preface this was a bang, 
the name of the Python file is 

05:04:15.489 --> 05:04:17.724
text generation, and I wanted to
create an upgrade, shift 

05:04:21.412 --> 05:04:24.282
enter, and it does its upgrading
magic and, very quickly, it 

05:04:24.283 --> 05:04:26.307
tells me all of the things that 
we need to change in order to 

05:04:26.308 --> 05:04:28.308
make it 2.

05:04:31.615 --> 05:04:35.710
0 compatible and create that 
file for me off to the side.  So

05:04:35.711 --> 05:04:38.577
if I wanted to run this model, 
it should be able to -- it 

05:04:38.578 --> 05:04:41.226
should be able to train as it 
would.

05:04:42.847 --> 05:04:46.109
So let's just check to make sure
that that would be the case.  

05:04:46.110 --> 05:04:49.791
LAWRENCE MORONEY: I think a lot 
of the errors you are seeing 

05:04:49.792 --> 05:04:54.072
here, it is renamed APIs rather 
than breaking changes within the

05:04:54.073 --> 05:04:56.073
APIs as well. 
PAIGE BAILEY: This is true.

05:05:00.599 --> 05:05:02.449
You have re-names and additional
key words.  So for more 

05:05:02.450 --> 05:05:04.450
information about that, 

05:05:06.073 --> 05:05:08.073
and to see it running

05:05:10.310 --> 05:05:12.553
in a video, you can go and take 
a look there. 

05:05:13.561 --> 05:05:15.561
LAWRENCE MORONEY: Sounds good, 
and you 

05:05:16.887 --> 05:05:18.887
have some gifs in there? 
PAIGE BAILEY: Absolutely. 

05:05:22.205 --> 05:05:25.453
LAWRENCE MORONEY: And any jifes 
for those that don't say gifs? 

05:05:25.454 --> 05:05:27.454
PAIGE BAILEY: It is in the 
works. 

05:05:27.916 --> 05:05:29.745
LAWRENCE MORONEY: [ Laughter ], 
so when it comes to upgrade, 

05:05:29.746 --> 05:05:31.791
there's a few little gotchas, so
hopefully the blog 

05:05:35.894 --> 05:05:37.894
post, the videos, will help you 
get around them. 

05:05:38.531 --> 05:05:41.176
PAIGE BAILEY: And the community 
you are mentioning before, we 

05:05:41.177 --> 05:05:43.418
had such an interest in testing 
TensorFlow 2.0 and trying it out

05:05:43.419 --> 05:05:47.913
against historic models that 
we've formed a weekly testing 

05:05:47.914 --> 05:05:49.914
stand-up, and also we have a 

05:05:51.798 --> 05:05:53.422
migration support hour that is 
being implemented with the 

05:05:53.423 --> 05:05:55.423
internal support hour.

05:05:57.590 --> 05:05:59.590
So if you have an external group
that 

05:06:00.861 --> 05:06:02.490
is interested in upgrading your 
model, please join the testing 

05:06:02.491 --> 05:06:04.491
group and we can get you 
situated. 

05:06:04.536 --> 05:06:06.536
LAWRENCE MORONEY: And a lot of 
the stuff 

05:06:07.789 --> 05:06:10.439
that we have seen in Keras, you 
have the great slide where she 

05:06:10.440 --> 05:06:11.655
is training -- 
PAIGE BAILEY: The code is 

05:06:11.656 --> 05:06:12.677
exactly the same. 
LAWRENCE MORONEY: Exactly the 

05:06:12.678 --> 05:06:16.973
same.  So while there is stuff 
changing under the hood, a lot 

05:06:16.974 --> 05:06:20.075
of the surface-level code that 
you are writing in Keras is not 

05:06:20.076 --> 05:06:21.893
changing. 
PAIGE BAILEY: If you are used 

05:06:21.894 --> 05:06:24.130
Keras, you are probably not 
going to have any problems. 

05:06:24.750 --> 05:06:26.384
LAWRENCE MORONEY: So good stuff.
Should we move on to the next 

05:06:26.385 --> 05:06:31.288
question? So I know we can talk 
about 2.0 all day, but we just 

05:06:31.289 --> 05:06:33.289
mentioned Keras, and it appears.

05:06:37.090 --> 05:06:39.329
PAIGE BAILEY: So I guess I can 
ask you this question.  What is 

05:06:39.330 --> 05:06:41.330
the purpose of keeping 

05:06:42.394 --> 05:06:44.438
estimators in Keras as separate 
APIs, is there something native 

05:06:44.439 --> 05:06:47.913
to Keras models that allows for 
distributed training, or train 

05:06:47.914 --> 05:06:49.340
and evaluate? 
LAWRENCE MORONEY: The purpose of

05:06:49.341 --> 05:06:53.398
keeping them, there are many 
purposes.  So for me, the main 

05:06:53.399 --> 05:06:56.664
purpose that I would like to 
think of is, because a lot of 

05:06:56.665 --> 05:06:58.514
people are using them. 
PAIGE BAILEY: Including internal

05:06:58.515 --> 05:07:00.515
Google 

05:07:02.378 --> 05:07:04.028
teams that would tar and feather
us if we removed them [ laughter

05:07:04.029 --> 05:07:05.040
]. 
LAWRENCE MORONEY: And so, when 

05:07:05.041 --> 05:07:08.546
it comes to estimators, 
estimators are really great for 

05:07:08.547 --> 05:07:10.547
large-scale training. 
PAIGE BAILEY: Yes. 

05:07:12.004 --> 05:07:13.828
LAWRENCE MORONEY: And it is like
a lot of times, you are doing 

05:07:13.829 --> 05:07:16.066
the large scale training, keep 
going with estimators, they are 

05:07:16.067 --> 05:07:19.943
great.  When I first started 
with TensorFlow, I started with 

05:07:19.944 --> 05:07:21.944
estimators, I could not figure 
out what a node was in a neural 

05:07:23.408 --> 05:07:25.644
network and all of these 
concepts that I had to learn, 

05:07:25.645 --> 05:07:27.645
where I had a simple estimator 
that I can use to do a DNN or 

05:07:27.914 --> 05:07:29.914
something like that.  And so, 
you know, they are there for a 

05:07:31.394 --> 05:07:33.244
reason and they are the same for
the reason.  Keras is one of the

05:07:33.245 --> 05:07:37.127
things that, from the point of 
view of making life easier for 

05:07:37.128 --> 05:07:39.128
developers that we have been 

05:07:41.022 --> 05:07:43.068
doubling down on TensorFlow 2, 
and the code is the same between

05:07:43.069 --> 05:07:48.379
1 and 2, and if -- the layers 
API makes it super simple for 

05:07:48.380 --> 05:07:49.601
you to design a neural network, 
and the fact that you can go low

05:07:49.602 --> 05:07:51.602
level 

05:07:53.893 --> 05:07:55.893
beyond that, you know, to define
your 

05:07:57.349 --> 05:08:00.214
own layers allows you to drive 
stick rather than automatic. 

05:08:00.215 --> 05:08:04.107
PAIGE BAILEY: And what is 
beautiful about Keras 2.0 is you

05:08:04.108 --> 05:08:06.400
have Keras the way you are used 
to using it, and if you want 

05:08:08.650 --> 05:08:10.275
customizations, there's a 
sub-component, if you wanted to 

05:08:10.276 --> 05:08:12.276
go lower, we have something 
called TS module and we have 

05:08:15.974 --> 05:08:17.029
even exposed some of the basic, 
most core ops in TensorFlow as 

05:08:17.030 --> 05:08:20.717
well.  So, at any sort of level 
you want to interact with the 

05:08:20.718 --> 05:08:21.981
API, you can. 
LAWRENCE MORONEY: Yep, and I 

05:08:21.982 --> 05:08:24.842
think there is another part of 
the question that is around 

05:08:24.843 --> 05:08:29.372
distributed training.  So you 
can scroll down there, and there

05:08:29.373 --> 05:08:30.407
is something called distributed 
strategy. 

05:08:30.408 --> 05:08:31.433
PAIGE BAILEY: Yep. 
LAWRENCE MORONEY: With Keras and

05:08:31.434 --> 05:08:35.506
with TensorFlow 2, and the whole
idea behind that is you can 

05:08:35.507 --> 05:08:39.809
distribute your training, maybe 
across multiple GPUs on the same

05:08:39.810 --> 05:08:42.068
machine, maybe across multiple 
GPUs on different machines, and 

05:08:42.069 --> 05:08:44.069
maybe 

05:08:45.319 --> 05:08:47.379
across CPUs all over the place. 
So distribution strategy is a 

05:08:47.380 --> 05:08:51.061
part of that, to help you with 
that.  So estimators in Keras, 

05:08:51.062 --> 05:08:54.947
we love them both, they are 
still there, and hopefully this 

05:08:54.948 --> 05:08:57.816
is something that will help you 
with a question.  I think we 

05:08:57.817 --> 05:08:59.653
have time for just one more. 
PAIGE BAILEY: Absolutely. 

05:08:59.654 --> 05:09:01.112
LAWRENCE MORONEY: Oh, this is a 
Paige question. 

05:09:01.113 --> 05:09:04.025
PAIGE BAILEY: This is totally a 
me question.  I'm the Python 

05:09:04.026 --> 05:09:06.026
person.  So ask TensorFlow, when
will 

05:09:09.313 --> 05:09:11.313
TensorFlow be supported in 
Python 3.

05:09:14.005 --> 05:09:18.503
7 and be accessed in Anaconda3? 
I can answer Python 3.7, and I 

05:09:18.504 --> 05:09:20.538
would love to speak a little bit
more about support for Python 

05:09:20.539 --> 05:09:21.759
going forward. 
LAWRENCE MORONEY: Please, 

05:09:21.760 --> 05:09:23.401
please. 
PAIGE BAILEY: So, to answer the 

05:09:23.402 --> 05:09:26.473
3.7 question, I'm going to 
bounce over to the TensorFlow 2.

05:09:28.760 --> 05:09:31.689
0 project tracker, and these are
all of the standing issues that 

05:09:31.690 --> 05:09:34.736
we have when doing development 
for TensorFlow 2.0, it is 

05:09:34.737 --> 05:09:36.594
transparent -- 
LAWRENCE MORONEY: You can see 

05:09:36.595 --> 05:09:38.595
your avatar. 

05:09:40.274 --> 05:09:41.710
PAIGE BAILEY: I have filed many 
issues and they are transparent 

05:09:41.711 --> 05:09:46.068
to the public.  If you want a 
context on where we stand 

05:09:46.069 --> 05:09:48.069
currently and what we have yet 
to 

05:09:49.406 --> 05:09:51.857
do, this project tracker is a 
great way to understand that.  

05:09:51.858 --> 05:09:54.095
And please take a look at 3.7 
and, there we go.

05:09:58.218 --> 05:10:00.857
In process of releasing binaries
for Python 3.5 and 3.

05:10:05.233 --> 05:10:09.320
7, we issued 25, 420, it is 
going off the screen, and 4.29. 

05:10:09.321 --> 05:10:11.321
You can take a look at that 
issue and 

05:10:13.004 --> 05:10:15.097
it is currently in progress, 
there is not really an eta, but 

05:10:15.098 --> 05:10:18.180
something that we want complete 
by the time that the Alpha RC is

05:10:18.181 --> 05:10:22.439
released.  So that is wonderful 
to see.  And there is also, um, 

05:10:22.440 --> 05:10:24.440
a website 

05:10:25.516 --> 05:10:28.285
called Python three statements, 
I think it is Python3statements.

05:10:30.125 --> 05:10:34.188
com, maybe it is .org.  There we
go!  

05:10:35.208 --> 05:10:38.285
LAWRENCE MORONEY: Yay. 
PAIGE BAILEY: Cool.  TensorFlow 

05:10:38.286 --> 05:10:40.286
made the commitment as of 

05:10:41.379 --> 05:10:43.419
January 1, 2020, we no longer 
support Python 2 and we have 

05:10:43.420 --> 05:10:45.420
done that with a plethora of our
Python community.

05:10:49.544 --> 05:10:51.544
So TensorFlow, Scikit-learn, 
etc.

05:10:52.799 --> 05:10:54.799
, we are committed to Python 3 
and Python 3 support.

05:10:56.705 --> 05:10:58.741
So you will -- you will be 
getting your Python 3 support 

05:10:58.742 --> 05:11:00.742
and we are firmly committed to 
having that. 

05:11:01.189 --> 05:11:03.189
LAWRENCE MORONEY: And a nice 
thing about 

05:11:04.262 --> 05:11:06.262
the issue tracker is it is not a
big, 

05:11:08.612 --> 05:11:10.649
hey, we have it, coming to a 
random point in the future, it 

05:11:10.650 --> 05:11:12.650
is transparent and you can see 
what we are doing. 

05:11:13.298 --> 05:11:15.298
PAIGE BAILEY: And you can see 
our people 

05:11:16.785 --> 05:11:18.828
commenting and the engineers 
commenting back, yeah, man, I 

05:11:18.829 --> 05:11:20.829
was running it last 

05:11:22.954 --> 05:11:23.785
night, one more test and it is 
it. 

05:11:23.786 --> 05:11:26.023
LAWRENCE MORONEY: That is all we
have time for, and remember, 

05:11:26.024 --> 05:11:28.024
#AskTensorFlow 

05:11:31.326 --> 05:11:33.180
on Twitter, Stack Overflow, 
YouTube. 

05:11:33.181 --> 05:11:35.431
PAIGE BAILEY: Send a carrier 
pigeon if that's your preferred 

05:11:35.432 --> 05:11:37.432
method of communication. 

05:11:39.346 --> 05:11:40.967
LAWRENCE MORONEY: I prefer owls.
PAIGE BAILEY: Owls are awesome. 

05:11:40.968 --> 05:11:45.069
LAWRENCE MORONEY: If you have 
questions, we will do the best 

05:11:45.070 --> 05:11:50.352
to answer them, we will put a 
lot on YouTube.com/TensorFlow.  

05:11:50.353 --> 05:11:52.636
So don't forget to subscribe, 
thank you so much for being 

05:11:52.637 --> 05:11:54.637
engaged. 
PAIGE BAILEY: Thank you.

05:12:01.201 --> 05:12:03.201
[Music].

05:35:56.941 --> 05:35:59.812
Often I hear sometimes from 
researchers, 

05:36:02.901 --> 05:36:05.148
I really only do research and 
care about training the machine 

05:36:05.149 --> 05:36:07.149
learning model and I don't mean 
these instruments. 

05:36:09.074 --> 05:36:10.968
What I would argue is research 
often leads to production and 

05:36:10.969 --> 05:36:12.969
where we want to 

05:36:14.643 --> 05:36:16.270
avoid is researchers having to 
reimplement their hard work in 

05:36:16.271 --> 05:36:18.271
the model 

05:36:20.347 --> 05:36:21.588
they have work when they want to
put the model into the 

05:36:21.589 --> 05:36:23.589
production.

05:36:25.031 --> 05:36:27.031
We really want the research 
community 

05:36:28.320 --> 05:36:30.320
to build the models and the 
framework in the community.

05:36:33.233 --> 05:36:35.233
A second comment that I hear 
often is 

05:36:38.127 --> 05:36:40.571
well, I only -- and all are 
built to scale up. I don't 

05:36:40.572 --> 05:36:42.397
really need all of these heavy 
tools. What we have seen time 

05:36:42.398 --> 05:36:44.398
and time again 

05:36:45.882 --> 05:36:46.898
at Google, is small data today 
becomes large data tomorrow. 

05:36:46.899 --> 05:36:48.899
There is really no reason why 
you would 

05:36:52.450 --> 05:36:53.871
have to reimplement your entire 
stack because just your dataset 

05:36:53.872 --> 05:36:58.201
grew. We want to make sure you 
can use the same tools early on 

05:36:58.202 --> 05:37:03.289
in your journey so the tools can
grow with you and your product 

05:37:03.290 --> 05:37:05.523
with the data so that you can 
scale the exact same code to 

05:37:05.524 --> 05:37:07.524
hundreds of machines.

05:37:11.678 --> 05:37:13.678
We are built TensorFlow Extended
and it 

05:37:16.006 --> 05:37:18.006
has had a profound impact in how
we do 

05:37:19.258 --> 05:37:21.258
production and becoming an AI 
first company.

05:37:22.573 --> 05:37:24.573
TFX powers some of the most 
important alphabet companies.

05:37:26.908 --> 05:37:28.131
It is used at six different 
alphabet companies and within 

05:37:28.132 --> 05:37:30.164
Google it is used with all of 
the major products and all 

05:37:33.863 --> 05:37:38.340
of the products that don't have 
billions of users. I said before

05:37:38.341 --> 05:37:42.650
we want to make TFX available to
all of you because we have seen 

05:37:42.651 --> 05:37:44.286
the profound impact it has had 
on our business and we are 

05:37:44.287 --> 05:37:46.780
really excited to see what you 
can do with the same tools in 

05:37:46.781 --> 05:37:51.080
your companies. A year ago, we 
talked about the libraries that 

05:37:51.081 --> 05:37:53.081
we had open sourced at that 
point in time. 

05:37:54.339 --> 05:37:56.339
We talked about TensorFlow 
transform, 

05:37:57.666 --> 05:38:00.121
the training libraries, 
estimators, and keras, and 

05:38:00.122 --> 05:38:03.992
TensorFlow Serving. I made the 
point that back then, as today, 

05:38:03.993 --> 05:38:06.503
all of these are just libraries.
They are low-level libraries you

05:38:06.504 --> 05:38:09.177
have to use independently and 
stitch together 

05:38:14.524 --> 05:38:16.524
to make use and trend for your 
own 

05:38:17.541 --> 05:38:19.541
use-casesuse-cases.

05:38:23.376 --> 05:38:26.455
We added TensorFlow validation 
later that year. It was valuable

05:38:26.456 --> 05:38:28.456
to release these 

05:38:30.615 --> 05:38:32.615
libraries because some of our 
most 

05:38:34.779 --> 05:38:38.910
important partners externally 
have had a huge impack. Airbnb 

05:38:38.911 --> 05:38:41.607
uses TensorFlow server and our 
friends at Twitter published a 

05:38:43.666 --> 05:38:45.666
fascinating blog post of how 
they use 

05:38:48.626 --> 05:38:50.626
TensorFlow to rank tweets on 
their home 

05:38:52.138 --> 05:38:54.138
timeline and used it to analyze 
data and 

05:38:55.242 --> 05:38:56.748
used TensorFlow Hub to share the
word embeddings they used for 

05:38:56.749 --> 05:38:58.749
these models.

05:39:01.852 --> 05:39:03.852
Coming back to this picture, for
those 

05:39:06.160 --> 05:39:07.173
who saw my talk last year, I 
promised everyone there would be

05:39:07.174 --> 05:39:09.174
more. 

05:39:10.635 --> 05:39:12.465
This is just a set of libraries 
right now. Today for the very 

05:39:12.466 --> 05:39:14.500
first time, we are sharing the 
horizontal layers that 

05:39:18.022 --> 05:39:20.022
integrate all of these libraries
into 

05:39:23.520 --> 05:39:24.537
one end platform into one end 
program called TensorFlow 

05:39:24.538 --> 05:39:26.538
Extended.

05:39:29.640 --> 05:39:31.640
First, we have to build 
components.

05:39:34.563 --> 05:39:38.986
At the top in orange are 
libraries from the past and in 

05:39:38.987 --> 05:39:40.987
blue is current libraries.

05:39:43.264 --> 05:39:45.264
Libraries are low level and 
flexible.

05:39:46.598 --> 05:39:48.445
We can build many components 
part of the machine learning 

05:39:48.446 --> 05:39:50.446
pipeline.

05:39:51.521 --> 05:39:53.521
I will go into detail in each 
one of 

05:39:55.227 --> 05:39:56.878
these components later. A 
component is no longer just a 

05:39:56.879 --> 05:39:58.879
library.

05:40:00.612 --> 05:40:02.034
It is a package binary or 
container that can be run as 

05:40:02.035 --> 05:40:04.035
part of a pipeline.

05:40:05.311 --> 05:40:07.364
It is well defined input and 
outputs. In the case of model 

05:40:07.365 --> 05:40:09.365
validation, it is 

05:40:10.826 --> 05:40:12.826
the last validated model.

05:40:15.143 --> 05:40:17.416
A new candidate model and 
validation. It has a well 

05:40:17.417 --> 05:40:19.864
defined configuration and most 
importantly it is one 

05:40:19.865 --> 05:40:22.114
configuration model for the 
entire pipeline.

05:40:25.439 --> 05:40:28.325
You can figure a TFX pipeline 
end-to-end. Some of you may have

05:40:28.326 --> 05:40:30.326
noticed because 

05:40:31.397 --> 05:40:33.042
model validation needs to last, 
it needs context and needs to 

05:40:33.043 --> 05:40:35.043
know what was the 

05:40:36.520 --> 05:40:41.628
last model validated. We need to
add a metadata store that keeps 

05:40:41.629 --> 05:40:43.629
a record of previous runs so SH 
of 

05:40:45.094 --> 05:40:47.094
the more advanced capabilities 
can be enabled.

05:40:48.830 --> 05:40:51.497
How does this context get 
created? The trainer produces 

05:40:51.498 --> 05:40:53.498
new model, the 

05:40:54.769 --> 05:40:56.828
model validator knows about the 
last validated model and 

05:40:56.829 --> 05:40:58.893
candidate model and downstream 
we take the new candidate 

05:41:03.225 --> 05:41:06.118
model and validation outcome and
if the out' come is positive we 

05:41:06.119 --> 05:41:08.119
push to the server and if it 
isn't we don't because 

05:41:11.928 --> 05:41:13.928
we tonight  -- 

05:41:15.622 --> 05:41:18.291
don't want to push a model that 
is not neter. The metadata store

05:41:18.292 --> 05:41:22.605
is new. When most people talk 
about machine learning learn 

05:41:22.606 --> 05:41:25.298
floes and pipelines they think 
about task dependency and think 

05:41:27.184 --> 05:41:28.391
about one component and when 
that is finished there is 

05:41:28.392 --> 05:41:31.451
another component that runs. 
However, all of you who do 

05:41:31.452 --> 05:41:36.973
machine learning in production 
know we need data dependencies 

05:41:36.974 --> 05:41:39.434
because all components consume 
and create artifacts and as the 

05:41:41.276 --> 05:41:43.276
example of model validation has 
showed, 

05:41:46.147 --> 05:41:49.233
it is incredibly important to 
know these dependency. We need a

05:41:49.234 --> 05:41:51.062
system that is task and data 
aware so each component has a 

05:41:51.063 --> 05:41:54.957
history of all the previous runs
and knows about all of the 

05:41:54.958 --> 05:41:59.450
artifacts. What is in this 
metadata store? Most 

05:41:59.451 --> 05:42:02.381
importantly, type definitions of
artifact and their properties. 

05:42:02.382 --> 05:42:07.076
For TFX, it contains the 
definition of all artifacts that

05:42:07.077 --> 05:42:10.972
have been consumeed and produced
by components and all of their 

05:42:10.973 --> 05:42:12.973
properties.

05:42:15.583 --> 05:42:19.598
It is an extensible type system 
and you can add new properties 

05:42:19.599 --> 05:42:24.072
to the artifacts if you need to 
track more properties of those. 

05:42:24.073 --> 05:42:26.533
Secondly, we keep a record of 
all of the execution of the 

05:42:26.534 --> 05:42:28.534
components. 

05:42:29.836 --> 05:42:31.836
With that execution, we store 
all of the 

05:42:34.469 --> 05:42:36.469
imported

05:42:38.895 --> 05:42:41.355
artifact and the output 
artifacts that were produced and

05:42:41.356 --> 05:42:43.356
the Runtime components.

05:42:46.425 --> 05:42:48.282
If you want to track the code 
snapshot, you can store it in 

05:42:48.283 --> 05:42:50.283
the metadata store as well.

05:42:53.595 --> 05:42:55.633
Putting these things together 
allows us to do something called

05:42:55.634 --> 05:42:59.963
linear tracking across all 
executions. If you know every 

05:42:59.964 --> 05:43:01.964
execution, all inputs 

05:43:03.618 --> 05:43:05.618
and outputs, you can piece 
together a 

05:43:07.114 --> 05:43:09.114
story of how an artifact was 
created.

05:43:11.006 --> 05:43:12.879
We can say what were the 
upstreams that went into 

05:43:12.880 --> 05:43:15.356
producing the artifact and what 
were the downstream runs 

05:43:15.357 --> 05:43:17.357
artifacts 

05:43:18.425 --> 05:43:21.900
that were produced using the 
artifact as an input. That is a 

05:43:21.901 --> 05:43:23.901
powerful capability and let 

05:43:25.446 --> 05:43:28.523
me talk you through some of the 
examples of what this enables. 

05:43:28.524 --> 05:43:30.396
The first is pretty 
straightforward. Let's say I 

05:43:30.397 --> 05:43:34.100
want to list all of the training
runs I are done in the past. I 

05:43:34.101 --> 05:43:36.760
am interested in the trainer and
I want to see all the training 

05:43:36.761 --> 05:43:38.761
runs that were recorded.

05:43:41.237 --> 05:43:46.392
In this case I had two training 
run and see all the properties. 

05:43:46.393 --> 05:43:51.115
This is pretty straightforward 
and nothing new to see here. We 

05:43:51.116 --> 05:43:53.993
can visualize the lineage and 
all the information we have. The

05:43:53.994 --> 05:43:58.543
first comment on this slide is 
we are working on a better UI. 

05:43:58.544 --> 05:44:00.544
This is just for demonstration 
purposes.

05:44:00.797 --> 05:44:02.621
If you look at the end of the 
graph, you see the model expert 

05:44:02.622 --> 05:44:04.622
path.

05:44:05.692 --> 05:44:07.343
This is the specific instance of
the model that was created and 

05:44:07.344 --> 05:44:09.344
we see the model was created by 
the trainer and the 

05:44:12.463 --> 05:44:15.372
trainer created this model by 
consuming a schema, transform 

05:44:15.373 --> 05:44:18.205
and examples. These are specific
instances so the IDs are not 

05:44:18.206 --> 05:44:20.206
just number.

05:44:23.133 --> 05:44:25.133
They are schema of ID number 4 
and for 

05:44:28.654 --> 05:44:30.654
each of those artifact WEEZsee 
they were created upstream.

05:44:32.542 --> 05:44:34.542
This allows forward and backward
in the 

05:44:35.645 --> 05:44:37.476
artifact and the narrative I 
used was walking back from the 

05:44:37.477 --> 05:44:39.477
model but you 

05:44:40.555 --> 05:44:43.031
could look at training data and 
say were what the artifacts 

05:44:43.032 --> 05:44:45.032
produced using that training 
data.

05:44:47.325 --> 05:44:49.403
This slide shows a visualization
of the data distribution that 

05:44:49.404 --> 05:44:51.404
went into the model.

05:44:52.658 --> 05:44:53.256
At first glance, this may not be
something like that that is 

05:44:53.257 --> 05:44:57.764
earth shattering because we have
done this before. We can compute

05:44:57.765 --> 05:44:59.765
and visualize statistics but if 
you look at the code snippet, we

05:45:00.832 --> 05:45:02.832
are fought referring data or 
statistics.

05:45:04.701 --> 05:45:06.701
We are referring to a model.

05:45:07.957 --> 05:45:10.390
We say show me the distribution 
of data the model was trained on

05:45:10.391 --> 05:45:14.108
and we can do this because we 
have a track record of all the 

05:45:14.109 --> 05:45:16.757
data and statistics that went 
into this model. We can do a 

05:45:16.758 --> 05:45:18.383
similar thing in the other 
direction saying for a specific 

05:45:18.384 --> 05:45:22.704
model show me the sliced metrics
that were produced downstream by

05:45:22.705 --> 05:45:24.705
TensorFlow model 

05:45:25.951 --> 05:45:28.873
analysis and we can get this 
visualization again just by 

05:45:28.874 --> 05:45:34.423
looking at a model and not 
specifically pointing to the 

05:45:34.424 --> 05:45:36.466
output of TensorFlow analysis. 
We know all the models that were

05:45:36.467 --> 05:45:38.467
trained 

05:45:40.542 --> 05:45:42.542
and where the checkpoints lie so
we can 

05:45:43.589 --> 05:45:46.239
start support to the historic 
runs and look at the tensor port

05:45:46.240 --> 05:45:48.240
for the models you have trained 
in the past.

05:45:51.158 --> 05:45:53.158
Because we have a track record, 
you can 

05:45:55.504 --> 05:45:57.504
launch tensor point and compare 
two models in the same instance.

05:45:59.213 --> 05:46:01.060
This is really model tracking 
and experiment comparison after 

05:46:01.061 --> 05:46:06.362
the fact. We enabled this by 
keeping a track record of all of

05:46:06.363 --> 05:46:08.801
this. 
If you have multiple models, you

05:46:08.802 --> 05:46:12.900
can look at the data 
distribution for multiple models

05:46:12.901 --> 05:46:16.604
and this usually helps with 
debugging a model. If you train 

05:46:16.605 --> 05:46:18.605
the same model twice, or 

05:46:22.963 --> 05:46:23.584
on data and it behaves 
differently, sometimes it can 

05:46:23.585 --> 05:46:28.075
pay off. We are overlaying two 
distributions of the statistics 

05:46:28.076 --> 05:46:30.076
for one model and the 

05:46:31.569 --> 05:46:33.834
other and you would see if there
is a considerable drift between 

05:46:33.835 --> 05:46:35.835
those two.

05:46:37.929 --> 05:46:39.929
All of these are enabled by this
lineage tracking.

05:46:42.231 --> 05:46:43.041
Another set of use-cases is 
visualizing previous runs over 

05:46:43.042 --> 05:46:48.168
time. So if you train the same 
model over time over new data we

05:46:48.169 --> 05:46:53.105
can give you a time series graph
of all metrics over time and you

05:46:53.106 --> 05:46:55.790
can see if your model improves 
or gets worse over time as you 

05:46:56.420 --> 05:47:00.965
re-train it. Another powerful 
use case is carrying over state 

05:47:00.966 --> 05:47:02.387
from previous models because we 
know you have trained the model 

05:47:02.388 --> 05:47:06.697
in the past we can do something 
called warm starting. We can 

05:47:06.698 --> 05:47:09.391
reinitialize the model with 
weights from a previous run. 

05:47:09.392 --> 05:47:13.696
Sometimes you want to 
reinitialize the entire model or

05:47:13.697 --> 05:47:15.345
just an embedding and in this 
way we can continue training 

05:47:15.346 --> 05:47:17.346
from 

05:47:19.247 --> 05:47:21.247
where we left off with a new 
dataset.

05:47:23.091 --> 05:47:25.091
And another very powerful 
application 

05:47:28.006 --> 05:47:30.262
of this is being able to reuse 
previously outputed outputs.

05:47:33.849 --> 05:47:36.349
If you have a pipeline that 
ingest data, applies 

05:47:36.350 --> 05:47:38.350
transformation to data and 

05:47:40.203 --> 05:47:42.652
then you train the model, every 
time you make a small change to 

05:47:42.653 --> 05:47:47.170
the model you don't want to 
recompute every upstream. There 

05:47:47.171 --> 05:47:50.236
is no reason just because you 
changed something in the model. 

05:47:50.237 --> 05:47:51.878
Because we have a track record 
of all the previous steps, we 

05:47:51.879 --> 05:47:56.577
can make a decision of saying 
your data hasn't changed, your 

05:47:56.578 --> 05:48:00.643
transfer code hasn't changed and
we will reuse the artifacts that

05:48:00.644 --> 05:48:01.897
produced the upstream and you 
can iterate much, much faster. 

05:48:01.898 --> 05:48:04.766
This improves iteration speeds 
and saves 

05:48:10.260 --> 05:48:13.122
compute because you are not 
reexot  -- recomputing things 

05:48:13.123 --> 05:48:15.123
you already computed in the 
past.

05:48:17.432 --> 05:48:19.062
We talked about components and 
how do we orchestrate TFX 

05:48:19.063 --> 05:48:21.909
pipelines? Every component has 
something we call a driver and 

05:48:21.910 --> 05:48:26.012
publisher. The driver's 
responsibility is to basically 

05:48:26.013 --> 05:48:28.262
retrieve state from the metadata
store to inform what work needs 

05:48:28.479 --> 05:48:31.360
to be done. In the example of 
model validation, the 

05:48:34.613 --> 05:48:36.613
driver looks into the metadata 
store to 

05:48:42.684 --> 05:48:45.136
find the last validated model. 
The publish keeps the record of 

05:48:46.782 --> 05:48:49.048
everything going in the 
component, produced and Runtime 

05:48:49.049 --> 05:48:51.049
configuration so we 

05:48:53.956 --> 05:48:55.956
can do the lineage tracking I 
mentioned earlier.

05:48:59.135 --> 05:49:01.387
In between is the executer who 
is unaware of the metadata 

05:49:01.388 --> 05:49:05.920
because it is important to make 
that piece relatively simple 

05:49:05.921 --> 05:49:06.936
because if you want to change 
the training code in a component

05:49:06.937 --> 05:49:11.442
you should not have to worry 
about drivers and publishes, 

05:49:11.443 --> 05:49:13.894
just the executer. It makes it 
easier to write new 

05:49:18.003 --> 05:49:20.662
components for the system, also.
We have one shared configuration

05:49:20.663 --> 05:49:22.663
model 

05:49:25.140 --> 05:49:27.140
on top that configures 
end-to-end pipeline.

05:49:30.047 --> 05:49:32.047
As you can see this is a Python 
dsl, 

05:49:35.435 --> 05:49:37.073
you see it has an object for 
each component from top to 

05:49:37.074 --> 05:49:39.131
bottom. 
The training component you can 

05:49:39.132 --> 05:49:41.217
see basically receives 
configuration and 

05:49:46.101 --> 05:49:48.669
says your input come from the 
transer outoutput and the schema

05:49:48.670 --> 05:49:50.670
that was inferred.

05:49:52.376 --> 05:49:53.840
Let's see what is inside the of 
the trainer and that is really 

05:49:53.841 --> 05:49:55.841
just TensorFlow code.

05:49:57.315 --> 05:49:59.536
We use an estimator and an 
estimator train and evaluate 

05:49:59.537 --> 05:50:01.537
method to train this model.

05:50:05.962 --> 05:50:08.005
It takes an estimator and we 
just use one of our pre-made 

05:50:08.006 --> 05:50:10.006
estimators.

05:50:12.985 --> 05:50:15.083
This is a wide and deep model 
you can instantiate and return. 

05:50:15.084 --> 05:50:16.742
We don't have an opinion on what
this code looks like. It is just

05:50:16.743 --> 05:50:18.743
TensorFlow. 

05:50:20.635 --> 05:50:21.660
Anything that produces a safe 
model as output is fair game. 

05:50:21.661 --> 05:50:26.992
You can use a keras model that 
produces an inference graph or 

05:50:26.993 --> 05:50:31.053
if you chose to you can use 
lower level APIs in TensorFlow 

05:50:31.054 --> 05:50:33.054
as long as it produces a safe 

05:50:34.144 --> 05:50:37.000
model in the right format that 
it can be useded in TensorFlow 

05:50:37.001 --> 05:50:40.076
or for model analysis, you can 
write any type of TensorFlow 

05:50:40.077 --> 05:50:42.077
code you want.

05:50:43.804 --> 05:50:46.444
So, if you have noticed, we 
still haven't talked about 

05:50:46.445 --> 05:50:48.445
orchestration.

05:50:50.951 --> 05:50:54.819
We are a configuration, 
components and metadata store. I

05:50:54.820 --> 05:50:57.079
know some of you might be 
thinking is he going to announce

05:50:57.080 --> 05:50:59.523
a new orchestration system and 
the good news is no. At least 

05:50:59.524 --> 05:51:01.524
not today.

05:51:02.618 --> 05:51:04.618
Instead we talked to a lot of 
users and 

05:51:10.210 --> 05:51:11.431
unsurprisingly found out that 
there is a significant install 

05:51:11.432 --> 05:51:13.432
base of 

05:51:14.495 --> 05:51:16.950
orchestration systems in your 
companies. We just heard from 

05:51:16.951 --> 05:51:18.951
Airbnb, of course they developed
airflow and there is a 

05:51:23.273 --> 05:51:25.321
lot of companies that use 
Kubeflow and there is a number 

05:51:25.322 --> 05:51:27.809
of other orchestration systems. 
We made the choice to select any

05:51:27.810 --> 05:51:29.810
number 

05:51:31.097 --> 05:51:33.158
of orchestration systems because
we don't want to make you adopt 

05:51:33.159 --> 05:51:35.159
a new 

05:51:36.822 --> 05:51:38.822
orchestration system to use the 
TFX pipelines.

05:51:40.089 --> 05:51:42.089
Reason number two is we want you
to extend TFX pipelines.

05:51:45.216 --> 05:51:47.656
We published our version of what
a TFX pipeline looks like and 

05:51:47.657 --> 05:51:52.779
the components we use at Google,
but we want to make it easy for 

05:51:52.780 --> 05:51:54.832
you to add components before, 
after and in parallel to 

05:51:54.833 --> 05:51:56.833
customize the pipeline to your 
own use-cases. 

05:51:59.320 --> 05:52:01.581
All these orchestrations are 
made to be able to express 

05:52:01.582 --> 05:52:04.232
arbitrary workflows and if you 
are familiar withing of those 

05:52:06.300 --> 05:52:08.167
orchestration systems you should
be able to use them for a use 

05:52:08.168 --> 05:52:13.870
case. Here are two examples of 
what it looks like with airflow 

05:52:13.871 --> 05:52:15.871
and Kubeflow pipelines.

05:52:18.416 --> 05:52:20.416
On the left you see the same TFX

05:52:25.818 --> 05:52:27.818
pipeline configured that is 
executed on AirFlow.

05:52:29.505 --> 05:52:31.505
In the Chicago taxi cab example 
we used 10,000 records.

05:52:35.044 --> 05:52:37.044
On the right side you see the 
same 

05:52:38.780 --> 05:52:41.214
pipeline executed on Kubeflow on
data Cloud so you can scale it 

05:52:41.215 --> 05:52:43.683
up to the 100 million rows in 
that dataset.

05:52:47.530 --> 05:52:49.173
It is the same configuration and
components running in both 

05:52:49.174 --> 05:52:53.460
environments and you can chose 
how you want to orchestrate them

05:52:53.461 --> 05:52:55.461
in your own favorite 
orchestration system. 

05:52:57.188 --> 05:52:59.188
This is what this looks like if 
it is put together.

05:53:01.277 --> 05:53:01.885
TFX goes all the way from raw 
data to your deployment 

05:53:01.886 --> 05:53:05.162
environment. We discussed a 
shared configuration model at 

05:53:05.163 --> 05:53:07.163
the top.

05:53:09.055 --> 05:53:11.916
The metadata system that keeps 
track of the runs no matter how 

05:53:11.917 --> 05:53:13.917
you orchestrator 

05:53:15.825 --> 05:53:18.099
the component and two ways to 
publish them with airflow and 

05:53:18.100 --> 05:53:20.974
Kubeflow but you can chose to 
orchestrate the TFX pipeline in 

05:53:20.975 --> 05:53:23.422
any way you want. 
All of this is available now so 

05:53:23.423 --> 05:53:25.423
you can 

05:53:30.331 --> 05:53:32.331
go on 

05:53:34.092 --> 05:53:36.092
GitHub/TensorFlow/TFX.

05:53:37.348 --> 05:53:39.348
Tomorrow we have a workshop 
where you 

05:53:42.691 --> 05:53:44.691
can get hands on experience with

05:53:46.009 --> 05:53:48.009
TensorFlow Extended and there is
no 

05:53:49.487 --> 05:53:51.487
prerecks and you don't to bring 
your own laptop.

05:53:54.193 --> 05:53:56.012
We will jump into an example 
with the Chicago taxicab 

05:53:56.013 --> 05:53:58.013
dataset.

05:54:00.282 --> 05:54:02.282
This is a record of cab rides in
Chicago for some period of time.

05:54:03.810 --> 05:54:06.060
It contains everything you would
expect. It contains when the 

05:54:06.061 --> 05:54:08.501
trip started, when it ended, 
where they started and ended, 

05:54:11.755 --> 05:54:13.755
how much was paid for it and how
it was paid.

05:54:15.631 --> 05:54:17.631
Some of the features need 

05:54:19.137 --> 05:54:21.137
transformation so longitude and 
latitude 

05:54:22.221 --> 05:54:24.221
features need to be packaged.

05:54:26.105 --> 05:54:28.578
We treat them as categorical 
features. 

05:54:30.204 --> 05:54:32.204
Strings need to be integerized 
and some 

05:54:33.279 --> 05:54:35.279
of the flows need to be 
normalized.

05:54:38.183 --> 05:54:39.833
We feed the dense features into 
the deep part and all the others

05:54:39.834 --> 05:54:41.834
use the wide part.

05:54:44.117 --> 05:54:46.372
The label we are trying to 
predict is a bullion which is if

05:54:46.373 --> 05:54:50.450
the tip is larger than 20% of 
the fare. Really, what we are 

05:54:50.451 --> 05:54:52.451
doing is building a 

05:54:54.442 --> 05:54:56.442
high tip predictor just in case 
if there 

05:54:58.949 --> 05:55:01.023
is any cab drivers in the 
audience or online, come find me

05:55:01.024 --> 05:55:06.420
later. I think this will be 
beneficial to you if you can 

05:55:06.421 --> 05:55:09.488
predict if a cab ride give as 
high tip or not. Let's jump in 

05:55:09.489 --> 05:55:12.758
and start with data validation 
transformation. The first part 

05:55:12.759 --> 05:55:15.202
of the TFX pipeline is ingesting
data, validating that data if 

05:55:19.082 --> 05:55:21.082
it is OK, and then transforming 
so it 

05:55:22.195 --> 05:55:24.195
can be fed into a TensorFlow 
graph.

05:55:29.920 --> 05:55:31.920
We start with the 

05:55:35.475 --> 05:55:38.242
examplexamplegenwhich 
takeexamplegen. This is 

05:55:38.243 --> 05:55:41.176
extensible. 
You can ingest any type of data 

05:55:38.243 --> 05:55:40.243
in the pipelines. What is 
important is after the step the 

05:55:42.179 --> 05:55:44.179
data is in a well defined place 
where we 

05:55:45.231 --> 05:55:46.910
can find it, in a well defined 
format because all downstream 

05:55:46.911 --> 05:55:49.577
components standardize and it is
split between training and 

05:55:49.578 --> 05:55:51.798
develop. 
You have seen the configuration 

05:55:51.799 --> 05:55:53.799
of these components before.

05:55:56.154 --> 05:55:58.154
It is very minimal configuration
in most of the cases.

05:56:01.042 --> 05:56:03.895
Next we move on to data analysis
and validation. I think a lot of

05:56:03.896 --> 05:56:06.343
you have a good intuition of why
it is important. Machine 

05:56:06.344 --> 05:56:08.344
learning is the process of 

05:56:09.426 --> 05:56:11.342
taking data and learning models 
that predict some field in your 

05:56:11.343 --> 05:56:15.661
data and you are always aware if
you feed garbage in you get 

05:56:15.662 --> 05:56:18.138
garbage out. 
There is no hope in learning a 

05:56:18.139 --> 05:56:21.424
good machine learning if the 
data is wrong or has errors in 

05:56:21.425 --> 05:56:23.425
them.

05:56:24.696 --> 05:56:27.132
This is reinforced if you have 
continuous pipelines produced on

05:56:27.133 --> 05:56:31.641
data from a bad model and you 
are reinforcing the same 

05:56:31.642 --> 05:56:35.732
problem.
Data understanding is critical 

05:56:35.733 --> 05:56:37.733
for model understanding.

05:56:40.380 --> 05:56:43.101
There is no hope in 
understanding why a model isn't 

05:56:45.355 --> 05:56:47.355
predicting something if the data
isn't right.

05:56:49.429 --> 05:56:51.429
The question you may ask as a 
cab 

05:56:54.797 --> 05:56:56.797
driver is why are trip 
predictions bad 

05:56:58.039 --> 05:57:00.039
in the morning hours and I will 
try to 

05:57:04.164 --> 05:57:06.434
answers these with the TFX 
toolsets we V. There is a lot of

05:57:06.435 --> 05:57:08.435
care taking with code.

05:57:10.107 --> 05:57:11.940
It is previewed, checked into 
repository, version controlled 

05:57:11.941 --> 05:57:17.072
and data needs to be first-class
citizens in these systems. With 

05:57:17.073 --> 05:57:19.513
the question what are expected 
values from payment types? That 

05:57:19.514 --> 05:57:21.972
is a question about the schema 
of your data and what we would 

05:57:21.973 --> 05:57:23.973
argue is that the schema needs 
to be treated with 

05:57:27.527 --> 05:57:30.405
the same care as you treat your 
code. And catching errors early 

05:57:30.406 --> 05:57:32.856
is absolutely critical because I
am sure as all of you 

05:57:36.152 --> 05:57:38.152
know errors propagate thew the 
system.

05:57:40.441 --> 05:57:42.441
If the data is not OK, every 
downstream goes wrong. 

05:57:44.798 --> 05:57:45.808
These errors are hard to fix if 
you catch them late in the 

05:57:45.809 --> 05:57:49.493
process. Really catching those 
problems as early as possible is

05:57:49.494 --> 05:57:51.494
absolutely critical.

05:57:53.148 --> 05:57:55.392
In the taxicab example, you 
would ask a question is this new

05:57:55.393 --> 05:57:57.393
company I have in 

05:57:59.253 --> 05:58:01.253
my dataset a typo or a real 
company 

05:58:02.447 --> 05:58:04.447
which is a

05:58:08.443 --> 05:58:10.443
natchal evolution of my dataset.

05:58:12.423 --> 05:58:14.042
The statistics training program 
takes in the data and it can be 

05:58:14.043 --> 05:58:16.043
training or 

05:58:17.336 --> 05:58:19.565
serving logs in which case you 
can look at the skew between 

05:58:19.566 --> 05:58:22.198
serving and training data. 
Statistics really capture the 

05:58:22.199 --> 05:58:25.888
shape of your data and the 
visualization components draw 

05:58:25.889 --> 05:58:28.125
your attention to things that 
need your attention such as if a

05:58:30.973 --> 05:58:34.625
feature is missing most of the 
time it is highlighted in red. 

05:58:34.626 --> 05:58:36.459
The configuration for this 
component is minimal as well. 

05:58:42.248 --> 05:58:44.730
Let me zoom into some of these 
visualizations. One question I 

05:58:44.731 --> 05:58:46.731
posted earlier was why 

05:58:47.790 --> 05:58:49.846
are my predictions bad in the 
morning hours. You could look at

05:58:49.847 --> 05:58:51.945
the dataset and see for trip 
start hour in the morning hours 

05:58:54.609 --> 05:58:56.609
before 2:00-6:00 A.M. you don't 
have much data because there 

05:58:59.861 --> 05:59:01.861
is not that much taxi trips at 
that time 

05:59:03.404 --> 05:59:06.049
and not having a lot of data in 
a specific area can mean your 

05:59:06.050 --> 05:59:08.562
model is not robust or has 
higher variance and this 

05:59:12.650 --> 05:59:14.650
could lead to worse predictions.

05:59:16.272 --> 05:59:18.272
Next schemagen.

05:59:21.798 --> 05:59:23.798
It takes the output and infers a
schema for you. 

05:59:24.802 --> 05:59:27.478
There is very few features in 
this case so you could handwrite

05:59:27.479 --> 05:59:29.479
the schema. If you have 
thousands of features, it 

05:59:33.851 --> 05:59:36.085
is hard to handwrite that 
expectation so we infer the 

05:59:36.086 --> 05:59:39.978
schema for you and it represents
what you expect from your data 

05:59:39.979 --> 05:59:42.851
and what good data looks like 
and what values your string 

05:59:42.852 --> 05:59:45.117
features can take on and so on. 

05:59:48.188 --> 05:59:50.040
Again, very minimal 
configuration. The question we 

05:59:50.041 --> 05:59:54.502
can answer is what are expected 
values for payment types. If you

05:59:54.503 --> 05:59:56.742
look at the bottom, you see the 
field payment type can cake ton 

05:59:56.743 --> 05:59:58.743
cash, 

06:00:01.038 --> 06:00:03.038
credit card, dispute, no charge,
p card and unknown.

06:00:05.765 --> 06:00:08.618
Next time I run this, and this 
field takes on a different 

06:00:08.619 --> 06:00:10.619
value, I will get 

06:00:12.275 --> 06:00:14.932
an anomaly which comes from the 
example validator. The example 

06:00:14.933 --> 06:00:19.784
validator takes the statistics 
and schema as input and produces

06:00:19.785 --> 06:00:22.843
on anomaly report. 
That anomaly report tell you if 

06:00:22.844 --> 06:00:24.844
the data 

06:00:27.133 --> 06:00:29.418
is missing feature, has the 
wrong relevancey, if the 

06:00:29.419 --> 06:00:33.520
distribution has shifted and 
deis important to highlight the 

06:00:33.521 --> 06:00:34.564
anomaly report is human readable
so you can look at it and 

06:00:34.565 --> 06:00:36.565
understand what 

06:00:37.650 --> 06:00:39.650
is going on but it is always 
machine 

06:00:40.951 --> 06:00:42.951
readable so you can 
automatically make 

06:00:47.077 --> 06:00:49.077
decisions on the 

06:00:52.299 --> 06:00:54.299
anomalanomaly. Here you can see 
the field company has 

06:00:56.304 --> 06:00:58.304
taken on unexpected string 
values.

06:01:00.648 --> 06:01:01.454
That just means these string 
values were not in your schema 

06:01:01.455 --> 06:01:03.455
before and that 

06:01:06.822 --> 06:01:08.822
can be a natural evolution of 
data.

06:01:10.827 --> 06:01:12.827
The first time you run it maybe 
you 

06:01:15.016 --> 06:01:16.455
didn't see trips from those 
taxicab companies but when you 

06:01:16.456 --> 06:01:20.755
look you see they are normal so 
you can update or if you saw a 

06:01:20.756 --> 06:01:22.579
lot of scrambled text you would 
know there is a problem in your 

06:01:22.580 --> 06:01:24.580
data that you would have to go 
and fix.

06:01:27.694 --> 06:01:29.970
Moving on, we actually get to 
transform. Let me just recap the

06:01:29.971 --> 06:01:31.971
types of transformations we want
to do.

06:01:35.985 --> 06:01:37.985
I highlighted them in blue.

06:01:43.214 --> 06:01:45.214
We want to convert the strings 
to ints 

06:01:48.041 --> 06:01:53.244
and for the tense features we 
want to normalize them. All of 

06:01:53.245 --> 06:01:55.245
these transformations require 
you to do a full pass.

06:01:59.710 --> 06:02:02.958
To pact  packatize you need to 
figure out the boundaries. For a

06:02:02.959 --> 06:02:04.959
string to int you need to see 
all 

06:02:06.025 --> 06:02:08.025
the string values.

06:02:10.487 --> 06:02:13.955
This is exactly what we build 
TensorFlow transform for. It 

06:02:13.956 --> 06:02:15.590
allows you to express the 
preprocessing function of your 

06:02:15.591 --> 06:02:20.759
data that contains some of these
transformations that require a 

06:02:20.760 --> 06:02:23.191
full pass of your data and 
enable automatically to run the 

06:02:23.192 --> 06:02:26.804
data processing graph to compute
those statistics. In this case, 

06:02:26.805 --> 06:02:28.805
you can see the orange 

06:02:30.851 --> 06:02:32.851
boxes are statistics that we 
require, 

06:02:34.017 --> 06:02:36.017
for minimalization we require a 
mean and 

06:02:37.300 --> 06:02:39.300
standard deviation and 
TensorFlow 

06:02:40.574 --> 06:02:43.047
transform says scale to C store 
and it will create a data 

06:02:43.048 --> 06:02:45.090
processing graph for you that 
will compute the standard mean 

06:02:48.996 --> 06:02:50.996
and data and return the results.

06:02:53.055 --> 06:02:54.495
That graph is a hematic graph 
that contains all the 

06:02:54.496 --> 06:02:58.788
information you need to apply 
your transformations. That graph

06:02:58.789 --> 06:03:01.270
can then be used in training and
surveying guaranteeing there is 

06:03:01.271 --> 06:03:03.271
no drift between them.

06:03:04.853 --> 06:03:06.853
This

06:03:09.049 --> 06:03:11.049
eliminates the chances of skew 
and at 

06:03:12.408 --> 06:03:14.288
serving time you need to feed in
the raw data and all the 

06:03:14.289 --> 06:03:19.414
transformations are done as part
of a TensorFlow graph. The 

06:03:19.415 --> 06:03:21.859
transfer component takes in 
data, the schema allows you to 

06:03:21.860 --> 06:03:23.860
parse the data, 

06:03:26.402 --> 06:03:28.402
and code in this case the system

06:03:30.250 --> 06:03:32.250
provided preprocessing function,
and it 

06:03:33.910 --> 06:03:37.023
produces a hematic graph that 
gets attached to your training. 

06:03:37.024 --> 06:03:39.024
It can materialize the transform
data 

06:03:43.068 --> 06:03:45.068
and that is a performance

06:03:46.595 --> 06:03:48.595
optimization you need.

06:03:51.023 --> 06:03:52.434
It can pay off to serialize some
of the training before the next 

06:03:52.435 --> 06:03:54.677
step. 
The component takes in a module 

06:03:54.678 --> 06:03:57.316
file, a file where you configure
preprocessing function.

06:04:02.381 --> 06:04:06.647
In this code snippet, the actual
code isn't important but I want 

06:04:02.381 --> 06:04:06.546
to highlight the last line in 
this code snippet and that is 

06:04:06.547 --> 06:04:08.547
how we transform our label 
because there label is a logic 

06:04:11.853 --> 06:04:15.110
expression of saying it is 
greater than 20% of my fare. I 

06:04:15.111 --> 06:04:17.161
want to highlight this because 
you don't need analyze phase for

06:04:17.162 --> 06:04:19.162
all of your transformations.

06:04:22.947 --> 06:04:25.405
In cases where you don't need 
analysis phases, the 

06:04:25.406 --> 06:04:28.066
transformation is just a regular
TensorFlow graph.

06:04:32.589 --> 06:04:34.605
To square something to C square 
and integerize schemes you need 

06:04:34.606 --> 06:04:36.606
these phases.

06:04:38.515 --> 06:04:40.515
This is the user code you would 
write 

06:04:41.793 --> 06:04:43.838
and transfer creates the graph 
and returns the transfer graph 

06:04:43.839 --> 06:04:45.839
you need to 

06:04:47.802 --> 06:04:49.802
apply these transformations.

06:04:51.165 --> 06:04:51.562
Now we are done with this we 
haven't trained a machine 

06:04:51.563 --> 06:04:56.238
learning model right but we have
made sure we know our data is in

06:04:56.239 --> 06:04:58.336
a place where we can find it, it
is in a format we can understand

06:04:58.337 --> 06:05:03.397
and split between training and 
deval and we know our data is 

06:05:03.398 --> 06:05:05.398
good because we 

06:05:07.484 --> 06:05:09.484
validated them and we are 
applying 

06:05:10.976 --> 06:05:12.976
transforms consistently between 
training and serving. 

06:05:13.856 --> 06:05:15.856
This bringstuse the training 
step.

06:05:17.934 --> 06:05:20.587
This is where the magic happens.
The training steps is the 

06:05:20.588 --> 06:05:24.242
TensorFlow graph and step. 
The training component takes in 

06:05:24.243 --> 06:05:28.388
the output of transform as 
mentioned which is the transform

06:05:28.389 --> 06:05:30.389
graph and optionally 

06:05:31.798 --> 06:05:33.798
the materialized data, schema 
and 

06:05:34.801 --> 06:05:36.801
training codes that you provide.

06:05:37.874 --> 06:05:40.496
It creates output to TensorFlow 
models. Those models are in the 

06:05:40.497 --> 06:05:42.497
safe model 

06:05:43.808 --> 06:05:45.314
format the standard serialized 
model in TensorFlow which you 

06:05:45.315 --> 06:05:48.378
heard quite a bit about this 
morning. We produced two of 

06:05:48.379 --> 06:05:53.926
them. One is the inference graph
and another is the evaluate 

06:05:53.927 --> 06:05:56.413
graph which contains the metrics
and necessary annotations to 

06:06:01.430 --> 06:06:02.663
perform TensorFlow model 
analysis. This is the 

06:06:02.664 --> 06:06:04.914
configuration you have seen 
earlier and again the trainer 

06:06:04.915 --> 06:06:08.181
takes in a module file and the 
code that is actually in that 

06:06:08.182 --> 06:06:10.842
module file, again, I am going 
to show you the same slides to 

06:06:12.297 --> 06:06:14.933
reiterate the point is just 
TensorFlow. In this case it is 

06:06:14.934 --> 06:06:16.934
the train and 

06:06:19.888 --> 06:06:20.907
evaluate method from estimators 
and the estimator being returned

06:06:20.908 --> 06:06:23.584
here. 
Again, just to make sure you are

06:06:23.585 --> 06:06:28.482
aware of this, any TensorFlow 
code here that produces the safe

06:06:28.483 --> 06:06:30.729
model in the right format is 
fair game. This works really 

06:06:30.730 --> 06:06:34.585
well. With this, we have now 
trained the TensorFlow model and

06:06:34.586 --> 06:06:36.832
now I am going to hand it off to
my colleague Christina 

06:06:41.270 --> 06:06:43.270
who will talk about model 
evaluation analysis.

06:06:48.404 --> 06:06:53.037
-- model validation. 
Thank you, Clemens. Hi, my name 

06:06:53.038 --> 06:06:54.660
is Christina and I am a software
engineer on the Google brain 

06:06:54.661 --> 06:06:56.661
team.

06:06:57.707 --> 06:06:59.916
I am here today to tell you 
about some tools my team and I 

06:06:59.917 --> 06:07:02.604
have built to help make the 
end-to-end lifecycle of the 

06:07:03.825 --> 06:07:05.654
machine learning pipeline 
easier. I am going to start by 

06:07:05.655 --> 06:07:07.655
talking about 

06:07:12.341 --> 06:07:15.114
model analysis and salad 
validation. These are two 

06:07:15.115 --> 06:07:17.115
different components in 

06:07:18.873 --> 06:07:20.641
TFX but they are similar in how 
they are executed. The main 

06:07:20.642 --> 06:07:22.920
difference is how you, as an end
user, will use them.

06:07:26.743 --> 06:07:28.743
I will start by talking about

06:07:32.591 --> 06:07:35.943
the evaluator. Why is this 
important? We have gathered 

06:07:35.944 --> 06:07:40.027
data, cleaned that data, trained
a model, but we really want to 

06:07:40.028 --> 06:07:42.686
make sure that model works and 
some model evaluation can help 

06:07:42.687 --> 06:07:44.687
you 

06:07:46.740 --> 06:07:48.740
assess the overall quality of 
your model.

06:07:50.410 --> 06:07:53.269
You also may want to analyze how
your model is performing on 

06:07:53.270 --> 06:07:56.917
specific Slices of the data. So,
in this case with the Chicago 

06:07:56.918 --> 06:08:01.662
taxi example that Clemens 
started us off with, why are my 

06:08:01.663 --> 06:08:04.523
tip predictions sometimes wrong?
Slicing the data and looking at 

06:08:04.524 --> 06:08:06.524
where 

06:08:07.990 --> 06:08:10.235
you are doing poorly can be a 
real benefit because it 

06:08:10.236 --> 06:08:12.275
identifies low-hanging fruit 
where you can get 

06:08:15.344 --> 06:08:17.344
gains in accuracy by adding more
data, 

06:08:18.384 --> 06:08:21.238
or making some other changes to 
make some of these segments 

06:08:21.239 --> 06:08:24.284
improve. You also want to track 
your performance over time.

06:08:26.133 --> 06:08:28.181
You are going to be continuously
training models and updating 

06:08:28.182 --> 06:08:33.092
them with fresh data so your 
models don't get stale and you 

06:08:33.093 --> 06:08:35.974
want to make sure that your 
metrics are improving over time 

06:08:35.975 --> 06:08:37.975
and 

06:08:42.429 --> 06:08:45.818
not regressing and model 
evaluation with help with this. 

06:08:49.060 --> 06:08:51.060
The component of TFX that 
supports this 

06:08:52.832 --> 06:08:54.832
is called

06:08:58.712 --> 06:09:00.758
evaluator.

06:09:05.810 --> 06:09:07.810
You need to specify the splits 
in data 

06:09:09.725 --> 06:09:12.723
you find more interesting so the
evaluateator

06:09:17.058 --> 06:09:22.499
evaluator. A process is run to 
generate metric for the overall 

06:09:22.500 --> 06:09:24.500
slice and the Slices you have 
specified. 

06:09:26.806 --> 06:09:30.074
The output of the evaluator is 
evaluation metrics. This is a 

06:09:30.075 --> 06:09:32.534
structured data format that has 
your data, the splits you 

06:09:32.535 --> 06:09:34.535
specified, 

06:09:36.355 --> 06:09:38.355
and the metrics that correspond 
to each split.

06:09:39.619 --> 06:09:41.909
The TensorFlow model analysis 
library also has a visualization

06:09:41.910 --> 06:09:44.365
tool that allows you to load up 
these metrics and 

06:09:48.848 --> 06:09:50.848
dig around in your data in a 
user-friendly way.

06:09:54.964 --> 06:09:57.199
So going back to our Chicago 
taxi example, you can see how 

06:09:57.200 --> 06:09:59.200
the model 

06:10:00.702 --> 06:10:03.154
evaluator can help you look at 
your top-line objective and how 

06:10:03.155 --> 06:10:05.155
well can you 

06:10:06.838 --> 06:10:08.838
predict trips that result in 
large tips.

06:10:10.805 --> 06:10:12.805
The

06:10:17.643 --> 06:10:19.643
TFMA visualization shows the 
overall data.

06:10:21.515 --> 06:10:23.515
Maybe you want to stay 95% 
accuracy.

06:10:25.591 --> 06:10:27.591
That is a lot better than 94.7. 
Maybe you want to bump that up.

06:10:31.966 --> 06:10:33.173
Then you can dig into why your 
tip predictions are sometimes 

06:10:33.174 --> 06:10:38.103
wrong. We sliced the data by the
hour of the day the trip starts 

06:10:38.104 --> 06:10:40.104
on and sorted by poor 
performance.

06:10:42.859 --> 06:10:45.696
When I look at this data, I see 
trips that start at 2-3:00 a.m.

06:10:52.505 --> 06:10:54.333
were performing quite poorly in 
these times. Because of the 

06:10:54.334 --> 06:10:56.334
statistics generation 

06:10:58.851 --> 06:11:00.851
tool Clemens talked about I know
the 

06:11:03.655 --> 06:11:05.655
data is sparse here but if where
didn't 

06:11:06.745 --> 06:11:08.745
I would think maybe there is 
something 

06:11:10.198 --> 06:11:12.198
that people that get taxies 
during that 

06:11:13.480 --> 06:11:15.480
time that causes erratic tipping
behavior.

06:11:17.305 --> 06:11:19.305
You also want to know if you can
get 

06:11:20.589 --> 06:11:21.199
better at predicting trips over 
time. You are continuously 

06:11:21.200 --> 06:11:25.526
training the model for new data 
and hoping you get better. The 

06:11:25.527 --> 06:11:27.785
TensorFlow model analysis tool 
that powers the evaluator in TFX

06:11:27.786 --> 06:11:29.786
can show you 

06:11:31.265 --> 06:11:34.108
the trends of your metrics over 
time. Here you see three 

06:11:34.109 --> 06:11:36.109
different models and 

06:11:37.589 --> 06:11:39.589
the performance over each with 
accuracy and AU C.

06:11:41.619 --> 06:11:43.059
Now I will move on to talking 
about the model validator 

06:11:43.060 --> 06:11:45.060
component.

06:11:48.625 --> 06:11:50.625
With the evaluator, you were an 
active 

06:11:51.760 --> 06:11:53.589
user, you generated the metrics,
you loaded them up into the UI, 

06:11:53.590 --> 06:11:57.680
you dug around in your data, you
looked for issues you could fix 

06:11:57.681 --> 06:11:59.681
to improve your 

06:12:01.795 --> 06:12:04.033
model, but eventually you are 
going to iterate, your data is 

06:12:04.034 --> 06:12:06.270
going to get better and model 
improve and you will be ready to

06:12:06.271 --> 06:12:09.154
launch. You will have a pipeline
continuously 

06:12:12.202 --> 06:12:14.854
feeding new data into this 
model. Every time you generate a

06:12:14.855 --> 06:12:18.134
new model with new data you 
don't want to have to do a 

06:12:18.135 --> 06:12:20.135
manual process of pushing this 
to a server somewhere.

06:12:22.047 --> 06:12:24.482
The model validator component of
TFX acts as a gate that keeps 

06:12:24.483 --> 06:12:26.483
you from pushing bad versions of
your model while 

06:12:31.798 --> 06:12:33.798
allowing you to auto

06:12:35.916 --> 06:12:37.916
automate pushing.

06:12:39.855 --> 06:12:40.801
We want to avoid pushing models 
with degrated quality 

06:12:40.802 --> 06:12:45.020
specifically in an automated 
fashion. If you train a model 

06:12:45.021 --> 06:12:47.060
with new data and the 
performance drops but say it 

06:12:49.107 --> 06:12:51.190
increases in certain segments of
the data you care about, maybe 

06:12:51.191 --> 06:12:55.109
you make the judgment call this 
is an improvement call overall 

06:12:55.110 --> 06:12:57.110
so we will launch it.

06:12:58.830 --> 06:13:00.254
But you don't want to do this 
automatically you want to have 

06:13:00.255 --> 06:13:03.363
say before you do this. This 
acts as your gatekeeper.

06:13:07.027 --> 06:13:09.027
You want to avoid breaking 
downstream 

06:13:11.898 --> 06:13:14.765
components if your model started
outputing something your server 

06:13:14.766 --> 06:13:18.430
binary couldn't handle you would
want to know that before you 

06:13:18.431 --> 06:13:20.501
push. 
The TFX component that supports 

06:13:20.502 --> 06:13:25.003
this is called the model 
validator taking similar input 

06:13:25.004 --> 06:13:27.004
and output to the model 

06:13:28.057 --> 06:13:30.057
evaluator and the libraries that
compute 

06:13:31.320 --> 06:13:33.320
the metrics are pretty much the 
same 

06:13:35.612 --> 06:13:37.612
under the hood, however, you 
provide two models.

06:13:42.591 --> 06:13:44.591
It then runs on your eval split 
data and 

06:13:45.843 --> 06:13:50.843
compares the metrics on the same
data between the two models. If 

06:13:50.844 --> 06:13:52.673
your metrics have stayed the 
same or improved, you go ahead 

06:13:52.674 --> 06:13:55.560
and bless the model. If the 
metrics that you care about have

06:13:59.373 --> 06:14:01.380
degraded, you will not bless the
model, get some information 

06:14:01.381 --> 06:14:03.396
about which metrics failed so 
you can do further analysis.

06:14:08.836 --> 06:14:10.836
The outcome of this is a 
valididation outcome.

06:14:12.583 --> 06:14:14.583
It just says blessed if 
everything went right.

06:14:17.088 --> 06:14:19.088
Another thing to mote about the 
model 

06:14:22.908 --> 06:14:26.384
validator is it allows next day 
eval of your next day push. 

06:14:26.385 --> 06:14:27.803
Maybe the last model you 
blessed, it was trained with old

06:14:27.804 --> 06:14:31.929
data. With the model evaluator, 
it evaluates it on the new data.

06:14:34.188 --> 06:14:36.188
And finally, I am going to talk 
about the pusher.

06:14:37.876 --> 06:14:39.876
It pusher is probably the 
simplest 

06:14:40.956 --> 06:14:43.864
component in the entire TFX 
pipeline but it does serve quite

06:14:43.865 --> 06:14:45.865
a useful purpose.

06:14:46.929 --> 06:14:48.929
It has one input which is that 
blessing 

06:14:50.184 --> 06:14:52.184
that you got from the model 
validator.

06:14:55.539 --> 06:14:57.539
Then the output is if you passed
your 

06:14:59.360 --> 06:15:01.360
validation then the pusher will 
copy 

06:15:03.065 --> 06:15:05.946
your saved model into a file 
system destination you have 

06:15:05.947 --> 06:15:08.010
specified. 
Now you are ready to serve your 

06:15:08.011 --> 06:15:10.830
model and make it useful to the 
world at-large.

06:15:14.763 --> 06:15:17.012
I am going to talk about model 
deployment next. So this is 

06:15:17.013 --> 06:15:19.904
where we are. 
We have a trained saved model.

06:15:25.451 --> 06:15:26.725
A saved model is a universal 
serialization format for 

06:15:26.726 --> 06:15:28.726
TensorFlow models.

06:15:30.622 --> 06:15:32.622
It contains your graph, your 
learned 

06:15:34.299 --> 06:15:36.728
variable weights, your assets 
like embeddings and vocabulary, 

06:15:36.729 --> 06:15:41.043
but to you this is just an 
implementation detail. Where you

06:15:41.044 --> 06:15:43.044
really want to be is you have 

06:15:46.133 --> 06:15:46.528
an API, you have a server that 
you can query to get answers in 

06:15:46.529 --> 06:15:48.529
real-time or 

06:15:50.400 --> 06:15:52.680
provide those answers to your 
users. We provide several 

06:15:52.681 --> 06:15:54.681
deployment options and many of 
them are going to be 

06:15:58.474 --> 06:16:00.116
discussed at other talks in this
session. 

06:16:00.117 --> 06:16:02.117
TensorFlow.

06:16:04.229 --> 06:16:06.229
js is optimized for serving in 
the browser. 

06:16:06.885 --> 06:16:09.336
TensorFlow Lite is optimized for
mobile devices. We heard a talk 

06:16:09.337 --> 06:16:11.337
about how Google 

06:16:12.430 --> 06:16:14.792
Assistant is using TensorFlow 
Lite to support model inference 

06:16:14.793 --> 06:16:18.714
on their Google Home devices. 
TensorFlow Hub is something new.

06:16:21.600 --> 06:16:22.809
Andr is going to come on in 
five minutes and tell you about 

06:16:22.810 --> 06:16:24.810
that so I am 

06:16:29.306 --> 06:16:29.711
not going to step on his toes. I
am going at a talk about 

06:16:29.712 --> 06:16:31.712
TensorFlow Serving.

06:16:32.983 --> 06:16:35.009
If you want to put up a resting 
API you would want to use 

06:16:35.010 --> 06:16:37.010
TensorFlow Serving. Why would 
you want to use this?

06:16:40.342 --> 06:16:41.851
For one thing, TensorFlow 
Serving has a lot of 

06:16:41.852 --> 06:16:43.852
flexibility.

06:16:45.811 --> 06:16:47.811
It supports multi tenancy.

06:16:49.286 --> 06:16:52.176
You can run multiple models on a
single server instance and run 

06:16:52.177 --> 06:16:55.261
multiple versions of the same 
model. This can be useful when 

06:16:55.262 --> 06:16:57.262
you are trying to canary a new 
model. 

06:16:58.767 --> 06:17:00.767
Say you have a tried and tested 
version 

06:17:02.448 --> 06:17:04.448
of your model, you have created 
a new 

06:17:05.501 --> 06:17:07.373
one, it past your evaluator and 
validation but you want to do 

06:17:07.374 --> 06:17:13.317
testing with real users before 
you completely switch over. 

06:17:13.318 --> 06:17:15.318
TensorFlow Serving supports 
this.

06:17:19.045 --> 06:17:21.045
We also support optimization 
with GPU and TensorFlow RT.

06:17:25.954 --> 06:17:28.843
You can expose a gRPC or REST 
API. TensorFlow Serving is 

06:17:28.844 --> 06:17:30.844
optimized for 

06:17:31.856 --> 06:17:33.856
high performance.

06:17:37.890 --> 06:17:39.890
It provides low let latency and 
request 

06:17:41.183 --> 06:17:43.815
batching and traffic isolation 
so if you are serving multiple 

06:17:43.816 --> 06:17:46.504
models on a single server, a 
traffic spike in one of those 

06:17:48.790 --> 06:17:50.790
models won't affect the serving 
of the other.

06:17:51.442 --> 06:17:54.115
And TensorFlow Serving is 
production ready, finally. This 

06:17:54.116 --> 06:17:56.403
is what we use to serve many of 
our models inside of Google.

06:18:01.823 --> 06:18:03.823
We have served millions of QPS 
with it.

06:18:04.840 --> 06:18:06.462
You can scale in minutes 
particularly if you use the 

06:18:06.463 --> 06:18:08.463
Docker image and scale up on 
Kubernetes.

06:18:10.299 --> 06:18:12.810
We support dynamic version 
refresh so you can specify a 

06:18:12.811 --> 06:18:14.876
version refresh policy to take 
the latest version of your model

06:18:16.924 --> 06:18:18.924
or you can pin to a specific 
version.

06:18:20.002 --> 06:18:21.844
This can be useful for rollbacks
if you find a problem with the 

06:18:21.845 --> 06:18:23.845
latest version 

06:18:25.122 --> 06:18:28.004
after you have already pushed. I
am going to go into a little bit

06:18:28.005 --> 06:18:32.125
more detail about how you might 
deploy a REST API for your 

06:18:32.126 --> 06:18:34.126
model.

06:18:35.176 --> 06:18:37.176
We have two different options 
for doing this.

06:18:39.475 --> 06:18:41.351
The first, the top command, is 
using Docker which we really 

06:18:41.352 --> 06:18:46.437
recommend. It requires a little 
bit of ramp up at the beginning,

06:18:46.438 --> 06:18:49.297
but you will really save time in
the long run by not having to 

06:18:50.826 --> 06:18:52.891
manage your environment and not 
having to manage your own 

06:18:52.892 --> 06:18:54.892
dependencies. 

06:18:57.993 --> 06:18:59.820
You can run locally on your own 
host but you have to do all that

06:18:59.821 --> 06:19:01.821
stuff long term.

06:19:04.292 --> 06:19:06.292
I will go into more detail on 
the Docker run command.

06:19:07.411 --> 06:19:09.411
You start with Docker run, chose
a port 

06:19:10.862 --> 06:19:12.862
you want to bind your API to, 
provide 

06:19:14.121 --> 06:19:15.339
the path to the saved model 
generated by your trainer and 

06:19:15.340 --> 06:19:17.585
hopefully it was pushed by the 
pusher, you provide the model 

06:19:22.017 --> 06:19:23.904
name, and you tell Docker to run
the TensorFlow Serving binary. 

06:19:23.905 --> 06:19:26.819
Another advantage of using 
Docker is 

06:19:31.341 --> 06:19:33.341
that you can easily enable 
hardware 

06:19:38.042 --> 06:19:40.042
acceleration if you are running 
on a 

06:19:43.663 --> 06:19:45.663
host with a GPU and Nvidia host 

06:19:46.741 --> 06:19:48.741
installed you can run on an 
accelerated hardware.

06:19:52.236 --> 06:19:54.236
If you need further optimization
we 

06:19:56.414 --> 06:19:58.042
support tensor RT which is 
optimized for deep learning 

06:19:58.043 --> 06:20:03.157
inference. Your Chicago taxi 
example that we have been using 

06:20:03.158 --> 06:20:05.901
here probably wouldn't benefit 
from this but if you had an 

06:20:08.382 --> 06:20:10.382
image recognition model, ResNet,
you 

06:20:12.475 --> 06:20:14.475
could get performance boosting 
and cost 

06:20:17.240 --> 06:20:19.240
savings by using TensorRT.

06:20:21.540 --> 06:20:24.007
We provide a command line 
allowing you to convert this 

06:20:24.008 --> 06:20:26.008
model and a simple change to the
command line and you are 

06:20:32.463 --> 06:20:34.463
running on accelerated GPU 
hardware TensorRT optimization.

06:20:36.931 --> 06:20:39.213
To put it all together again, we
introduced TensorFlow Extended 

06:20:39.214 --> 06:20:43.368
or TFX and showed you how the 
different components that TFX 

06:20:43.369 --> 06:20:45.369
consist of can work together to 
help you manage the 

06:20:47.398 --> 06:20:49.398
end-to-end lifecycle of your 
machine learning pipeline.

06:20:51.281 --> 06:20:53.115
First, you have your data and we
have tools to help you make 

06:20:53.116 --> 06:20:57.388
sense of that, process it, and 
prepare it for training. We then

06:20:57.389 --> 06:20:59.233
support training your model and 
after you train your model we 

06:20:59.234 --> 06:21:03.126
provide tools that allow you to 
make sense of what you are 

06:21:03.127 --> 06:21:05.777
seeing, of what your model is 
doing and to make improvements, 

06:21:05.778 --> 06:21:10.142
also to make sure you don't 
regress. Then we have the pusher

06:21:10.143 --> 06:21:13.044
that allows you to push to 
various deployment options 

06:21:17.124 --> 06:21:20.808
and make sure model available to
serve users in the real world. 

06:21:20.809 --> 06:21:23.063
To get started with TensorFlow 
Extended, please visit us on 

06:21:23.064 --> 06:21:25.064
GitHub.

06:21:27.876 --> 06:21:29.289
There is more documentation at 
tensorflow.org/TFX and some of 

06:21:29.290 --> 06:21:32.116
my teammates are running a 
workshop tomorrow and they would

06:21:32.117 --> 06:21:37.255
love to see you there. You don't
need to bring a laptop. We have 

06:21:37.256 --> 06:21:39.256
machines setup and ready to go 

06:21:42.342 --> 06:21:44.342
and you can get hands-on 
experience 

06:21:45.353 --> 06:21:49.156
using TensorFlow Extended. Thank
you. Please welcome Andr who 

06:21:49.157 --> 06:21:51.157
will talk about TensorFlow Hub. 

06:21:53.279 --> 06:21:55.279
Thank you, Christina.

06:22:01.551 --> 06:22:03.802
Could I get the notes on the 
presentation? Thank you. Hello. 

06:22:03.803 --> 06:22:08.767
My name is Andr and I am a 
software engineer in Zurich and 

06:22:08.768 --> 06:22:10.768
I am the 

06:22:11.774 --> 06:22:14.635
technical lead for the 
TensorFlow Hub project. It is a 

06:22:14.636 --> 06:22:16.636
library and project for using 
machine learning.

06:22:18.462 --> 06:22:22.790
We Open Sourced it last year. 
Today I will give you an update 

06:22:22.791 --> 06:22:26.824
on the project. Let's start by 
talking about when you want to 

06:22:26.825 --> 06:22:30.008
use it. 
If you have problems collecting 

06:22:30.009 --> 06:22:32.009
enough 

06:22:34.817 --> 06:22:38.623
data to train models from 
scratch than this is for you. 

06:22:38.624 --> 06:22:43.142
Additionally, since it is so 
easy to reuse computations and 

06:22:43.143 --> 06:22:45.143
weights, it becomes possible to 
leverage features 

06:22:47.842 --> 06:22:49.842
without having to learn how to 
fit them into neural networks.

06:22:52.276 --> 06:22:54.937
Images, text and videos are 
features you can use with a 

06:22:54.938 --> 06:22:57.235
single line of code. 
You might also have encountered 

06:22:57.236 --> 06:23:00.094
problems where codebases become 
really coupled 

06:23:03.185 --> 06:23:05.185
and experimentation becomes 
slower over time.

06:23:06.236 --> 06:23:09.085
By defining an artifact that 
does not depend on code, hub 

06:23:09.086 --> 06:23:11.086
allows for more 

06:23:12.152 --> 06:23:13.396
maintainable systems similar to 
how libraries help software 

06:23:13.397 --> 06:23:15.397
engineers.

06:23:16.511 --> 06:23:18.511
The main concept is this 
pre-trained 

06:23:19.815 --> 06:23:22.459
building blocks that we call 
modules. Typically we start by 

06:23:22.460 --> 06:23:26.556
training a large model with the 
right algorithm and data. From 

06:23:26.557 --> 06:23:28.603
this large model, we are 
interested in just a part which 

06:23:28.604 --> 06:23:30.604
is reusable.

06:23:31.688 --> 06:23:34.169
This is typically a bottleneck 
layer or disrooted 

06:23:34.170 --> 06:23:36.170
representation.

06:23:37.432 --> 06:23:39.432
Then we can package them into a 
saved 

06:23:41.453 --> 06:23:43.453
model that defines the 
commutation and 

06:23:44.738 --> 06:23:46.738
no longer depends on the 
original code. 

06:23:48.793 --> 06:23:51.844
You can share with web servers, 
cloud and artifact system. You 

06:23:51.845 --> 06:23:53.267
can bring the module back as a 
piece of a new model to solve a 

06:23:53.268 --> 06:23:55.268
new task.

06:23:57.833 --> 06:23:59.833
Since the model is defined by 

06:24:01.899 --> 06:24:03.899
TensorFlow primitives you can 
fine tune 

06:24:06.630 --> 06:24:09.485
the waits  weights and adjust 
them. For the last few months, 

06:24:09.486 --> 06:24:13.980
we have been making it easier to
use. This concept of saving a 

06:24:13.981 --> 06:24:17.691
part of the model and loading it
back is getting integrated right

06:24:17.692 --> 06:24:19.692
in the core of TensorFlow.

06:24:20.968 --> 06:24:23.620
We have added saveModel features
that make it possible to share 

06:24:23.621 --> 06:24:25.621
more than just 

06:24:26.881 --> 06:24:28.881
signatures and with eager 
execution it 

06:24:30.386 --> 06:24:31.982
becomes even easier to select 
the part the network that gets 

06:24:31.983 --> 06:24:35.504
exports. Let's look at a few 
examples. In TensorFlow 2.

06:24:42.662 --> 06:24:47.024
0, we can load a module with 
hub.load. We can use tf.saved 

06:24:47.025 --> 06:24:49.025
model.

06:24:50.313 --> 06:24:52.979
load if it is on the system. 
Once loaded, we can call it 

06:24:52.980 --> 06:24:54.980
right away.

06:24:57.899 --> 06:25:00.544
Due to the new capabilites you 
can share any object composed of

06:25:00.545 --> 06:25:05.260
TensorFlow primitives. Text 
features has two members. Call 

06:25:05.261 --> 06:25:07.942
which is a TFX function and 
embedding which is a TF 

06:25:07.943 --> 06:25:10.403
variable. We are really excited 
that we added 

06:25:13.747 --> 06:25:17.264
support for polymorphic 
functions on saved model. This 

06:25:17.265 --> 06:25:19.265
will provide a more natural 

06:25:21.152 --> 06:25:23.385
interface than we had before 
with signatures. Here we see an 

06:25:23.386 --> 06:25:25.386
image representation 

06:25:28.396 --> 06:25:30.859
module being loaded and being 
used in inference mode or used 

06:25:30.860 --> 06:25:34.731
during training mode where batch
mode is on. When it is on, we 

06:25:34.732 --> 06:25:36.732
can control some of its 
parameters.

06:25:41.317 --> 06:25:43.176
This is just backed by TF graphs
so we no longer need to be 

06:25:43.177 --> 06:25:45.177
selecting things here and there.

06:25:49.135 --> 06:25:51.135
We just get the API that looks 
very like Python. 

06:25:53.875 --> 06:25:57.790
Additionally, we have added the 
new symbol. Hub.keraslayer. This

06:25:57.791 --> 06:25:59.791
makes integrating hub models 
into a keras model easier.

06:26:02.827 --> 06:26:04.827
In this example, we see how easy
it is 

06:26:06.728 --> 06:26:08.393
to build a text sentence -- a 
sentence classification model. 

06:26:08.394 --> 06:26:10.394
We have three layers.

06:26:14.840 --> 06:26:16.840
The top model is this keras 
layer, is a 

06:26:18.828 --> 06:26:21.479
layer that receives inputs and 
outputs representation. Then we 

06:26:21.480 --> 06:26:23.480
have a dense layer and a 
classification layer.

06:26:29.888 --> 06:26:31.739
Since the keras layer with this 
MLM layer, we can just feed 

06:26:31.740 --> 06:26:34.605
sentences straight into the 
model. We never have to define 

06:26:34.606 --> 06:26:36.606
the logic. 

06:26:38.476 --> 06:26:40.476
If we wanted to try other text 
models we 

06:26:41.761 --> 06:26:43.761
could just change that string.

06:26:45.576 --> 06:26:47.576
That is the easiest way to try 
the new research.

06:26:50.567 --> 06:26:54.635
We have released 0.3 version of 
TensorFlow Hub. We have hub.load

06:26:54.636 --> 06:26:58.966
and hub.keraslayer and they are 
usable in TensorFlow 2.0 both in

06:26:58.967 --> 06:27:00.967
eager and in graph mode.

06:27:04.021 --> 06:27:06.021
To let you preview this 

06:27:07.933 --> 06:27:09.822
functionality, we have published
modules in this format and the 

06:27:09.823 --> 06:27:11.823
next step for us is to backport 
existing models.

06:27:13.870 --> 06:27:15.870
A bit more practical now.

06:27:19.224 --> 06:27:21.224
Google AI and DeepMind teams 
have been 

06:27:22.934 --> 06:27:24.934
sharing information on 
tfhub.dev.

06:27:27.050 --> 06:27:29.050
This was launched last year.

06:27:30.083 --> 06:27:32.083
One of the most popular was the 
universal sentence encoder.

06:27:34.985 --> 06:27:37.231
This encoded short sentences 
into a vector that could be used

06:27:37.232 --> 06:27:39.232
for many natural language tasks.

06:27:42.749 --> 06:27:45.012
The team added a cross-lingual 
version of these. Sentences with

06:27:45.013 --> 06:27:47.013
similar meaning, 

06:27:51.612 --> 06:27:54.883
independent of the language, end
up in points close together. You

06:27:54.884 --> 06:27:57.350
can learn the classifier using 
English data and you could run 

06:27:57.351 --> 06:27:59.351
it on other languages.

06:28:04.909 --> 06:28:06.909
We have also added image 
augmentation models.

06:28:08.162 --> 06:28:10.839
They have been shown to transfer
to new tasks. An interesting 

06:28:10.840 --> 06:28:12.840
thing is you could grab 

06:28:14.521 --> 06:28:15.742
this module and one of the image
representation modules and you 

06:28:15.743 --> 06:28:18.617
could string them together. The 
image augmentation model would 

06:28:22.309 --> 06:28:23.310
reduce the amount of data by 
data augmentation and the image 

06:28:23.311 --> 06:28:25.311
feature 

06:28:26.321 --> 06:28:28.693
vector by transfer learning. 
There are many more. 

06:28:40.348 --> 06:28:43.514
We have BERT model for tasks, we
have biggan models and more. 

06:28:43.515 --> 06:28:46.355
Some were designed especially 
for low resources.

06:28:50.383 --> 06:28:52.005
Additionally, we have been 
working to make the modules more

06:28:52.006 --> 06:28:54.006
integrated.

06:28:57.133 --> 06:28:59.133
We have a common line utility to

06:29:00.727 --> 06:29:02.727
convert the hub model into TF 
model.

06:29:05.233 --> 06:29:07.233
It can be used with a library 
for ML 

06:29:08.749 --> 06:29:10.749
and they can also be used inside
TF transform.

06:29:15.460 --> 06:29:17.460
If you want to try it you can go
to

06:29:20.304 --> 06:29:24.326
tfhub.dev. Most of them include 
the link where you can see them 

06:29:24.327 --> 06:29:26.327
in action. Thank you.

06:29:31.481 --> 06:29:33.481
Thank you very much. OK.

06:29:36.390 --> 06:29:38.058
Now you have all learned 
hopefully training neural 

06:29:38.059 --> 06:29:40.059
networks that if I show up there
is food.

06:29:41.838 --> 06:29:42.892
So, we are going to come back 
here at 4:45 to do a little wrap

06:29:42.893 --> 06:29:46.972
up. The wrap up session is going
to be amazing not that this 

06:29:46.973 --> 06:29:51.467
wasn't also amazing. We will be 
talking about probablyability, 

06:29:51.468 --> 06:29:53.931
reinforcement learning, we will 
be talking about -- Elena is 

06:29:55.556 --> 06:29:57.604
coming back to tell us about her
stuff. It is going to be great. 

06:29:57.605 --> 06:30:02.522
After that there is a party. 
Please stay with us. Come back. 

06:30:02.523 --> 06:30:08.059
Don't forget to go upstairs and 
check out the demos. Thank you 

06:30:08.060 --> 06:30:10.060
so much.

07:30:19.614 --> 07:30:21.614
Aida net

07:30:32.050 --> 07:30:34.050
et

07:30:42.885 --> 07:30:44.885
AdaNet

07:30:48.387 --> 07:30:50.387
da net

07:31:00.423 --> 07:31:02.423
auto M L

07:41:04.183 --> 07:41:06.183
test test test test

07:42:32.890 --> 07:42:34.890
Extreme weather is changing. 

07:42:37.120 --> 07:42:39.120
There is more extreme rainfall,

07:42:41.244 --> 07:42:43.244
heavy flooding, forest fires. 

07:42:44.744 --> 07:42:46.744
Being able to predict these 
extreme 

07:42:47.782 --> 07:42:49.782
events more accurately is the 
biggest changes we are facing.

07:42:52.893 --> 07:42:54.519
There is 100 Tarabytes every 
day. Climate data is a big 

07:42:54.520 --> 07:42:57.306
problem.
We need things that are fast and

07:42:57.307 --> 07:42:59.537
can sift through the data 
accurately and rapidly.

07:43:04.027 --> 07:43:06.027
Deep learning is poised for 
problems in climate science.

07:43:10.325 --> 07:43:12.325
A lot of

07:43:16.063 --> 07:43:18.575
NERSC are using TensorFlow. We 
use TensorFlow to layer and to 

07:43:20.611 --> 07:43:23.252
create the deep learning model, 
we started from segmentation 

07:43:23.253 --> 07:43:25.253
models which 

07:43:28.558 --> 07:43:30.558
have proven to be successful and
we use 

07:43:32.218 --> 07:43:34.218
TensorFlow to enhance the models
until 

07:43:35.930 --> 07:43:37.589
we found a set of models that 
perform well enough for the 

07:43:37.590 --> 07:43:39.590
specific task.

07:43:40.711 --> 07:43:43.180
The network required 14 
teraflops. If you want to do 

07:43:43.181 --> 07:43:45.181
this on your work 

07:43:47.460 --> 07:43:49.460
station, it would take months to
train. 

07:43:50.107 --> 07:43:52.155
It takes the largest 
computational resources on the 

07:43:52.156 --> 07:43:55.834
planet. Two tennis courts in 
size, this computer is 

07:43:55.835 --> 07:43:58.693
state-of-the-art and a million 
times faster than your common 

07:43:58.694 --> 07:44:00.694
laptop. 3.3 exaflops.

07:44:03.846 --> 07:44:05.683
Imagine what you do at your work
station and imagine having 

07:44:05.684 --> 07:44:07.684
27,000 times that power. We can 
do that now. 

07:44:10.576 --> 07:44:12.576
We were surprised how good it 
scales.

07:44:13.651 --> 07:44:15.651
2,000 nodes, 5,000 nodes. 

07:44:17.533 --> 07:44:19.533
This was the first time anyone 
has ran 

07:44:20.812 --> 07:44:22.812
an AI application at this scale.

07:44:25.298 --> 07:44:27.298
The climate scientists could 
express 

07:44:28.577 --> 07:44:30.624
things in Python in TensorFlow 
and get all the code people are 

07:44:30.625 --> 07:44:32.625
used to.

07:44:34.502 --> 07:44:36.502
We are entering a state where AI
can 

07:44:37.606 --> 07:44:39.606
contribute to the predictions of
these 

07:44:40.826 --> 07:44:43.547
extreme weather events. 
We can tackle things we never 

07:44:43.548 --> 07:44:48.810
thought. Understanding diseases 
like Alzheimer's and cancer. 

07:44:48.811 --> 07:44:52.965
That is like incredible. 
We have shown with a PRAM work 

07:44:52.966 --> 07:44:54.966
like 

07:44:56.467 --> 07:45:00.133
TensorFlow you can get to 
massive scale and accomplish 

07:44:56.467 --> 07:44:58.467
goals.

07:44:59.526 --> 07:45:01.526
Genetics, neuroscience, physics 
-- that 

07:45:05.276 --> 07:45:07.276
is immensely exciting for me.

07:45:13.585 --> 07:45:15.585
-- framework --

07:45:24.137 --> 07:45:26.137
[Applause] 
Hello, everyone.

07:45:28.634 --> 07:45:29.860
I am Josh Dillon and I am a lead
on the TensorFlow Probability 

07:45:29.861 --> 07:45:33.366
team and today I will talk about
probability stuff and how it 

07:45:33.367 --> 07:45:37.424
relates to TensorFlow stuff. 
Let's find out what that means. 

07:45:37.425 --> 07:45:39.425
OK.

07:45:41.176 --> 07:45:43.627
These days machine learning 
often means specifying deep 

07:45:43.628 --> 07:45:45.628
model architectures and 

07:45:47.709 --> 07:45:49.213
fitting them under loss and 
keras makes specifying this 

07:45:49.214 --> 07:45:51.214
architecture easily but what 
about the loss? 

07:45:52.472 --> 07:45:54.472
Choosing the right loss is 
tough.

07:45:57.771 --> 07:45:59.771
Improving even a reasonable one 
it be tougher. 

07:46:00.814 --> 07:46:02.814
Once you fit the model, how do 
you know it is good?

07:46:07.343 --> 07:46:09.181
Why not use entropy or mode? 
Wouldn't it be nice if there was

07:46:09.182 --> 07:46:11.819
a plug and play with keras and 
the rest of TF model?

07:46:14.889 --> 07:46:16.889
This would make comparing models
easier 

07:46:18.584 --> 07:46:20.584
by maximizing likelihood and 
having as 

07:46:21.593 --> 07:46:26.549
it SKT -- rapidly available 
statistics we can reject the bad

07:46:26.550 --> 07:46:28.550
ones and accept the good ones. 

07:46:30.644 --> 07:46:33.302
Wouldn't it be great to say I 
want to maximize the log 

07:46:33.303 --> 07:46:36.806
likelihood and summarize what I 
learned easily and in a unified 

07:46:36.807 --> 07:46:40.086
way. Let's play with that idea. 
Here we have a dataset.

07:46:44.403 --> 07:46:46.403
These blue dots and our task is 
to 

07:46:48.276 --> 07:46:50.276
predict, pretend task, is to 
predict the 

07:46:51.333 --> 07:46:53.333
Y coordinate from the X 
coordinate.

07:46:54.782 --> 07:46:56.782
You might do this by specifying 
a deep 

07:46:57.815 --> 07:46:59.307
model and choosing the mean 
squared error as the loss 

07:46:59.308 --> 07:47:01.308
function.

07:47:06.827 --> 07:47:08.827
But our wish here is to think 
probalistically and that means 

07:47:11.860 --> 07:47:12.867
maximizing the log likelihood 
like here with this Lambda 

07:47:12.868 --> 07:47:17.858
function. We want to get back a 
distribution, a thing that has 

07:47:17.859 --> 07:47:19.908
attached to it statistics we can
use to evaluate what we just 

07:47:19.909 --> 07:47:21.909
learned.

07:47:23.023 --> 07:47:25.023
If only such a thing were 
probably.

07:47:27.339 --> 07:47:29.586
Of course, it is and you can do 
this now. Using TensorFlow 

07:47:29.587 --> 07:47:31.587
Probability 

07:47:33.055 --> 07:47:35.055
distribution layers you can 
specify the 

07:47:36.743 --> 07:47:39.617
model as part of your deep net. 
The loss now is actually part of

07:47:39.618 --> 07:47:42.264
the model, sort of the way it 
used to be, 

07:47:46.282 --> 07:47:50.627
the way it is meant to be. We 
have two dense layers.

07:47:54.889 --> 07:47:56.889
The second one outputs one float
that 

07:47:58.171 --> 07:48:00.171
is parameterizing a normal 
distrubution 

07:48:01.243 --> 07:48:03.492
mean through this distribution 
Lambda layer. Being we are able 

07:48:03.493 --> 07:48:05.493
to find this line, 

07:48:08.353 --> 07:48:11.218
that looks great, and the best 
part is once we instantite this 

07:48:11.219 --> 07:48:13.901
model with test points we have 
back a distribution for 

07:48:17.369 --> 07:48:19.211
which you get not just the mean 
but intropy variance and 

07:48:19.212 --> 07:48:21.212
standard deviation and all these
things. 

07:48:22.884 --> 07:48:24.884
You can compare between this and
other 

07:48:26.324 --> 07:48:28.164
distributions as we will see 
later -- entropy. If we look at 

07:48:28.165 --> 07:48:32.237
the data, something is fishy 
here, right? Notice as the 

07:48:32.238 --> 07:48:34.238
magnitude of X increases, 

07:48:35.239 --> 07:48:37.239
the

07:48:44.758 --> 07:48:49.087
variance of my increases also. 
What can we do to fix this? 

07:48:49.088 --> 07:48:51.088
Answer? Learn the variance too. 

07:48:53.337 --> 07:48:55.337
If we are fitting a normal why 
on earth 

07:48:56.371 --> 07:48:57.417
do we think the variance would 
be one? That is what you are 

07:48:57.418 --> 07:48:59.418
doing when you use mean squared 
error.

07:49:01.668 --> 07:49:04.336
To achieve this, all I had to do
was make my previous layer 

07:49:04.337 --> 07:49:06.337
output two floats. Passing one 
to the mean as normal, one 

07:49:10.387 --> 07:49:12.251
in as the standard deviation of 
the normal, and press no change 

07:49:12.252 --> 07:49:14.252
and now I 

07:49:16.301 --> 07:49:17.955
have learned the standard 
deviation from the data itself. 

07:49:17.956 --> 07:49:19.956
That is what the green lines 
are.

07:49:25.627 --> 07:49:27.627
This is cool because if you are 
a

07:49:33.635 --> 07:49:35.635
statistician, it means you are 
learning known unknowns.

07:49:38.155 --> 07:49:40.155
The data had variance and you 
learned 

07:49:41.422 --> 07:49:43.876
it and it cost you basically 
nothing to do but a few key 

07:49:43.877 --> 07:49:48.779
strokes. The way you saw to do 
this was self evident from the 

07:49:48.780 --> 07:49:50.780
fact you were using a normal 
distribution for a curious 

07:49:51.655 --> 07:49:53.655
constant sitting there.

07:49:55.967 --> 07:49:57.588
This is good but I don't know is
there enough data for which we 

07:49:57.589 --> 07:49:59.589
can claim this 

07:50:00.844 --> 07:50:02.844
red line is the mean and the 
green lines 

07:50:04.818 --> 07:50:07.792
are the standard deviation? How 
would we know? Is there anything

07:50:07.793 --> 07:50:11.246
else we can do? Of course there 
is. A keras dense layer has two 

07:50:11.247 --> 07:50:14.111
components. A kernel matrix and 
a bias vector.

07:50:17.766 --> 07:50:19.595
What makes you think those point
estimates are the best 

07:50:19.596 --> 07:50:22.239
especially given your dataset 
itself is random and 

07:50:28.184 --> 07:50:30.806
possibly inadequate to 
meaningfully and reliable learn 

07:50:30.807 --> 07:50:33.253
those estimates. You can learn a
distribution over 

07:50:38.578 --> 07:50:40.578
weight, this is the same as 
learning an 

07:50:43.591 --> 07:50:45.591
ensemble that is 

07:50:48.322 --> 07:50:50.322
infinitely large.

07:50:54.311 --> 07:50:56.311
As you can see, all I had to do 
was replace keras.

07:51:02.031 --> 07:51:04.031
dense with with dense variation.

07:51:06.939 --> 07:51:09.376
The keyword here -- I like to 
think of the unknowns unknowns.

07:51:14.881 --> 07:51:16.881
As a consequence, of course,this
means 

07:51:18.136 --> 07:51:20.559
any model or instantiation is a 
random variable because the 

07:51:20.560 --> 07:51:22.415
weight is a random available. 
That is why you see all the 

07:51:22.416 --> 07:51:24.416
lines, there 

07:51:25.667 --> 07:51:27.301
are many lines, we have an 
ensemble of them but if you 

07:51:27.302 --> 07:51:29.177
averaged them and took the 
standard deviation that would 

07:51:29.178 --> 07:51:34.367
give you an estimate of credible
intervals over your prediction 

07:51:34.368 --> 07:51:36.368
and you can go to 

07:51:38.175 --> 07:51:40.447
your customer and say here is 
what I think will happen and 

07:51:40.448 --> 07:51:42.448
this is how much you can trust 
me.

07:51:50.068 --> 07:51:52.068
We lost the head

07:51:56.191 --> 07:51:59.025
-- I just output two floats and 
feed that into my output layer 

07:51:59.026 --> 07:52:01.912
and press no change. I am 
learning both known and unknown 

07:52:05.597 --> 07:52:07.597
knowns and all it cost me was a 
few key strokes. 

07:52:10.372 --> 07:52:12.372
So what you see here is an 
ensemble of 

07:52:14.897 --> 07:52:17.342
standard deviations associated 
with the known-unknown parts, 

07:52:17.343 --> 07:52:19.586
the variances observable in the 
Y axis, as well as a 

07:52:23.085 --> 07:52:26.761
number or ensemble of these mean
regressions. OK. That's cool. I 

07:52:26.762 --> 07:52:29.055
like where this is going but I 
have to ask what makes you think

07:52:29.056 --> 07:52:34.373
a line is even the right thing 
to fit here? Is there another 

07:52:34.374 --> 07:52:37.240
distribution we could chose? A 
richer distribution that would 

07:52:39.889 --> 07:52:41.889
actually find the right form of 
the data?

07:52:43.559 --> 07:52:45.559
Of course the answer is yes.

07:52:46.976 --> 07:52:48.976
It is a

07:52:52.815 --> 07:52:56.845
Galsian process.
The

07:53:01.399 --> 07:53:03.435
Gaussian process can see this. 
How can you do that if you are 

07:53:06.517 --> 07:53:08.517
specifying mean squared error as
your loss? You can't. 

07:53:09.827 --> 07:53:11.827
It has to be part of the model. 

07:53:14.112 --> 07:53:16.112
When you bake these ideas into 
one model 

07:53:17.368 --> 07:53:18.811
you get to move it around 
fluidly between weight 

07:53:18.812 --> 07:53:21.496
uncertainty and variance in the 
data and even uncertainty in the

07:53:24.361 --> 07:53:26.361
loss function you are fitting 
itself.

07:53:29.656 --> 07:53:31.656
So the question is, how can this
all be so easy?

07:53:33.734 --> 07:53:37.805
How did it fit together? With 
TensorFlow Probability. 

07:53:37.806 --> 07:53:39.806
TensorFlow Probability is a 
collection 

07:53:41.516 --> 07:53:42.331
of tools designed to make 
probalistic reasoning in 

07:53:42.332 --> 07:53:46.209
TensorFlow easier. It is not 
going to make your job easy. It 

07:53:46.210 --> 07:53:48.256
is just going to give you the 
tools you need to express the 

07:53:48.257 --> 07:53:50.298
ideas you have. You still have 
to have domain knowledge 

07:53:53.503 --> 07:53:56.092
and expertise but you can encode
that in a 

07:53:59.291 --> 07:54:01.291
probalistic 

07:54:02.817 --> 07:54:04.817
formulism and TensorFlow Hub has
the way to do that.

07:54:07.117 --> 07:54:09.117
Gone are the days of hacking a 
model in 

07:54:16.445 --> 07:54:19.344
R and importing it to a faster 
language. If you predict the 

07:54:19.345 --> 07:54:21.345
light is green, you 

07:54:22.608 --> 07:54:24.608
better be confident you should 
go.

07:54:27.942 --> 07:54:29.983
You can do that with probalistic
modeling and TensorFlow 

07:54:29.984 --> 07:54:31.984
Probability.

07:54:35.337 --> 07:54:37.568
We saw one small part of TFP. 
The tools are broken into two 

07:54:37.569 --> 07:54:39.569
components. 

07:54:43.701 --> 07:54:45.701
Those tools use FWL -- for 
building 

07:54:49.974 --> 07:54:51.974
models and those useful for do 
analyzing.

07:54:54.542 --> 07:54:56.542
It is summary statistics like it
is in other libraries.

07:55:00.692 --> 07:55:02.140
Our distributions support batch 
shape but for the most part they

07:55:02.141 --> 07:55:06.253
should be pretty natural and 
easy to use. We also have 

07:55:06.254 --> 07:55:08.495
something we call bijectors 
which is a library for 

07:55:10.140 --> 07:55:13.234
transforming random variables. 
This can be like taking the X of

07:55:13.235 --> 07:55:15.235
a normal and now you have a log 
normal.

07:55:19.131 --> 07:55:21.131
In comply

07:55:22.251 --> 07:55:24.251
-- complicated cases it is like 

07:55:27.574 --> 07:55:29.574
mascots, regressive flows, MVPs 
and 

07:55:30.595 --> 07:55:32.595
other

07:55:38.642 --> 07:55:41.344
sophisticated probalistic 
models. Edward2 helps you 

07:55:41.345 --> 07:55:43.345
combine different variables as 
one.

07:55:47.874 --> 07:55:50.511
On the inference side, no 
library would be compete without

07:55:50.512 --> 07:55:52.512
Monte Carlo tools.

07:55:55.408 --> 07:55:57.408
We have transitions components 
one is 

07:56:03.398 --> 07:56:04.625
called Markov chain Monte Carlo 
and variational inference tools.

07:56:11.351 --> 07:56:13.351
And of course we have our own 
optimizers that come up.

07:56:16.157 --> 07:56:17.789
The point is this toolbox has 
maybe not everything but 

07:56:17.790 --> 07:56:19.790
certainly it has most of 

07:56:24.075 --> 07:56:26.859
what you might need to do 
fancier modeling to get more. It

07:56:26.860 --> 07:56:28.860
doesn't have to be hard.

07:56:30.136 --> 07:56:34.137
You saw the keras examples were 
sequences of one-line changes. 

07:56:34.138 --> 07:56:36.450
Of course, TensorFlow 
Probability is used widely 

07:56:36.451 --> 07:56:38.893
around alphabet. DeepMind and 
Google brain and Google 

07:56:40.950 --> 07:56:42.787
accelerated sciences, product 
areas, infrastructure areas use 

07:56:42.788 --> 07:56:44.788
it for planning 

07:56:46.051 --> 07:56:48.051
purposes, but it is also used 
outside of Google. 

07:56:49.693 --> 07:56:51.693
Baker

07:57:00.315 --> 07:57:02.315
Hughes uses this to predict 
anomalies.

07:57:04.213 --> 07:57:06.213
For example, anyone who flew out
here 

07:57:08.505 --> 07:57:10.951
would be happy to know that bhge
uses their prediction to predict

07:57:10.952 --> 07:57:12.952
the lifespan of jet engines.

07:57:15.443 --> 07:57:17.472
If we had a dataset using a 
failing engine that would be a 

07:57:17.473 --> 07:57:20.745
tragedy. 
Using math they get around this 

07:57:20.746 --> 07:57:22.746
by 

07:57:24.884 --> 07:57:26.884
modeling models and then trying 
to in 

07:57:28.612 --> 07:57:30.612
the abstract figure out if they 
are going to be good models.

07:57:32.920 --> 07:57:34.920
The orange boxes use TensorFlow 
Probability extensively.

07:57:37.849 --> 07:57:39.849
The orange border box is where 
they use TensorFlow.

07:57:41.532 --> 07:57:43.774
The basic flow is to try to 
treat the model itself as a 

07:57:43.775 --> 07:57:47.871
random available and determine 
if it will be a good model on an

07:57:47.872 --> 07:57:49.872
otherwise incomplete dataset. 

07:57:51.972 --> 07:57:53.972
From this they get remarkable 
results 

07:57:55.635 --> 07:57:57.887
and dramatic increases in false 
positives and false negatives 

07:57:57.888 --> 07:57:59.888
over large 

07:58:01.554 --> 07:58:04.410
datasets in complicated systems.
So the question is who will be 

07:58:04.411 --> 07:58:09.309
the next success story? Try it 
out. It is an open source Python

07:58:09.310 --> 07:58:11.363
package built using TensorFlow 
that makes it 

07:58:14.613 --> 07:58:17.466
easy to combine deep learning 
with probalistic models.

07:58:20.781 --> 07:58:23.446
You can pip install it. 
If you are interested in 

07:58:23.447 --> 07:58:25.447
learning more 

07:58:26.657 --> 07:58:28.657
about the

07:58:35.065 --> 07:58:37.073
bayessian methods, check out 
thus  this book.

07:58:40.088 --> 07:58:44.810
If you are not a bayesian, that 
is OK too. If you are doing 

07:58:44.811 --> 07:58:46.875
linear aggression, it could 
solve the problem on the order 

07:58:46.876 --> 07:58:52.611
of 30 iterations which can't be 
said of standard gradient 

07:58:52.612 --> 07:58:54.612
dissent.

07:58:56.507 --> 07:58:58.507
If you want to find out more, 
you can 

07:58:59.559 --> 07:59:01.576
check out our GitHub repository 
where you can find several 

07:59:01.577 --> 07:59:05.271
Jupyter notebooks. Thanks. 
[Applause] 

07:59:07.914 --> 07:59:09.914
And with that, I would like to 
hand it 

07:59:11.814 --> 07:59:13.814
off to Eugene and Sergio who 
will be 

07:59:14.896 --> 07:59:15.500
talking about reinforcement 
learning. 

07:59:15.501 --> 07:59:17.501
Thank you.

07:59:18.526 --> 07:59:24.261
Hello, everyone. How is it 
going? Very packed day? Lots of 

07:59:24.262 --> 07:59:31.829
good stuff? Are you ready for 
more? Or not? OK. Good. So my 

07:59:31.830 --> 07:59:34.068
name is Sergio Guadarrama and I 
am a software engineer at Google

07:59:34.069 --> 07:59:36.069
brain 

07:59:39.156 --> 07:59:41.156
and I work in the TF agent 
steam. 

07:59:45.063 --> 07:59:47.063
And I am Eugene Brevdo working 
on reinforcement learning. 

07:59:49.531 --> 07:59:51.531
How many of you remember how you
learned how to walk?

07:59:53.391 --> 07:59:55.437
You stumble a little bit, you 
try one step, doesn't work, you 

07:59:55.438 --> 07:59:57.438
lose your balance, you try 
again.

08:00:02.579 --> 08:00:04.718
When you try to learn something 
hard practice you need to try 

08:00:04.719 --> 08:00:07.578
many times. This is moving the 
legs and trying to 

08:00:11.048 --> 08:00:13.048
learn how to walk.

08:00:15.355 --> 08:00:17.355
After learning to try multiple 
times, in 

08:00:21.851 --> 08:00:23.085
this case is  1,000 times, it 
falls. It will train a little 

08:00:23.086 --> 08:00:25.086
longer and then 

08:00:26.259 --> 08:00:27.689
it is able to walk around and go
from one place to another and 

08:00:27.690 --> 08:00:30.329
find their way around the room.

08:00:34.016 --> 08:00:36.016
You have probably heard about 
all the 

08:00:37.920 --> 08:00:39.920
applications including 
recommended 

08:00:42.878 --> 08:00:44.878
assistance, data -- 

08:00:46.139 --> 08:00:48.358
real robots like that, chemistry
and math and always alpha go 

08:00:48.359 --> 08:00:51.423
that play Go better than any 
human. Now, I have a question 

08:00:51.424 --> 08:00:53.424
for you.

08:00:56.127 --> 08:00:58.154
How many of you have tried to 
auto implement an algorithm? I 

08:00:58.155 --> 08:01:01.848
see quite a bit of hands. Very 
good. It is hard. 

08:01:01.849 --> 08:01:05.734
Yeah. We went through that pain 
too. You know, many people who 

08:01:05.735 --> 08:01:10.213
try get the first prototype 
right away. It seems to be 

08:01:10.214 --> 08:01:12.852
working but then you miss a lot 
of different pieces.

08:01:20.409 --> 08:01:22.409
All the details have to be 
right, everything.

08:01:23.451 --> 08:01:26.292
There is a lot of pieces and a 
lot of things you need to do. We

08:01:26.293 --> 08:01:28.293
suffer through the same problem 
at 

08:01:30.173 --> 08:01:32.990
Google and decided to implement 
a library that many people can 

08:01:32.991 --> 08:01:37.306
use and we are happy to announce
it is available online. You can 

08:01:37.307 --> 08:01:39.307
go to GitHub and pip install it 

08:01:40.364 --> 08:01:41.578
and start using it right away. 
Hopefully, you will provide 

08:01:41.579 --> 08:01:45.287
feedback contributions so we can
make it better over time.

08:01:50.138 --> 08:01:52.138
Now, where is

08:01:55.259 --> 08:01:57.259
TF age NLT

08:01:59.775 --> 08:02:02.834
-- agents? 
For people new, we have Colabs 

08:02:02.835 --> 08:02:06.535
and things with documentation 
and samples so you can learn 

08:02:06.536 --> 08:02:08.579
about it. For people that want 
to solve the 

08:02:11.729 --> 08:02:14.369
complex problems, we have 
already ways to take their 

08:02:14.370 --> 08:02:16.821
algorithm and apply it. 
For people who are researching 

08:02:16.822 --> 08:02:18.822
and want 

08:02:20.624 --> 08:02:22.624
to develop new algorithm

08:02:24.797 --> 08:02:25.560
they will not need to build all 
the pieces. They can build on 

08:02:25.561 --> 08:02:27.561
top of it.

08:02:28.667 --> 08:02:30.667
You can start doing experiments 
right away.

08:02:35.399 --> 08:02:37.399
We build on top of the goodies 
in TensorFlow 2.0 you saw today.

08:02:38.311 --> 08:02:41.705
Tf.keras to build the networks 
and tf.functions when you want 

08:02:41.706 --> 08:02:44.143
thinks to go faster and we make 
it mobile and 

08:02:48.404 --> 08:02:51.654
extensible so you can cherrypick
the pieces you need. For those 

08:02:51.655 --> 08:02:53.655
who are not ready for the 

08:02:55.387 --> 08:02:58.072
change yet, we make it 
compatible with TensorFlow 1.14.

08:03:00.727 --> 08:03:04.390
We go back to the sample of the 
little robot trying to work. 

08:03:04.391 --> 08:03:07.039
This is how the call looks like.
You have it define networks.

08:03:14.029 --> 08:03:16.029
An actor distribution network 
and a criteriaic and actor.

08:03:19.053 --> 08:03:21.958
Assuming we have data collected 
we can just train through it.

08:03:29.808 --> 08:03:31.808
TF-agents provide a lot of 
environments already.

08:03:34.938 --> 08:03:36.938
We also provide a 
state-of-the-art 

08:03:39.242 --> 08:03:40.256
algorithms including dqn, and 
many others. 

08:03:40.257 --> 08:03:42.292
There are more coming soon. They
are fully tested with quality 

08:03:45.544 --> 08:03:48.418
version tests and a speed test 
to make things keep working. As 

08:03:48.419 --> 08:03:53.390
an overview of the system, it 
looks like that. On the left 

08:03:53.391 --> 08:03:56.465
side you have all the collection
aspects and we will have policy 

08:03:56.466 --> 08:03:57.689
that will interact with 
environments and collects on 

08:03:57.690 --> 08:04:01.758
experience. We probably put in 
some replay buffer to work later

08:04:01.759 --> 08:04:05.456
and on the right side we have 
all the training pipeline. We 

08:04:05.457 --> 08:04:06.879
are going to write from this 
experience and our agent is 

08:04:06.880 --> 08:04:08.880
going to 

08:04:10.550 --> 08:04:14.056
learn to improve the policy by 
training a neural network. Let's

08:04:14.057 --> 08:04:16.057
focus for a little bit on this 
environment. 

08:04:16.748 --> 08:04:17.593
How we define a problem and new 
task. Let's take another 

08:04:17.594 --> 08:04:21.047
example. In this case it is 
breakout. The idea is you have 

08:04:21.048 --> 08:04:25.029
to play this game move the power
left and right and try to break 

08:04:25.030 --> 08:04:29.917
the bricks on the top. You break
the bricks you get rewards and 

08:04:29.918 --> 08:04:31.918
the points go up.

08:04:34.000 --> 08:04:36.000
You let the ball drop and the 
points go down.

08:04:37.887 --> 08:04:39.722
The agent refers multiple frames
and it will decide which action 

08:04:39.723 --> 08:04:43.596
to take and based on that it 
will get reward and just loop 

08:04:43.597 --> 08:04:46.583
again. How this look into the 
code is something like this.

08:04:49.648 --> 08:04:51.648
You define the specific 
specification.

08:04:52.672 --> 08:04:54.699
What kind of observation this 
environment provides. Any other 

08:04:54.700 --> 08:04:56.700
information.
Multiple things.

08:05:00.596 --> 08:05:03.488
Then the action speckspecs. In 
this case left and right but in 

08:05:03.489 --> 08:05:07.643
many environments we have 
multiple actions. And a reset 

08:05:07.644 --> 08:05:09.862
method because we will play the 
game a lot and have to reset the

08:05:11.934 --> 08:05:14.048
environment and a step method 
that taking an action will 

08:05:14.049 --> 08:05:16.851
provide a new observation and 
give us a reward.

08:05:21.958 --> 08:05:23.228
Given that we define the policy,
create the environment, reset 

08:05:23.229 --> 08:05:25.229
the environment and start 
looking over it and start 

08:05:25.819 --> 08:05:28.460
playing the game. Your policy is
very good you will get a good 

08:05:28.461 --> 08:05:30.461
score.

08:05:31.549 --> 08:05:32.962
To make the learning even 
faster, we make these parallel 

08:05:32.963 --> 08:05:35.213
environments and you can run 
these games in parallel multiple

08:05:36.434 --> 08:05:38.434
times and wrap in TensorFlow so 
it will 

08:05:39.491 --> 08:05:41.530
go even faster and then do the 
same loop again. What happened 

08:05:41.531 --> 08:05:43.531
in general is like we 

08:05:45.337 --> 08:05:47.337
don't want to define this policy
by hand.

08:05:48.379 --> 08:05:50.824
Let me hand it over to Eugene 
who is going to explain how to 

08:05:50.825 --> 08:05:52.825
learn those policies. 
Thank you, Sergio.

08:05:57.439 --> 08:05:59.482
Yeah, as Sergio said, I am 
giving an example of how you 

08:05:59.483 --> 08:06:03.545
would interact with the 
environment by a policy. We are 

08:06:03.546 --> 08:06:06.397
going to go into the a little 
bit more detail and talk about 

08:06:06.398 --> 08:06:08.398
how to 

08:06:10.493 --> 08:06:12.493
train policies to maximize the 
rewards.

08:06:13.968 --> 08:06:15.968
So kind of going over it again, 

08:06:18.705 --> 08:06:20.295
policies take observations and 
emit parameters of a -- a 

08:06:20.296 --> 08:06:22.296
distribution over the actions.

08:06:24.765 --> 08:06:26.765
In this case, the 

08:06:29.853 --> 08:06:31.867
observations are an image, there
is an underlying neural network 

08:06:31.868 --> 08:06:33.960
that converts those images to 
the parameter of the 

08:06:35.997 --> 08:06:37.831
distribution and the policy 
emits that distribution or you 

08:06:37.832 --> 08:06:40.072
might sample from it to actually
take actions.

08:06:43.579 --> 08:06:45.579
So, let's talk about networks.

08:06:47.446 --> 08:06:48.939
I think you have seen some 
variation of this slide over and

08:06:48.940 --> 08:06:50.940
over again today.

08:06:52.837 --> 08:06:55.694
A network in this case network 
used for deep Q learning is 

08:06:55.695 --> 08:06:57.750
essentially a container for a 
bunch of keras layers.

08:07:01.113 --> 08:07:02.744
In this case, your inputs go 
through a convolutional layer 

08:07:02.745 --> 08:07:04.745
and so on and so 

08:07:10.063 --> 08:07:12.811
forth and the final layer emits 
logits over the actions you 

08:07:12.812 --> 08:07:18.101
might take. The core method of 
the network is the call. It 

08:07:18.102 --> 08:07:20.102
takes observations in a state, 

08:07:24.353 --> 08:07:26.008
possibly an RNN state, and emits
the logits and you update the 

08:07:26.009 --> 08:07:31.125
state. OK. So let's talk about 
policies. First of all, we 

08:07:31.126 --> 08:07:35.201
provide a large number of 
policies. Some of them 

08:07:35.202 --> 08:07:37.202
specifically tailored to 

08:07:38.790 --> 08:07:40.790
particular algorithms and 
particular 

08:07:41.820 --> 08:07:43.658
agents but you can also 
implement your own. It is useful

08:07:43.659 --> 08:07:45.659
to go through that.

08:07:47.741 --> 08:07:49.741
A policy takes one or more 
networks.

08:07:51.350 --> 08:07:53.316
The fundamental method on a 
policy is the distribution 

08:07:53.317 --> 08:07:55.317
method.

08:07:56.380 --> 08:07:58.380
This takes the time that is 
essentially 

08:08:01.001 --> 08:08:03.266
containing the observations, 
passes that through one or more 

08:08:03.267 --> 08:08:05.917
networks and emits the 
parameters of the output 

08:08:08.771 --> 08:08:10.771
distribution in this case 
logits.

08:08:12.276 --> 08:08:14.276
It then returns a tuple of three
things.

08:08:15.958 --> 08:08:18.446
The first is an actual 
distribution object. Josh spoke 

08:08:18.447 --> 08:08:20.689
about the probability and here 
is this low probability category

08:08:22.943 --> 08:08:25.581
called distribution built from 
the logits, it emits the next 

08:08:25.582 --> 08:08:30.273
state possibly containing RNN 
state information, and it also 

08:08:30.274 --> 08:08:32.274
emits site information.

08:08:34.636 --> 08:08:36.673
Site information is useful -- 
side information is useful 

08:08:36.674 --> 08:08:38.674
perhaps you want to emit 
information you want to login 

08:08:40.770 --> 08:08:42.647
your metrics and that is not the
action, or maybe you want to log

08:08:42.648 --> 08:08:45.945
some information that is 
necessarily for training later 

08:08:45.946 --> 08:08:47.946
on so the agent is going 

08:08:49.646 --> 08:08:51.646
to use that information to 
actually train. OK.

08:08:54.734 --> 08:08:56.734
So now let's actually talk about
training. 

08:09:00.296 --> 08:09:03.134
The agent class encompasses the 
main ML algorithm and that 

08:09:03.135 --> 08:09:05.135
includes the training 

08:09:08.808 --> 08:09:10.041
and reading batches of data and 
trajectries to update the neural

08:09:10.042 --> 08:09:15.752
network. Here is a simple 
example. First, you create a 

08:09:15.753 --> 08:09:17.753
deep queue learning 

08:09:20.222 --> 08:09:22.431
agent, you give it a network, 
you can access a policy 

08:09:22.432 --> 08:09:26.807
specifically a collection policy
from that agent, that policy 

08:09:26.808 --> 08:09:28.808
uses the underlying network that

08:09:30.100 --> 08:09:32.100
you passed in, and maybe 
performs some 

08:09:35.662 --> 08:09:37.708
additional work like maybe 
performs exploration. It also 

08:09:37.709 --> 08:09:39.733
logs site information that is 
going to be necessary to be able

08:09:39.734 --> 08:09:41.734
to train the agent.

08:09:44.470 --> 08:09:46.470
The main method on the agent is 
called train.

08:09:48.305 --> 08:09:50.305
It takes experience in the form 
of 

08:09:52.810 --> 08:09:54.810
batch 

08:09:58.342 --> 08:10:00.310
trajectories from a replay 
buffer. Assuming you have 

08:10:00.311 --> 08:10:02.311
trained the data and 

08:10:03.374 --> 08:10:06.212
performed well you might want to
take a policy that provides more

08:10:06.213 --> 08:10:11.117
greedy actions and just exploits
and takes the best actions it 

08:10:11.118 --> 08:10:13.118
thinks are the best and 

08:10:14.572 --> 08:10:17.042
doesn't log any side 
information. That is the plan 

08:10:17.043 --> 08:10:19.043
and policy.

08:10:21.980 --> 08:10:24.451
You can save this to a saved 
model and put it into the plain.

08:10:30.152 --> 08:10:32.152
For a complete example, here we 
have a 

08:10:34.874 --> 08:10:36.874
deep Q learning network.

08:10:38.148 --> 08:10:40.243
It has other arguments 
describing what kind of keras 

08:10:40.244 --> 08:10:43.318
layers to combine.
You build the agent with that 

08:10:43.319 --> 08:10:45.969
network. Then you get a tf.data 
dataset.

08:10:49.731 --> 08:10:51.773
In this case, you get it from a 
replay buffer object, but you 

08:10:51.774 --> 08:10:53.774
can get it from 

08:10:57.524 --> 08:10:59.155
any other dataset that emits the
correct batch trajectory 

08:10:59.156 --> 08:11:01.156
information and you 

08:11:04.060 --> 08:11:06.506
iterate over that dataset 
calling agent.train to update 

08:11:06.507 --> 08:11:08.507
the underlying neural networks 
that are updated in the 

08:11:08.744 --> 08:11:10.975
reflected policies. Let's talk a
little bit about collection. 

08:11:15.033 --> 08:11:17.033
Given a collection policy, it 
doesn't 

08:11:18.903 --> 08:11:20.123
have to be trained, it can have 
random parameters, you want to 

08:11:20.124 --> 08:11:25.025
be able to collect data and we 
provide a number of tools for 

08:11:25.026 --> 08:11:30.386
that, again, if your environment
is something that is in Python 

08:11:30.387 --> 08:11:32.387
you can wrap it.

08:11:34.380 --> 08:11:36.022
The core tool for this is the 
driver. Going through that first

08:11:36.023 --> 08:11:40.066
you create your batched 
environments at the top. Then 

08:11:40.067 --> 08:11:44.804
you create a replay buffer. In 
this case we have a tf.uniform 

08:11:44.805 --> 08:11:48.903
replay buffer. This is backed by
TensorFlow variables. Then you 

08:11:48.904 --> 08:11:50.904
create the driver.

08:11:53.008 --> 08:11:54.627
The driver accepts the 
environment and the collect 

08:11:54.628 --> 08:11:59.326
policy from the agent and a 
number of callbacks. When you 

08:11:59.327 --> 08:12:02.220
call driver.run what it will do 
is it will iterate, 

08:12:06.737 --> 08:12:08.737
in this case it will take 100 
steps 

08:12:10.595 --> 08:12:12.595
between a policy and 
environment, it 

08:12:16.655 --> 08:12:18.655
will create trajecto
trajectories.

08:12:22.009 --> 08:12:24.009
After the buffer finished, you 
have 100 more frames of data.

08:12:27.097 --> 08:12:29.097
Here is kind of the complete 
picture.

08:12:32.993 --> 08:12:35.431
Again, you create your 
environment, you interact with 

08:12:35.432 --> 08:12:37.432
that environment to the 

08:12:39.317 --> 08:12:41.317
driver given a policy, those 

08:12:43.303 --> 08:12:43.917
interactions get stored in the 
replay buffer, the replay buffer

08:12:43.918 --> 08:12:45.918
you read from with a tf.

08:12:47.990 --> 08:12:49.990
data dataset and then the agent 
trains 

08:12:51.565 --> 08:12:53.565
with batches from that dataset 
and 

08:12:55.425 --> 08:12:57.425
updates the network underlying 
the policy. 

08:12:58.675 --> 08:13:00.675
Here is kind of the set of 
commands to do that.

08:13:02.731 --> 08:13:04.731
If you look at the bottom, here 
is that 

08:13:05.789 --> 08:13:08.462
loop, you call driver run to 
collect data, it stores data in 

08:13:08.463 --> 08:13:13.400
the replay buffer, and then you 
read from the dataset generated 

08:13:13.401 --> 08:13:15.401
from that replay 

08:13:17.354 --> 08:13:20.334
buffer and train the agent and 
you can iterate this over and 

08:13:20.335 --> 08:13:22.923
over again. 
We have a lot of exciting things

08:13:22.924 --> 08:13:26.892
coming up. We have a number of 
new agents we are going to 

08:13:26.893 --> 08:13:28.893
release. C51 and so on.

08:13:34.543 --> 08:13:35.993
We are adding support for 
contextual bandits, we are going

08:13:35.994 --> 08:13:37.994
to release a 

08:13:39.633 --> 08:13:40.669
number of baselines and as well 
as a number of new replay 

08:13:40.670 --> 08:13:42.670
buffers.

08:13:43.952 --> 08:13:45.952
In particular, we are going to 
be 

08:13:47.194 --> 08:13:49.462
releasing some distributed 
replay buffers, in the next 

08:13:49.463 --> 08:13:51.905
couple of quarters, and those 
will be used for distributed 

08:13:51.906 --> 08:13:55.768
collection. 
Distributed collection allows 

08:13:55.769 --> 08:13:59.432
you to parallelize your data 
collection across many machines 

08:13:59.433 --> 08:14:01.433
and be able to maximize 

08:14:03.272 --> 08:14:05.272
the throughput of your training 
URL algorithm that way.

08:14:06.961 --> 08:14:09.226
We are working on distributed 
training also using TensorFlow 

08:14:09.227 --> 08:14:11.227
new distribution 

08:14:13.904 --> 08:14:16.139
strategy API allowing you to 
train at a massive scale on many

08:14:16.140 --> 08:14:18.140
GPUs and TPUs and 

08:14:21.047 --> 08:14:23.047
we are adding support for more 
environments.

08:14:25.947 --> 08:14:27.947
Please check out TF-agents on 
GitHub. 

08:14:30.448 --> 08:14:32.448
We have a number of Colabs, I 
think 8-9, 

08:14:33.827 --> 08:14:35.827
exploring different parts of the
system.

08:14:37.899 --> 08:14:39.899
As Sergio said, TF-agents is 
built to 

08:14:41.370 --> 08:14:43.370
solve many real-world problems 
and in particular we are 

08:14:43.371 --> 08:14:46.419
interested in seeing your 
problems. For example, we 

08:14:46.420 --> 08:14:48.420
welcome contributions 

08:14:49.468 --> 08:14:51.468
for new environments, new RL 
algorithms 

08:14:54.975 --> 08:14:57.227
for those of you out there who 
are RL experts. Please come chat

08:14:57.228 --> 08:14:59.228
with me or Sergio 

08:15:01.182 --> 08:15:03.332
after the talks or file an issue
on the GitHub issue tracker.

08:15:07.369 --> 08:15:09.937
And let us know what you think. 
Thank you very much. 

08:15:10.138 --> 08:15:13.598
Thank you. 
[Applause] 

08:15:16.451 --> 08:15:18.451
And now I would like to pass it 
off to 

08:15:22.761 --> 08:15:24.761
Nick and Yannick who will talk 
about TensorFlow.js.

08:15:25.817 --> 08:15:27.817
Thanks, guys.

08:15:34.829 --> 08:15:38.095
Hey, everybody. My name is Nick 
and this is my colleague 

08:15:38.096 --> 08:15:42.814
Yannick. We are going to talk 
about JavaScript and ML.

08:15:42.815 --> 08:15:46.401
TensorFlow.js is a library we 
launched last year. It is a 

08:15:46.402 --> 08:15:48.670
library for training and 
deploying ML models in the 

08:15:48.671 --> 08:15:51.516
browser and on node.js. We want 
to showcase what you can do 

08:15:55.199 --> 08:15:57.462
today with a platform and where 
we are going. One of the great 

08:15:57.463 --> 08:16:01.355
parts about the library is there
is really no drivers to install.

08:16:01.356 --> 08:16:04.854
If you run it in the browser you
can get out of the box GPU 

08:16:04.855 --> 08:16:06.855
acceleration.

08:16:10.374 --> 08:16:12.374
The browser itself tends to be 
interactive and builds great 

08:16:14.030 --> 08:16:16.030
applications and demos for using
ML.

08:16:18.109 --> 08:16:20.314
Privacy is a very important part
to the library. You can run 

08:16:20.315 --> 08:16:22.356
inference and training locally 
on the client which works around

08:16:23.972 --> 08:16:25.972
all sorts of privacy issues you 
might 

08:16:28.405 --> 08:16:30.405
have with doing server side 
inference or training.

08:16:32.422 --> 08:16:33.438
And what can you do today with 
the library? We have a 

08:16:33.439 --> 08:16:38.540
collection of, like, pre-trained
off the shelf models you can use

08:16:38.541 --> 08:16:40.816
without any knowledge of ML. We 
also have the ability to take 

08:16:44.484 --> 08:16:46.923
existing Python models and 
convert them and run them in 

08:16:46.924 --> 08:16:48.924
TensorFlow.js.

08:16:51.359 --> 08:16:53.123
We also have a full-stack for 
training, inference and low 

08:16:53.124 --> 08:16:55.124
level linear algebra 

08:16:56.613 --> 08:16:58.613
and that runs in a browser in 
node.js.

08:17:00.307 --> 08:17:02.141
We also have a bunch of 
platforms that JavaScript can 

08:17:02.142 --> 08:17:04.142
run on outside of just the 
browser.

08:17:05.801 --> 08:17:07.801
The first thing I want to 
showcase is 

08:17:09.481 --> 08:17:11.481
some of our new off the shelf 
models we have launched.

08:17:14.776 --> 08:17:16.620
The first one is a bunch of 
pre-trained -- or off the shelf 

08:17:16.621 --> 08:17:18.884
models are a bunch of 
pre-trained models.

08:17:22.608 --> 08:17:26.484
They are image, audio and text 
classification models. The APIs 

08:17:26.485 --> 08:17:28.485
are all user-friendly.

08:17:31.829 --> 08:17:33.829
You don't have to worry about 

08:17:34.913 --> 08:17:36.913
converting images to tensors.

08:17:40.639 --> 08:17:42.103
They are available on MPM and we
have precompiled hosted scripts 

08:17:42.104 --> 08:17:47.010
a well. We are working on this a
lot in the upcoming year and we 

08:17:47.011 --> 08:17:49.011
will have pore and more models 
as we go forward.

08:17:51.306 --> 08:17:53.191
The first model was body pics. I
want to show you how easy it is 

08:17:53.192 --> 08:17:55.192
to 

08:17:56.405 --> 08:17:58.405
use this model.

08:18:00.731 --> 08:18:02.948
First include the library and 
body pics. It is two simple 

08:18:02.949 --> 08:18:04.949
imports.

08:18:11.900 --> 08:18:13.900
The next step is to create an 
image

08:18:18.896 --> 08:18:24.740
DOM. This is Frank. He is trying
to do something on the couch but

08:18:24.741 --> 08:18:26.391
I want to actually load the 
model and find body parts on 

08:18:26.392 --> 08:18:28.854
Frank. 
The first thing to do is to load

08:18:28.855 --> 08:18:34.950
the body pics model. Just a 
simple one-line call. The next 

08:18:34.951 --> 08:18:37.409
step is to call one of the 
methods we expose which is 

08:18:37.410 --> 08:18:39.410
estimate 

08:18:44.487 --> 08:18:46.487
person segmentation and I can 
pass in a

08:18:52.324 --> 08:18:54.358
DOM element. 
There is also a bunch of really 

08:18:54.359 --> 08:18:59.242
easy to use methods for doing 
filtering on the image. So I can

08:18:59.243 --> 08:19:01.243
take the results of that and 

08:19:03.274 --> 08:19:07.390
render it directly on there DOM.
It shows head, body, arm and so 

08:19:07.391 --> 08:19:09.391
on. Another model we launched a 
couple 

08:19:14.154 --> 08:19:16.154
weeks ago is the toxicity model.

08:19:20.487 --> 08:19:21.743
To use this model, we will use 
the prehosted scripts. 

08:19:21.744 --> 08:19:23.744
Two lines of code.

08:19:24.809 --> 08:19:26.809
I will load the toxicity model 
and ask 

08:19:28.277 --> 08:19:30.277
the model to classify just some 
really 

08:19:32.552 --> 08:19:35.408
lovely text, pretty pg, you 
suck, and I will get a result 

08:19:35.409 --> 08:19:37.409
back as a JavaScript 

08:19:39.506 --> 08:19:41.506
object that has seven labels we 
identify 

08:19:44.236 --> 08:19:46.073
different types of toxic-type 
text and the probabilities and 

08:19:46.074 --> 08:19:48.522
if it matches.
We also have the ability to take

08:19:50.341 --> 08:19:52.353
pre-trained Python models for 
run them directly in browsers. 

08:19:55.421 --> 08:19:56.431
If you have a pre-trained model 
trained in Python world we have 

08:19:56.432 --> 08:20:01.759
a command line tool that makes 
it easy to serialize the model 

08:20:01.760 --> 08:20:04.400
as a JSON object and the weights
and distribute them in a web 

08:20:04.401 --> 08:20:06.401
format.

08:20:09.487 --> 08:20:11.749
We support saveModel, TensorFlow
Hub and keras models.

08:20:17.730 --> 08:20:18.337
It supports over 170 and 
counting ops and we will be 

08:20:18.338 --> 08:20:22.355
TensorFlow 2.0 compatible. I 
want to walkthrough how simple 

08:20:22.356 --> 08:20:28.372
it is to use this. I is a Python
model and run it through the 

08:20:28.373 --> 08:20:31.341
command line tool and I can 
easily load that in my 

08:20:31.342 --> 08:20:33.342
JavaScript application. Very 
simple.

08:20:37.745 --> 08:20:39.745
I want to hand it off to Yannick
to 

08:20:41.313 --> 08:20:43.339
walk through the pre-trained 
model. 

08:20:43.340 --> 08:20:45.816
TensorFlow.js allows you to 
train in the browser and in 

08:20:45.817 --> 08:20:47.817
node. The primary tool for this 
is the layers 

08:20:56.234 --> 08:20:58.234
API which is a keras-compatible 
API for authoring models.

08:21:01.938 --> 08:21:03.938
We are going to take a quick 
look at 

08:21:05.870 --> 08:21:07.870
what TF.js code looks like.

08:21:10.237 --> 08:21:11.670
The first step to is import the 
library and when working in 

08:21:11.671 --> 08:21:15.358
node.js you can also use the 
node.js bindings which execute 

08:21:15.359 --> 08:21:17.359
the TensorFlow 

08:21:20.263 --> 08:21:22.263
operations using native compile 
C++ code.

08:21:25.833 --> 08:21:27.833
If you are an a system that 
supports 

08:21:32.336 --> 08:21:34.336
kuda, you can import -- cuda -- 

08:21:35.575 --> 08:21:37.208
you can -- this is what creating
a convolutional model for a 

08:21:37.209 --> 08:21:39.209
classification task looks like. 

08:21:41.521 --> 08:21:43.521
It is similar to keras code in 
Python.

08:21:47.045 --> 08:21:49.449
We start by instantiating a 
model and add a flat operation 

08:21:49.450 --> 08:21:51.450
and a dense layer with output 
classes.

08:21:55.028 --> 08:21:57.878
Similar to Python we use model.
compile and specify loss 

08:21:57.879 --> 08:22:00.816
function and optimizer and 
model.fit is the function that 

08:22:00.817 --> 08:22:02.817
drives the train loop.

08:22:04.671 --> 08:22:06.671
In JavaScript it is an a sync 
function 

08:22:08.340 --> 08:22:10.340
so we want to wait for the 
result or to be done. 

08:22:11.635 --> 08:22:13.635
Once it is done training we can 
save the model.

08:22:15.599 --> 08:22:18.384
We support saving to a number of
targets both on the client and 

08:22:18.385 --> 08:22:20.475
server. 
And you can use model.

08:22:24.136 --> 08:22:26.717
predict to get a result from the
model, finally. Over the past 

08:22:26.718 --> 08:22:29.420
year, we have also heard 
feedback from the community on 

08:22:29.421 --> 08:22:31.421
ways we 

08:22:34.536 --> 08:22:38.814
can improve the experience of 
training with TensorFlow.js. We 

08:22:38.815 --> 08:22:40.815
would like to show you progress 
we have made.

08:22:42.224 --> 08:22:44.224
First up it is tf.data.

08:22:48.574 --> 08:22:50.848
It is an  an API for managing 
data training. It provides a 

08:22:50.849 --> 08:22:55.142
whole set of utility functions 
for dataset transformation. 

08:22:55.143 --> 08:22:57.581
Finally, it works with streams 
and the lazy evaluation allows 

08:22:57.582 --> 08:22:59.582
you to work with 

08:23:03.038 --> 08:23:05.927
data that doesn't fit in memory 
which can be quite important. 

08:23:05.928 --> 08:23:07.928
Let's look at a simple example.

08:23:11.189 --> 08:23:13.189
We load up a

08:23:16.732 --> 08:23:18.732
CSV file and use the is label 
attribute.

08:23:20.218 --> 08:23:22.218
For example, in this map 

08:23:24.081 --> 08:23:26.749
transformation, the price data 
has been separated out into that

08:23:26.750 --> 08:23:29.444
Y object and the rest of the 
features are in the X object.

08:23:32.704 --> 08:23:35.543
Once we flattened the data we 
can apply typical ML 

08:23:35.544 --> 08:23:37.544
transformations including 

08:23:38.600 --> 08:23:40.362
things like shuffling, an ML 
best practice, and batting which

08:23:40.363 --> 08:23:42.363
will do the 

08:23:43.607 --> 08:23:46.282
work of creating properly sized 
mini batches for training and 

08:23:46.283 --> 08:23:49.230
know what to pull into memory 
when the training loop is 

08:23:49.231 --> 08:23:51.231
running.

08:23:53.857 --> 08:23:55.857
Other things include things like
normalization.

08:23:57.397 --> 08:23:59.397
Finally we run the train loop.

08:24:00.458 --> 08:24:04.156
Model.fitdataset supports models
and pulls the right stuff into 

08:24:04.157 --> 08:24:06.196
memory as needed. And that's 
tf.data.

08:24:09.460 --> 08:24:11.460
So the other area we have been 

08:24:13.787 --> 08:24:15.787
responding to community feedback
is visualization.

08:24:18.685 --> 08:24:20.685
I want to talk about tfjs-vis 
which is 

08:24:22.965 --> 08:24:24.965
model of internals as well as 
evaluation 

08:24:27.426 --> 08:24:28.852
metricsism you can view 
internals and evaluation 

08:24:28.853 --> 08:24:31.698
metrics. First we import the 
library and you 

08:24:35.140 --> 08:24:37.140
should node we provide tfjs-vis 
as a separate package.

08:24:41.665 --> 08:24:43.665
To visualize training behavior 
we can 

08:24:45.292 --> 08:24:48.249
use this showshow.fitscallbacks 
function.

08:24:51.311 --> 08:24:53.311
In one line, fit callbacks will 
plot the selected metrics.

08:24:56.589 --> 08:24:58.589
In this case loss and accuracy 
metrics 

08:25:01.213 --> 08:25:03.213
on the batch end and at the end 
of each epoch.

08:25:04.926 --> 08:25:06.926
This allows us to adjust 
hyperparameters as usual.

08:25:11.151 --> 08:25:13.151
You can look at model internals 
with show.

08:25:15.625 --> 08:25:17.858
model summary and show.layer 
summary. Here we see the 

08:25:17.859 --> 08:25:19.859
architecture of the 

08:25:22.693 --> 08:25:24.905
model including output shapes of
SARSAR -- various layers.

08:25:28.762 --> 08:25:31.018
We see the distribution of 
layers in the first 

08:25:31.019 --> 08:25:33.019
convolutional layers including 

08:25:35.119 --> 08:25:37.968
statistics like nans and zero 
counts which are useful for 

08:25:37.969 --> 08:25:39.969
debugging models.

08:25:42.426 --> 08:25:44.426
We want to finally announce 
TensorBoard support in node.js.

08:25:47.472 --> 08:25:52.538
You can monitor when using the 
TensorBoard layer in node.js. A 

08:25:52.539 --> 08:25:54.539
single line will generate the 

08:25:55.817 --> 08:25:57.817
necessary callbacks to write the
methods 

08:26:00.478 --> 08:26:02.116
to a TensorBoard log file using 
this command. Then you can open 

08:26:02.117 --> 08:26:06.626
in TensorBoard and look at how 
your training is going just like

08:26:06.627 --> 08:26:10.908
you may be used to. With that, I
am going to hand it back to Nick

08:26:10.909 --> 08:26:13.745
to talk about some of the 
platforms we execute on. 

08:26:17.422 --> 08:26:19.253
JavaScript is an interesting 
language because it runs in a 

08:26:19.254 --> 08:26:22.043
lot more places than you think. 
There is the traditional browser

08:26:22.044 --> 08:26:25.288
front for running JavaScript in 
the browser. We all know about 

08:26:25.289 --> 08:26:27.289
that. Node.

08:26:29.356 --> 08:26:32.402
js is a big server-side 
solution. Very popular. But 

08:26:32.403 --> 08:26:34.403
there is also a growing trend 
with JavaScript in more places.

08:26:37.490 --> 08:26:39.490
One of them is desktop 
applications and 

08:26:41.146 --> 08:26:43.795
electron is a very popular 
platform for developing 

08:26:43.796 --> 08:26:45.796
applications.

08:26:46.867 --> 08:26:48.891
Those who have used Spotify or 
visual code those are good 

08:26:48.892 --> 08:26:50.892
examples of electron.

08:26:52.627 --> 08:26:55.490
JavaScript is moving into the 
mobile space also. I want to 

08:26:55.491 --> 08:26:57.491
highlight a couple examples 

08:26:59.172 --> 08:27:01.172
we have seen in the industry on 
all four platforms. 

08:27:02.459 --> 08:27:04.459
First is the browser.

08:27:05.571 --> 08:27:07.571
Our friends at Google creative 
labs 

08:27:09.244 --> 08:27:10.869
have built experiments on how 
creative tools could be more 

08:27:10.870 --> 08:27:12.870
accessible to everyone. There is
going to be a great lightning 

08:27:13.924 --> 08:27:16.361
talk on this and they will talk 
about everything they have built

08:27:16.362 --> 08:27:18.362
with this project.

08:27:20.832 --> 08:27:22.832
Uber has built an in-browser 
tool for 

08:27:25.962 --> 08:27:26.778
model agnostic visualization of 
ML performance. They use 

08:27:26.779 --> 08:27:28.779
TensorFlow.

08:27:30.888 --> 08:27:32.923
js for acceleration of their 
linear algebra. K means 

08:27:32.924 --> 08:27:35.188
clustering and so on. They are 
also giving a great lightning 

08:27:37.914 --> 08:27:39.914
talk about how they use 
TensorFlow.

08:27:40.960 --> 08:27:42.999
js to solve this problem for 
their platform. This is all in 

08:27:43.000 --> 08:27:45.000
browser, again. 

08:27:47.292 --> 08:27:49.719
Another cool industry example is
Airbnb. Airbnb built an identity

08:27:49.720 --> 08:27:51.720
document 

08:27:53.803 --> 08:27:55.854
detection model that they use as
a full TensorFlow ecosystem 

08:27:55.855 --> 08:27:58.467
solution. On our Airbnb profile,
if you were to 

08:28:04.404 --> 08:28:07.289
upload a government-issued ID it
is a very big trust issue. They 

08:28:07.290 --> 08:28:09.142
built a TensorFlow model to 
detect if a profile picture that

08:28:09.143 --> 08:28:14.217
you are trying to upload 
directly in the client contains 

08:28:14.218 --> 08:28:16.677
government-issued IDs. They use 
this in the browser using 

08:28:16.678 --> 08:28:18.678
TensorFlow.

08:28:21.761 --> 08:28:24.623
js as well as on their mobile 
devices for TFLite.

08:28:24.828 --> 08:28:28.541
On node.js, a good example of 
this being used is clinic 

08:28:28.542 --> 08:28:32.208
doctor. This is a node.js 
performance analysis tool. They 

08:28:32.209 --> 08:28:34.209
use our node.

08:28:36.885 --> 08:28:38.885
js bindings to filter out gc 
spikes on 

08:28:39.938 --> 08:28:42.936
node processes that are running 
to give a true accurate CPU 

08:28:42.937 --> 08:28:44.937
performance benchmark.

08:28:47.820 --> 08:28:49.820
On the desktop, our team here at
Google 

08:28:52.103 --> 08:28:54.103
with magenta and their music 
generation 

08:28:56.666 --> 08:28:58.696
models, have built a series of 
desktop plugins for the Ableton 

08:28:58.697 --> 08:29:03.997
studio. These are full desktop 
applications that use the 

08:29:03.998 --> 08:29:05.998
magenta models and 

08:29:07.645 --> 08:29:09.645
accelerate them all into the GPU
and desktop applications.

08:29:11.711 --> 08:29:13.711
We have a demo at your booth for
how this works.

08:29:14.976 --> 08:29:16.976
The cool part is all JavaScript 
and GPU 

08:29:18.647 --> 08:29:20.066
acceleration on the desktop with
no cuda drivers all through the 

08:29:20.067 --> 08:29:22.067
findings.

08:29:23.785 --> 08:29:25.785
Mobile is another interesting 
space.

08:29:27.375 --> 08:29:28.594
WeChat, for example, is one of 
the most popular apps in the 

08:29:28.595 --> 08:29:30.595
world with over 1 

08:29:32.081 --> 08:29:34.721
billion total users and a 
subapplication platform called 

08:29:34.722 --> 08:29:37.373
the mini programs. They are 
great because it is no-need 

08:29:39.416 --> 08:29:41.416
install app in advance used on 
demand 

08:29:43.900 --> 08:29:46.546
and has over 1 million apps and 
1.5 million developers.

08:29:47.986 --> 08:29:50.658
The mini program is built using 
JavaScript and makes it easy for

08:29:53.117 --> 08:29:55.117
developers to create, deploy and
share on the WeChat platform.

08:29:59.935 --> 08:30:01.935
I want to show a demo of a tfjs 
models running on WeChat.

08:30:04.625 --> 08:30:06.625
I have just a regular iOS device
and I will open WeChat.

08:30:12.239 --> 08:30:13.330
Someone shared this tfjs example
and I can load up the 

08:30:13.331 --> 08:30:15.331
application.

08:30:17.516 --> 08:30:19.516
It is running our post net model
and if 

08:30:21.801 --> 08:30:23.801
I aim it at Yannick here, there 
we go, 

08:30:25.504 --> 08:30:27.504
this is just purely done in 
JavaScript 

08:30:29.158 --> 08:30:31.194
and it is running our off the 
shelf mobile model doing about 

08:30:31.195 --> 08:30:33.195
30 frames a second.

08:30:35.688 --> 08:30:37.356
This is all done with the WeChat
JavaScript platform.

08:30:37.357 --> 08:30:39.357
platform.

08:30:41.009 --> 08:30:43.009
Thank you. 

08:30:44.944 --> 08:30:46.766
So, all this work over the past 
year and the fantastic projects 

08:30:46.767 --> 08:30:48.767
created by the 

08:30:50.244 --> 08:30:51.662
community, makes us very excited
to announce TensorFlow.js 1.0 

08:30:51.663 --> 08:30:56.782
today. It is available now and 
we are super excited to see what

08:30:56.783 --> 08:30:58.783
the community builds 

08:31:01.500 --> 08:31:03.340
with it and hopes the API 
stability makes it easier going 

08:31:03.341 --> 08:31:05.341
forward.

08:31:06.431 --> 08:31:08.266
We are focused on providing a 
stable API you can build 

08:31:08.267 --> 08:31:10.267
applications on and manage 
upgrade easier and bringing 

08:31:11.937 --> 08:31:13.816
market improvements in 
performance particularly on 

08:31:13.817 --> 08:31:18.131
mobile devices. We will look at 
that in a bit more detail. To 

08:31:18.132 --> 08:31:22.420
look at the update closer, since
announcng TensorFlow.js last 

08:31:22.421 --> 08:31:24.876
year at the Dev Summit we have 
been working on improvements 

08:31:24.877 --> 08:31:26.877
across a number of platforms.

08:31:29.768 --> 08:31:31.768
We see increases of about 1.

08:31:33.454 --> 08:31:35.454
4X to 9X in some extreme cases.

08:31:36.553 --> 08:31:38.553
This chart shows inference 
performance 

08:31:41.052 --> 08:31:43.052
with a batch size of 1 on mobile
net in Chrome.

08:31:44.730 --> 08:31:46.730
It is a mobile image 
classification and 

08:31:48.384 --> 08:31:50.384
we see inference times going 
from about 

08:31:54.797 --> 08:31:58.674
15 milliseconds to about 150 
milliseconds on the pixel 2. We 

08:31:58.675 --> 08:32:00.322
have done performance on the iOS
devices as well so really 

08:32:00.323 --> 08:32:06.051
excited for you to try this. 
What is next for us? Well, we 

08:32:06.052 --> 08:32:09.719
want to enable you to execute 
saved models on our node.js back

08:32:09.720 --> 08:32:12.168
end without going through the 
conversion process and this will

08:32:12.169 --> 08:32:14.169
open up 

08:32:21.412 --> 08:32:23.412
many more models to serve using 
the js.

08:32:25.725 --> 08:32:27.725
node stack.

08:32:29.404 --> 08:32:31.404
We are always keeping an eye on 
browser 

08:32:38.016 --> 08:32:40.016
acceleration proposals like simd
and WASM.

08:32:41.270 --> 08:32:43.270
The browser is only going to get
faster and so will we.

08:32:47.806 --> 08:32:49.806
We want to expand the platforms 
on which TensorFlow.js can run.

08:32:52.300 --> 08:32:54.300
We are working on platforms like
the 

08:32:56.170 --> 08:32:57.185
Raspberry Pi and other hybrid 
mobile platforms that run 

08:32:57.186 --> 08:33:00.038
JavaScript. 
So thanks and for more 

08:33:00.039 --> 08:33:02.039
information on 

08:33:03.321 --> 08:33:05.321
the things we talked about, you 
can 

08:33:09.685 --> 08:33:11.944
visit any one of these links. 
Thank you. 

08:33:17.578 --> 08:33:19.578
And last up is Elena to talk 
about her 

08:33:20.828 --> 08:33:24.514
work using machine translation 
in the Vatican secret archive. 

08:33:27.632 --> 08:33:29.632
Thank you, Nick and Yanni.

08:33:30.680 --> 08:33:32.680
It is great to be here. Just one
second.

08:33:41.345 --> 08:33:43.345
I am excited to be here and to 
talk to 

08:33:45.434 --> 08:33:47.716
you about the In Codice Ratio 
project that is going on at Roma

08:33:47.717 --> 08:33:50.167
university. To talk to you about
TensorFlow help us 

08:33:56.237 --> 08:33:58.237
build a model that is able to

08:34:02.306 --> 08:34:04.306
transcribe ancient manuscripts.

08:34:09.461 --> 08:34:11.491
You can see the polyographers 
and archivist and on the left is

08:34:11.492 --> 08:34:14.550
the data science team. That is 
why I think the name, In Codice 

08:34:21.137 --> 08:34:23.137
Ratio, reflects us well.

08:34:25.215 --> 08:34:27.215
In Latin, it means a known edge 
for 

08:34:28.228 --> 08:34:30.228
transcript

08:34:31.962 --> 08:34:33.385
but in Italian it means software
code so it is knowledge through 

08:34:33.386 --> 08:34:35.386
software.

08:34:36.841 --> 08:34:38.841
You might ask 
yourselvesyourselves, 

08:34:40.341 --> 08:34:42.341
what brings polyographer and 
archivist 

08:34:43.850 --> 08:34:45.850
and data scientists together.

08:34:49.738 --> 08:34:51.738
They both want to 

08:34:52.897 --> 08:34:54.897
discover knowledge from big 
data.

08:34:56.832 --> 08:34:58.832
Historical archives are an 
endless 

08:35:00.108 --> 08:35:03.164
source of important cultural and
historical information. Just to 

08:35:03.165 --> 08:35:04.592
give you a scale of how large 
this information can be, let's 

08:35:04.593 --> 08:35:06.593
compare 

08:35:07.654 --> 08:35:09.654
for a second the sides of the 
Vatican 

08:35:11.724 --> 08:35:14.792
secret archive to the height of 
Mt. Everest. If you were to take

08:35:14.793 --> 08:35:16.793
each shelve of the 

08:35:18.470 --> 08:35:21.149
Vatican secret archive, you 
would get to 85 kilometers tall.

08:35:24.825 --> 08:35:26.678
That is about 10 times the size 
of Mt. Everest and the content 

08:35:26.679 --> 08:35:30.368
spans centuries and continents. 
There you have examples of 

08:35:30.369 --> 08:35:32.369
letters 

08:35:34.875 --> 08:35:36.875
coming from China, from Europe, 
from 

08:35:38.747 --> 08:35:40.747
Africa, and of course from the 
Americas.

08:35:41.798 --> 08:35:43.798
What is our goal?

08:35:45.862 --> 08:35:48.313
To build tools and technology 
that enables in general to 

08:35:48.314 --> 08:35:53.471
perform large scale analysis on 
historical archives because 

08:35:53.472 --> 08:35:56.899
right now the process is 
entirely manually. You have to 

08:35:56.900 --> 08:36:01.453
go there, consult the documents 
manually and be able to read the

08:36:01.454 --> 08:36:03.304
challenging handwriting and if 
you find information that may be

08:36:03.305 --> 08:36:05.305
linked to 

08:36:06.800 --> 08:36:08.800
another collection, you have to 
do it all by yourself.

08:36:13.950 --> 08:36:15.372
But first, we have to face the 
very first challenge and that is

08:36:15.373 --> 08:36:20.074
when you are dealing with web 
content, for example, if you 

08:36:20.075 --> 08:36:22.075
want to extract data from the 
internet, that is already text.

08:36:25.190 --> 08:36:26.639
We said we are dealing with 
historical documents and that is

08:36:26.640 --> 08:36:29.962
often scans. 
Traditionally OCR is fine for 

08:36:29.963 --> 08:36:32.821
printed text but then you get to
this.

08:36:37.100 --> 08:36:38.115
This is a medieval handwriting 
and a language nobody uses 

08:36:38.116 --> 08:36:40.116
anymore.

08:36:41.591 --> 08:36:43.421
It is a handwriting nobody is 
able to write or read anymore 

08:36:43.422 --> 08:36:45.422
for that matter.

08:36:47.242 --> 08:36:53.267
It is heavily aggregated and you
want to get text out of it. You 

08:36:53.268 --> 08:36:55.928
might want to train a machine 
learning model but then we come 

08:36:55.929 --> 08:36:59.845
to the second challenge and that
is scalability in the dataset 

08:36:59.846 --> 08:37:01.846
collection process.

08:37:03.543 --> 08:37:05.589
The graph you see there is an 
algorithmic scale and it might 

08:37:05.590 --> 08:37:07.590
show you something you know that
it is known as 

08:37:11.708 --> 08:37:13.708
the zip flow that tells you that
there 

08:37:16.426 --> 08:37:18.882
is very few words and most of 
the words do not occur that 

08:37:18.883 --> 08:37:20.883
often. What does that mean for 
us?

08:37:25.472 --> 08:37:28.334
If we want to collect data at 
vocabulary or word level, we 

08:37:28.335 --> 08:37:30.335
have to 

08:37:31.614 --> 08:37:33.614
annotate thousands of lines of 
text 

08:37:35.744 --> 08:37:37.403
which means thousands of -- 
hundreds of pages. Similar 

08:37:37.404 --> 08:37:40.101
systems do exist. There are 
state-of-the-art systems but 

08:37:43.724 --> 08:37:46.527
most of the polyographer even 
when they know of the tools get 

08:37:47.948 --> 08:37:49.996
discouraged using them because 
they say it is not 

08:37:49.997 --> 08:37:51.997
cost-effective because it can 

08:37:55.733 --> 08:37:57.733
take up to months or even years 
on these 

08:37:59.213 --> 08:38:01.453
documents to get a transcription
service they will use once or 

08:38:01.454 --> 08:38:05.197
twice whereas they would like to
do it faster. We asked ourselves

08:38:05.198 --> 08:38:07.198
how can we scale on this task?

08:38:10.727 --> 08:38:12.727
We decided to go by easier 
steps, simpler steps. 

08:38:14.425 --> 08:38:16.285
The very first things we did was
to select data for single 

08:38:16.286 --> 08:38:18.286
characters.

08:38:20.412 --> 08:38:21.630
This enabled us not to involve 
polyographer but people are less

08:38:21.631 --> 08:38:23.631
experience.

08:38:25.523 --> 08:38:27.523
We built a custom crowd sourcing

08:38:30.299 --> 08:38:32.299
platform that worked like capcha
solving.

08:38:33.780 --> 08:38:36.667
You see an actual screen there. 
The workers were presented with 

08:38:36.668 --> 08:38:41.147
an image and target and had to 
match the target and select the 

08:38:41.148 --> 08:38:43.148
areas inside the image.

08:38:44.622 --> 08:38:47.497
In this way, we were able to 
involve more than 500 high 

08:38:47.498 --> 08:38:51.601
school students and in about two
week's work we made more than 

08:38:51.602 --> 08:38:53.602
40,000 annotations.

08:38:55.465 --> 08:38:57.465
Now we had the data and we 
wanted to build a model. 

08:38:59.339 --> 08:39:01.339
When I started working at the 
project, I 

08:39:03.210 --> 08:39:05.443
was pretty much a beginner in 
machine learning and so 

08:39:05.444 --> 08:39:07.444
TensorFlow helped me put 

08:39:10.084 --> 08:39:13.008
in practice what I was studying 
in thereytheory. It was a great 

08:39:13.009 --> 08:39:15.009
help that I could rely 

08:39:17.349 --> 08:39:19.382
on tutorials and the community 
and where everything else failed

08:39:19.383 --> 08:39:21.383
even the source code.

08:39:23.469 --> 08:39:25.469
So, we started experimenting and
we decided to start small first.

08:39:26.743 --> 08:39:28.743
We didn't want to overkill.

08:39:29.868 --> 08:39:31.868
We wanted the model to fit 
exactly our data. 

08:39:35.005 --> 08:39:37.650
We started small and proceeded 
incrementally and in a constant 

08:39:37.651 --> 08:39:41.557
cycle of tuning hyperparameters 
and the best optimizer and 

08:39:41.558 --> 08:39:43.558
initializers and the number of 
layer and the type of layers 

08:39:48.375 --> 08:39:51.088
and then evaluaing and training 
again. Then we used keras which 

08:39:51.089 --> 08:39:55.632
allowed us to keep the code 
spall and readable. This is what

08:39:55.633 --> 08:39:57.633
we settled for.

08:40:01.429 --> 08:40:03.429
It might look trivial but it 
allowed us 

08:40:08.232 --> 08:40:10.232
to get up to 94% average 
accuracy. 

08:40:13.497 --> 08:40:15.497
Where does this fit in the whole
scheme 

08:40:17.137 --> 08:40:19.137
of the

08:40:21.948 --> 08:40:24.641
transcription center? We have 
the input image.

08:40:27.513 --> 08:40:29.565
We are relying on over 
segmentation and that is a bit 

08:40:29.566 --> 08:40:32.046
old school but it allows us to 
feed the single characters or 

08:40:34.111 --> 08:40:36.111
combinations of characters 
inside of the 

08:40:39.004 --> 08:40:41.252
classifier which then produces 
different transcription 

08:40:41.253 --> 08:40:43.253
according to a latent 

08:40:44.911 --> 08:40:45.931
language model which we also 
built from publically available 

08:40:45.932 --> 08:40:47.932
sources. How good do we get?

08:40:51.692 --> 08:40:53.692
We get about 65% exact 
transcription 

08:40:55.198 --> 08:40:58.058
and we can get up to 80% if we 
consider minor spelling errors 

08:40:58.059 --> 08:41:00.059
or if the segmentation is 
perfect.

08:41:02.952 --> 08:41:04.952
If we had perfect segmentation 
we could get up to 80%.

08:41:07.876 --> 08:41:10.119
We can see it can be more 
challenging. What are plans for 

08:41:10.120 --> 08:41:12.789
the future? We are very excited 
about the 

08:41:15.796 --> 08:41:17.796
integration

08:41:18.823 --> 08:41:20.823
integration of TensorFlow and 
keras.

08:41:23.513 --> 08:41:24.925
What we found out was that 
sometimes some were lagging 

08:41:24.926 --> 08:41:26.926
behind and sometimes 

08:41:28.624 --> 08:41:30.666
we wanted to get to one part of 
the features or from keras or 

08:41:30.667 --> 08:41:32.667
TensorFlow and we found 
ourselves doing lots of -- I 

08:41:33.784 --> 08:41:36.247
don't know if that is your 
experience as well but we found 

08:41:36.248 --> 08:41:38.248
ourselves doing back and forth 
between TensorFlow and keras 

08:41:41.959 --> 08:41:43.401
and now we get the best of the 
two worlds and are very excited 

08:41:43.402 --> 08:41:46.504
about that. How do we plan to 
expand our machine learning 

08:41:46.505 --> 08:41:48.505
system?

08:41:52.804 --> 08:41:54.804
First thing first, we are trying

08:41:58.256 --> 08:42:01.531
units that achieved good results
on medical imaging.

08:42:04.807 --> 08:42:06.807
We are planning to use them to 
get rid 

08:42:12.115 --> 08:42:14.826
of the segmentation and achieve 
the result of classifying 

08:42:14.827 --> 08:42:16.827
together. Those examples worked 
particularly well.

08:42:20.368 --> 08:42:22.800
Of course, since there could be 
ambiguitywe could do error 

08:42:22.801 --> 08:42:24.801
correction 

08:42:25.840 --> 08:42:27.840
and

08:42:32.954 --> 08:42:35.233
transcripttranscription. We want
to evolve and move to word 

08:42:38.949 --> 08:42:40.949
level and even sentence level 
annotated 

08:42:43.403 --> 08:42:45.403
characters, but still our focus 
is 

08:42:47.342 --> 08:42:48.786
calbility and we want to involve
polyographers as little as 

08:42:48.787 --> 08:42:50.787
possible.

08:42:56.157 --> 08:42:58.394
This is generated input from GAN
but we plan to use an encoder to

08:42:58.395 --> 08:43:00.395
evolve the 

08:43:02.479 --> 08:43:03.690
dataset with little human 
interaction, like the less we 

08:43:03.691 --> 08:43:05.691
can.

08:43:09.109 --> 08:43:11.134
And in the end this brings us to
use sequence model that will 

08:43:11.135 --> 08:43:13.135
take full 

08:43:15.229 --> 08:43:17.086
advantage of the sentence level 
context, for example, and could 

08:43:17.087 --> 08:43:19.087
be able to solve 

08:43:20.347 --> 08:43:22.347
things that we could not be able
to 

08:43:24.460 --> 08:43:25.879
solve with single, let's say, 
with single character 

08:43:25.880 --> 08:43:27.880
classification, for example 
abbreviation.

08:43:31.017 --> 08:43:33.017
Many words occur aggregated in 
this 

08:43:34.075 --> 08:43:36.075
text just like you would text 
saying me 

08:43:37.575 --> 08:43:39.575
too and use 2 the number or for 
you and 

08:43:41.844 --> 08:43:42.675
that is the same with this kind 
of manuscripts. That is one 

08:43:42.676 --> 08:43:44.676
application you could have.

08:43:48.034 --> 08:43:50.888
We are planning to use sequence 
models to get to a neural model 

08:43:50.889 --> 08:43:55.834
because so far we have only 
experimented with statistics. 

08:43:55.835 --> 08:43:57.492
And one last thing, I mentioned 
the people in the team but there

08:43:57.493 --> 08:44:02.403
is so many people I would like 
to thank that were not in those 

08:44:02.404 --> 08:44:04.404
slides.

08:44:09.127 --> 08:44:11.127
First of all Simone who should 
have 

08:44:12.611 --> 08:44:14.611
been here but he couldn't make 
it. 

08:44:18.381 --> 08:44:20.381
And then py school of AI and 
Luke for 

08:44:21.441 --> 08:44:23.517
their amazing mentoring and a 
high school teacher that 

08:44:23.518 --> 08:44:25.518
actually allowed us 

08:44:27.190 --> 08:44:29.216
to involve those students that 
work for the platform.

08:44:32.819 --> 08:44:33.751
And of course, all the graduate 
and undergraduate students that 

08:44:33.752 --> 08:44:35.752
worked with 

08:44:36.838 --> 08:44:39.329
us and helped us achieve what we
have achieved and what we plan 

08:44:39.330 --> 08:44:41.330
to achieve in the future.

08:44:44.459 --> 08:44:46.916
And of course, that you can  -- 
thank you for your attention. 

08:44:46.917 --> 08:44:50.989
[Applause] 
Thank you, Elena. All right. 

08:44:50.990 --> 08:44:55.696
This is the end of day 1. We 
hope you had a great time.

08:44:59.198 --> 08:45:01.685
A big thank you to our speakers,
our staff, our crew, and to the 

08:45:01.686 --> 08:45:03.686
audience 

08:45:04.742 --> 08:45:07.629
here in Sunnyvale and all around
the world. With that, let's take

08:45:07.630 --> 08:45:09.630
the livestream 

08:45:11.309 --> 08:45:13.309
out with a look at some 
highlights.

08:46:16.851 --> 08:46:18.851
Hello, everybody.

08:46:21.952 --> 08:46:26.239
Right now, right now, before you
leave, there is a survey. 

08:46:26.240 --> 08:46:29.316
Please, it has been emailed to 
you right now. Please do the 

08:46:29.317 --> 08:46:33.885
survey. This is machine 
learning. We need data. No, 

08:46:33.886 --> 08:46:35.886
seriously, like we can't bring 

08:46:37.157 --> 08:46:40.154
these events if you can't find 
out what you think of them. 

08:46:40.155 --> 08:46:42.403
Please, please fill out the 
survey. Next we have a social 

08:46:42.404 --> 08:46:44.466
night. Tomorrow we have talks, 
breakout 

08:46:48.379 --> 08:46:50.379
sessions, we have coding, we 
have workshops. 

08:46:51.319 --> 08:46:53.319
Doors open at 8:00 A.M.

08:46:54.827 --> 08:46:56.827
and lightning talks start at 
9:50. 

08:46:58.503 --> 08:47:00.503
We will

08:47:03.277 --> 08:47:05.739
have breakfast and start 
lightning talks. We have 

08:47:05.740 --> 08:47:07.947
breakout on performance, mobile 
and 2.0 and we have an amazing 

08:47:07.948 --> 08:47:10.865
series of research talks. 

08:47:13.973 --> 08:47:15.973
Please, please, we will see you 
tomorrow.

08:47:17.019 --> 08:47:19.019
Go have a drink. 

08:47:20.224 --> 08:47:22.224
[Applause]

