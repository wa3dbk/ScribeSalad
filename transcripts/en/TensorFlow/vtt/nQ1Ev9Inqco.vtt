WEBVTT
Kind: captions
Language: en

00:00:00.160 --> 00:00:03.058
♪ (music) ♪

00:00:07.068 --> 00:00:09.316
Alright, hi, everybody.

00:00:09.316 --> 00:00:12.218
I'm Alex, from the Brain Robotics team,

00:00:12.218 --> 00:00:15.787
and in this presentation, I'll be talking 
about how we use simulation

00:00:15.787 --> 00:00:19.721
and domain adaptation in some of
our real-world robot learning problems.

00:00:20.621 --> 00:00:24.457
So, first let me start by
introducing robot learning.

00:00:24.457 --> 00:00:28.036
The goal of robot learning
is to use machine learning

00:00:28.036 --> 00:00:29.826
to learn robotic skills

00:00:29.826 --> 00:00:31.995
that work in general environments.

00:00:31.995 --> 00:00:33.921
What we've seen so far is that

00:00:33.921 --> 00:00:35.731
if you control your environment a lot,

00:00:35.731 --> 00:00:38.634
you can get robots to do
pretty impressive things,

00:00:38.634 --> 00:00:40.168
and where techniques start to break down

00:00:40.168 --> 00:00:42.792
is when you try to apply
these same techniques

00:00:42.792 --> 00:00:44.651
to more general environments.

00:00:44.651 --> 00:00:47.214
And the thinking is that
if you use machine learning,

00:00:47.214 --> 00:00:48.934
then you can learn
from your environment,

00:00:48.934 --> 00:00:52.340
and this can help you address
these generalization issues.

00:00:52.340 --> 00:00:54.158
So, as a step in this direction,

00:00:54.158 --> 00:00:57.279
we've been looking at the problem
of robotic grasping.

00:00:57.279 --> 00:00:59.282
This is a project that we've been
working on

00:00:59.282 --> 00:01:01.783
in collaboration with some people at X.

00:01:01.783 --> 00:01:03.784
And to explain our problem setup a bit,

00:01:03.784 --> 00:01:05.551
we're going to have a real robot arm

00:01:05.551 --> 00:01:08.622
which is learning to pick up objects
out of a bin.

00:01:08.622 --> 00:01:11.655
There is going to be a camera looking down

00:01:11.655 --> 00:01:14.325
over the shoulder of the arm into the bin,

00:01:14.325 --> 00:01:17.760
and from this RGB image
we're going to train a neural network

00:01:17.760 --> 00:01:20.558
to learn what commands
it should send to the robot

00:01:20.558 --> 00:01:22.924
to successfully pick up objects.

00:01:22.924 --> 00:01:27.265
Now, we want to try to solve this task
using as few assumptions as possible.

00:01:27.265 --> 00:01:30.199
So, importantly, we're not going
to give any information

00:01:30.199 --> 00:01:33.499
about the geometry of what kinds
of objects we are trying to pick up,

00:01:33.499 --> 00:01:35.048
and we're also not going to give

00:01:35.048 --> 00:01:38.239
any information about
the depth of the scene.

00:01:38.239 --> 00:01:40.373
So in order to solve the task,

00:01:40.373 --> 00:01:42.577
the model needs to learn
hand-eye coordination

00:01:42.577 --> 00:01:45.212
or it needs to see where it is
within the camera image,

00:01:45.212 --> 00:01:48.015
and then figure out
where in the scene it is,

00:01:48.015 --> 00:01:50.729
and then combine these two to figure out
how it should move around.

00:01:51.869 --> 00:01:55.505
Now, in order to train this model,
we're going to need a lot of data

00:01:55.505 --> 00:01:57.666
because it's a pretty large scale
image model.

00:01:57.666 --> 00:02:02.822
And our solution at the time for this
was to simply use more robots.

00:02:02.822 --> 00:02:05.561
So this is what we call the "Arm Farm."

00:02:05.561 --> 00:02:08.691
These are six robots
collecting data in parallel.

00:02:08.691 --> 00:02:11.554
And if you have six robots,
you can collect data a lot faster

00:02:11.554 --> 00:02:13.261
than if you only have one robot.

00:02:13.261 --> 00:02:16.231
So using these robots,
we were able to collect

00:02:16.231 --> 00:02:18.697
over a million attempted grasps,

00:02:18.697 --> 00:02:20.923
over a total of thousands of robot hours,

00:02:20.923 --> 00:02:22.671
and then using this data we were able

00:02:22.671 --> 00:02:26.602
to successfully train models to learn
how to pick up objects.

00:02:26.602 --> 00:02:29.406
Now, this works,
but it still took a lot of time

00:02:29.406 --> 00:02:31.040
to collect this dataset.

00:02:31.040 --> 00:02:33.041
So this motivated looking into ways

00:02:33.041 --> 00:02:35.099
to reduce the amount 
of real-world data needed

00:02:35.099 --> 00:02:37.156
to learn these behaviors.

00:02:38.986 --> 00:02:41.656
One approach for doing this is simulation.

00:02:41.656 --> 00:02:43.314
So in the left video here,

00:02:43.314 --> 00:02:45.861
you can see the images that are
going into our model

00:02:45.861 --> 00:02:47.783
in our real world setup,

00:02:47.783 --> 00:02:49.785
and on the right here you can see

00:02:49.785 --> 00:02:52.700
our simulated recreation of that setup.

00:02:52.700 --> 00:02:55.222
Now, the advantage 
of moving things into simulation

00:02:55.222 --> 00:02:58.326
is that simulated robots
are a lot easier to scale.

00:02:58.326 --> 00:03:01.460
We've been able to spin up
thousands of simulated robots

00:03:01.460 --> 00:03:03.328
grasping various objects,

00:03:03.328 --> 00:03:06.436
and using this setup we were able 
to collect millions of grasps

00:03:06.436 --> 00:03:08.127
in just over eight hours,

00:03:08.127 --> 00:03:11.008
instead of the weeks that were required
for our original dataset.

00:03:12.836 --> 00:03:15.931
Now, this is good 
for getting a lot of data,

00:03:15.931 --> 00:03:18.535
but unfortunately models
trained in simulation

00:03:18.535 --> 00:03:21.751
tend not to transfer
to the actual real world robot.

00:03:21.751 --> 00:03:24.707
There are a lot of systematic differences
between the two.

00:03:24.707 --> 00:03:28.020
One big one is the visual
appearances of different things.

00:03:28.020 --> 00:03:30.346
And another big one is just
physical differences

00:03:30.346 --> 00:03:31.943
between our real-world physics

00:03:31.943 --> 00:03:34.179
and our simulated physics.

00:03:34.179 --> 00:03:37.181
So what we did was,
we were able to very quickly

00:03:37.181 --> 00:03:41.383
train our model on simulation
to get to around 90% grasp success.

00:03:41.383 --> 00:03:43.649
We then deployed to the real robot,

00:03:43.649 --> 00:03:46.397
and it succeeds just over 20% of the time,

00:03:46.397 --> 00:03:48.419
which is a very big performance drop.

00:03:49.039 --> 00:03:51.520
So in order to actually
get good performance,

00:03:51.520 --> 00:03:53.841
we need to do something
a bit more clever.

00:03:53.841 --> 00:03:56.690
So this motivated looking 
into Sim-to-Real transfer,

00:03:56.690 --> 00:03:59.363
which is a set of
transfer-learning techniques

00:03:59.363 --> 00:04:01.264
for trying to use simulated data

00:04:01.264 --> 00:04:04.499
to improve your
real-world sample efficiency.

00:04:06.549 --> 00:04:09.969
Now, there are a few different ways
you can do this.

00:04:09.969 --> 00:04:11.634
One approach for doing this is

00:04:11.634 --> 00:04:14.035
adding more randomization
into your simulator.

00:04:14.035 --> 00:04:16.469
You can do this by changing
around the textures

00:04:16.469 --> 00:04:17.979
that you apply to different objects,

00:04:17.979 --> 00:04:19.953
changing around their colors,

00:04:19.953 --> 00:04:22.945
changing how lighting is interacting
with your scene,

00:04:22.945 --> 00:04:27.113
and you can also play around with changing
the geometry of what kinds of objects

00:04:27.113 --> 00:04:28.565
you're trying to pick up.

00:04:28.565 --> 00:04:31.946
Another way of doing this
is domain adaptation,

00:04:31.946 --> 00:04:34.252
which is a set of techniques for learning

00:04:34.252 --> 00:04:38.121
when you have two domains of data
that have some common structure,

00:04:38.121 --> 00:04:39.857
but are still somewhat different.

00:04:39.857 --> 00:04:42.730
In our case the two domains are
going to be our simulated robot data

00:04:42.730 --> 00:04:45.303
and our real robot data.

00:04:45.303 --> 00:04:47.456
And there are feature-level
ways of doing this

00:04:47.456 --> 00:04:49.825
and there are pixel-level
ways of doing this.

00:04:49.825 --> 00:04:53.134
Now, in this work, we tried
all of these approaches,

00:04:53.134 --> 00:04:55.678
and in this presentation,
I'm going to focus primarily

00:04:55.678 --> 00:04:58.113
on the domain adaptation side of things.

00:05:00.503 --> 00:05:02.937
So, in feature-level domain adaptation

00:05:02.937 --> 00:05:06.437
what we're going to do is we're going 
to take our simulated data,

00:05:06.437 --> 00:05:07.804
take our real data,

00:05:07.804 --> 00:05:10.670
train the same model on both datasets,

00:05:10.670 --> 00:05:13.108
but then at an intermediate
feature layer of the network,

00:05:13.108 --> 00:05:15.647
we're going to attach a similarity loss.

00:05:15.647 --> 00:05:19.483
And the similarity loss is going to
encourage the distribution of features

00:05:19.483 --> 00:05:21.530
to be the same across both domains.

00:05:21.530 --> 00:05:24.938
Now, one approach for doing this
which has worked well recently

00:05:24.938 --> 00:05:27.755
is called Domain-Adversarial
Neural Networks.

00:05:27.755 --> 00:05:31.422
And the way these work is that the
similarity loss is implemented

00:05:31.422 --> 00:05:34.725
as a small neural net that tries
to predict the domain

00:05:34.725 --> 00:05:37.410
based on the input features 
it's receiving,

00:05:37.410 --> 00:05:39.123
and then the rest of the model is trying

00:05:39.123 --> 00:05:42.243
to confuse this domain classifier
as much as possible.

00:05:44.673 --> 00:05:48.066
Now, pixel-level methods try to work
at the problem

00:05:48.066 --> 00:05:49.701
from a different point of view.

00:05:49.701 --> 00:05:52.703
Instead of trying to learn
domain invariant features,

00:05:52.703 --> 00:05:55.767
we're going to try to transform
our images at the pixel level

00:05:55.767 --> 00:05:57.507
to look more realistic.

00:05:57.507 --> 00:06:01.073
So what we do here is we take 
a generative-adversarial network;

00:06:01.073 --> 00:06:04.777
we feed it an image from our simulator,

00:06:04.777 --> 00:06:08.578
and then it's going to output an image
that looks more realistic.

00:06:08.578 --> 00:06:11.514
And then we're going to use the output
of this generator

00:06:11.514 --> 00:06:15.411
to train whatever task model
that we want to train.

00:06:15.411 --> 00:06:16.952
Now we're going to train both

00:06:16.952 --> 00:06:19.354
the generator and the task model
at the same time.

00:06:19.354 --> 00:06:21.387
We found that in practice, this was useful

00:06:21.387 --> 00:06:23.821
because it helps ground
the generator output

00:06:23.821 --> 00:06:27.082
to be useful for actually training
your downstream task.

00:06:29.232 --> 00:06:31.927
Alright. So taking a step back,

00:06:31.927 --> 00:06:36.001
feature-level methods can learn
domain-invariant features

00:06:36.001 --> 00:06:38.100
when you have data from related domains

00:06:38.100 --> 00:06:40.269
that aren't quite identical.

00:06:40.269 --> 00:06:43.602
Meanwhile, pixel-level methods
can transform your data

00:06:43.602 --> 00:06:46.403
to look more like your real-world data,

00:06:46.403 --> 00:06:48.138
but in practice they don't work perfectly,

00:06:48.138 --> 00:06:50.205
and there are still some small artifacts

00:06:50.205 --> 00:06:52.743
and inaccuracies from
the generator output.

00:06:52.743 --> 00:06:56.712
So our thinking went, "Why don't we simply
combine both of these approaches?"

00:06:56.712 --> 00:06:59.191
We can apply a pixel-level method

00:06:59.191 --> 00:07:01.913
to try to transform the data
as much as possible,

00:07:01.913 --> 00:07:04.250
and this isn't going to get us
all the way there,

00:07:04.250 --> 00:07:06.951
but then we can attach a
feature-level method on top of this

00:07:06.951 --> 00:07:09.915
to try to close 
the reality gap even further,

00:07:09.915 --> 00:07:12.508
and combined these form what we call
the grasp gen

00:07:12.508 --> 00:07:13.966
which is a combination of both

00:07:13.966 --> 00:07:16.222
pixel-level and feature-level
domain adaptation.

00:07:16.222 --> 00:07:18.257
In the left half of the video here

00:07:18.257 --> 00:07:19.857
you can see a simulated grasp.

00:07:19.857 --> 00:07:22.861
In the right half you can see 
the output of our generator.

00:07:22.861 --> 00:07:25.228
And you can see that it's
learning some pretty cool things

00:07:25.228 --> 00:07:27.730
in terms of drawing
what the tray should look like,

00:07:27.730 --> 00:07:30.333
drawing more realistic textures
on the arm,

00:07:30.333 --> 00:07:32.235
drawing shadows 
that the objects are casting.

00:07:32.235 --> 00:07:34.377
It's also learned how to even draw shadows

00:07:34.377 --> 00:07:36.435
as the arm is moving around in the scene.

00:07:36.435 --> 00:07:38.403
And it certainly isn't perfect.

00:07:38.403 --> 00:07:41.805
There are still these little
odd splotches of color around,

00:07:41.805 --> 00:07:43.808
but it's definitely learning something

00:07:43.808 --> 00:07:47.028
about what it means for an image 
to look more realistic.

00:07:47.028 --> 00:07:51.147
Now, this is good for getting
a lot of pretty images,

00:07:51.147 --> 00:07:54.813
but what matters for our problem is
whether these images are actually useful

00:07:54.813 --> 00:07:58.300
for reducing them 
onto real-world data required.

00:07:58.960 --> 00:08:01.417
And we find that it does.

00:08:01.417 --> 00:08:03.386
So, to explain this chart a bit:

00:08:03.386 --> 00:08:07.136
On the x-axis is the number
of real-world samples used,

00:08:07.136 --> 00:08:09.646
and we compared the performance
of different methods

00:08:09.646 --> 00:08:13.057
as we vary them onto real-world data
given to the model.

00:08:13.057 --> 00:08:16.727
The blue bar is our performance
when we use only simulated data.

00:08:16.727 --> 00:08:20.129
The red bar is our performance
when we use only real data,

00:08:20.129 --> 00:08:24.297
and the orange bar is our performance
when we use both simulated and real data

00:08:24.297 --> 00:08:27.667
and the domain adaptation methods
that I've been talking about.

00:08:27.667 --> 00:08:30.611
And what we found is that
when we use just 2%

00:08:30.611 --> 00:08:32.801
of our original real-world dataset,

00:08:32.801 --> 00:08:35.470
and we apply domain adaptation to it,

00:08:35.470 --> 00:08:37.696
we're able to get
the same level of performance

00:08:37.696 --> 00:08:41.238
so this reduces the number 
of real-world samples we needed

00:08:41.238 --> 00:08:43.366
by up to 50 times,
which is really exciting

00:08:43.366 --> 00:08:46.515
in terms of not needing to run
robots for a large amount of time

00:08:46.515 --> 00:08:48.749
to learn these grasping behaviors.

00:08:48.749 --> 00:08:52.149
Additionally, we found 
that even when we give

00:08:52.149 --> 00:08:54.652
all of the real-world data to the model,

00:08:54.652 --> 00:08:56.586
when we give simulated data as well,

00:08:56.586 --> 00:08:59.055
we're still able to see
improved performance

00:08:59.055 --> 00:09:01.589
so that implies that we haven't hit
the data capacity limits

00:09:01.589 --> 00:09:03.824
for this grasping problem.

00:09:03.824 --> 00:09:07.561
And finally, there's a way
to train this setup

00:09:07.561 --> 00:09:09.859
without having real-world labels,

00:09:09.859 --> 00:09:11.762
and when we trained 
the model in this setting,

00:09:11.762 --> 00:09:14.665
we found that we were still able to get
pretty good performance

00:09:14.665 --> 00:09:17.031
on the real-world robot.

00:09:19.091 --> 00:09:21.536
Now, this was the work of a large team

00:09:21.536 --> 00:09:24.170
across both Brain as well as X.

00:09:24.170 --> 00:09:26.406
I'd like to thank all of my collaborators.

00:09:26.406 --> 00:09:28.641
Here's a link to the original paper.

00:09:28.641 --> 00:09:31.341
And I believe there is also a blog post,

00:09:31.341 --> 00:09:34.565
if people are interested
in hearing more details.

00:09:34.565 --> 00:09:35.675
Thanks.

00:09:35.675 --> 00:09:38.119
(applause)

00:09:38.781 --> 00:09:41.928
♪ (music) ♪

