WEBVTT
Kind: captions
Language: en

00:00:07.257 --> 00:00:09.309
This is a thought experiment.

00:00:09.309 --> 00:00:11.917
Let's say at some point
in the not so distant future,

00:00:11.917 --> 00:00:15.505
you're barreling down the highway
in your self-driving car,

00:00:15.505 --> 00:00:19.809
and you find yourself boxed in
on all sides by other cars.

00:00:19.809 --> 00:00:24.213
Suddenly, a large, heavy object
falls off the truck in front of you.

00:00:24.213 --> 00:00:27.364
Your car can't stop in time
to avoid the collision,

00:00:27.364 --> 00:00:29.415
so it needs to make a decision:

00:00:29.415 --> 00:00:31.673
go straight and hit the object,

00:00:31.673 --> 00:00:33.953
swerve left into an SUV,

00:00:33.953 --> 00:00:36.940
or swerve right into a motorcycle.

00:00:36.940 --> 00:00:40.450
Should it prioritize your safety
by hitting the motorcycle,

00:00:40.450 --> 00:00:43.267
minimize danger to others by not swerving,

00:00:43.267 --> 00:00:47.346
even if it means hitting the large object
and sacrificing your life,

00:00:47.346 --> 00:00:50.104
or take the middle ground
by hitting the SUV,

00:00:50.104 --> 00:00:53.087
which has a high passenger safety rating?

00:00:53.087 --> 00:00:56.298
So what should the self-driving car do?

00:00:56.298 --> 00:00:59.505
If we were driving that boxed in car
in manual mode,

00:00:59.505 --> 00:01:03.034
whichever way we'd react
would be understood as just that,

00:01:03.034 --> 00:01:04.320
a reaction,

00:01:04.320 --> 00:01:06.571
not a deliberate decision.

00:01:06.571 --> 00:01:10.869
It would be an instinctual panicked move
with no forethought or malice.

00:01:10.869 --> 00:01:14.519
But if a programmer were to instruct
the car to make the same move,

00:01:14.519 --> 00:01:17.316
given conditions it may 
sense in the future,

00:01:17.316 --> 00:01:21.619
well, that looks more
like premeditated homicide.

00:01:21.619 --> 00:01:22.637
Now, to be fair,

00:01:22.637 --> 00:01:26.709
self-driving cars are are predicted 
to dramatically reduce traffic accidents

00:01:26.709 --> 00:01:27.956
and fatalities

00:01:27.956 --> 00:01:31.398
by removing human error 
from the driving equation.

00:01:31.398 --> 00:01:33.512
Plus, there may be all sorts 
of other benefits:

00:01:33.512 --> 00:01:35.150
eased road congestion,

00:01:35.150 --> 00:01:36.723
decreased harmful emissions,

00:01:36.723 --> 00:01:41.345
and minimized unproductive
and stressful driving time.

00:01:41.345 --> 00:01:43.716
But accidents can and will still happen,

00:01:43.716 --> 00:01:44.685
and when they do,

00:01:44.685 --> 00:01:49.147
their outcomes may be determined
months or years in advance

00:01:49.147 --> 00:01:51.752
by programmers or policy makers.

00:01:51.752 --> 00:01:54.252
And they'll have 
some difficult decisions to make.

00:01:54.252 --> 00:01:57.204
It's tempting to offer up general 
decision-making principles,

00:01:57.204 --> 00:01:59.099
like minimize harm,

00:01:59.099 --> 00:02:02.475
but even that quickly leads 
to morally murky decisions.

00:02:02.475 --> 00:02:03.633
For example,

00:02:03.633 --> 00:02:05.641
let's say we have the same initial set up,

00:02:05.641 --> 00:02:08.512
but now there's a motorcyclist 
wearing a helmet to your left

00:02:08.512 --> 00:02:11.309
and another one without 
a helmet to your right.

00:02:11.309 --> 00:02:14.360
Which one should 
your robot car crash into?

00:02:14.360 --> 00:02:18.442
If you say the biker with the helmet
because she's more likely to survive,

00:02:18.442 --> 00:02:21.771
then aren't you penalizing 
the responsible motorist?

00:02:21.771 --> 00:02:24.115
If, instead, you save the biker 
without the helmet

00:02:24.115 --> 00:02:26.106
because he's acting irresponsibly,

00:02:26.106 --> 00:02:31.017
then you've gone way beyond the initial
design principle about minimizing harm,

00:02:31.017 --> 00:02:34.871
and the robot car is now 
meting out street justice.

00:02:34.871 --> 00:02:38.403
The ethical considerations 
get more complicated here.

00:02:38.403 --> 00:02:39.807
In both of our scenarios,

00:02:39.807 --> 00:02:44.400
the underlying design is functioning
as a targeting algorithm of sorts.

00:02:44.400 --> 00:02:45.299
In other words,

00:02:45.299 --> 00:02:47.809
it's systematically favoring 
or discriminating

00:02:47.809 --> 00:02:51.298
against a certain type 
of object to crash into.

00:02:51.298 --> 00:02:53.613
And the owners of the target vehicles

00:02:53.613 --> 00:02:56.662
will suffer the negative consequences
of this algorithm

00:02:56.662 --> 00:02:58.747
through no fault of their own.

00:02:58.747 --> 00:03:03.401
Our new technologies are opening up
many other novel ethical dilemmas.

00:03:03.401 --> 00:03:05.482
For instance, if you had to 
choose between

00:03:05.482 --> 00:03:09.534
a car that would always save
as many lives as possible in an accident,

00:03:09.534 --> 00:03:12.565
or one that would save you at any cost,

00:03:12.565 --> 00:03:14.262
which would you buy?

00:03:14.262 --> 00:03:17.577
What happens if the cars start analyzing
and factoring in

00:03:17.577 --> 00:03:21.047
the passengers of the cars
and the particulars of their lives?

00:03:21.047 --> 00:03:23.219
Could it be the case 
that a random decision

00:03:23.219 --> 00:03:28.125
is still better than a predetermined one
designed to minimize harm?

00:03:28.125 --> 00:03:30.878
And who should be making 
all of these decisions anyhow?

00:03:30.878 --> 00:03:31.972
Programmers?

00:03:31.972 --> 00:03:32.908
Companies?

00:03:32.908 --> 00:03:34.352
Governments?

00:03:34.352 --> 00:03:37.566
Reality may not play out exactly
like our thought experiments,

00:03:37.566 --> 00:03:39.232
but that's not the point.

00:03:39.232 --> 00:03:43.603
They're designed to isolate 
and stress test our intuitions on ethics,

00:03:43.603 --> 00:03:46.589
just like science experiments do
for the physical world.

00:03:46.589 --> 00:03:49.979
Spotting these moral hairpin turns now

00:03:49.979 --> 00:03:53.550
will help us maneuver the unfamiliar road
of technology ethics,

00:03:53.550 --> 00:03:57.301
and allow us to cruise confidently
and conscientiously

00:03:57.301 --> 00:03:59.648
into our brave new future.

