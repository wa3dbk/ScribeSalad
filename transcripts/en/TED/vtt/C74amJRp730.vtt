WEBVTT
Kind: captions
Language: en

00:00:12.960 --> 00:00:14.160
I want you to imagine

00:00:15.000 --> 00:00:16.200
walking into a room,

00:00:17.480 --> 00:00:19.616
a control room with a bunch of people,

00:00:19.640 --> 00:00:22.480
a hundred people, hunched
over a desk with little dials,

00:00:23.280 --> 00:00:24.800
and that that control room

00:00:25.680 --> 00:00:29.376
will shape the thoughts and feelings

00:00:29.400 --> 00:00:30.640
of a billion people.

00:00:32.560 --> 00:00:34.440
This might sound like science fiction,

00:00:35.320 --> 00:00:37.536
but this actually exists

00:00:37.560 --> 00:00:38.760
right now, today.

00:00:40.040 --> 00:00:43.400
I know because I used to be
in one of those control rooms.

00:00:44.159 --> 00:00:46.456
I was a design ethicist at Google,

00:00:46.480 --> 00:00:49.840
where I studied how do you ethically
steer people's thoughts?

00:00:50.560 --> 00:00:53.456
Because what we don't talk about
is how the handful of people

00:00:53.480 --> 00:00:56.016
working at a handful
of technology companies

00:00:56.040 --> 00:01:01.080
through their choices will steer
what a billion people are thinking today.

00:01:02.400 --> 00:01:04.136
Because when you pull out your phone

00:01:04.160 --> 00:01:07.256
and they design how this works
or what's on the feed,

00:01:07.280 --> 00:01:10.496
it's scheduling little blocks
of time in our minds.

00:01:10.520 --> 00:01:13.656
If you see a notification,
it schedules you to have thoughts

00:01:13.680 --> 00:01:15.720
that maybe you didn't intend to have.

00:01:16.400 --> 00:01:19.016
If you swipe over that notification,

00:01:19.040 --> 00:01:21.421
it schedules you into spending
a little bit of time

00:01:21.445 --> 00:01:22.826
getting sucked into something

00:01:22.850 --> 00:01:25.805
that maybe you didn't intend
to get sucked into.

00:01:27.320 --> 00:01:28.840
When we talk about technology,

00:01:30.040 --> 00:01:32.736
we tend to talk about it
as this blue sky opportunity.

00:01:32.760 --> 00:01:34.240
It could go any direction.

00:01:35.400 --> 00:01:37.256
And I want to get serious for a moment

00:01:37.280 --> 00:01:39.960
and tell you why it's going
in a very specific direction.

00:01:40.840 --> 00:01:43.040
Because it's not evolving randomly.

00:01:44.000 --> 00:01:46.016
There's a hidden goal
driving the direction

00:01:46.040 --> 00:01:48.176
of all of the technology we make,

00:01:48.200 --> 00:01:51.120
and that goal is the race
for our attention.

00:01:52.840 --> 00:01:55.576
Because every news site,

00:01:55.600 --> 00:01:58.336
TED, elections, politicians,

00:01:58.360 --> 00:02:00.336
games, even meditation apps

00:02:00.360 --> 00:02:02.320
have to compete for one thing,

00:02:03.160 --> 00:02:04.896
which is our attention,

00:02:04.920 --> 00:02:06.520
and there's only so much of it.

00:02:08.440 --> 00:02:10.856
And the best way to get people's attention

00:02:10.880 --> 00:02:13.320
is to know how someone's mind works.

00:02:13.800 --> 00:02:16.136
And there's a whole bunch
of persuasive techniques

00:02:16.160 --> 00:02:19.656
that I learned in college at a lab
called the Persuasive Technology Lab

00:02:19.680 --> 00:02:21.280
to get people's attention.

00:02:21.880 --> 00:02:23.360
A simple example is YouTube.

00:02:24.000 --> 00:02:26.936
YouTube wants to maximize
how much time you spend.

00:02:26.960 --> 00:02:28.160
And so what do they do?

00:02:28.840 --> 00:02:31.120
They autoplay the next video.

00:02:31.760 --> 00:02:33.576
And let's say that works really well.

00:02:33.600 --> 00:02:36.016
They're getting a little bit
more of people's time.

00:02:36.040 --> 00:02:38.416
Well, if you're Netflix,
you look at that and say,

00:02:38.440 --> 00:02:40.298
well, that's shrinking my market share,

00:02:40.322 --> 00:02:42.322
so I'm going to autoplay the next episode.

00:02:43.320 --> 00:02:44.696
But then if you're Facebook,

00:02:44.720 --> 00:02:47.056
you say, that's shrinking
all of my market share,

00:02:47.080 --> 00:02:49.736
so now I have to autoplay
all the videos in the newsfeed

00:02:49.760 --> 00:02:51.522
before waiting for you to click play.

00:02:52.320 --> 00:02:55.480
So the internet is not evolving at random.

00:02:56.320 --> 00:03:00.736
The reason it feels
like it's sucking us in the way it is

00:03:00.760 --> 00:03:03.136
is because of this race for attention.

00:03:03.160 --> 00:03:04.576
We know where this is going.

00:03:04.600 --> 00:03:06.120
Technology is not neutral,

00:03:07.320 --> 00:03:10.736
and it becomes this race
to the bottom of the brain stem

00:03:10.760 --> 00:03:12.960
of who can go lower to get it.

00:03:13.920 --> 00:03:16.256
Let me give you an example of Snapchat.

00:03:16.280 --> 00:03:19.976
If you didn't know,
Snapchat is the number one way

00:03:20.000 --> 00:03:22.256
that teenagers in
the United States communicate.

00:03:22.280 --> 00:03:26.456
So if you're like me, and you use
text messages to communicate,

00:03:26.480 --> 00:03:28.256
Snapchat is that for teenagers,

00:03:28.280 --> 00:03:30.976
and there's, like,
a hundred million of them that use it.

00:03:31.000 --> 00:03:33.216
And they invented
a feature called Snapstreaks,

00:03:33.240 --> 00:03:35.136
which shows the number of days in a row

00:03:35.160 --> 00:03:37.776
that two people have
communicated with each other.

00:03:37.800 --> 00:03:39.656
In other words, what they just did

00:03:39.680 --> 00:03:42.640
is they gave two people
something they don't want to lose.

00:03:44.000 --> 00:03:47.456
Because if you're a teenager,
and you have 150 days in a row,

00:03:47.480 --> 00:03:49.456
you don't want that to go away.

00:03:49.480 --> 00:03:53.640
And so think of the little blocks of time
that that schedules in kids' minds.

00:03:54.160 --> 00:03:56.496
This isn't theoretical:
when kids go on vacation,

00:03:56.520 --> 00:03:59.776
it's been shown they give their passwords
to up to five other friends

00:03:59.800 --> 00:04:02.016
to keep their Snapstreaks going,

00:04:02.040 --> 00:04:04.056
even when they can't do it.

00:04:04.080 --> 00:04:06.016
And they have, like, 30 of these things,

00:04:06.040 --> 00:04:09.416
and so they have to get through
taking photos of just pictures or walls

00:04:09.440 --> 00:04:11.920
or ceilings just to get through their day.

00:04:13.200 --> 00:04:15.896
So it's not even like
they're having real conversations.

00:04:15.920 --> 00:04:17.856
We have a temptation to think about this

00:04:17.880 --> 00:04:20.576
as, oh, they're just using Snapchat

00:04:20.600 --> 00:04:22.616
the way we used to
gossip on the telephone.

00:04:22.640 --> 00:04:23.840
It's probably OK.

00:04:24.480 --> 00:04:26.736
Well, what this misses
is that in the 1970s,

00:04:26.760 --> 00:04:29.375
when you were just
gossiping on the telephone,

00:04:29.399 --> 00:04:32.416
there wasn't a hundred engineers
on the other side of the screen

00:04:32.440 --> 00:04:34.496
who knew exactly
how your psychology worked

00:04:34.520 --> 00:04:37.160
and orchestrated you
into a double bind with each other.

00:04:38.440 --> 00:04:41.840
Now, if this is making you
feel a little bit of outrage,

00:04:42.680 --> 00:04:45.256
notice that that thought
just comes over you.

00:04:45.280 --> 00:04:48.600
Outrage is a really good way also
of getting your attention,

00:04:49.880 --> 00:04:51.456
because we don't choose outrage.

00:04:51.480 --> 00:04:52.896
It happens to us.

00:04:52.920 --> 00:04:54.776
And if you're the Facebook newsfeed,

00:04:54.800 --> 00:04:56.216
whether you'd want to or not,

00:04:56.240 --> 00:04:58.976
you actually benefit when there's outrage.

00:04:59.000 --> 00:05:01.936
Because outrage
doesn't just schedule a reaction

00:05:01.960 --> 00:05:04.840
in emotional time, space, for you.

00:05:05.440 --> 00:05:07.856
We want to share that outrage
with other people.

00:05:07.880 --> 00:05:09.456
So we want to hit share and say,

00:05:09.480 --> 00:05:11.520
"Can you believe the thing
that they said?"

00:05:12.520 --> 00:05:15.896
And so outrage works really well
at getting attention,

00:05:15.920 --> 00:05:19.816
such that if Facebook had a choice
between showing you the outrage feed

00:05:19.840 --> 00:05:21.160
and a calm newsfeed,

00:05:22.120 --> 00:05:24.256
they would want
to show you the outrage feed,

00:05:24.280 --> 00:05:26.336
not because someone
consciously chose that,

00:05:26.360 --> 00:05:29.040
but because that worked better
at getting your attention.

00:05:31.120 --> 00:05:36.600
And the newsfeed control room
is not accountable to us.

00:05:37.040 --> 00:05:39.336
It's only accountable
to maximizing attention.

00:05:39.360 --> 00:05:40.576
It's also accountable,

00:05:40.600 --> 00:05:42.976
because of the business model
of advertising,

00:05:43.000 --> 00:05:46.336
for anybody who can pay the most
to actually walk into the control room

00:05:46.360 --> 00:05:47.936
and say, "That group over there,

00:05:47.960 --> 00:05:50.600
I want to schedule these thoughts
into their minds."

00:05:51.760 --> 00:05:52.960
So you can target,

00:05:54.040 --> 00:05:55.976
you can precisely target a lie

00:05:56.000 --> 00:05:58.920
directly to the people
who are most susceptible.

00:06:00.080 --> 00:06:02.960
And because this is profitable,
it's only going to get worse.

00:06:05.040 --> 00:06:06.840
So I'm here today

00:06:08.160 --> 00:06:10.160
because the costs are so obvious.

00:06:12.280 --> 00:06:14.416
I don't know a more urgent
problem than this,

00:06:14.440 --> 00:06:17.560
because this problem
is underneath all other problems.

00:06:18.720 --> 00:06:21.896
It's not just taking away our agency

00:06:21.920 --> 00:06:24.520
to spend our attention
and live the lives that we want,

00:06:25.720 --> 00:06:29.256
it's changing the way
that we have our conversations,

00:06:29.280 --> 00:06:31.016
it's changing our democracy,

00:06:31.040 --> 00:06:33.656
and it's changing our ability
to have the conversations

00:06:33.680 --> 00:06:35.680
and relationships we want with each other.

00:06:37.160 --> 00:06:38.936
And it affects everyone,

00:06:38.960 --> 00:06:42.320
because a billion people
have one of these in their pocket.

00:06:45.360 --> 00:06:46.760
So how do we fix this?

00:06:49.080 --> 00:06:52.016
We need to make three radical changes

00:06:52.040 --> 00:06:53.840
to technology and to our society.

00:06:55.720 --> 00:06:59.520
The first is we need to acknowledge
that we are persuadable.

00:07:00.840 --> 00:07:02.216
Once you start understanding

00:07:02.240 --> 00:07:05.016
that your mind can be scheduled
into having little thoughts

00:07:05.040 --> 00:07:07.616
or little blocks of time
that you didn't choose,

00:07:07.640 --> 00:07:09.696
wouldn't we want to use that understanding

00:07:09.720 --> 00:07:11.880
and protect against the way
that that happens?

00:07:12.600 --> 00:07:15.896
I think we need to see ourselves
fundamentally in a new way.

00:07:15.920 --> 00:07:18.136
It's almost like a new period
of human history,

00:07:18.160 --> 00:07:19.376
like the Enlightenment,

00:07:19.400 --> 00:07:21.616
but almost a kind of
self-aware Enlightenment,

00:07:21.640 --> 00:07:23.720
that we can be persuaded,

00:07:24.320 --> 00:07:26.560
and there might be something
we want to protect.

00:07:27.400 --> 00:07:31.976
The second is we need new models
and accountability systems

00:07:32.000 --> 00:07:35.496
so that as the world gets better
and more and more persuasive over time --

00:07:35.520 --> 00:07:37.856
because it's only going
to get more persuasive --

00:07:37.880 --> 00:07:39.736
that the people in those control rooms

00:07:39.760 --> 00:07:42.216
are accountable and transparent
to what we want.

00:07:42.240 --> 00:07:44.936
The only form of ethical
persuasion that exists

00:07:44.960 --> 00:07:46.896
is when the goals of the persuader

00:07:46.920 --> 00:07:49.120
are aligned with the goals
of the persuadee.

00:07:49.640 --> 00:07:53.480
And that involves questioning big things,
like the business model of advertising.

00:07:54.720 --> 00:07:56.296
Lastly,

00:07:56.320 --> 00:07:58.000
we need a design renaissance,

00:07:59.080 --> 00:08:02.136
because once you have
this view of human nature,

00:08:02.160 --> 00:08:05.136
that you can steer the timelines
of a billion people --

00:08:05.160 --> 00:08:07.896
just imagine, there's people
who have some desire

00:08:07.920 --> 00:08:10.776
about what they want to do
and what they want to be thinking

00:08:10.800 --> 00:08:13.936
and what they want to be feeling
and how they want to be informed,

00:08:13.960 --> 00:08:16.496
and we're all just tugged
into these other directions.

00:08:16.520 --> 00:08:20.216
And you have a billion people just tugged
into all these different directions.

00:08:20.240 --> 00:08:22.296
Well, imagine an entire design renaissance

00:08:22.320 --> 00:08:25.416
that tried to orchestrate
the exact and most empowering

00:08:25.440 --> 00:08:28.576
time-well-spent way
for those timelines to happen.

00:08:28.600 --> 00:08:30.256
And that would involve two things:

00:08:30.280 --> 00:08:32.416
one would be protecting
against the timelines

00:08:32.440 --> 00:08:34.296
that we don't want to be experiencing,

00:08:34.320 --> 00:08:36.736
the thoughts that we
wouldn't want to be happening,

00:08:36.760 --> 00:08:40.096
so that when that ding happens,
not having the ding that sends us away;

00:08:40.120 --> 00:08:43.736
and the second would be empowering us
to live out the timeline that we want.

00:08:43.760 --> 00:08:45.640
So let me give you a concrete example.

00:08:46.280 --> 00:08:48.736
Today, let's say your friend
cancels dinner on you,

00:08:48.760 --> 00:08:52.535
and you are feeling a little bit lonely.

00:08:52.559 --> 00:08:54.376
And so what do you do in that moment?

00:08:54.400 --> 00:08:55.679
You open up Facebook.

00:08:56.960 --> 00:08:58.656
And in that moment,

00:08:58.680 --> 00:09:02.056
the designers in the control room
want to schedule exactly one thing,

00:09:02.080 --> 00:09:05.120
which is to maximize how much time
you spend on the screen.

00:09:06.640 --> 00:09:10.536
Now, instead, imagine if those designers
created a different timeline

00:09:10.560 --> 00:09:14.056
that was the easiest way,
using all of their data,

00:09:14.080 --> 00:09:17.176
to actually help you get out
with the people that you care about?

00:09:17.200 --> 00:09:22.616
Just think, alleviating
all loneliness in society,

00:09:22.640 --> 00:09:26.136
if that was the timeline that Facebook
wanted to make possible for people.

00:09:26.160 --> 00:09:27.875
Or imagine a different conversation.

00:09:27.899 --> 00:09:31.216
Let's say you wanted to post
something supercontroversial on Facebook,

00:09:31.240 --> 00:09:33.656
which is a really important
thing to be able to do,

00:09:33.680 --> 00:09:35.376
to talk about controversial topics.

00:09:35.400 --> 00:09:37.736
And right now, when there's
that big comment box,

00:09:37.760 --> 00:09:41.136
it's almost asking you,
what key do you want to type?

00:09:41.160 --> 00:09:43.976
In other words, it's scheduling
a little timeline of things

00:09:44.000 --> 00:09:46.136
you're going to continue
to do on the screen.

00:09:46.160 --> 00:09:49.136
And imagine instead that there was
another button there saying,

00:09:49.160 --> 00:09:51.216
what would be most
time well spent for you?

00:09:51.240 --> 00:09:52.816
And you click "host a dinner."

00:09:52.840 --> 00:09:54.936
And right there
underneath the item it said,

00:09:54.960 --> 00:09:56.656
"Who wants to RSVP for the dinner?"

00:09:56.680 --> 00:09:59.936
And so you'd still have a conversation
about something controversial,

00:09:59.960 --> 00:10:03.696
but you'd be having it in the most
empowering place on your timeline,

00:10:03.720 --> 00:10:06.736
which would be at home that night
with a bunch of a friends over

00:10:06.760 --> 00:10:07.960
to talk about it.

00:10:09.000 --> 00:10:12.160
So imagine we're running, like,
a find and replace

00:10:13.000 --> 00:10:15.576
on all of the timelines
that are currently steering us

00:10:15.600 --> 00:10:18.160
towards more and more
screen time persuasively

00:10:19.080 --> 00:10:21.616
and replacing all of those timelines

00:10:21.640 --> 00:10:23.280
with what do we want in our lives.

00:10:26.960 --> 00:10:28.440
It doesn't have to be this way.

00:10:30.360 --> 00:10:32.616
Instead of handicapping our attention,

00:10:32.640 --> 00:10:35.456
imagine if we used all of this data
and all of this power

00:10:35.480 --> 00:10:37.096
and this new view of human nature

00:10:37.120 --> 00:10:39.976
to give us a superhuman ability to focus

00:10:40.000 --> 00:10:44.136
and a superhuman ability to put
our attention to what we cared about

00:10:44.160 --> 00:10:46.776
and a superhuman ability
to have the conversations

00:10:46.800 --> 00:10:48.800
that we need to have for democracy.

00:10:51.600 --> 00:10:54.280
The most complex challenges in the world

00:10:56.280 --> 00:10:59.400
require not just us
to use our attention individually.

00:11:00.440 --> 00:11:03.760
They require us to use our attention
and coordinate it together.

00:11:04.440 --> 00:11:07.256
Climate change is going to require
that a lot of people

00:11:07.280 --> 00:11:09.376
are being able
to coordinate their attention

00:11:09.400 --> 00:11:11.296
in the most empowering way together.

00:11:11.320 --> 00:11:14.400
And imagine creating
a superhuman ability to do that.

00:11:19.000 --> 00:11:23.160
Sometimes the world's
most pressing and important problems

00:11:24.040 --> 00:11:27.880
are not these hypothetical future things
that we could create in the future.

00:11:28.560 --> 00:11:30.296
Sometimes the most pressing problems

00:11:30.320 --> 00:11:32.656
are the ones that are
right underneath our noses,

00:11:32.680 --> 00:11:35.800
the things that are already directing
a billion people's thoughts.

00:11:36.600 --> 00:11:39.976
And maybe instead of getting excited
about the new augmented reality

00:11:40.000 --> 00:11:43.296
and virtual reality
and these cool things that could happen,

00:11:43.320 --> 00:11:46.616
which are going to be susceptible
to the same race for attention,

00:11:46.640 --> 00:11:48.816
if we could fix the race for attention

00:11:48.840 --> 00:11:51.560
on the thing that's already
in a billion people's pockets.

00:11:52.040 --> 00:11:53.616
Maybe instead of getting excited

00:11:53.640 --> 00:11:57.816
about the most exciting
new cool fancy education apps,

00:11:57.840 --> 00:12:00.736
we could fix the way
kids' minds are getting manipulated

00:12:00.760 --> 00:12:03.240
into sending empty messages
back and forth.

00:12:04.040 --> 00:12:08.336
(Applause)

00:12:08.360 --> 00:12:09.616
Maybe instead of worrying

00:12:09.640 --> 00:12:13.416
about hypothetical future
runaway artificial intelligences

00:12:13.440 --> 00:12:15.320
that are maximizing for one goal,

00:12:16.680 --> 00:12:19.336
we could solve the runaway
artificial intelligence

00:12:19.360 --> 00:12:21.416
that already exists right now,

00:12:21.440 --> 00:12:24.360
which are these newsfeeds
maximizing for one thing.

00:12:26.080 --> 00:12:29.896
It's almost like instead of running away
to colonize new planets,

00:12:29.920 --> 00:12:31.976
we could fix the one
that we're already on.

00:12:32.000 --> 00:12:36.120
(Applause)

00:12:40.040 --> 00:12:41.816
Solving this problem

00:12:41.840 --> 00:12:45.640
is critical infrastructure
for solving every other problem.

00:12:46.600 --> 00:12:50.616
There's nothing in your life
or in our collective problems

00:12:50.640 --> 00:12:54.200
that does not require our ability
to put our attention where we care about.

00:12:55.800 --> 00:12:57.040
At the end of our lives,

00:12:58.240 --> 00:13:00.880
all we have is our attention and our time.

00:13:01.800 --> 00:13:03.696
What will be time well spent for ours?

00:13:03.720 --> 00:13:04.936
Thank you.

00:13:04.960 --> 00:13:08.080
(Applause)

00:13:17.760 --> 00:13:20.696
Chris Anderson: Tristan, thank you.
Hey, stay up here a sec.

00:13:20.720 --> 00:13:22.056
First of all, thank you.

00:13:22.080 --> 00:13:24.856
I know we asked you to do this talk
on pretty short notice,

00:13:24.880 --> 00:13:27.096
and you've had quite a stressful week

00:13:27.120 --> 00:13:29.560
getting this thing together, so thank you.

00:13:30.680 --> 00:13:34.656
Some people listening might say,
what you complain about is addiction,

00:13:34.680 --> 00:13:38.176
and all these people doing this stuff,
for them it's actually interesting.

00:13:38.200 --> 00:13:39.456
All these design decisions

00:13:39.480 --> 00:13:42.576
have built user content
that is fantastically interesting.

00:13:42.600 --> 00:13:45.016
The world's more interesting
than it ever has been.

00:13:45.040 --> 00:13:46.296
What's wrong with that?

00:13:46.320 --> 00:13:48.576
Tristan Harris:
I think it's really interesting.

00:13:48.600 --> 00:13:52.616
One way to see this
is if you're just YouTube, for example,

00:13:52.640 --> 00:13:55.296
you want to always show
the more interesting next video.

00:13:55.320 --> 00:13:58.336
You want to get better and better
at suggesting that next video,

00:13:58.360 --> 00:14:00.816
but even if you could propose
the perfect next video

00:14:00.840 --> 00:14:02.496
that everyone would want to watch,

00:14:02.520 --> 00:14:05.856
it would just be better and better
at keeping you hooked on the screen.

00:14:05.880 --> 00:14:07.536
So what's missing in that equation

00:14:07.560 --> 00:14:09.696
is figuring out what
our boundaries would be.

00:14:09.720 --> 00:14:12.936
You would want YouTube to know
something about, say, falling asleep.

00:14:12.960 --> 00:14:14.576
The CEO of Netflix recently said,

00:14:14.600 --> 00:14:17.336
"our biggest competitors
are Facebook, YouTube and sleep."

00:14:17.360 --> 00:14:21.816
And so what we need to recognize
is that the human architecture is limited

00:14:21.840 --> 00:14:24.816
and that we have certain boundaries
or dimensions of our lives

00:14:24.840 --> 00:14:26.816
that we want to be honored and respected,

00:14:26.840 --> 00:14:28.656
and technology could help do that.

00:14:28.680 --> 00:14:31.296
(Applause)

00:14:31.320 --> 00:14:33.016
CA: I mean, could you make the case

00:14:33.040 --> 00:14:39.096
that part of the problem here is that
we've got a naÃ¯ve model of human nature?

00:14:39.120 --> 00:14:41.856
So much of this is justified
in terms of human preference,

00:14:41.880 --> 00:14:44.496
where we've got these algorithms
that do an amazing job

00:14:44.520 --> 00:14:46.216
of optimizing for human preference,

00:14:46.240 --> 00:14:47.576
but which preference?

00:14:47.600 --> 00:14:51.096
There's the preferences
of things that we really care about

00:14:51.120 --> 00:14:52.496
when we think about them

00:14:52.520 --> 00:14:55.576
versus the preferences
of what we just instinctively click on.

00:14:55.600 --> 00:15:00.256
If we could implant that more nuanced
view of human nature in every design,

00:15:00.280 --> 00:15:01.736
would that be a step forward?

00:15:01.760 --> 00:15:03.736
TH: Absolutely. I mean, I think right now

00:15:03.760 --> 00:15:07.256
it's as if all of our technology
is basically only asking our lizard brain

00:15:07.280 --> 00:15:09.776
what's the best way
to just impulsively get you to do

00:15:09.800 --> 00:15:11.936
the next tiniest thing with your time,

00:15:11.960 --> 00:15:13.616
instead of asking you in your life

00:15:13.640 --> 00:15:15.816
what we would be most
time well spent for you?

00:15:15.840 --> 00:15:19.136
What would be the perfect timeline
that might include something later,

00:15:19.160 --> 00:15:22.336
would be time well spent for you
here at TED in your last day here?

00:15:22.360 --> 00:15:25.336
CA: So if Facebook and Google
and everyone said to us first up,

00:15:25.360 --> 00:15:28.256
"Hey, would you like us
to optimize for your reflective brain

00:15:28.280 --> 00:15:29.936
or your lizard brain? You choose."

00:15:29.960 --> 00:15:32.040
TH: Right. That would be one way. Yes.

00:15:34.358 --> 00:15:37.216
CA: You said persuadability,
that's an interesting word to me

00:15:37.240 --> 00:15:40.096
because to me there's
two different types of persuadability.

00:15:40.120 --> 00:15:42.656
There's the persuadability
that we're trying right now

00:15:42.680 --> 00:15:44.856
of reason and thinking
and making an argument,

00:15:44.880 --> 00:15:47.576
but I think you're almost
talking about a different kind,

00:15:47.590 --> 00:15:49.496
a more visceral type of persuadability,

00:15:49.520 --> 00:15:52.416
of being persuaded without
even knowing that you're thinking.

00:15:52.440 --> 00:15:55.296
TH: Exactly. The reason
I care about this problem so much is

00:15:55.320 --> 00:15:58.496
I studied at a lab called
the Persuasive Technology Lab at Stanford

00:15:58.520 --> 00:16:01.066
that taught [students how to recognize]
exactly these techniques.

00:16:01.106 --> 00:16:04.096
There's conferences and workshops
that teach people all these covert ways

00:16:04.120 --> 00:16:07.096
of getting people's attention
and orchestrating people's lives.

00:16:07.120 --> 00:16:09.776
And it's because most people
don't know that that exists

00:16:09.800 --> 00:16:11.696
that this conversation is so important.

00:16:11.720 --> 00:16:15.496
CA: Tristan, you and I, we both know
so many people from all these companies.

00:16:15.520 --> 00:16:17.496
There are actually many here in the room,

00:16:17.520 --> 00:16:19.997
and I don't know about you,
but my experience of them

00:16:20.021 --> 00:16:22.096
is that there is
no shortage of good intent.

00:16:22.120 --> 00:16:24.296
People want a better world.

00:16:24.320 --> 00:16:27.840
They are actually -- they really want it.

00:16:28.320 --> 00:16:32.496
And I don't think anything you're saying
is that these are evil people.

00:16:32.520 --> 00:16:36.216
It's a system where there's
these unintended consequences

00:16:36.240 --> 00:16:38.096
that have really got out of control --

00:16:38.120 --> 00:16:39.616
TH: Of this race for attention.

00:16:39.640 --> 00:16:42.816
It's the classic race to the bottom
when you have to get attention,

00:16:42.840 --> 00:16:44.056
and it's so tense.

00:16:44.080 --> 00:16:46.816
The only way to get more
is to go lower on the brain stem,

00:16:46.840 --> 00:16:49.256
to go lower into outrage,
to go lower into emotion,

00:16:49.280 --> 00:16:50.976
to go lower into the lizard brain.

00:16:51.000 --> 00:16:54.816
CA: Well, thank you so much for helping us
all get a little bit wiser about this.

00:16:54.840 --> 00:16:57.256
Tristan Harris, thank you.
TH: Thank you very much.

00:16:57.280 --> 00:16:59.520
(Applause)

