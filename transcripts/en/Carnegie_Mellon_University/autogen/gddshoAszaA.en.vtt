WEBVTT
Kind: captions
Language: en

00:13:23.800 --> 00:13:28.710
you

00:13:28.710 --> 00:13:28.720
 

00:13:28.720 --> 00:15:32.150
you

00:15:32.150 --> 00:15:32.160
 

00:15:32.160 --> 00:15:44.100
[Music]

00:15:44.100 --> 00:15:44.110
 

00:15:44.110 --> 00:20:45.049
you

00:20:45.049 --> 00:20:45.059
 

00:20:45.059 --> 00:20:52.669
you

00:20:52.669 --> 00:20:52.679
 

00:20:52.679 --> 00:23:11.270
you

00:23:11.270 --> 00:23:11.280
 

00:23:11.280 --> 00:23:14.300
ladies and gentlemen foreign of Jahannam

00:23:14.300 --> 00:23:14.310
ladies and gentlemen foreign of Jahannam
 

00:23:14.310 --> 00:23:18.420
ladies and gentlemen foreign of Jahannam
president Carnegie Mellon University

00:23:18.420 --> 00:23:18.430
president Carnegie Mellon University
 

00:23:18.430 --> 00:23:28.130
president Carnegie Mellon University
[Applause]

00:23:28.130 --> 00:23:28.140
 

00:23:28.140 --> 00:23:33.120
well good afternoon okay we're gonna try

00:23:33.120 --> 00:23:33.130
well good afternoon okay we're gonna try
 

00:23:33.130 --> 00:23:35.310
well good afternoon okay we're gonna try
this again good afternoon

00:23:35.310 --> 00:23:35.320
this again good afternoon
 

00:23:35.320 --> 00:23:39.690
this again good afternoon
Oh much better much better it is my

00:23:39.690 --> 00:23:39.700
Oh much better much better it is my
 

00:23:39.700 --> 00:23:42.960
Oh much better much better it is my
great pleasure to extend a warm welcome

00:23:42.960 --> 00:23:42.970
great pleasure to extend a warm welcome
 

00:23:42.970 --> 00:23:46.860
great pleasure to extend a warm welcome
to all of you as we kick off this

00:23:46.860 --> 00:23:46.870
to all of you as we kick off this
 

00:23:46.870 --> 00:23:50.690
to all of you as we kick off this
inaugural CMU K&amp;L gates conference on

00:23:50.690 --> 00:23:50.700
inaugural CMU K&amp;L gates conference on
 

00:23:50.700 --> 00:23:53.270
inaugural CMU K&amp;L gates conference on
ethics and artificial intelligence

00:23:53.270 --> 00:23:53.280
ethics and artificial intelligence
 

00:23:53.280 --> 00:23:56.790
ethics and artificial intelligence
before I get started we pick this date

00:23:56.790 --> 00:23:56.800
before I get started we pick this date
 

00:23:56.800 --> 00:24:02.580
before I get started we pick this date
in April because we knew that this would

00:24:02.580 --> 00:24:02.590
in April because we knew that this would
 

00:24:02.590 --> 00:24:05.510
in April because we knew that this would
be the third week of spring and

00:24:05.510 --> 00:24:05.520
be the third week of spring and
 

00:24:05.520 --> 00:24:08.850
be the third week of spring and
especially for many of our friends and

00:24:08.850 --> 00:24:08.860
especially for many of our friends and
 

00:24:08.860 --> 00:24:11.400
especially for many of our friends and
colleagues from the Bay Area from the

00:24:11.400 --> 00:24:11.410
colleagues from the Bay Area from the
 

00:24:11.410 --> 00:24:15.300
colleagues from the Bay Area from the
west coast and from Texas we knew that

00:24:15.300 --> 00:24:15.310
west coast and from Texas we knew that
 

00:24:15.310 --> 00:24:18.660
west coast and from Texas we knew that
you wanted to enjoy spring in Pittsburgh

00:24:18.660 --> 00:24:18.670
you wanted to enjoy spring in Pittsburgh
 

00:24:18.670 --> 00:24:24.540
you wanted to enjoy spring in Pittsburgh
so you are welcome really happy to

00:24:24.540 --> 00:24:24.550
so you are welcome really happy to
 

00:24:24.550 --> 00:24:27.930
so you are welcome really happy to
welcome all of you on behalf of my

00:24:27.930 --> 00:24:27.940
welcome all of you on behalf of my
 

00:24:27.940 --> 00:24:29.550
welcome all of you on behalf of my
colleagues at Carnegie Mellon we're

00:24:29.550 --> 00:24:29.560
colleagues at Carnegie Mellon we're
 

00:24:29.560 --> 00:24:32.370
colleagues at Carnegie Mellon we're
joined here today by several of our

00:24:32.370 --> 00:24:32.380
joined here today by several of our
 

00:24:32.380 --> 00:24:34.410
joined here today by several of our
trustees and members of the university

00:24:34.410 --> 00:24:34.420
trustees and members of the university
 

00:24:34.420 --> 00:24:38.510
trustees and members of the university
leadership team and by a wide array of

00:24:38.510 --> 00:24:38.520
leadership team and by a wide array of
 

00:24:38.520 --> 00:24:42.030
leadership team and by a wide array of
distinguished CMU faculty and thought

00:24:42.030 --> 00:24:42.040
distinguished CMU faculty and thought
 

00:24:42.040 --> 00:24:44.670
distinguished CMU faculty and thought
leaders from across the country you will

00:24:44.670 --> 00:24:44.680
leaders from across the country you will
 

00:24:44.680 --> 00:24:47.280
leaders from across the country you will
hear from them throughout the day today

00:24:47.280 --> 00:24:47.290
hear from them throughout the day today
 

00:24:47.290 --> 00:24:50.880
hear from them throughout the day today
and tomorrow as one of Carnegie Mellon's

00:24:50.880 --> 00:24:50.890
and tomorrow as one of Carnegie Mellon's
 

00:24:50.890 --> 00:24:54.150
and tomorrow as one of Carnegie Mellon's
newest initiatives and annual traditions

00:24:54.150 --> 00:24:54.160
newest initiatives and annual traditions
 

00:24:54.160 --> 00:24:57.030
newest initiatives and annual traditions
this conference brings together thought

00:24:57.030 --> 00:24:57.040
this conference brings together thought
 

00:24:57.040 --> 00:24:59.250
this conference brings together thought
leaders from diverse perspectives to

00:24:59.250 --> 00:24:59.260
leaders from diverse perspectives to
 

00:24:59.260 --> 00:25:03.660
leaders from diverse perspectives to
discuss the ethical social and policy

00:25:03.660 --> 00:25:03.670
discuss the ethical social and policy
 

00:25:03.670 --> 00:25:06.900
discuss the ethical social and policy
ramifications brought on by advances in

00:25:06.900 --> 00:25:06.910
ramifications brought on by advances in
 

00:25:06.910 --> 00:25:08.340
ramifications brought on by advances in
artificial intelligence and

00:25:08.340 --> 00:25:08.350
artificial intelligence and
 

00:25:08.350 --> 00:25:11.460
artificial intelligence and
computational technologies this two-day

00:25:11.460 --> 00:25:11.470
computational technologies this two-day
 

00:25:11.470 --> 00:25:14.460
computational technologies this two-day
event has been made possible by an

00:25:14.460 --> 00:25:14.470
event has been made possible by an
 

00:25:14.470 --> 00:25:17.940
event has been made possible by an
exceptional coalition of support first I

00:25:17.940 --> 00:25:17.950
exceptional coalition of support first I
 

00:25:17.950 --> 00:25:19.950
exceptional coalition of support first I
want to take a moment to thank our ex

00:25:19.950 --> 00:25:19.960
want to take a moment to thank our ex
 

00:25:19.960 --> 00:25:23.790
want to take a moment to thank our ex
ordinary partners at K&amp;L gates ever

00:25:23.790 --> 00:25:23.800
ordinary partners at K&amp;L gates ever
 

00:25:23.800 --> 00:25:25.830
ordinary partners at K&amp;L gates ever
since we began discussing the

00:25:25.830 --> 00:25:25.840
since we began discussing the
 

00:25:25.840 --> 00:25:28.950
since we began discussing the
possibility of this new form I've been

00:25:28.950 --> 00:25:28.960
possibility of this new form I've been
 

00:25:28.960 --> 00:25:32.460
possibility of this new form I've been
impressed by the thought and foresight

00:25:32.460 --> 00:25:32.470
impressed by the thought and foresight
 

00:25:32.470 --> 00:25:36.690
impressed by the thought and foresight
of our K&amp;L gates partners who understood

00:25:36.690 --> 00:25:36.700
of our K&amp;L gates partners who understood
 

00:25:36.700 --> 00:25:39.630
of our K&amp;L gates partners who understood
the far-reaching the effects of these

00:25:39.630 --> 00:25:39.640
the far-reaching the effects of these
 

00:25:39.640 --> 00:25:43.020
the far-reaching the effects of these
new technologies not only in the legal

00:25:43.020 --> 00:25:43.030
new technologies not only in the legal
 

00:25:43.030 --> 00:25:45.720
new technologies not only in the legal
world but also throughout society and

00:25:45.720 --> 00:25:45.730
world but also throughout society and
 

00:25:45.730 --> 00:25:48.060
world but also throughout society and
they've been willing to invest in work

00:25:48.060 --> 00:25:48.070
they've been willing to invest in work
 

00:25:48.070 --> 00:25:50.910
they've been willing to invest in work
to make sure that these technologies and

00:25:50.910 --> 00:25:50.920
to make sure that these technologies and
 

00:25:50.920 --> 00:25:53.450
to make sure that these technologies and
these unprecedented vance's that we see

00:25:53.450 --> 00:25:53.460
these unprecedented vance's that we see
 

00:25:53.460 --> 00:25:56.540
these unprecedented vance's that we see
benefit society and benefits humanity

00:25:56.540 --> 00:25:56.550
benefit society and benefits humanity
 

00:25:56.550 --> 00:25:59.100
benefit society and benefits humanity
because of their generosity can all

00:25:59.100 --> 00:25:59.110
because of their generosity can all
 

00:25:59.110 --> 00:26:00.600
because of their generosity can all
gates Endowment for Ethics and

00:26:00.600 --> 00:26:00.610
gates Endowment for Ethics and
 

00:26:00.610 --> 00:26:03.150
gates Endowment for Ethics and
computational technologies will advance

00:26:03.150 --> 00:26:03.160
computational technologies will advance
 

00:26:03.160 --> 00:26:05.160
computational technologies will advance
research and education in this critical

00:26:05.160 --> 00:26:05.170
research and education in this critical
 

00:26:05.170 --> 00:26:08.430
research and education in this critical
domain and foster dialogues like this

00:26:08.430 --> 00:26:08.440
domain and foster dialogues like this
 

00:26:08.440 --> 00:26:10.800
domain and foster dialogues like this
week's conference I'm delighted to

00:26:10.800 --> 00:26:10.810
week's conference I'm delighted to
 

00:26:10.810 --> 00:26:13.860
week's conference I'm delighted to
welcome several representatives from K&amp;L

00:26:13.860 --> 00:26:13.870
welcome several representatives from K&amp;L
 

00:26:13.870 --> 00:26:17.160
welcome several representatives from K&amp;L
gates this afternoon including Jim

00:26:17.160 --> 00:26:17.170
gates this afternoon including Jim
 

00:26:17.170 --> 00:26:19.800
gates this afternoon including Jim
Segura Dahle global managing partner who

00:26:19.800 --> 00:26:19.810
Segura Dahle global managing partner who
 

00:26:19.810 --> 00:26:21.480
Segura Dahle global managing partner who
will provide remarks in just a few

00:26:21.480 --> 00:26:21.490
will provide remarks in just a few
 

00:26:21.490 --> 00:26:22.280
will provide remarks in just a few
minutes

00:26:22.280 --> 00:26:22.290
minutes
 

00:26:22.290 --> 00:26:25.260
minutes
Michael cutsies chairman of the

00:26:25.260 --> 00:26:25.270
Michael cutsies chairman of the
 

00:26:25.270 --> 00:26:27.500
Michael cutsies chairman of the
management committee and david lehman

00:26:27.500 --> 00:26:27.510
management committee and david lehman
 

00:26:27.510 --> 00:26:30.000
management committee and david lehman
partner and co head of the K&amp;L gates

00:26:30.000 --> 00:26:30.010
partner and co head of the K&amp;L gates
 

00:26:30.010 --> 00:26:32.070
partner and co head of the K&amp;L gates
artificially intelligent initiative and

00:26:32.070 --> 00:26:32.080
artificially intelligent initiative and
 

00:26:32.080 --> 00:26:34.950
artificially intelligent initiative and
I should note that Michael is also a

00:26:34.950 --> 00:26:34.960
I should note that Michael is also a
 

00:26:34.960 --> 00:26:38.790
I should note that Michael is also a
proud CMU alone Dietrich

00:26:38.790 --> 00:26:38.800
proud CMU alone Dietrich
 

00:26:38.800 --> 00:26:41.340
proud CMU alone Dietrich
a college class of 82 Michael was 10

00:26:41.340 --> 00:26:41.350
a college class of 82 Michael was 10
 

00:26:41.350 --> 00:26:43.580
a college class of 82 Michael was 10
years old when he graduated from college

00:26:43.580 --> 00:26:43.590
years old when he graduated from college
 

00:26:43.590 --> 00:26:45.870
years old when he graduated from college
ladies and gentlemen please join me in

00:26:45.870 --> 00:26:45.880
ladies and gentlemen please join me in
 

00:26:45.880 --> 00:26:47.640
ladies and gentlemen please join me in
welcoming and thanking all of our

00:26:47.640 --> 00:26:47.650
welcoming and thanking all of our
 

00:26:47.650 --> 00:26:49.250
welcoming and thanking all of our
partners from K&amp;L gates

00:26:49.250 --> 00:26:49.260
partners from K&amp;L gates
 

00:26:49.260 --> 00:26:54.720
partners from K&amp;L gates
[Applause]

00:26:54.720 --> 00:26:54.730
 

00:26:54.730 --> 00:26:57.940
I'm also police to acknowledge all of

00:26:57.940 --> 00:26:57.950
I'm also police to acknowledge all of
 

00:26:57.950 --> 00:27:00.160
I'm also police to acknowledge all of
our conference speakers and panelists

00:27:00.160 --> 00:27:00.170
our conference speakers and panelists
 

00:27:00.170 --> 00:27:02.860
our conference speakers and panelists
including my friend Eric Horvitz

00:27:02.860 --> 00:27:02.870
including my friend Eric Horvitz
 

00:27:02.870 --> 00:27:05.290
including my friend Eric Horvitz
director of Microsoft Research Lab who

00:27:05.290 --> 00:27:05.300
director of Microsoft Research Lab who
 

00:27:05.300 --> 00:27:07.660
director of Microsoft Research Lab who
is here today as our distinguished

00:27:07.660 --> 00:27:07.670
is here today as our distinguished
 

00:27:07.670 --> 00:27:10.960
is here today as our distinguished
speaker for this event welcome Eric pre

00:27:10.960 --> 00:27:10.970
speaker for this event welcome Eric pre
 

00:27:10.970 --> 00:27:13.060
speaker for this event welcome Eric pre
praying such an extraordinary agenda

00:27:13.060 --> 00:27:13.070
praying such an extraordinary agenda
 

00:27:13.070 --> 00:27:15.580
praying such an extraordinary agenda
requires exceptional leadership so I'm

00:27:15.580 --> 00:27:15.590
requires exceptional leadership so I'm
 

00:27:15.590 --> 00:27:17.470
requires exceptional leadership so I'm
grateful to the conference organizers

00:27:17.470 --> 00:27:17.480
grateful to the conference organizers
 

00:27:17.480 --> 00:27:20.250
grateful to the conference organizers
the steering committee and of course my

00:27:20.250 --> 00:27:20.260
the steering committee and of course my
 

00:27:20.260 --> 00:27:23.050
the steering committee and of course my
colleagues the co-chairs David Danks and

00:27:23.050 --> 00:27:23.060
colleagues the co-chairs David Danks and
 

00:27:23.060 --> 00:27:24.210
colleagues the co-chairs David Danks and
Eleanor Bach

00:27:24.210 --> 00:27:24.220
Eleanor Bach
 

00:27:24.220 --> 00:27:26.820
Eleanor Bach
finally this conference has been

00:27:26.820 --> 00:27:26.830
finally this conference has been
 

00:27:26.830 --> 00:27:29.380
finally this conference has been
supported by countless professionals

00:27:29.380 --> 00:27:29.390
supported by countless professionals
 

00:27:29.390 --> 00:27:32.830
supported by countless professionals
including dorothy robinson of counsel at

00:27:32.830 --> 00:27:32.840
including dorothy robinson of counsel at
 

00:27:32.840 --> 00:27:35.470
including dorothy robinson of counsel at
K&amp;L gates and the entire planning

00:27:35.470 --> 00:27:35.480
K&amp;L gates and the entire planning
 

00:27:35.480 --> 00:27:38.110
K&amp;L gates and the entire planning
committee at carnegie mellon and at kano

00:27:38.110 --> 00:27:38.120
committee at carnegie mellon and at kano
 

00:27:38.120 --> 00:27:40.960
committee at carnegie mellon and at kano
gates please join me in thanking all of

00:27:40.960 --> 00:27:40.970
gates please join me in thanking all of
 

00:27:40.970 --> 00:27:42.940
gates please join me in thanking all of
our special guests as well as our

00:27:42.940 --> 00:27:42.950
our special guests as well as our
 

00:27:42.950 --> 00:27:51.850
our special guests as well as our
conference organizers there is little

00:27:51.850 --> 00:27:51.860
conference organizers there is little
 

00:27:51.860 --> 00:27:54.250
conference organizers there is little
doubt that emerging technologies and

00:27:54.250 --> 00:27:54.260
doubt that emerging technologies and
 

00:27:54.260 --> 00:27:57.460
doubt that emerging technologies and
artificial intelligence or at the center

00:27:57.460 --> 00:27:57.470
artificial intelligence or at the center
 

00:27:57.470 --> 00:28:00.130
artificial intelligence or at the center
of an ongoing economic and societal

00:28:00.130 --> 00:28:00.140
of an ongoing economic and societal
 

00:28:00.140 --> 00:28:03.130
of an ongoing economic and societal
transformation that will no doubt will

00:28:03.130 --> 00:28:03.140
transformation that will no doubt will
 

00:28:03.140 --> 00:28:06.460
transformation that will no doubt will
continue for decades to come as we

00:28:06.460 --> 00:28:06.470
continue for decades to come as we
 

00:28:06.470 --> 00:28:08.670
continue for decades to come as we
embrace the Internet of Things

00:28:08.670 --> 00:28:08.680
embrace the Internet of Things
 

00:28:08.680 --> 00:28:11.950
embrace the Internet of Things
unprecedented access to massive amount

00:28:11.950 --> 00:28:11.960
unprecedented access to massive amount
 

00:28:11.960 --> 00:28:14.740
unprecedented access to massive amount
of data and the rise of automation and

00:28:14.740 --> 00:28:14.750
of data and the rise of automation and
 

00:28:14.750 --> 00:28:18.550
of data and the rise of automation and
robotics we're barreling toward a future

00:28:18.550 --> 00:28:18.560
robotics we're barreling toward a future
 

00:28:18.560 --> 00:28:22.210
robotics we're barreling toward a future
run by cyber enabled systems while these

00:28:22.210 --> 00:28:22.220
run by cyber enabled systems while these
 

00:28:22.220 --> 00:28:25.050
run by cyber enabled systems while these
technologies will enhance our comfort

00:28:25.050 --> 00:28:25.060
technologies will enhance our comfort
 

00:28:25.060 --> 00:28:29.320
technologies will enhance our comfort
security and quality of life for society

00:28:29.320 --> 00:28:29.330
security and quality of life for society
 

00:28:29.330 --> 00:28:32.380
security and quality of life for society
for for all of us their deployment has

00:28:32.380 --> 00:28:32.390
for for all of us their deployment has
 

00:28:32.390 --> 00:28:35.530
for for all of us their deployment has
had unprecedent consequences for our

00:28:35.530 --> 00:28:35.540
had unprecedent consequences for our
 

00:28:35.540 --> 00:28:38.280
had unprecedent consequences for our
workforce our education system

00:28:38.280 --> 00:28:38.290
workforce our education system
 

00:28:38.290 --> 00:28:40.630
workforce our education system
potentially for social justice for

00:28:40.630 --> 00:28:40.640
potentially for social justice for
 

00:28:40.640 --> 00:28:43.420
potentially for social justice for
fairness for privacy and many other

00:28:43.420 --> 00:28:43.430
fairness for privacy and many other
 

00:28:43.430 --> 00:28:46.390
fairness for privacy and many other
aspects of society the effect of

00:28:46.390 --> 00:28:46.400
aspects of society the effect of
 

00:28:46.400 --> 00:28:48.910
aspects of society the effect of
technology on society is not a brand new

00:28:48.910 --> 00:28:48.920
technology on society is not a brand new
 

00:28:48.920 --> 00:28:52.030
technology on society is not a brand new
concern but as we see these emerging

00:28:52.030 --> 00:28:52.040
concern but as we see these emerging
 

00:28:52.040 --> 00:28:55.660
concern but as we see these emerging
technologies pervade every aspect of our

00:28:55.660 --> 00:28:55.670
technologies pervade every aspect of our
 

00:28:55.670 --> 00:28:59.080
technologies pervade every aspect of our
lives and disrupt markets and disrupt

00:28:59.080 --> 00:28:59.090
lives and disrupt markets and disrupt
 

00:28:59.090 --> 00:29:02.680
lives and disrupt markets and disrupt
industries these issues have become even

00:29:02.680 --> 00:29:02.690
industries these issues have become even
 

00:29:02.690 --> 00:29:04.140
industries these issues have become even
more urgent

00:29:04.140 --> 00:29:04.150
more urgent
 

00:29:04.150 --> 00:29:07.020
more urgent
consider for a moment a few phenomena

00:29:07.020 --> 00:29:07.030
consider for a moment a few phenomena
 

00:29:07.030 --> 00:29:09.960
consider for a moment a few phenomena
Mokhtar technology-driven world as data

00:29:09.960 --> 00:29:09.970
Mokhtar technology-driven world as data
 

00:29:09.970 --> 00:29:13.500
Mokhtar technology-driven world as data
has become a new currency for business

00:29:13.500 --> 00:29:13.510
has become a new currency for business
 

00:29:13.510 --> 00:29:16.440
has become a new currency for business
and for technology we're seeing a

00:29:16.440 --> 00:29:16.450
and for technology we're seeing a
 

00:29:16.450 --> 00:29:19.760
and for technology we're seeing a
dramatic surge in the collection storage

00:29:19.760 --> 00:29:19.770
dramatic surge in the collection storage
 

00:29:19.770 --> 00:29:24.180
dramatic surge in the collection storage
analysis and monetization of our

00:29:24.180 --> 00:29:24.190
analysis and monetization of our
 

00:29:24.190 --> 00:29:28.280
analysis and monetization of our
personal and often most sensitive data

00:29:28.280 --> 00:29:28.290
personal and often most sensitive data
 

00:29:28.290 --> 00:29:31.800
personal and often most sensitive data
meanwhile the most cherished attributes

00:29:31.800 --> 00:29:31.810
meanwhile the most cherished attributes
 

00:29:31.810 --> 00:29:34.680
meanwhile the most cherished attributes
of the internet and the web its speed

00:29:34.680 --> 00:29:34.690
of the internet and the web its speed
 

00:29:34.690 --> 00:29:38.610
of the internet and the web its speed
its reach its openness and the notion of

00:29:38.610 --> 00:29:38.620
its reach its openness and the notion of
 

00:29:38.620 --> 00:29:41.460
its reach its openness and the notion of
anonymity are being used to undermine

00:29:41.460 --> 00:29:41.470
anonymity are being used to undermine
 

00:29:41.470 --> 00:29:45.170
anonymity are being used to undermine
our democracy and our civil liberties as

00:29:45.170 --> 00:29:45.180
our democracy and our civil liberties as
 

00:29:45.180 --> 00:29:49.260
our democracy and our civil liberties as
AI enhances and augments and in some

00:29:49.260 --> 00:29:49.270
AI enhances and augments and in some
 

00:29:49.270 --> 00:29:53.430
AI enhances and augments and in some
cases outpaces human capabilities human

00:29:53.430 --> 00:29:53.440
cases outpaces human capabilities human
 

00:29:53.440 --> 00:29:55.860
cases outpaces human capabilities human
machine interaction is reimagining the

00:29:55.860 --> 00:29:55.870
machine interaction is reimagining the
 

00:29:55.870 --> 00:30:00.180
machine interaction is reimagining the
future of work at the same time the very

00:30:00.180 --> 00:30:00.190
future of work at the same time the very
 

00:30:00.190 --> 00:30:04.380
future of work at the same time the very
nature of employment is involving skill

00:30:04.380 --> 00:30:04.390
nature of employment is involving skill
 

00:30:04.390 --> 00:30:08.300
nature of employment is involving skill
cycles are shorter than ever before

00:30:08.300 --> 00:30:08.310
cycles are shorter than ever before
 

00:30:08.310 --> 00:30:10.770
cycles are shorter than ever before
consider for a moment that according to

00:30:10.770 --> 00:30:10.780
consider for a moment that according to
 

00:30:10.780 --> 00:30:14.310
consider for a moment that according to
a recent study 65% of the jobs that

00:30:14.310 --> 00:30:14.320
a recent study 65% of the jobs that
 

00:30:14.320 --> 00:30:17.400
a recent study 65% of the jobs that
generation Z will perform don't even

00:30:17.400 --> 00:30:17.410
generation Z will perform don't even
 

00:30:17.410 --> 00:30:21.540
generation Z will perform don't even
exist today this will have dramatic

00:30:21.540 --> 00:30:21.550
exist today this will have dramatic
 

00:30:21.550 --> 00:30:23.400
exist today this will have dramatic
implications for educating the next

00:30:23.400 --> 00:30:23.410
implications for educating the next
 

00:30:23.410 --> 00:30:26.040
implications for educating the next
generation it's not just a scale and

00:30:26.040 --> 00:30:26.050
generation it's not just a scale and
 

00:30:26.050 --> 00:30:29.220
generation it's not just a scale and
ubiquity of these advances that are

00:30:29.220 --> 00:30:29.230
ubiquity of these advances that are
 

00:30:29.230 --> 00:30:34.080
ubiquity of these advances that are
remarkable the pace of advances as well

00:30:34.080 --> 00:30:34.090
remarkable the pace of advances as well
 

00:30:34.090 --> 00:30:35.760
remarkable the pace of advances as well
as the acceleration of their economic

00:30:35.760 --> 00:30:35.770
as the acceleration of their economic
 

00:30:35.770 --> 00:30:38.910
as the acceleration of their economic
impact is also unparalleled in human

00:30:38.910 --> 00:30:38.920
impact is also unparalleled in human
 

00:30:38.920 --> 00:30:42.450
impact is also unparalleled in human
history as an example it took landline

00:30:42.450 --> 00:30:42.460
history as an example it took landline
 

00:30:42.460 --> 00:30:45.960
history as an example it took landline
phones about 45 years to go from 5

00:30:45.960 --> 00:30:45.970
phones about 45 years to go from 5
 

00:30:45.970 --> 00:30:49.380
phones about 45 years to go from 5
percent to 50 percent penetration among

00:30:49.380 --> 00:30:49.390
percent to 50 percent penetration among
 

00:30:49.390 --> 00:30:52.860
percent to 50 percent penetration among
US households by comparison smart phones

00:30:52.860 --> 00:30:52.870
US households by comparison smart phones
 

00:30:52.870 --> 00:30:55.110
US households by comparison smart phones
went from 5 percent to 40 percent in

00:30:55.110 --> 00:30:55.120
went from 5 percent to 40 percent in
 

00:30:55.120 --> 00:30:57.720
went from 5 percent to 40 percent in
about just 4 years and that was during a

00:30:57.720 --> 00:30:57.730
about just 4 years and that was during a
 

00:30:57.730 --> 00:31:01.260
about just 4 years and that was during a
recession and in 2017 that number

00:31:01.260 --> 00:31:01.270
recession and in 2017 that number
 

00:31:01.270 --> 00:31:04.620
recession and in 2017 that number
reached more than 77 percent today we're

00:31:04.620 --> 00:31:04.630
reached more than 77 percent today we're
 

00:31:04.630 --> 00:31:06.720
reached more than 77 percent today we're
at an inflection point for the

00:31:06.720 --> 00:31:06.730
at an inflection point for the
 

00:31:06.730 --> 00:31:08.790
at an inflection point for the
proliferation of AI robotics and

00:31:08.790 --> 00:31:08.800
proliferation of AI robotics and
 

00:31:08.800 --> 00:31:11.130
proliferation of AI robotics and
automation as this innovation moves

00:31:11.130 --> 00:31:11.140
automation as this innovation moves
 

00:31:11.140 --> 00:31:14.880
automation as this innovation moves
forward at warp speed our deployment of

00:31:14.880 --> 00:31:14.890
forward at warp speed our deployment of
 

00:31:14.890 --> 00:31:16.470
forward at warp speed our deployment of
ethics our development of ethics I

00:31:16.470 --> 00:31:16.480
ethics our development of ethics I
 

00:31:16.480 --> 00:31:17.470
ethics our development of ethics I
should say

00:31:17.470 --> 00:31:17.480
should say
 

00:31:17.480 --> 00:31:21.100
should say
policy must keep up now of course is a

00:31:21.100 --> 00:31:21.110
policy must keep up now of course is a
 

00:31:21.110 --> 00:31:23.560
policy must keep up now of course is a
time for partners from across public and

00:31:23.560 --> 00:31:23.570
time for partners from across public and
 

00:31:23.570 --> 00:31:26.799
time for partners from across public and
private sectors to come together with

00:31:26.799 --> 00:31:26.809
private sectors to come together with
 

00:31:26.809 --> 00:31:29.470
private sectors to come together with
effective communities to ensure that

00:31:29.470 --> 00:31:29.480
effective communities to ensure that
 

00:31:29.480 --> 00:31:32.820
effective communities to ensure that
technology is used to benefit humanity

00:31:32.820 --> 00:31:32.830
technology is used to benefit humanity
 

00:31:32.830 --> 00:31:35.080
technology is used to benefit humanity
individually of course as well as the

00:31:35.080 --> 00:31:35.090
individually of course as well as the
 

00:31:35.090 --> 00:31:40.150
individually of course as well as the
entire society as the University that

00:31:40.150 --> 00:31:40.160
entire society as the University that
 

00:31:40.160 --> 00:31:42.010
entire society as the University that
has been intimately involved in the

00:31:42.010 --> 00:31:42.020
has been intimately involved in the
 

00:31:42.020 --> 00:31:44.020
has been intimately involved in the
creation and evolution of artificial

00:31:44.020 --> 00:31:44.030
creation and evolution of artificial
 

00:31:44.030 --> 00:31:46.480
creation and evolution of artificial
intelligence we've definitely been

00:31:46.480 --> 00:31:46.490
intelligence we've definitely been
 

00:31:46.490 --> 00:31:48.460
intelligence we've definitely been
responsible for some really

00:31:48.460 --> 00:31:48.470
responsible for some really
 

00:31:48.470 --> 00:31:51.159
responsible for some really
groundbreaking innovations but we've

00:31:51.159 --> 00:31:51.169
groundbreaking innovations but we've
 

00:31:51.169 --> 00:31:53.039
groundbreaking innovations but we've
also built an interdisciplinary culture

00:31:53.039 --> 00:31:53.049
also built an interdisciplinary culture
 

00:31:53.049 --> 00:31:54.730
also built an interdisciplinary culture
focused on making sure that

00:31:54.730 --> 00:31:54.740
focused on making sure that
 

00:31:54.740 --> 00:31:57.539
focused on making sure that
technological progress benefits society

00:31:57.539 --> 00:31:57.549
technological progress benefits society
 

00:31:57.549 --> 00:32:00.760
technological progress benefits society
through our partnerships we're K&amp;L gates

00:32:00.760 --> 00:32:00.770
through our partnerships we're K&amp;L gates
 

00:32:00.770 --> 00:32:02.980
through our partnerships we're K&amp;L gates
as well as our block Center for

00:32:02.980 --> 00:32:02.990
as well as our block Center for
 

00:32:02.990 --> 00:32:04.630
as well as our block Center for
technology and society which we just

00:32:04.630 --> 00:32:04.640
technology and society which we just
 

00:32:04.640 --> 00:32:07.000
technology and society which we just
recently launched CMU and our

00:32:07.000 --> 00:32:07.010
recently launched CMU and our
 

00:32:07.010 --> 00:32:08.650
recently launched CMU and our
collaborators are helping to shape the

00:32:08.650 --> 00:32:08.660
collaborators are helping to shape the
 

00:32:08.660 --> 00:32:10.960
collaborators are helping to shape the
world in which people policy and

00:32:10.960 --> 00:32:10.970
world in which people policy and
 

00:32:10.970 --> 00:32:14.620
world in which people policy and
technology are better connected some of

00:32:14.620 --> 00:32:14.630
technology are better connected some of
 

00:32:14.630 --> 00:32:17.980
technology are better connected some of
CMU's most passionate partners have been

00:32:17.980 --> 00:32:17.990
CMU's most passionate partners have been
 

00:32:17.990 --> 00:32:20.320
CMU's most passionate partners have been
right here in Pittsburgh which has been

00:32:20.320 --> 00:32:20.330
right here in Pittsburgh which has been
 

00:32:20.330 --> 00:32:21.970
right here in Pittsburgh which has been
serving as a living laboratory for

00:32:21.970 --> 00:32:21.980
serving as a living laboratory for
 

00:32:21.980 --> 00:32:24.370
serving as a living laboratory for
technology that is integrated with

00:32:24.370 --> 00:32:24.380
technology that is integrated with
 

00:32:24.380 --> 00:32:28.090
technology that is integrated with
society our Mayor Bill Peduto has been

00:32:28.090 --> 00:32:28.100
society our Mayor Bill Peduto has been
 

00:32:28.100 --> 00:32:29.830
society our Mayor Bill Peduto has been
at the forefront of these collaborative

00:32:29.830 --> 00:32:29.840
at the forefront of these collaborative
 

00:32:29.840 --> 00:32:32.409
at the forefront of these collaborative
efforts and has established himself as a

00:32:32.409 --> 00:32:32.419
efforts and has established himself as a
 

00:32:32.419 --> 00:32:35.620
efforts and has established himself as a
national leader in progressive inclusive

00:32:35.620 --> 00:32:35.630
national leader in progressive inclusive
 

00:32:35.630 --> 00:32:38.770
national leader in progressive inclusive
and sustainable innovation of American

00:32:38.770 --> 00:32:38.780
and sustainable innovation of American
 

00:32:38.780 --> 00:32:41.590
and sustainable innovation of American
societies I'm delighted that he's able

00:32:41.590 --> 00:32:41.600
societies I'm delighted that he's able
 

00:32:41.600 --> 00:32:43.780
societies I'm delighted that he's able
to join us today to offer a few words

00:32:43.780 --> 00:32:43.790
to join us today to offer a few words
 

00:32:43.790 --> 00:32:46.510
to join us today to offer a few words
ladies and gentlemen please join me in

00:32:46.510 --> 00:32:46.520
ladies and gentlemen please join me in
 

00:32:46.520 --> 00:32:49.390
ladies and gentlemen please join me in
welcoming Pittsburgh Mayor Bill Peduto

00:32:49.390 --> 00:32:49.400
welcoming Pittsburgh Mayor Bill Peduto
 

00:32:49.400 --> 00:32:57.470
welcoming Pittsburgh Mayor Bill Peduto
[Applause]

00:32:57.470 --> 00:32:57.480
[Applause]
 

00:32:57.480 --> 00:33:01.320
[Applause]
Thank You mr. president and thank you to

00:33:01.320 --> 00:33:01.330
Thank You mr. president and thank you to
 

00:33:01.330 --> 00:33:03.920
Thank You mr. president and thank you to
Carnegie Mellon University and K&amp;L gates

00:33:03.920 --> 00:33:03.930
Carnegie Mellon University and K&amp;L gates
 

00:33:03.930 --> 00:33:07.350
Carnegie Mellon University and K&amp;L gates
for taking on this issue and being able

00:33:07.350 --> 00:33:07.360
for taking on this issue and being able
 

00:33:07.360 --> 00:33:09.960
for taking on this issue and being able
to have a discussion and I also want to

00:33:09.960 --> 00:33:09.970
to have a discussion and I also want to
 

00:33:09.970 --> 00:33:11.370
to have a discussion and I also want to
thank the person who wrote this speech

00:33:11.370 --> 00:33:11.380
thank the person who wrote this speech
 

00:33:11.380 --> 00:33:12.930
thank the person who wrote this speech
for me because it's one of the best

00:33:12.930 --> 00:33:12.940
for me because it's one of the best
 

00:33:12.940 --> 00:33:16.080
for me because it's one of the best
political speeches that I've not going

00:33:16.080 --> 00:33:16.090
political speeches that I've not going
 

00:33:16.090 --> 00:33:20.730
political speeches that I've not going
to read I always try to speak off the

00:33:20.730 --> 00:33:20.740
to read I always try to speak off the
 

00:33:20.740 --> 00:33:24.510
to read I always try to speak off the
cuff so let me explain what these next

00:33:24.510 --> 00:33:24.520
cuff so let me explain what these next
 

00:33:24.520 --> 00:33:27.270
cuff so let me explain what these next
two days mean for policy makers you know

00:33:27.270 --> 00:33:27.280
two days mean for policy makers you know
 

00:33:27.280 --> 00:33:30.830
two days mean for policy makers you know
Pittsburgh was an industry leader right

00:33:30.830 --> 00:33:30.840
Pittsburgh was an industry leader right
 

00:33:30.840 --> 00:33:34.590
Pittsburgh was an industry leader right
the industrial revolution during World

00:33:34.590 --> 00:33:34.600
the industrial revolution during World
 

00:33:34.600 --> 00:33:37.370
the industrial revolution during World
War two we produced more steel than

00:33:37.370 --> 00:33:37.380
War two we produced more steel than
 

00:33:37.380 --> 00:33:41.880
War two we produced more steel than
Germany in Japan combined and when we

00:33:41.880 --> 00:33:41.890
Germany in Japan combined and when we
 

00:33:41.890 --> 00:33:44.250
Germany in Japan combined and when we
built out this industry in this great

00:33:44.250 --> 00:33:44.260
built out this industry in this great
 

00:33:44.260 --> 00:33:47.280
built out this industry in this great
wealth that came to this region it came

00:33:47.280 --> 00:33:47.290
wealth that came to this region it came
 

00:33:47.290 --> 00:33:51.630
wealth that came to this region it came
at a very heavy cost we had air that was

00:33:51.630 --> 00:33:51.640
at a very heavy cost we had air that was
 

00:33:51.640 --> 00:33:54.720
at a very heavy cost we had air that was
dangerous to breathe we had water that

00:33:54.720 --> 00:33:54.730
dangerous to breathe we had water that
 

00:33:54.730 --> 00:33:57.540
dangerous to breathe we had water that
was poisonous to drink and we had the

00:33:57.540 --> 00:33:57.550
was poisonous to drink and we had the
 

00:33:57.550 --> 00:33:59.760
was poisonous to drink and we had the
greatest disparity between the people

00:33:59.760 --> 00:33:59.770
greatest disparity between the people
 

00:33:59.770 --> 00:34:02.070
greatest disparity between the people
who worked in those mines and mills and

00:34:02.070 --> 00:34:02.080
who worked in those mines and mills and
 

00:34:02.080 --> 00:34:05.490
who worked in those mines and mills and
those that owned and operated them the

00:34:05.490 --> 00:34:05.500
those that owned and operated them the
 

00:34:05.500 --> 00:34:07.500
those that owned and operated them the
greatest disparity in American history

00:34:07.500 --> 00:34:07.510
greatest disparity in American history
 

00:34:07.510 --> 00:34:10.470
greatest disparity in American history
and those were the costs that came with

00:34:10.470 --> 00:34:10.480
and those were the costs that came with
 

00:34:10.480 --> 00:34:13.230
and those were the costs that came with
it and it would take decades and decades

00:34:13.230 --> 00:34:13.240
it and it would take decades and decades
 

00:34:13.240 --> 00:34:16.080
it and it would take decades and decades
to be able to change that what we did

00:34:16.080 --> 00:34:16.090
to be able to change that what we did
 

00:34:16.090 --> 00:34:19.110
to be able to change that what we did
what Pittsburghers do we created the

00:34:19.110 --> 00:34:19.120
what Pittsburghers do we created the
 

00:34:19.120 --> 00:34:21.900
what Pittsburghers do we created the
first Clean Air Act in American history

00:34:21.900 --> 00:34:21.910
first Clean Air Act in American history
 

00:34:21.910 --> 00:34:25.320
first Clean Air Act in American history
and we went about to clean our air we

00:34:25.320 --> 00:34:25.330
and we went about to clean our air we
 

00:34:25.330 --> 00:34:27.960
and we went about to clean our air we
created public/private partnerships in

00:34:27.960 --> 00:34:27.970
created public/private partnerships in
 

00:34:27.970 --> 00:34:30.300
created public/private partnerships in
order to be able to clean our water and

00:34:30.300 --> 00:34:30.310
order to be able to clean our water and
 

00:34:30.310 --> 00:34:33.480
order to be able to clean our water and
we organized in those mines and in those

00:34:33.480 --> 00:34:33.490
we organized in those mines and in those
 

00:34:33.490 --> 00:34:35.970
we organized in those mines and in those
mills and in the process of building

00:34:35.970 --> 00:34:35.980
mills and in the process of building
 

00:34:35.980 --> 00:34:38.040
mills and in the process of building
America every Bridge and every

00:34:38.040 --> 00:34:38.050
America every Bridge and every
 

00:34:38.050 --> 00:34:41.570
America every Bridge and every
skyscraper we created the middle class

00:34:41.570 --> 00:34:41.580
skyscraper we created the middle class
 

00:34:41.580 --> 00:34:45.570
skyscraper we created the middle class
but they came as afterthoughts to the

00:34:45.570 --> 00:34:45.580
but they came as afterthoughts to the
 

00:34:45.580 --> 00:34:48.480
but they came as afterthoughts to the
Industrial Revolution not a component of

00:34:48.480 --> 00:34:48.490
Industrial Revolution not a component of
 

00:34:48.490 --> 00:34:50.490
Industrial Revolution not a component of
the Industrial Revolution

00:34:50.490 --> 00:34:50.500
the Industrial Revolution
 

00:34:50.500 --> 00:34:53.040
the Industrial Revolution
so as we find ourselves today in the

00:34:53.040 --> 00:34:53.050
so as we find ourselves today in the
 

00:34:53.050 --> 00:34:56.190
so as we find ourselves today in the
fourth Industrial Revolution we have to

00:34:56.190 --> 00:34:56.200
fourth Industrial Revolution we have to
 

00:34:56.200 --> 00:34:59.220
fourth Industrial Revolution we have to
look beyond simply what this technology

00:34:59.220 --> 00:34:59.230
look beyond simply what this technology
 

00:34:59.230 --> 00:35:01.800
look beyond simply what this technology
can provide the venture capitalists out

00:35:01.800 --> 00:35:01.810
can provide the venture capitalists out
 

00:35:01.810 --> 00:35:04.470
can provide the venture capitalists out
of California we have to think about

00:35:04.470 --> 00:35:04.480
of California we have to think about
 

00:35:04.480 --> 00:35:07.620
of California we have to think about
what it will provide to the people who

00:35:07.620 --> 00:35:07.630
what it will provide to the people who
 

00:35:07.630 --> 00:35:09.580
what it will provide to the people who
live in cities throughout

00:35:09.580 --> 00:35:09.590
live in cities throughout
 

00:35:09.590 --> 00:35:13.530
live in cities throughout
country we went through decades of

00:35:13.530 --> 00:35:13.540
country we went through decades of
 

00:35:13.540 --> 00:35:16.930
country we went through decades of
redlining out neighborhoods based upon

00:35:16.930 --> 00:35:16.940
redlining out neighborhoods based upon
 

00:35:16.940 --> 00:35:20.680
redlining out neighborhoods based upon
race and based upon income we cannot

00:35:20.680 --> 00:35:20.690
race and based upon income we cannot
 

00:35:20.690 --> 00:35:23.200
race and based upon income we cannot
read line out communities in this new

00:35:23.200 --> 00:35:23.210
read line out communities in this new
 

00:35:23.210 --> 00:35:23.950
read line out communities in this new
economy

00:35:23.950 --> 00:35:23.960
economy
 

00:35:23.960 --> 00:35:26.080
economy
based on whether or not you have a

00:35:26.080 --> 00:35:26.090
based on whether or not you have a
 

00:35:26.090 --> 00:35:27.100
based on whether or not you have a
cellphone

00:35:27.100 --> 00:35:27.110
cellphone
 

00:35:27.110 --> 00:35:30.280
cellphone
whether you or not you have credit we

00:35:30.280 --> 00:35:30.290
whether you or not you have credit we
 

00:35:30.290 --> 00:35:32.170
whether you or not you have credit we
have to be able to make sure everybody

00:35:32.170 --> 00:35:32.180
have to be able to make sure everybody
 

00:35:32.180 --> 00:35:35.290
have to be able to make sure everybody
is having that same opportunity and the

00:35:35.290 --> 00:35:35.300
is having that same opportunity and the
 

00:35:35.300 --> 00:35:38.500
is having that same opportunity and the
technology will help us to expand it we

00:35:38.500 --> 00:35:38.510
technology will help us to expand it we
 

00:35:38.510 --> 00:35:41.290
technology will help us to expand it we
have to look beyond just what the

00:35:41.290 --> 00:35:41.300
have to look beyond just what the
 

00:35:41.300 --> 00:35:43.420
have to look beyond just what the
technology will do in stark tech

00:35:43.420 --> 00:35:43.430
technology will do in stark tech
 

00:35:43.430 --> 00:35:46.330
technology will do in stark tech
understand how it can minimize negative

00:35:46.330 --> 00:35:46.340
understand how it can minimize negative
 

00:35:46.340 --> 00:35:48.850
understand how it can minimize negative
effects to the environment and we have

00:35:48.850 --> 00:35:48.860
effects to the environment and we have
 

00:35:48.860 --> 00:35:51.850
effects to the environment and we have
to look and see how it will enhance the

00:35:51.850 --> 00:35:51.860
to look and see how it will enhance the
 

00:35:51.860 --> 00:35:54.610
to look and see how it will enhance the
places we call home in Pittsburgh we

00:35:54.610 --> 00:35:54.620
places we call home in Pittsburgh we
 

00:35:54.620 --> 00:36:00.090
places we call home in Pittsburgh we
call it P for people planet Place and

00:36:00.090 --> 00:36:00.100
call it P for people planet Place and
 

00:36:00.100 --> 00:36:04.000
call it P for people planet Place and
performance a new metric for a 21st

00:36:04.000 --> 00:36:04.010
performance a new metric for a 21st
 

00:36:04.010 --> 00:36:07.810
performance a new metric for a 21st
century not old economics of the 19th

00:36:07.810 --> 00:36:07.820
century not old economics of the 19th
 

00:36:07.820 --> 00:36:10.960
century not old economics of the 19th
century a quadruple bottom line of being

00:36:10.960 --> 00:36:10.970
century a quadruple bottom line of being
 

00:36:10.970 --> 00:36:13.150
century a quadruple bottom line of being
able to understand how to measure

00:36:13.150 --> 00:36:13.160
able to understand how to measure
 

00:36:13.160 --> 00:36:17.050
able to understand how to measure
success and for policymakers it's

00:36:17.050 --> 00:36:17.060
success and for policymakers it's
 

00:36:17.060 --> 00:36:20.470
success and for policymakers it's
absolutely critical because when we look

00:36:20.470 --> 00:36:20.480
absolutely critical because when we look
 

00:36:20.480 --> 00:36:23.770
absolutely critical because when we look
at the ethics of where AI can take us

00:36:23.770 --> 00:36:23.780
at the ethics of where AI can take us
 

00:36:23.780 --> 00:36:28.270
at the ethics of where AI can take us
it goes beyond purchasing drones for a

00:36:28.270 --> 00:36:28.280
it goes beyond purchasing drones for a
 

00:36:28.280 --> 00:36:30.730
it goes beyond purchasing drones for a
city that will come up with issues of

00:36:30.730 --> 00:36:30.740
city that will come up with issues of
 

00:36:30.740 --> 00:36:35.500
city that will come up with issues of
privacy or being able to throw a drone

00:36:35.500 --> 00:36:35.510
privacy or being able to throw a drone
 

00:36:35.510 --> 00:36:38.620
privacy or being able to throw a drone
behind a wall and having somebody have

00:36:38.620 --> 00:36:38.630
behind a wall and having somebody have
 

00:36:38.630 --> 00:36:41.200
behind a wall and having somebody have
already have programmed when it takes a

00:36:41.200 --> 00:36:41.210
already have programmed when it takes a
 

00:36:41.210 --> 00:36:44.200
already have programmed when it takes a
human life at what point then do we

00:36:44.200 --> 00:36:44.210
human life at what point then do we
 

00:36:44.210 --> 00:36:47.080
human life at what point then do we
start to think about creating machines

00:36:47.080 --> 00:36:47.090
start to think about creating machines
 

00:36:47.090 --> 00:36:49.840
start to think about creating machines
and algorithms that have the ability to

00:36:49.840 --> 00:36:49.850
and algorithms that have the ability to
 

00:36:49.850 --> 00:36:52.720
and algorithms that have the ability to
take a human life we are at the

00:36:52.720 --> 00:36:52.730
take a human life we are at the
 

00:36:52.730 --> 00:36:54.880
take a human life we are at the
forefront of all of this coming together

00:36:54.880 --> 00:36:54.890
forefront of all of this coming together
 

00:36:54.890 --> 00:36:58.150
forefront of all of this coming together
and it's coming together so fast but if

00:36:58.150 --> 00:36:58.160
and it's coming together so fast but if
 

00:36:58.160 --> 00:37:00.820
and it's coming together so fast but if
we don't plan for the negative

00:37:00.820 --> 00:37:00.830
we don't plan for the negative
 

00:37:00.830 --> 00:37:03.280
we don't plan for the negative
consequences in the beginning

00:37:03.280 --> 00:37:03.290
consequences in the beginning
 

00:37:03.290 --> 00:37:06.190
consequences in the beginning
we'll have made the mistake that we made

00:37:06.190 --> 00:37:06.200
we'll have made the mistake that we made
 

00:37:06.200 --> 00:37:09.220
we'll have made the mistake that we made
in the industrial revolution and we will

00:37:09.220 --> 00:37:09.230
in the industrial revolution and we will
 

00:37:09.230 --> 00:37:12.100
in the industrial revolution and we will
spend decades trying to solve those

00:37:12.100 --> 00:37:12.110
spend decades trying to solve those
 

00:37:12.110 --> 00:37:14.490
spend decades trying to solve those
problems

00:37:14.490 --> 00:37:14.500
problems
 

00:37:14.500 --> 00:37:24.290
problems
[Applause]

00:37:24.290 --> 00:37:24.300
 

00:37:24.300 --> 00:37:26.760
thank you very much bill for your

00:37:26.760 --> 00:37:26.770
thank you very much bill for your
 

00:37:26.770 --> 00:37:29.339
thank you very much bill for your
leadership and for making Pittsburgh be

00:37:29.339 --> 00:37:29.349
leadership and for making Pittsburgh be
 

00:37:29.349 --> 00:37:32.490
leadership and for making Pittsburgh be
such an inclusive community I'm happy to

00:37:32.490 --> 00:37:32.500
such an inclusive community I'm happy to
 

00:37:32.500 --> 00:37:34.620
such an inclusive community I'm happy to
share with you that later this evening

00:37:34.620 --> 00:37:34.630
share with you that later this evening
 

00:37:34.630 --> 00:37:37.020
share with you that later this evening
we'll will celebrate the first two

00:37:37.020 --> 00:37:37.030
we'll will celebrate the first two
 

00:37:37.030 --> 00:37:40.680
we'll will celebrate the first two
recipients of new professorships funded

00:37:40.680 --> 00:37:40.690
recipients of new professorships funded
 

00:37:40.690 --> 00:37:43.980
recipients of new professorships funded
through their K&amp;L gates endowment these

00:37:43.980 --> 00:37:43.990
through their K&amp;L gates endowment these
 

00:37:43.990 --> 00:37:46.650
through their K&amp;L gates endowment these
recipients exemplify interdisciplinary

00:37:46.650 --> 00:37:46.660
recipients exemplify interdisciplinary
 

00:37:46.660 --> 00:37:48.480
recipients exemplify interdisciplinary
nature of these complex issues

00:37:48.480 --> 00:37:48.490
nature of these complex issues
 

00:37:48.490 --> 00:37:50.819
nature of these complex issues
the first is illenore Bock who is an

00:37:50.819 --> 00:37:50.829
the first is illenore Bock who is an
 

00:37:50.829 --> 00:37:52.880
the first is illenore Bock who is an
expert of course in our computer science

00:37:52.880 --> 00:37:52.890
expert of course in our computer science
 

00:37:52.890 --> 00:37:55.319
expert of course in our computer science
school while the other Molly write

00:37:55.319 --> 00:37:55.329
school while the other Molly write
 

00:37:55.329 --> 00:37:58.200
school while the other Molly write
Stinson investigates past and present

00:37:58.200 --> 00:37:58.210
Stinson investigates past and present
 

00:37:58.210 --> 00:38:00.690
Stinson investigates past and present
implications of AI and computation on

00:38:00.690 --> 00:38:00.700
implications of AI and computation on
 

00:38:00.700 --> 00:38:02.609
implications of AI and computation on
design and architecture with

00:38:02.609 --> 00:38:02.619
design and architecture with
 

00:38:02.619 --> 00:38:04.740
design and architecture with
appointments in our School of Design and

00:38:04.740 --> 00:38:04.750
appointments in our School of Design and
 

00:38:04.750 --> 00:38:07.109
appointments in our School of Design and
our school of architecture in addition

00:38:07.109 --> 00:38:07.119
our school of architecture in addition
 

00:38:07.119 --> 00:38:09.839
our school of architecture in addition
the K&amp;L Gates Presidential fellowship

00:38:09.839 --> 00:38:09.849
the K&amp;L Gates Presidential fellowship
 

00:38:09.849 --> 00:38:12.900
the K&amp;L Gates Presidential fellowship
endowed fund will support for

00:38:12.900 --> 00:38:12.910
endowed fund will support for
 

00:38:12.910 --> 00:38:16.040
endowed fund will support for
outstanding doctoral students veronica

00:38:16.040 --> 00:38:16.050
outstanding doctoral students veronica
 

00:38:16.050 --> 00:38:19.200
outstanding doctoral students veronica
Allante Zachary and Abigail whom you'll

00:38:19.200 --> 00:38:19.210
Allante Zachary and Abigail whom you'll
 

00:38:19.210 --> 00:38:20.750
Allante Zachary and Abigail whom you'll
meet later today whose work spans

00:38:20.750 --> 00:38:20.760
meet later today whose work spans
 

00:38:20.760 --> 00:38:23.520
meet later today whose work spans
information systems Computer Science

00:38:23.520 --> 00:38:23.530
information systems Computer Science
 

00:38:23.530 --> 00:38:27.059
information systems Computer Science
Engineering public policy and social and

00:38:27.059 --> 00:38:27.069
Engineering public policy and social and
 

00:38:27.069 --> 00:38:28.650
Engineering public policy and social and
decision science I'm delighted to

00:38:28.650 --> 00:38:28.660
decision science I'm delighted to
 

00:38:28.660 --> 00:38:30.870
decision science I'm delighted to
congratulate these pioneering scholars

00:38:30.870 --> 00:38:30.880
congratulate these pioneering scholars
 

00:38:30.880 --> 00:38:33.890
congratulate these pioneering scholars
on their prestigious professorship and

00:38:33.890 --> 00:38:33.900
on their prestigious professorship and
 

00:38:33.900 --> 00:38:35.640
on their prestigious professorship and
scholarship please join me in

00:38:35.640 --> 00:38:35.650
scholarship please join me in
 

00:38:35.650 --> 00:38:38.250
scholarship please join me in
congratulating all day awardees

00:38:38.250 --> 00:38:38.260
congratulating all day awardees
 

00:38:38.260 --> 00:38:41.039
congratulating all day awardees
[Applause]

00:38:41.039 --> 00:38:41.049
[Applause]
 

00:38:41.049 --> 00:38:44.049
[Applause]
as I mentioned earlier we are truly

00:38:44.049 --> 00:38:44.059
as I mentioned earlier we are truly
 

00:38:44.059 --> 00:38:46.000
as I mentioned earlier we are truly
fortunate to have K&amp;L gates as our

00:38:46.000 --> 00:38:46.010
fortunate to have K&amp;L gates as our
 

00:38:46.010 --> 00:38:49.000
fortunate to have K&amp;L gates as our
partner who's generous support will help

00:38:49.000 --> 00:38:49.010
partner who's generous support will help
 

00:38:49.010 --> 00:38:51.730
partner who's generous support will help
us who helped put us at the center of

00:38:51.730 --> 00:38:51.740
us who helped put us at the center of
 

00:38:51.740 --> 00:38:54.630
us who helped put us at the center of
some of the most pressing conversations

00:38:54.630 --> 00:38:54.640
some of the most pressing conversations
 

00:38:54.640 --> 00:38:57.279
some of the most pressing conversations
facing society at this time it is my

00:38:57.279 --> 00:38:57.289
facing society at this time it is my
 

00:38:57.289 --> 00:38:59.829
facing society at this time it is my
great pleasure to introduce Jim Sagar

00:38:59.829 --> 00:38:59.839
great pleasure to introduce Jim Sagar
 

00:38:59.839 --> 00:39:02.470
great pleasure to introduce Jim Sagar
Dahl as global managing partner he

00:39:02.470 --> 00:39:02.480
Dahl as global managing partner he
 

00:39:02.480 --> 00:39:04.359
Dahl as global managing partner he
serves as a firm's chief executive

00:39:04.359 --> 00:39:04.369
serves as a firm's chief executive
 

00:39:04.369 --> 00:39:07.180
serves as a firm's chief executive
officer and native Pittsburgher so you

00:39:07.180 --> 00:39:07.190
officer and native Pittsburgher so you
 

00:39:07.190 --> 00:39:08.529
officer and native Pittsburgher so you
can tell you all about the weather

00:39:08.529 --> 00:39:08.539
can tell you all about the weather
 

00:39:08.539 --> 00:39:10.720
can tell you all about the weather
Jim is a board member of the Allegheny

00:39:10.720 --> 00:39:10.730
Jim is a board member of the Allegheny
 

00:39:10.730 --> 00:39:12.059
Jim is a board member of the Allegheny
Conference for Community Development

00:39:12.059 --> 00:39:12.069
Conference for Community Development
 

00:39:12.069 --> 00:39:14.740
Conference for Community Development
please join me in extending him a warm

00:39:14.740 --> 00:39:14.750
please join me in extending him a warm
 

00:39:14.750 --> 00:39:29.980
please join me in extending him a warm
welcome to Jim Hall Thank You President

00:39:29.980 --> 00:39:29.990
welcome to Jim Hall Thank You President
 

00:39:29.990 --> 00:39:32.140
welcome to Jim Hall Thank You President
Cheney and I'm thrilled to be here it's

00:39:32.140 --> 00:39:32.150
Cheney and I'm thrilled to be here it's
 

00:39:32.150 --> 00:39:35.799
Cheney and I'm thrilled to be here it's
very exciting event just terrific to be

00:39:35.799 --> 00:39:35.809
very exciting event just terrific to be
 

00:39:35.809 --> 00:39:38.049
very exciting event just terrific to be
in the company of so many brilliant

00:39:38.049 --> 00:39:38.059
in the company of so many brilliant
 

00:39:38.059 --> 00:39:41.380
in the company of so many brilliant
people working on truly important

00:39:41.380 --> 00:39:41.390
people working on truly important
 

00:39:41.390 --> 00:39:45.700
people working on truly important
matters I am pleased and honored to

00:39:45.700 --> 00:39:45.710
matters I am pleased and honored to
 

00:39:45.710 --> 00:39:47.170
matters I am pleased and honored to
welcome all of you to the first

00:39:47.170 --> 00:39:47.180
welcome all of you to the first
 

00:39:47.180 --> 00:39:49.210
welcome all of you to the first
inaugural conference of ethics and

00:39:49.210 --> 00:39:49.220
inaugural conference of ethics and
 

00:39:49.220 --> 00:39:51.519
inaugural conference of ethics and
artificial intelligence which I am proud

00:39:51.519 --> 00:39:51.529
artificial intelligence which I am proud
 

00:39:51.529 --> 00:39:54.160
artificial intelligence which I am proud
to say is funded by the kano Gates

00:39:54.160 --> 00:39:54.170
to say is funded by the kano Gates
 

00:39:54.170 --> 00:39:56.799
to say is funded by the kano Gates
Endowment for Ethics and computational

00:39:56.799 --> 00:39:56.809
Endowment for Ethics and computational
 

00:39:56.809 --> 00:39:59.920
Endowment for Ethics and computational
technologies with this initiative we

00:39:59.920 --> 00:39:59.930
technologies with this initiative we
 

00:39:59.930 --> 00:40:02.859
technologies with this initiative we
honor not only our long-standing

00:40:02.859 --> 00:40:02.869
honor not only our long-standing
 

00:40:02.869 --> 00:40:05.740
honor not only our long-standing
relationship with CMU but also the

00:40:05.740 --> 00:40:05.750
relationship with CMU but also the
 

00:40:05.750 --> 00:40:07.990
relationship with CMU but also the
commitment of both organizations to be

00:40:07.990 --> 00:40:08.000
commitment of both organizations to be
 

00:40:08.000 --> 00:40:10.390
commitment of both organizations to be
at the forefront in furthering the

00:40:10.390 --> 00:40:10.400
at the forefront in furthering the
 

00:40:10.400 --> 00:40:12.370
at the forefront in furthering the
understanding of the opportunities and

00:40:12.370 --> 00:40:12.380
understanding of the opportunities and
 

00:40:12.380 --> 00:40:14.849
understanding of the opportunities and
the challenges presented by the

00:40:14.849 --> 00:40:14.859
the challenges presented by the
 

00:40:14.859 --> 00:40:17.710
the challenges presented by the
ever-evolving role of Technology in our

00:40:17.710 --> 00:40:17.720
ever-evolving role of Technology in our
 

00:40:17.720 --> 00:40:20.620
ever-evolving role of Technology in our
society including the role of artificial

00:40:20.620 --> 00:40:20.630
society including the role of artificial
 

00:40:20.630 --> 00:40:22.599
society including the role of artificial
intelligence machine learning and

00:40:22.599 --> 00:40:22.609
intelligence machine learning and
 

00:40:22.609 --> 00:40:25.299
intelligence machine learning and
robotics our law firm has had a long

00:40:25.299 --> 00:40:25.309
robotics our law firm has had a long
 

00:40:25.309 --> 00:40:28.029
robotics our law firm has had a long
relationship with CMU over many decades

00:40:28.029 --> 00:40:28.039
relationship with CMU over many decades
 

00:40:28.039 --> 00:40:30.490
relationship with CMU over many decades
that relationship has been a source of

00:40:30.490 --> 00:40:30.500
that relationship has been a source of
 

00:40:30.500 --> 00:40:33.460
that relationship has been a source of
tremendous pride for us although our

00:40:33.460 --> 00:40:33.470
tremendous pride for us although our
 

00:40:33.470 --> 00:40:35.380
tremendous pride for us although our
core missions are different we have

00:40:35.380 --> 00:40:35.390
core missions are different we have
 

00:40:35.390 --> 00:40:37.930
core missions are different we have
worked together shared civic involvement

00:40:37.930 --> 00:40:37.940
worked together shared civic involvement
 

00:40:37.940 --> 00:40:42.160
worked together shared civic involvement
and even shared leaders for example one

00:40:42.160 --> 00:40:42.170
and even shared leaders for example one
 

00:40:42.170 --> 00:40:43.990
and even shared leaders for example one
of my predecessors as Managing Partner

00:40:43.990 --> 00:40:44.000
of my predecessors as Managing Partner
 

00:40:44.000 --> 00:40:47.710
of my predecessors as Managing Partner
of kano Gates Chuck Queenan is a former

00:40:47.710 --> 00:40:47.720
of kano Gates Chuck Queenan is a former
 

00:40:47.720 --> 00:40:50.620
of kano Gates Chuck Queenan is a former
chair of the board at CMU he's currently

00:40:50.620 --> 00:40:50.630
chair of the board at CMU he's currently
 

00:40:50.630 --> 00:40:52.700
chair of the board at CMU he's currently
an emeritus trustee of the University

00:40:52.700 --> 00:40:52.710
an emeritus trustee of the University
 

00:40:52.710 --> 00:40:54.950
an emeritus trustee of the University
continues to provide wisdom and guidance

00:40:54.950 --> 00:40:54.960
continues to provide wisdom and guidance
 

00:40:54.960 --> 00:40:57.890
continues to provide wisdom and guidance
to both of our organizations with this

00:40:57.890 --> 00:40:57.900
to both of our organizations with this
 

00:40:57.900 --> 00:41:00.230
to both of our organizations with this
conference we have added another

00:41:00.230 --> 00:41:00.240
conference we have added another
 

00:41:00.240 --> 00:41:02.720
conference we have added another
dimension to that relationship as you

00:41:02.720 --> 00:41:02.730
dimension to that relationship as you
 

00:41:02.730 --> 00:41:04.970
dimension to that relationship as you
know it will be a biennial event and

00:41:04.970 --> 00:41:04.980
know it will be a biennial event and
 

00:41:04.980 --> 00:41:07.670
know it will be a biennial event and
will be an opportunity that we hope will

00:41:07.670 --> 00:41:07.680
will be an opportunity that we hope will
 

00:41:07.680 --> 00:41:10.579
will be an opportunity that we hope will
stimulate great discussion and important

00:41:10.579 --> 00:41:10.589
stimulate great discussion and important
 

00:41:10.589 --> 00:41:15.349
stimulate great discussion and important
dialogue you might reasonably ask why is

00:41:15.349 --> 00:41:15.359
dialogue you might reasonably ask why is
 

00:41:15.359 --> 00:41:18.140
dialogue you might reasonably ask why is
the field of ethics and intelligence

00:41:18.140 --> 00:41:18.150
the field of ethics and intelligence
 

00:41:18.150 --> 00:41:20.270
the field of ethics and intelligence
artificial intelligence important decay

00:41:20.270 --> 00:41:20.280
artificial intelligence important decay
 

00:41:20.280 --> 00:41:23.540
artificial intelligence important decay
no gates a law firm that's a reasonable

00:41:23.540 --> 00:41:23.550
no gates a law firm that's a reasonable
 

00:41:23.550 --> 00:41:26.540
no gates a law firm that's a reasonable
question our firms work and our

00:41:26.540 --> 00:41:26.550
question our firms work and our
 

00:41:26.550 --> 00:41:29.920
question our firms work and our
footprint is global as well as national

00:41:29.920 --> 00:41:29.930
footprint is global as well as national
 

00:41:29.930 --> 00:41:32.569
footprint is global as well as national
we like to think that we are a leader in

00:41:32.569 --> 00:41:32.579
we like to think that we are a leader in
 

00:41:32.579 --> 00:41:34.790
we like to think that we are a leader in
the practice of law as it relates to

00:41:34.790 --> 00:41:34.800
the practice of law as it relates to
 

00:41:34.800 --> 00:41:37.240
the practice of law as it relates to
technology innovation and development

00:41:37.240 --> 00:41:37.250
technology innovation and development
 

00:41:37.250 --> 00:41:39.890
technology innovation and development
we're fortunate to represent some of the

00:41:39.890 --> 00:41:39.900
we're fortunate to represent some of the
 

00:41:39.900 --> 00:41:41.809
we're fortunate to represent some of the
leading technology entities in the world

00:41:41.809 --> 00:41:41.819
leading technology entities in the world
 

00:41:41.819 --> 00:41:44.480
leading technology entities in the world
as well as numerous startup companies

00:41:44.480 --> 00:41:44.490
as well as numerous startup companies
 

00:41:44.490 --> 00:41:48.109
as well as numerous startup companies
that aspire to be in that sphere we care

00:41:48.109 --> 00:41:48.119
that aspire to be in that sphere we care
 

00:41:48.119 --> 00:41:49.880
that aspire to be in that sphere we care
about what our clients care about and

00:41:49.880 --> 00:41:49.890
about what our clients care about and
 

00:41:49.890 --> 00:41:52.520
about what our clients care about and
forward-looking clients everywhere care

00:41:52.520 --> 00:41:52.530
forward-looking clients everywhere care
 

00:41:52.530 --> 00:41:54.950
forward-looking clients everywhere care
about artificial intelligence and its

00:41:54.950 --> 00:41:54.960
about artificial intelligence and its
 

00:41:54.960 --> 00:41:57.530
about artificial intelligence and its
potential impact on their businesses and

00:41:57.530 --> 00:41:57.540
potential impact on their businesses and
 

00:41:57.540 --> 00:42:01.700
potential impact on their businesses and
society at large evidence of this is the

00:42:01.700 --> 00:42:01.710
society at large evidence of this is the
 

00:42:01.710 --> 00:42:04.490
society at large evidence of this is the
fact that our artificial intelligence

00:42:04.490 --> 00:42:04.500
fact that our artificial intelligence
 

00:42:04.500 --> 00:42:07.160
fact that our artificial intelligence
practice area now comprises over 80

00:42:07.160 --> 00:42:07.170
practice area now comprises over 80
 

00:42:07.170 --> 00:42:10.670
practice area now comprises over 80
lawyers the legal app the legal

00:42:10.670 --> 00:42:10.680
lawyers the legal app the legal
 

00:42:10.680 --> 00:42:13.250
lawyers the legal app the legal
implications of the application of AI on

00:42:13.250 --> 00:42:13.260
implications of the application of AI on
 

00:42:13.260 --> 00:42:15.799
implications of the application of AI on
our clients is broad deep and

00:42:15.799 --> 00:42:15.809
our clients is broad deep and
 

00:42:15.809 --> 00:42:18.289
our clients is broad deep and
interdisciplinary it will affect

00:42:18.289 --> 00:42:18.299
interdisciplinary it will affect
 

00:42:18.299 --> 00:42:20.390
interdisciplinary it will affect
numerous different practice areas for us

00:42:20.390 --> 00:42:20.400
numerous different practice areas for us
 

00:42:20.400 --> 00:42:22.460
numerous different practice areas for us
and for clients around the world

00:42:22.460 --> 00:42:22.470
and for clients around the world
 

00:42:22.470 --> 00:42:25.250
and for clients around the world
including mergers and acquisitions

00:42:25.250 --> 00:42:25.260
including mergers and acquisitions
 

00:42:25.260 --> 00:42:27.530
including mergers and acquisitions
healthcare the protection of

00:42:27.530 --> 00:42:27.540
healthcare the protection of
 

00:42:27.540 --> 00:42:30.200
healthcare the protection of
intellectual property employment law and

00:42:30.200 --> 00:42:30.210
intellectual property employment law and
 

00:42:30.210 --> 00:42:34.910
intellectual property employment law and
a host of regulatory issues further the

00:42:34.910 --> 00:42:34.920
a host of regulatory issues further the
 

00:42:34.920 --> 00:42:36.770
a host of regulatory issues further the
practice of law itself is undergoing

00:42:36.770 --> 00:42:36.780
practice of law itself is undergoing
 

00:42:36.780 --> 00:42:40.160
practice of law itself is undergoing
major shifts in AI increasingly will be

00:42:40.160 --> 00:42:40.170
major shifts in AI increasingly will be
 

00:42:40.170 --> 00:42:42.620
major shifts in AI increasingly will be
important to how legal services are

00:42:42.620 --> 00:42:42.630
important to how legal services are
 

00:42:42.630 --> 00:42:45.380
important to how legal services are
delivered to clients we approach what we

00:42:45.380 --> 00:42:45.390
delivered to clients we approach what we
 

00:42:45.390 --> 00:42:48.410
delivered to clients we approach what we
do in a forward-looking way with a

00:42:48.410 --> 00:42:48.420
do in a forward-looking way with a
 

00:42:48.420 --> 00:42:50.720
do in a forward-looking way with a
forward-looking mindset that's part of

00:42:50.720 --> 00:42:50.730
forward-looking mindset that's part of
 

00:42:50.730 --> 00:42:52.910
forward-looking mindset that's part of
our DNA and that's why we're so excited

00:42:52.910 --> 00:42:52.920
our DNA and that's why we're so excited
 

00:42:52.920 --> 00:42:54.770
our DNA and that's why we're so excited
to be part of this important discussion

00:42:54.770 --> 00:42:54.780
to be part of this important discussion
 

00:42:54.780 --> 00:42:58.640
to be part of this important discussion
and exchange of ideas all of us know

00:42:58.640 --> 00:42:58.650
and exchange of ideas all of us know
 

00:42:58.650 --> 00:43:00.440
and exchange of ideas all of us know
that advancements and computer related

00:43:00.440 --> 00:43:00.450
that advancements and computer related
 

00:43:00.450 --> 00:43:02.960
that advancements and computer related
technologies many of them born of

00:43:02.960 --> 00:43:02.970
technologies many of them born of
 

00:43:02.970 --> 00:43:04.490
technologies many of them born of
research here at Carnegie Mellon

00:43:04.490 --> 00:43:04.500
research here at Carnegie Mellon
 

00:43:04.500 --> 00:43:06.520
research here at Carnegie Mellon
University going back to her

00:43:06.520 --> 00:43:06.530
University going back to her
 

00:43:06.530 --> 00:43:08.440
University going back to her
Simon and his colleagues will

00:43:08.440 --> 00:43:08.450
Simon and his colleagues will
 

00:43:08.450 --> 00:43:10.180
Simon and his colleagues will
increasingly bring about profound

00:43:10.180 --> 00:43:10.190
increasingly bring about profound
 

00:43:10.190 --> 00:43:12.880
increasingly bring about profound
changes that affect our society and

00:43:12.880 --> 00:43:12.890
changes that affect our society and
 

00:43:12.890 --> 00:43:16.290
changes that affect our society and
humanity in many ways we can see that a

00:43:16.290 --> 00:43:16.300
humanity in many ways we can see that a
 

00:43:16.300 --> 00:43:19.270
humanity in many ways we can see that a
critical dimension to many of the

00:43:19.270 --> 00:43:19.280
critical dimension to many of the
 

00:43:19.280 --> 00:43:20.920
critical dimension to many of the
choices that will have to be made

00:43:20.920 --> 00:43:20.930
choices that will have to be made
 

00:43:20.930 --> 00:43:23.910
choices that will have to be made
whether implicitly or explicitly as

00:43:23.910 --> 00:43:23.920
whether implicitly or explicitly as
 

00:43:23.920 --> 00:43:27.820
whether implicitly or explicitly as
technology is developed is the ethical

00:43:27.820 --> 00:43:27.830
technology is developed is the ethical
 

00:43:27.830 --> 00:43:30.010
technology is developed is the ethical
dimension and this is going to be

00:43:30.010 --> 00:43:30.020
dimension and this is going to be
 

00:43:30.020 --> 00:43:33.190
dimension and this is going to be
important as our policymakers grapple

00:43:33.190 --> 00:43:33.200
important as our policymakers grapple
 

00:43:33.200 --> 00:43:35.470
important as our policymakers grapple
with difficult issue as time goes on and

00:43:35.470 --> 00:43:35.480
with difficult issue as time goes on and
 

00:43:35.480 --> 00:43:39.610
with difficult issue as time goes on and
as advances are made we all have a stake

00:43:39.610 --> 00:43:39.620
as advances are made we all have a stake
 

00:43:39.620 --> 00:43:41.400
as advances are made we all have a stake
in the outcome of these discussions

00:43:41.400 --> 00:43:41.410
in the outcome of these discussions
 

00:43:41.410 --> 00:43:44.260
in the outcome of these discussions
getting the best minds from across the

00:43:44.260 --> 00:43:44.270
getting the best minds from across the
 

00:43:44.270 --> 00:43:46.840
getting the best minds from across the
relevant fields of study from robotics

00:43:46.840 --> 00:43:46.850
relevant fields of study from robotics
 

00:43:46.850 --> 00:43:50.530
relevant fields of study from robotics
to philosophy in between and around to

00:43:50.530 --> 00:43:50.540
to philosophy in between and around to
 

00:43:50.540 --> 00:43:52.750
to philosophy in between and around to
focus on these problems is essential to

00:43:52.750 --> 00:43:52.760
focus on these problems is essential to
 

00:43:52.760 --> 00:43:55.410
focus on these problems is essential to
address in these matters efficiently and

00:43:55.410 --> 00:43:55.420
address in these matters efficiently and
 

00:43:55.420 --> 00:43:59.350
address in these matters efficiently and
ethically in making our gift to fund an

00:43:59.350 --> 00:43:59.360
ethically in making our gift to fund an
 

00:43:59.360 --> 00:44:01.330
ethically in making our gift to fund an
endowment and ethics and computational

00:44:01.330 --> 00:44:01.340
endowment and ethics and computational
 

00:44:01.340 --> 00:44:03.850
endowment and ethics and computational
technologies we made an investment for

00:44:03.850 --> 00:44:03.860
technologies we made an investment for
 

00:44:03.860 --> 00:44:06.640
technologies we made an investment for
the long term in an area important to us

00:44:06.640 --> 00:44:06.650
the long term in an area important to us
 

00:44:06.650 --> 00:44:09.790
the long term in an area important to us
as a law firm into society at large with

00:44:09.790 --> 00:44:09.800
as a law firm into society at large with
 

00:44:09.800 --> 00:44:11.410
as a law firm into society at large with
this conference we are pleased to open

00:44:11.410 --> 00:44:11.420
this conference we are pleased to open
 

00:44:11.420 --> 00:44:13.030
this conference we are pleased to open
this new forum for discussion and

00:44:13.030 --> 00:44:13.040
this new forum for discussion and
 

00:44:13.040 --> 00:44:15.250
this new forum for discussion and
development of the important work of the

00:44:15.250 --> 00:44:15.260
development of the important work of the
 

00:44:15.260 --> 00:44:18.310
development of the important work of the
speakers and participants welcome and I

00:44:18.310 --> 00:44:18.320
speakers and participants welcome and I
 

00:44:18.320 --> 00:44:19.960
speakers and participants welcome and I
hope you enjoy and benefit from the

00:44:19.960 --> 00:44:19.970
hope you enjoy and benefit from the
 

00:44:19.970 --> 00:44:21.550
hope you enjoy and benefit from the
conference

00:44:21.550 --> 00:44:21.560
conference
 

00:44:21.560 --> 00:44:36.000
conference
[Applause]

00:44:36.000 --> 00:44:36.010
 

00:44:36.010 --> 00:44:38.980
Thank You President Joe Haney and mr.

00:44:38.980 --> 00:44:38.990
Thank You President Joe Haney and mr.
 

00:44:38.990 --> 00:44:41.290
Thank You President Joe Haney and mr.
Sager doll and Mayor Peduto I'm David

00:44:41.290 --> 00:44:41.300
Sager doll and Mayor Peduto I'm David
 

00:44:41.300 --> 00:44:43.180
Sager doll and Mayor Peduto I'm David
Danks Thurston professor of philosophy

00:44:43.180 --> 00:44:43.190
Danks Thurston professor of philosophy
 

00:44:43.190 --> 00:44:47.800
Danks Thurston professor of philosophy
and psychology here at CMU and co-chair

00:44:47.800 --> 00:44:47.810
and psychology here at CMU and co-chair
 

00:44:47.810 --> 00:44:49.350
and psychology here at CMU and co-chair
of this event with Eleanor Bosch

00:44:49.350 --> 00:44:49.360
of this event with Eleanor Bosch
 

00:44:49.360 --> 00:44:51.640
of this event with Eleanor Bosch
currently professor of robotics for

00:44:51.640 --> 00:44:51.650
currently professor of robotics for
 

00:44:51.650 --> 00:44:53.920
currently professor of robotics for
about another two hours at which point

00:44:53.920 --> 00:44:53.930
about another two hours at which point
 

00:44:53.930 --> 00:44:56.050
about another two hours at which point
he becomes the inaugural K&amp;L gates

00:44:56.050 --> 00:44:56.060
he becomes the inaugural K&amp;L gates
 

00:44:56.060 --> 00:44:58.170
he becomes the inaugural K&amp;L gates
professor of ethics and computational

00:44:58.170 --> 00:44:58.180
professor of ethics and computational
 

00:44:58.180 --> 00:45:00.940
professor of ethics and computational
technologies and it is our great

00:45:00.940 --> 00:45:00.950
technologies and it is our great
 

00:45:00.950 --> 00:45:03.400
technologies and it is our great
pleasure to welcome you all to this

00:45:03.400 --> 00:45:03.410
pleasure to welcome you all to this
 

00:45:03.410 --> 00:45:05.670
pleasure to welcome you all to this
inaugural conference on ethics and AI

00:45:05.670 --> 00:45:05.680
inaugural conference on ethics and AI
 

00:45:05.680 --> 00:45:09.790
inaugural conference on ethics and AI
here in Pittsburgh I want to say a few

00:45:09.790 --> 00:45:09.800
here in Pittsburgh I want to say a few
 

00:45:09.800 --> 00:45:12.010
here in Pittsburgh I want to say a few
words about why this conference needs to

00:45:12.010 --> 00:45:12.020
words about why this conference needs to
 

00:45:12.020 --> 00:45:13.870
words about why this conference needs to
happen now and why it has to have the

00:45:13.870 --> 00:45:13.880
happen now and why it has to have the
 

00:45:13.880 --> 00:45:16.030
happen now and why it has to have the
format that it has no doubt you're all

00:45:16.030 --> 00:45:16.040
format that it has no doubt you're all
 

00:45:16.040 --> 00:45:17.170
format that it has no doubt you're all
well aware that artificial intelligence

00:45:17.170 --> 00:45:17.180
well aware that artificial intelligence
 

00:45:17.180 --> 00:45:19.770
well aware that artificial intelligence
has reached fever pitch in society today

00:45:19.770 --> 00:45:19.780
has reached fever pitch in society today
 

00:45:19.780 --> 00:45:22.210
has reached fever pitch in society today
thanks to certain companies what they've

00:45:22.210 --> 00:45:22.220
thanks to certain companies what they've
 

00:45:22.220 --> 00:45:24.310
thanks to certain companies what they've
done in terms of rushing forth with

00:45:24.310 --> 00:45:24.320
done in terms of rushing forth with
 

00:45:24.320 --> 00:45:26.890
done in terms of rushing forth with
deployments in the real world and thanks

00:45:26.890 --> 00:45:26.900
deployments in the real world and thanks
 

00:45:26.900 --> 00:45:27.940
deployments in the real world and thanks
to discourse by outstanding

00:45:27.940 --> 00:45:27.950
to discourse by outstanding
 

00:45:27.950 --> 00:45:29.590
to discourse by outstanding
investigative journalists who have

00:45:29.590 --> 00:45:29.600
investigative journalists who have
 

00:45:29.600 --> 00:45:31.060
investigative journalists who have
turned the public's attention to these

00:45:31.060 --> 00:45:31.070
turned the public's attention to these
 

00:45:31.070 --> 00:45:33.460
turned the public's attention to these
issues but the people in this room know

00:45:33.460 --> 00:45:33.470
issues but the people in this room know
 

00:45:33.470 --> 00:45:35.680
issues but the people in this room know
something else because many of our

00:45:35.680 --> 00:45:35.690
something else because many of our
 

00:45:35.690 --> 00:45:37.390
something else because many of our
friends who are here from policy makers

00:45:37.390 --> 00:45:37.400
friends who are here from policy makers
 

00:45:37.400 --> 00:45:39.880
friends who are here from policy makers
to business folks to researchers are

00:45:39.880 --> 00:45:39.890
to business folks to researchers are
 

00:45:39.890 --> 00:45:41.290
to business folks to researchers are
flying around the world at a breakneck

00:45:41.290 --> 00:45:41.300
flying around the world at a breakneck
 

00:45:41.300 --> 00:45:43.600
flying around the world at a breakneck
pace now between meetings and other

00:45:43.600 --> 00:45:43.610
pace now between meetings and other
 

00:45:43.610 --> 00:45:45.750
pace now between meetings and other
meetings discussing regulation

00:45:45.750 --> 00:45:45.760
meetings discussing regulation
 

00:45:45.760 --> 00:45:47.980
meetings discussing regulation
discussing entrepreneurship and venture

00:45:47.980 --> 00:45:47.990
discussing entrepreneurship and venture
 

00:45:47.990 --> 00:45:50.590
discussing entrepreneurship and venture
capital funding to AI discussing the

00:45:50.590 --> 00:45:50.600
capital funding to AI discussing the
 

00:45:50.600 --> 00:45:52.450
capital funding to AI discussing the
question of how we engineer ethics into

00:45:52.450 --> 00:45:52.460
question of how we engineer ethics into
 

00:45:52.460 --> 00:45:54.700
question of how we engineer ethics into
artificial intelligence systems and all

00:45:54.700 --> 00:45:54.710
artificial intelligence systems and all
 

00:45:54.710 --> 00:45:56.170
artificial intelligence systems and all
those meetings are fantastic and must

00:45:56.170 --> 00:45:56.180
those meetings are fantastic and must
 

00:45:56.180 --> 00:45:58.330
those meetings are fantastic and must
happen but this meeting has to happen

00:45:58.330 --> 00:45:58.340
happen but this meeting has to happen
 

00:45:58.340 --> 00:45:59.410
happen but this meeting has to happen
because there's a fundamental question

00:45:59.410 --> 00:45:59.420
because there's a fundamental question
 

00:45:59.420 --> 00:46:01.810
because there's a fundamental question
that is actually inherently

00:46:01.810 --> 00:46:01.820
that is actually inherently
 

00:46:01.820 --> 00:46:05.650
that is actually inherently
transdisciplinary what will artificial

00:46:05.650 --> 00:46:05.660
transdisciplinary what will artificial
 

00:46:05.660 --> 00:46:08.110
transdisciplinary what will artificial
intelligence do to our sense of human

00:46:08.110 --> 00:46:08.120
intelligence do to our sense of human
 

00:46:08.120 --> 00:46:11.410
intelligence do to our sense of human
identity what will we be in the age that

00:46:11.410 --> 00:46:11.420
identity what will we be in the age that
 

00:46:11.420 --> 00:46:13.900
identity what will we be in the age that
we can see already on the horizon that's

00:46:13.900 --> 00:46:13.910
we can see already on the horizon that's
 

00:46:13.910 --> 00:46:16.180
we can see already on the horizon that's
the fundamental question of identity and

00:46:16.180 --> 00:46:16.190
the fundamental question of identity and
 

00:46:16.190 --> 00:46:17.800
the fundamental question of identity and
the fundamental question of human

00:46:17.800 --> 00:46:17.810
the fundamental question of human
 

00:46:17.810 --> 00:46:20.200
the fundamental question of human
dignity and those are the issues that we

00:46:20.200 --> 00:46:20.210
dignity and those are the issues that we
 

00:46:20.210 --> 00:46:22.030
dignity and those are the issues that we
will grapple with during this conference

00:46:22.030 --> 00:46:22.040
will grapple with during this conference
 

00:46:22.040 --> 00:46:23.860
will grapple with during this conference
so we've designed a conference for you

00:46:23.860 --> 00:46:23.870
so we've designed a conference for you
 

00:46:23.870 --> 00:46:25.270
so we've designed a conference for you
that brings together outstanding

00:46:25.270 --> 00:46:25.280
that brings together outstanding
 

00:46:25.280 --> 00:46:27.730
that brings together outstanding
journalists and experts and sets them up

00:46:27.730 --> 00:46:27.740
journalists and experts and sets them up
 

00:46:27.740 --> 00:46:29.980
journalists and experts and sets them up
not to talk within Disciplinary confines

00:46:29.980 --> 00:46:29.990
not to talk within Disciplinary confines
 

00:46:29.990 --> 00:46:31.450
not to talk within Disciplinary confines
but rather to talk in a

00:46:31.450 --> 00:46:31.460
but rather to talk in a
 

00:46:31.460 --> 00:46:32.380
but rather to talk in a
transdisciplinary

00:46:32.380 --> 00:46:32.390
transdisciplinary
 

00:46:32.390 --> 00:46:34.540
transdisciplinary
manner about the core issues of identity

00:46:34.540 --> 00:46:34.550
manner about the core issues of identity
 

00:46:34.550 --> 00:46:37.450
manner about the core issues of identity
of humanity itself that's a conversation

00:46:37.450 --> 00:46:37.460
of humanity itself that's a conversation
 

00:46:37.460 --> 00:46:39.620
of humanity itself that's a conversation
that we think has to start now

00:46:39.620 --> 00:46:39.630
that we think has to start now
 

00:46:39.630 --> 00:46:41.690
that we think has to start now
has to repeat over and over again so

00:46:41.690 --> 00:46:41.700
has to repeat over and over again so
 

00:46:41.700 --> 00:46:44.630
has to repeat over and over again so
that's the conversation that you're part

00:46:44.630 --> 00:46:44.640
that's the conversation that you're part
 

00:46:44.640 --> 00:46:46.100
that's the conversation that you're part
of and that we're very happy to welcome

00:46:46.100 --> 00:46:46.110
of and that we're very happy to welcome
 

00:46:46.110 --> 00:46:49.970
of and that we're very happy to welcome
you to but enough about the highfalutin

00:46:49.970 --> 00:46:49.980
you to but enough about the highfalutin
 

00:46:49.980 --> 00:46:52.960
you to but enough about the highfalutin
stuff some logistical notes very quickly

00:46:52.960 --> 00:46:52.970
stuff some logistical notes very quickly
 

00:46:52.970 --> 00:46:55.340
stuff some logistical notes very quickly
you'll notice on all of the tables that

00:46:55.340 --> 00:46:55.350
you'll notice on all of the tables that
 

00:46:55.350 --> 00:46:57.590
you'll notice on all of the tables that
there are cubes that have the schedule

00:46:57.590 --> 00:46:57.600
there are cubes that have the schedule
 

00:46:57.600 --> 00:47:00.470
there are cubes that have the schedule
have information about Wi-Fi for those

00:47:00.470 --> 00:47:00.480
have information about Wi-Fi for those
 

00:47:00.480 --> 00:47:02.210
have information about Wi-Fi for those
who are interested in that Twitter

00:47:02.210 --> 00:47:02.220
who are interested in that Twitter
 

00:47:02.220 --> 00:47:05.810
who are interested in that Twitter
hashtag and so forth throughout the the

00:47:05.810 --> 00:47:05.820
hashtag and so forth throughout the the
 

00:47:05.820 --> 00:47:08.270
hashtag and so forth throughout the the
conference over the next two days there

00:47:08.270 --> 00:47:08.280
conference over the next two days there
 

00:47:08.280 --> 00:47:10.460
conference over the next two days there
will be opportunities for you as members

00:47:10.460 --> 00:47:10.470
will be opportunities for you as members
 

00:47:10.470 --> 00:47:12.920
will be opportunities for you as members
of the audience to participate in terms

00:47:12.920 --> 00:47:12.930
of the audience to participate in terms
 

00:47:12.930 --> 00:47:14.990
of the audience to participate in terms
of providing questions for panelists to

00:47:14.990 --> 00:47:15.000
of providing questions for panelists to
 

00:47:15.000 --> 00:47:17.300
of providing questions for panelists to
answer that will all be done using a

00:47:17.300 --> 00:47:17.310
answer that will all be done using a
 

00:47:17.310 --> 00:47:19.280
answer that will all be done using a
technology refer to a slide Oh an app

00:47:19.280 --> 00:47:19.290
technology refer to a slide Oh an app
 

00:47:19.290 --> 00:47:20.510
technology refer to a slide Oh an app
and there will be some more information

00:47:20.510 --> 00:47:20.520
and there will be some more information
 

00:47:20.520 --> 00:47:23.300
and there will be some more information
provided about that as we get to the

00:47:23.300 --> 00:47:23.310
provided about that as we get to the
 

00:47:23.310 --> 00:47:26.330
provided about that as we get to the
first panel the conference is being

00:47:26.330 --> 00:47:26.340
first panel the conference is being
 

00:47:26.340 --> 00:47:28.490
first panel the conference is being
live-streamed so if there are people

00:47:28.490 --> 00:47:28.500
live-streamed so if there are people
 

00:47:28.500 --> 00:47:29.990
live-streamed so if there are people
that you know wanted to participate but

00:47:29.990 --> 00:47:30.000
that you know wanted to participate but
 

00:47:30.000 --> 00:47:31.820
that you know wanted to participate but
we're unable to be here please let them

00:47:31.820 --> 00:47:31.830
we're unable to be here please let them
 

00:47:31.830 --> 00:47:32.570
we're unable to be here please let them
know about that

00:47:32.570 --> 00:47:32.580
know about that
 

00:47:32.580 --> 00:47:34.910
know about that
and the conference elements will all

00:47:34.910 --> 00:47:34.920
and the conference elements will all
 

00:47:34.920 --> 00:47:37.790
and the conference elements will all
over the coming weeks be rolled out on

00:47:37.790 --> 00:47:37.800
over the coming weeks be rolled out on
 

00:47:37.800 --> 00:47:40.160
over the coming weeks be rolled out on
the webpage so we encourage you to

00:47:40.160 --> 00:47:40.170
the webpage so we encourage you to
 

00:47:40.170 --> 00:47:43.250
the webpage so we encourage you to
continue to look back at that ok back to

00:47:43.250 --> 00:47:43.260
continue to look back at that ok back to
 

00:47:43.260 --> 00:47:46.370
continue to look back at that ok back to
the important things this conference as

00:47:46.370 --> 00:47:46.380
the important things this conference as
 

00:47:46.380 --> 00:47:48.620
the important things this conference as
ela said is about having a conversation

00:47:48.620 --> 00:47:48.630
ela said is about having a conversation
 

00:47:48.630 --> 00:47:51.650
ela said is about having a conversation
of the ways that technology particularly

00:47:51.650 --> 00:47:51.660
of the ways that technology particularly
 

00:47:51.660 --> 00:47:54.430
of the ways that technology particularly
AI and robotic technologies are

00:47:54.430 --> 00:47:54.440
AI and robotic technologies are
 

00:47:54.440 --> 00:47:56.960
AI and robotic technologies are
influencing impacting providing

00:47:56.960 --> 00:47:56.970
influencing impacting providing
 

00:47:56.970 --> 00:48:00.050
influencing impacting providing
opportunities and challenges for us

00:48:00.050 --> 00:48:00.060
opportunities and challenges for us
 

00:48:00.060 --> 00:48:03.200
opportunities and challenges for us
humans for societies for nations for

00:48:03.200 --> 00:48:03.210
humans for societies for nations for
 

00:48:03.210 --> 00:48:06.050
humans for societies for nations for
peoples and in particular we've decided

00:48:06.050 --> 00:48:06.060
peoples and in particular we've decided
 

00:48:06.060 --> 00:48:08.210
peoples and in particular we've decided
to organize this conference around four

00:48:08.210 --> 00:48:08.220
to organize this conference around four
 

00:48:08.220 --> 00:48:09.650
to organize this conference around four
different sessions each of which is

00:48:09.650 --> 00:48:09.660
different sessions each of which is
 

00:48:09.660 --> 00:48:12.680
different sessions each of which is
anchored in a particular human interest

00:48:12.680 --> 00:48:12.690
anchored in a particular human interest
 

00:48:12.690 --> 00:48:14.960
anchored in a particular human interest
or human value so rather than having a

00:48:14.960 --> 00:48:14.970
or human value so rather than having a
 

00:48:14.970 --> 00:48:17.720
or human value so rather than having a
session on AI and self-driving cars we

00:48:17.720 --> 00:48:17.730
session on AI and self-driving cars we
 

00:48:17.730 --> 00:48:20.240
session on AI and self-driving cars we
have sessions on equity of access and

00:48:20.240 --> 00:48:20.250
have sessions on equity of access and
 

00:48:20.250 --> 00:48:23.930
have sessions on equity of access and
equity of impact trust policy and

00:48:23.930 --> 00:48:23.940
equity of impact trust policy and
 

00:48:23.940 --> 00:48:26.330
equity of impact trust policy and
governance and agency and empowerment

00:48:26.330 --> 00:48:26.340
governance and agency and empowerment
 

00:48:26.340 --> 00:48:29.480
governance and agency and empowerment
which we hope will help to focus the

00:48:29.480 --> 00:48:29.490
which we hope will help to focus the
 

00:48:29.490 --> 00:48:31.430
which we hope will help to focus the
interdisciplinary and transdisciplinary

00:48:31.430 --> 00:48:31.440
interdisciplinary and transdisciplinary
 

00:48:31.440 --> 00:48:33.370
interdisciplinary and transdisciplinary
and multidisciplinary conversations

00:48:33.370 --> 00:48:33.380
and multidisciplinary conversations
 

00:48:33.380 --> 00:48:36.290
and multidisciplinary conversations
around these core human values and

00:48:36.290 --> 00:48:36.300
around these core human values and
 

00:48:36.300 --> 00:48:38.660
around these core human values and
interests so that we can start to make

00:48:38.660 --> 00:48:38.670
interests so that we can start to make
 

00:48:38.670 --> 00:48:40.940
interests so that we can start to make
real progress over the course of the

00:48:40.940 --> 00:48:40.950
real progress over the course of the
 

00:48:40.950 --> 00:48:44.090
real progress over the course of the
coming days and so with that said I'd

00:48:44.090 --> 00:48:44.100
coming days and so with that said I'd
 

00:48:44.100 --> 00:48:45.650
coming days and so with that said I'd
like to turn things over to ela to

00:48:45.650 --> 00:48:45.660
like to turn things over to ela to
 

00:48:45.660 --> 00:48:48.650
like to turn things over to ela to
introduce the first session thank you

00:48:48.650 --> 00:48:48.660
introduce the first session thank you
 

00:48:48.660 --> 00:48:49.040
introduce the first session thank you
David

00:48:49.040 --> 00:48:49.050
David
 

00:48:49.050 --> 00:48:51.110
David
session 1 we're gonna dive right into

00:48:51.110 --> 00:48:51.120
session 1 we're gonna dive right into
 

00:48:51.120 --> 00:48:53.300
session 1 we're gonna dive right into
this equity of access and they could

00:48:53.300 --> 00:48:53.310
this equity of access and they could
 

00:48:53.310 --> 00:48:55.760
this equity of access and they could
impact so let me just say 30 seconds

00:48:55.760 --> 00:48:55.770
impact so let me just say 30 seconds
 

00:48:55.770 --> 00:48:57.440
impact so let me just say 30 seconds
about that what are the interesting

00:48:57.440 --> 00:48:57.450
about that what are the interesting
 

00:48:57.450 --> 00:48:59.030
about that what are the interesting
things about artificial intelligence and

00:48:59.030 --> 00:48:59.040
things about artificial intelligence and
 

00:48:59.040 --> 00:49:01.070
things about artificial intelligence and
computational technologies writ large is

00:49:01.070 --> 00:49:01.080
computational technologies writ large is
 

00:49:01.080 --> 00:49:02.990
computational technologies writ large is
that they can increase the apparent

00:49:02.990 --> 00:49:03.000
that they can increase the apparent
 

00:49:03.000 --> 00:49:04.880
that they can increase the apparent
productivity of human society they can

00:49:04.880 --> 00:49:04.890
productivity of human society they can
 

00:49:04.890 --> 00:49:06.920
productivity of human society they can
allow us to do more they can allow us

00:49:06.920 --> 00:49:06.930
allow us to do more they can allow us
 

00:49:06.930 --> 00:49:09.350
allow us to do more they can allow us
new kinds of discoveries new spaces to

00:49:09.350 --> 00:49:09.360
new kinds of discoveries new spaces to
 

00:49:09.360 --> 00:49:11.810
new kinds of discoveries new spaces to
in which we can discover and create new

00:49:11.810 --> 00:49:11.820
in which we can discover and create new
 

00:49:11.820 --> 00:49:13.970
in which we can discover and create new
knowledge the challenge with that form

00:49:13.970 --> 00:49:13.980
knowledge the challenge with that form
 

00:49:13.980 --> 00:49:16.910
knowledge the challenge with that form
of innovation is of course stemming from

00:49:16.910 --> 00:49:16.920
of innovation is of course stemming from
 

00:49:16.920 --> 00:49:19.160
of innovation is of course stemming from
the fact that AI and information itself

00:49:19.160 --> 00:49:19.170
the fact that AI and information itself
 

00:49:19.170 --> 00:49:21.260
the fact that AI and information itself
has ownership and as a result as you

00:49:21.260 --> 00:49:21.270
has ownership and as a result as you
 

00:49:21.270 --> 00:49:24.110
has ownership and as a result as you
create new AI technologies new

00:49:24.110 --> 00:49:24.120
create new AI technologies new
 

00:49:24.120 --> 00:49:26.750
create new AI technologies new
innovations you have the danger that you

00:49:26.750 --> 00:49:26.760
innovations you have the danger that you
 

00:49:26.760 --> 00:49:28.520
innovations you have the danger that you
can limit the access to that AI

00:49:28.520 --> 00:49:28.530
can limit the access to that AI
 

00:49:28.530 --> 00:49:30.260
can limit the access to that AI
technology to the few you have the

00:49:30.260 --> 00:49:30.270
technology to the few you have the
 

00:49:30.270 --> 00:49:32.980
technology to the few you have the
further danger that you can in fact

00:49:32.980 --> 00:49:32.990
further danger that you can in fact
 

00:49:32.990 --> 00:49:35.570
further danger that you can in fact
augment the wealth of the few through

00:49:35.570 --> 00:49:35.580
augment the wealth of the few through
 

00:49:35.580 --> 00:49:37.280
augment the wealth of the few through
those very same technologies thus

00:49:37.280 --> 00:49:37.290
those very same technologies thus
 

00:49:37.290 --> 00:49:39.680
those very same technologies thus
increasing disparity between the those

00:49:39.680 --> 00:49:39.690
increasing disparity between the those
 

00:49:39.690 --> 00:49:41.870
increasing disparity between the those
who have and those who have not so

00:49:41.870 --> 00:49:41.880
who have and those who have not so
 

00:49:41.880 --> 00:49:43.460
who have and those who have not so
fundamentally we have on the one hand

00:49:43.460 --> 00:49:43.470
fundamentally we have on the one hand
 

00:49:43.470 --> 00:49:45.200
fundamentally we have on the one hand
technology that can make the world a

00:49:45.200 --> 00:49:45.210
technology that can make the world a
 

00:49:45.210 --> 00:49:45.890
technology that can make the world a
better place

00:49:45.890 --> 00:49:45.900
better place
 

00:49:45.900 --> 00:49:48.350
better place
on average the same time we have a

00:49:48.350 --> 00:49:48.360
on average the same time we have a
 

00:49:48.360 --> 00:49:50.360
on average the same time we have a
technology that while the average case

00:49:50.360 --> 00:49:50.370
technology that while the average case
 

00:49:50.370 --> 00:49:50.990
technology that while the average case
might be better

00:49:50.990 --> 00:49:51.000
might be better
 

00:49:51.000 --> 00:49:53.270
might be better
the extrema might be far worse and so

00:49:53.270 --> 00:49:53.280
the extrema might be far worse and so
 

00:49:53.280 --> 00:49:56.030
the extrema might be far worse and so
that fundamental challenge in terms of

00:49:56.030 --> 00:49:56.040
that fundamental challenge in terms of
 

00:49:56.040 --> 00:49:58.520
that fundamental challenge in terms of
both access and in terms of how the

00:49:58.520 --> 00:49:58.530
both access and in terms of how the
 

00:49:58.530 --> 00:50:00.530
both access and in terms of how the
world as a whole is impacted by AI

00:50:00.530 --> 00:50:00.540
world as a whole is impacted by AI
 

00:50:00.540 --> 00:50:02.720
world as a whole is impacted by AI
that's the focus of our first session

00:50:02.720 --> 00:50:02.730
that's the focus of our first session
 

00:50:02.730 --> 00:50:04.400
that's the focus of our first session
now each of our sessions is structured

00:50:04.400 --> 00:50:04.410
now each of our sessions is structured
 

00:50:04.410 --> 00:50:05.510
now each of our sessions is structured
differently so we think you're gonna

00:50:05.510 --> 00:50:05.520
differently so we think you're gonna
 

00:50:05.520 --> 00:50:07.190
differently so we think you're gonna
enjoy that there are spotlight speakers

00:50:07.190 --> 00:50:07.200
enjoy that there are spotlight speakers
 

00:50:07.200 --> 00:50:09.170
enjoy that there are spotlight speakers
there are videos that help to set the

00:50:09.170 --> 00:50:09.180
there are videos that help to set the
 

00:50:09.180 --> 00:50:11.300
there are videos that help to set the
stage and there are panel discussions in

00:50:11.300 --> 00:50:11.310
stage and there are panel discussions in
 

00:50:11.310 --> 00:50:13.370
stage and there are panel discussions in
this session we're gonna start with a

00:50:13.370 --> 00:50:13.380
this session we're gonna start with a
 

00:50:13.380 --> 00:50:15.290
this session we're gonna start with a
video and I'm going to ask to Q that

00:50:15.290 --> 00:50:15.300
video and I'm going to ask to Q that
 

00:50:15.300 --> 00:50:16.730
video and I'm going to ask to Q that
right away and then I'll introduce the

00:50:16.730 --> 00:50:16.740
right away and then I'll introduce the
 

00:50:16.740 --> 00:50:18.080
right away and then I'll introduce the
spotlight speaker to get things started

00:50:18.080 --> 00:50:18.090
spotlight speaker to get things started
 

00:50:18.090 --> 00:50:28.950
spotlight speaker to get things started
so let's run with the video please

00:50:28.950 --> 00:50:28.960
 

00:50:28.960 --> 00:50:33.490
we really as engineers need to take

00:50:33.490 --> 00:50:33.500
we really as engineers need to take
 

00:50:33.500 --> 00:50:35.410
we really as engineers need to take
responsibility to work on the things

00:50:35.410 --> 00:50:35.420
responsibility to work on the things
 

00:50:35.420 --> 00:50:37.000
responsibility to work on the things
that will make the world a better place

00:50:37.000 --> 00:50:37.010
that will make the world a better place
 

00:50:37.010 --> 00:50:39.970
that will make the world a better place
taking responsibility having a research

00:50:39.970 --> 00:50:39.980
taking responsibility having a research
 

00:50:39.980 --> 00:50:42.580
taking responsibility having a research
strategy instead of just going wherever

00:50:42.580 --> 00:50:42.590
strategy instead of just going wherever
 

00:50:42.590 --> 00:50:44.980
strategy instead of just going wherever
your curiosity takes you AI and

00:50:44.980 --> 00:50:44.990
your curiosity takes you AI and
 

00:50:44.990 --> 00:50:46.900
your curiosity takes you AI and
optimization have very different

00:50:46.900 --> 00:50:46.910
optimization have very different
 

00:50:46.910 --> 00:50:49.450
optimization have very different
strengths than humans do humans are

00:50:49.450 --> 00:50:49.460
strengths than humans do humans are
 

00:50:49.460 --> 00:50:51.250
strengths than humans do humans are
really good at actually figuring out

00:50:51.250 --> 00:50:51.260
really good at actually figuring out
 

00:50:51.260 --> 00:50:53.740
really good at actually figuring out
what the ins are what the value

00:50:53.740 --> 00:50:53.750
what the ins are what the value
 

00:50:53.750 --> 00:50:55.690
what the ins are what the value
judgments are in general how it should

00:50:55.690 --> 00:50:55.700
judgments are in general how it should
 

00:50:55.700 --> 00:50:57.250
judgments are in general how it should
trade-off between different things like

00:50:57.250 --> 00:50:57.260
trade-off between different things like
 

00:50:57.260 --> 00:50:59.260
trade-off between different things like
various forms of efficiency and various

00:50:59.260 --> 00:50:59.270
various forms of efficiency and various
 

00:50:59.270 --> 00:51:02.680
various forms of efficiency and various
forms of fairness but he must think of

00:51:02.680 --> 00:51:02.690
forms of fairness but he must think of
 

00:51:02.690 --> 00:51:05.650
forms of fairness but he must think of
them in the context of special cases and

00:51:05.650 --> 00:51:05.660
them in the context of special cases and
 

00:51:05.660 --> 00:51:08.440
them in the context of special cases and
humans are very bad at actually sifting

00:51:08.440 --> 00:51:08.450
humans are very bad at actually sifting
 

00:51:08.450 --> 00:51:10.480
humans are very bad at actually sifting
through all of the possible special

00:51:10.480 --> 00:51:10.490
through all of the possible special
 

00:51:10.490 --> 00:51:12.820
through all of the possible special
cases of which there are more than the

00:51:12.820 --> 00:51:12.830
cases of which there are more than the
 

00:51:12.830 --> 00:51:14.710
cases of which there are more than the
number of atoms in the universe we

00:51:14.710 --> 00:51:14.720
number of atoms in the universe we
 

00:51:14.720 --> 00:51:16.360
number of atoms in the universe we
developed this new framework which we

00:51:16.360 --> 00:51:16.370
developed this new framework which we
 

00:51:16.370 --> 00:51:19.120
developed this new framework which we
call future match where we take as input

00:51:19.120 --> 00:51:19.130
call future match where we take as input
 

00:51:19.130 --> 00:51:22.570
call future match where we take as input
to humans value judgments and then using

00:51:22.570 --> 00:51:22.580
to humans value judgments and then using
 

00:51:22.580 --> 00:51:27.760
to humans value judgments and then using
AI based simulation and past data we can

00:51:27.760 --> 00:51:27.770
AI based simulation and past data we can
 

00:51:27.770 --> 00:51:30.070
AI based simulation and past data we can
actually optimize the policy parameters

00:51:30.070 --> 00:51:30.080
actually optimize the policy parameters
 

00:51:30.080 --> 00:51:34.420
actually optimize the policy parameters
as to how do you best achieve those

00:51:34.420 --> 00:51:34.430
as to how do you best achieve those
 

00:51:34.430 --> 00:51:36.580
as to how do you best achieve those
goals so we're separating the means and

00:51:36.580 --> 00:51:36.590
goals so we're separating the means and
 

00:51:36.590 --> 00:51:38.500
goals so we're separating the means and
the ends the humans are talking about

00:51:38.500 --> 00:51:38.510
the ends the humans are talking about
 

00:51:38.510 --> 00:51:41.050
the ends the humans are talking about
ends and the AI is figuring out the

00:51:41.050 --> 00:51:41.060
ends and the AI is figuring out the
 

00:51:41.060 --> 00:51:43.030
ends and the AI is figuring out the
means and I think that this is a very

00:51:43.030 --> 00:51:43.040
means and I think that this is a very
 

00:51:43.040 --> 00:51:47.020
means and I think that this is a very
important separation for the future in

00:51:47.020 --> 00:51:47.030
important separation for the future in
 

00:51:47.030 --> 00:51:52.330
important separation for the future in
many different areas of AI that's almost

00:51:52.330 --> 00:51:52.340
many different areas of AI that's almost
 

00:51:52.340 --> 00:51:53.800
many different areas of AI that's almost
sent home our colleague right here at

00:51:53.800 --> 00:51:53.810
sent home our colleague right here at
 

00:51:53.810 --> 00:51:55.030
sent home our colleague right here at
Carnegie Mellon University and the

00:51:55.030 --> 00:51:55.040
Carnegie Mellon University and the
 

00:51:55.040 --> 00:51:56.950
Carnegie Mellon University and the
school of computer science now I'm very

00:51:56.950 --> 00:51:56.960
school of computer science now I'm very
 

00:51:56.960 --> 00:51:58.750
school of computer science now I'm very
pleased to introduce our first speaker

00:51:58.750 --> 00:51:58.760
pleased to introduce our first speaker
 

00:51:58.760 --> 00:52:00.730
pleased to introduce our first speaker
he's gonna set the stage for about 20

00:52:00.730 --> 00:52:00.740
he's gonna set the stage for about 20
 

00:52:00.740 --> 00:52:01.690
he's gonna set the stage for about 20
minutes and then we're gonna have a

00:52:01.690 --> 00:52:01.700
minutes and then we're gonna have a
 

00:52:01.700 --> 00:52:03.130
minutes and then we're gonna have a
little discussion the comfortable

00:52:03.130 --> 00:52:03.140
little discussion the comfortable
 

00:52:03.140 --> 00:52:05.230
little discussion the comfortable
armchairs in the middle our first

00:52:05.230 --> 00:52:05.240
armchairs in the middle our first
 

00:52:05.240 --> 00:52:08.140
armchairs in the middle our first
speaker is Sean dae-ho Shobha and I'm

00:52:08.140 --> 00:52:08.150
speaker is Sean dae-ho Shobha and I'm
 

00:52:08.150 --> 00:52:09.340
speaker is Sean dae-ho Shobha and I'm
happy to welcome him to the stage

00:52:09.340 --> 00:52:09.350
happy to welcome him to the stage
 

00:52:09.350 --> 00:52:11.260
happy to welcome him to the stage
what Shonda is an engineer at RAND

00:52:11.260 --> 00:52:11.270
what Shonda is an engineer at RAND
 

00:52:11.270 --> 00:52:12.760
what Shonda is an engineer at RAND
Corporation and he's a professor at the

00:52:12.760 --> 00:52:12.770
Corporation and he's a professor at the
 

00:52:12.770 --> 00:52:15.280
Corporation and he's a professor at the
party ran school his background is

00:52:15.280 --> 00:52:15.290
party ran school his background is
 

00:52:15.290 --> 00:52:16.840
party ran school his background is
perfect for this his background is in

00:52:16.840 --> 00:52:16.850
perfect for this his background is in
 

00:52:16.850 --> 00:52:18.760
perfect for this his background is in
machine learning optimization control

00:52:18.760 --> 00:52:18.770
machine learning optimization control
 

00:52:18.770 --> 00:52:21.460
machine learning optimization control
but he's also branched from machine

00:52:21.460 --> 00:52:21.470
but he's also branched from machine
 

00:52:21.470 --> 00:52:23.140
but he's also branched from machine
learning into health defense and

00:52:23.140 --> 00:52:23.150
learning into health defense and
 

00:52:23.150 --> 00:52:25.990
learning into health defense and
Technology Policy so from there he's

00:52:25.990 --> 00:52:26.000
Technology Policy so from there he's
 

00:52:26.000 --> 00:52:27.730
Technology Policy so from there he's
going to data privacy and accountability

00:52:27.730 --> 00:52:27.740
going to data privacy and accountability
 

00:52:27.740 --> 00:52:30.010
going to data privacy and accountability
so you notice I mentioned pretty much

00:52:30.010 --> 00:52:30.020
so you notice I mentioned pretty much
 

00:52:30.020 --> 00:52:32.230
so you notice I mentioned pretty much
all the key words as we look at the way

00:52:32.230 --> 00:52:32.240
all the key words as we look at the way
 

00:52:32.240 --> 00:52:34.900
all the key words as we look at the way
AI integrates into society before

00:52:34.900 --> 00:52:34.910
AI integrates into society before
 

00:52:34.910 --> 00:52:37.000
AI integrates into society before
joining Rand or Sunday was at USC and

00:52:37.000 --> 00:52:37.010
joining Rand or Sunday was at USC and
 

00:52:37.010 --> 00:52:38.980
joining Rand or Sunday was at USC and
indeed got his PhD there in electrical

00:52:38.980 --> 00:52:38.990
indeed got his PhD there in electrical
 

00:52:38.990 --> 00:52:39.700
indeed got his PhD there in electrical
engineering

00:52:39.700 --> 00:52:39.710
engineering
 

00:52:39.710 --> 00:52:41.049
engineering
and without further ado welcome ocean

00:52:41.049 --> 00:52:41.059
and without further ado welcome ocean
 

00:52:41.059 --> 00:52:50.140
and without further ado welcome ocean
day so today we're going to be talking

00:52:50.140 --> 00:52:50.150
day so today we're going to be talking
 

00:52:50.150 --> 00:52:51.790
day so today we're going to be talking
about equity of access and equity of

00:52:51.790 --> 00:52:51.800
about equity of access and equity of
 

00:52:51.800 --> 00:52:54.270
about equity of access and equity of
impact and how artificial intelligence

00:52:54.270 --> 00:52:54.280
impact and how artificial intelligence
 

00:52:54.280 --> 00:52:57.250
impact and how artificial intelligence
mediates transforms affects those two

00:52:57.250 --> 00:52:57.260
mediates transforms affects those two
 

00:52:57.260 --> 00:53:02.349
mediates transforms affects those two
things I will be suggesting that we look

00:53:02.349 --> 00:53:02.359
things I will be suggesting that we look
 

00:53:02.359 --> 00:53:05.260
things I will be suggesting that we look
to the Past look to antiquity to sort of

00:53:05.260 --> 00:53:05.270
to the Past look to antiquity to sort of
 

00:53:05.270 --> 00:53:07.420
to the Past look to antiquity to sort of
help us see what's going on in the

00:53:07.420 --> 00:53:07.430
help us see what's going on in the
 

00:53:07.430 --> 00:53:10.660
help us see what's going on in the
future I want us to focus on these two

00:53:10.660 --> 00:53:10.670
future I want us to focus on these two
 

00:53:10.670 --> 00:53:15.670
future I want us to focus on these two
individuals Plato and Socrates my

00:53:15.670 --> 00:53:15.680
individuals Plato and Socrates my
 

00:53:15.680 --> 00:53:17.980
individuals Plato and Socrates my
contention is that the interaction the

00:53:17.980 --> 00:53:17.990
contention is that the interaction the
 

00:53:17.990 --> 00:53:19.210
contention is that the interaction the
difference is in the way these two

00:53:19.210 --> 00:53:19.220
difference is in the way these two
 

00:53:19.220 --> 00:53:21.730
difference is in the way these two
people work kind of sort of pre figures

00:53:21.730 --> 00:53:21.740
people work kind of sort of pre figures
 

00:53:21.740 --> 00:53:24.400
people work kind of sort of pre figures
are complexities the complexity of

00:53:24.400 --> 00:53:24.410
are complexities the complexity of
 

00:53:24.410 --> 00:53:26.349
are complexities the complexity of
dealing with today when it comes to AI

00:53:26.349 --> 00:53:26.359
dealing with today when it comes to AI
 

00:53:26.359 --> 00:53:30.220
dealing with today when it comes to AI
and equity if there is a key takeaway

00:53:30.220 --> 00:53:30.230
and equity if there is a key takeaway
 

00:53:30.230 --> 00:53:32.799
and equity if there is a key takeaway
point from from this conversation I hope

00:53:32.799 --> 00:53:32.809
point from from this conversation I hope
 

00:53:32.809 --> 00:53:35.349
point from from this conversation I hope
it would be the idea of participatory

00:53:35.349 --> 00:53:35.359
it would be the idea of participatory
 

00:53:35.359 --> 00:53:38.020
it would be the idea of participatory
equity I will be making the contention

00:53:38.020 --> 00:53:38.030
equity I will be making the contention
 

00:53:38.030 --> 00:53:39.549
equity I will be making the contention
that that should be the foundation of

00:53:39.549 --> 00:53:39.559
that that should be the foundation of
 

00:53:39.559 --> 00:53:41.950
that that should be the foundation of
any successful infrastructure for

00:53:41.950 --> 00:53:41.960
any successful infrastructure for
 

00:53:41.960 --> 00:53:46.620
any successful infrastructure for
ethically aligned what fish or fair AI

00:53:46.620 --> 00:53:46.630
ethically aligned what fish or fair AI
 

00:53:46.630 --> 00:53:49.210
ethically aligned what fish or fair AI
now that I have given you the key point

00:53:49.210 --> 00:53:49.220
now that I have given you the key point
 

00:53:49.220 --> 00:53:52.750
now that I have given you the key point
we can all take naps basically back to

00:53:52.750 --> 00:53:52.760
we can all take naps basically back to
 

00:53:52.760 --> 00:53:55.750
we can all take naps basically back to
Plato so the mental image I have a Plato

00:53:55.750 --> 00:53:55.760
Plato so the mental image I have a Plato
 

00:53:55.760 --> 00:53:59.109
Plato so the mental image I have a Plato
is of this undoubted genius but he's

00:53:59.109 --> 00:53:59.119
is of this undoubted genius but he's
 

00:53:59.119 --> 00:54:02.470
is of this undoubted genius but he's
also this guy who's deeply deeply

00:54:02.470 --> 00:54:02.480
also this guy who's deeply deeply
 

00:54:02.480 --> 00:54:04.539
also this guy who's deeply deeply
scarred by one pivotal event in his life

00:54:04.539 --> 00:54:04.549
scarred by one pivotal event in his life
 

00:54:04.549 --> 00:54:07.210
scarred by one pivotal event in his life
and that's the state sanctioned

00:54:07.210 --> 00:54:07.220
and that's the state sanctioned
 

00:54:07.220 --> 00:54:09.490
and that's the state sanctioned
execution of his teacher and mentor of

00:54:09.490 --> 00:54:09.500
execution of his teacher and mentor of
 

00:54:09.500 --> 00:54:14.549
execution of his teacher and mentor of
Socrates Socrates Plato reacts to that

00:54:14.549 --> 00:54:14.559
Socrates Socrates Plato reacts to that
 

00:54:14.559 --> 00:54:17.799
Socrates Socrates Plato reacts to that
event by retreating from the marketplace

00:54:17.799 --> 00:54:17.809
event by retreating from the marketplace
 

00:54:17.809 --> 00:54:20.319
event by retreating from the marketplace
of ideas from the from the population

00:54:20.319 --> 00:54:20.329
of ideas from the from the population
 

00:54:20.329 --> 00:54:22.589
of ideas from the from the population
from the community and he sets up this

00:54:22.589 --> 00:54:22.599
from the community and he sets up this
 

00:54:22.599 --> 00:54:27.280
from the community and he sets up this
he sets up this archetype for the

00:54:27.280 --> 00:54:27.290
he sets up this archetype for the
 

00:54:27.290 --> 00:54:30.130
he sets up this archetype for the
solitary contemplative philosopher and

00:54:30.130 --> 00:54:30.140
solitary contemplative philosopher and
 

00:54:30.140 --> 00:54:34.020
solitary contemplative philosopher and
he he desires you to peers in which

00:54:34.020 --> 00:54:34.030
he he desires you to peers in which
 

00:54:34.030 --> 00:54:37.660
he he desires you to peers in which
basically the demos the people that he

00:54:37.660 --> 00:54:37.670
basically the demos the people that he
 

00:54:37.670 --> 00:54:41.230
basically the demos the people that he
that he kind of pulls back from sort of

00:54:41.230 --> 00:54:41.240
that he kind of pulls back from sort of
 

00:54:41.240 --> 00:54:44.650
that he kind of pulls back from sort of
always subject to some philosopher king

00:54:44.650 --> 00:54:44.660
always subject to some philosopher king
 

00:54:44.660 --> 00:54:46.660
always subject to some philosopher king
and that is the idea what a fair society

00:54:46.660 --> 00:54:46.670
and that is the idea what a fair society
 

00:54:46.670 --> 00:54:48.640
and that is the idea what a fair society
is and it does is without engaging with

00:54:48.640 --> 00:54:48.650
is and it does is without engaging with
 

00:54:48.650 --> 00:54:50.230
is and it does is without engaging with
the people at all he just designed this

00:54:50.230 --> 00:54:50.240
the people at all he just designed this
 

00:54:50.240 --> 00:54:51.040
the people at all he just designed this
from

00:54:51.040 --> 00:54:51.050
from
 

00:54:51.050 --> 00:54:53.890
from
now contrast this to how Socrates does

00:54:53.890 --> 00:54:53.900
now contrast this to how Socrates does
 

00:54:53.900 --> 00:54:58.420
now contrast this to how Socrates does
inquiry he engages deeply and very very

00:54:58.420 --> 00:54:58.430
inquiry he engages deeply and very very
 

00:54:58.430 --> 00:55:00.580
inquiry he engages deeply and very very
critically with the community he's

00:55:00.580 --> 00:55:00.590
critically with the community he's
 

00:55:00.590 --> 00:55:02.740
critically with the community he's
engaged he's around when he asked when

00:55:02.740 --> 00:55:02.750
engaged he's around when he asked when
 

00:55:02.750 --> 00:55:03.730
engaged he's around when he asked when
he has to answer the simple question

00:55:03.730 --> 00:55:03.740
he has to answer the simple question
 

00:55:03.740 --> 00:55:08.200
he has to answer the simple question
what is wisdom now he doesn't get his

00:55:08.200 --> 00:55:08.210
what is wisdom now he doesn't get his
 

00:55:08.210 --> 00:55:09.940
what is wisdom now he doesn't get his
answer he doesn't get a definite I'm

00:55:09.940 --> 00:55:09.950
answer he doesn't get a definite I'm
 

00:55:09.950 --> 00:55:11.590
answer he doesn't get a definite I'm
definitely to answer from that question

00:55:11.590 --> 00:55:11.600
definitely to answer from that question
 

00:55:11.600 --> 00:55:13.240
definitely to answer from that question
from an interaction but at least he can

00:55:13.240 --> 00:55:13.250
from an interaction but at least he can
 

00:55:13.250 --> 00:55:16.510
from an interaction but at least he can
claim some defense against the biases

00:55:16.510 --> 00:55:16.520
claim some defense against the biases
 

00:55:16.520 --> 00:55:18.640
claim some defense against the biases
that his personal perspective

00:55:18.640 --> 00:55:18.650
that his personal perspective
 

00:55:18.650 --> 00:55:21.600
that his personal perspective
necessarily brings to that conversation

00:55:21.600 --> 00:55:21.610
necessarily brings to that conversation
 

00:55:21.610 --> 00:55:24.850
necessarily brings to that conversation
think about how these two men might have

00:55:24.850 --> 00:55:24.860
think about how these two men might have
 

00:55:24.860 --> 00:55:26.440
think about how these two men might have
thought about the questions that lay

00:55:26.440 --> 00:55:26.450
thought about the questions that lay
 

00:55:26.450 --> 00:55:27.430
thought about the questions that lay
before us today

00:55:27.430 --> 00:55:27.440
before us today
 

00:55:27.440 --> 00:55:30.250
before us today
the idea of equity what is equity and

00:55:30.250 --> 00:55:30.260
the idea of equity what is equity and
 

00:55:30.260 --> 00:55:32.110
the idea of equity what is equity and
how do our giri's make decision-making

00:55:32.110 --> 00:55:32.120
how do our giri's make decision-making
 

00:55:32.120 --> 00:55:37.060
how do our giri's make decision-making
artifacts interact with equity Aristotle

00:55:37.060 --> 00:55:37.070
artifacts interact with equity Aristotle
 

00:55:37.070 --> 00:55:39.520
artifacts interact with equity Aristotle
makes an appearance in discussions of

00:55:39.520 --> 00:55:39.530
makes an appearance in discussions of
 

00:55:39.530 --> 00:55:41.290
makes an appearance in discussions of
equity also but we won't focus on him

00:55:41.290 --> 00:55:41.300
equity also but we won't focus on him
 

00:55:41.300 --> 00:55:45.850
equity also but we won't focus on him
today back in the day a few months ago I

00:55:45.850 --> 00:55:45.860
today back in the day a few months ago I
 

00:55:45.860 --> 00:55:48.100
today back in the day a few months ago I
was introduced as done as an engineer of

00:55:48.100 --> 00:55:48.110
was introduced as done as an engineer of
 

00:55:48.110 --> 00:55:51.190
was introduced as done as an engineer of
fairness that's a little bit of a

00:55:51.190 --> 00:55:51.200
fairness that's a little bit of a
 

00:55:51.200 --> 00:55:55.050
fairness that's a little bit of a
paradoxical title engineers have this

00:55:55.050 --> 00:55:55.060
paradoxical title engineers have this
 

00:55:55.060 --> 00:55:58.480
paradoxical title engineers have this
image of being objective precise

00:55:58.480 --> 00:55:58.490
image of being objective precise
 

00:55:58.490 --> 00:56:00.760
image of being objective precise
analysts while fairness is this very

00:56:00.760 --> 00:56:00.770
analysts while fairness is this very
 

00:56:00.770 --> 00:56:03.160
analysts while fairness is this very
contextual very socially defined very

00:56:03.160 --> 00:56:03.170
contextual very socially defined very
 

00:56:03.170 --> 00:56:06.540
contextual very socially defined very
fuzzy concept that slippery to define I

00:56:06.540 --> 00:56:06.550
fuzzy concept that slippery to define I
 

00:56:06.550 --> 00:56:10.270
fuzzy concept that slippery to define I
argue that any any discussion of

00:56:10.270 --> 00:56:10.280
argue that any any discussion of
 

00:56:10.280 --> 00:56:11.680
argue that any any discussion of
artificial intelligence and equity is

00:56:11.680 --> 00:56:11.690
artificial intelligence and equity is
 

00:56:11.690 --> 00:56:13.900
artificial intelligence and equity is
going to be doomed to this compress of

00:56:13.900 --> 00:56:13.910
going to be doomed to this compress of
 

00:56:13.910 --> 00:56:17.080
going to be doomed to this compress of
opposite qualities it used to be that

00:56:17.080 --> 00:56:17.090
opposite qualities it used to be that
 

00:56:17.090 --> 00:56:19.660
opposite qualities it used to be that
when we talked about AI equity people

00:56:19.660 --> 00:56:19.670
when we talked about AI equity people
 

00:56:19.670 --> 00:56:22.270
when we talked about AI equity people
sneer at us they'll say things like well

00:56:22.270 --> 00:56:22.280
sneer at us they'll say things like well
 

00:56:22.280 --> 00:56:23.980
sneer at us they'll say things like well
what is equity in the same way that

00:56:23.980 --> 00:56:23.990
what is equity in the same way that
 

00:56:23.990 --> 00:56:26.440
what is equity in the same way that
Pontius Pilate probably asked what is

00:56:26.440 --> 00:56:26.450
Pontius Pilate probably asked what is
 

00:56:26.450 --> 00:56:31.720
Pontius Pilate probably asked what is
truth and typical conversations about AI

00:56:31.720 --> 00:56:31.730
truth and typical conversations about AI
 

00:56:31.730 --> 00:56:33.520
truth and typical conversations about AI
policies they were focused on these

00:56:33.520 --> 00:56:33.530
policies they were focused on these
 

00:56:33.530 --> 00:56:36.490
policies they were focused on these
future looking scenarios this Terminator

00:56:36.490 --> 00:56:36.500
future looking scenarios this Terminator
 

00:56:36.500 --> 00:56:38.970
future looking scenarios this Terminator
scenarios these are existential crises

00:56:38.970 --> 00:56:38.980
scenarios these are existential crises
 

00:56:38.980 --> 00:56:41.800
scenarios these are existential crises
and I would argue that that that focus

00:56:41.800 --> 00:56:41.810
and I would argue that that that focus
 

00:56:41.810 --> 00:56:44.440
and I would argue that that that focus
that's a product of a privileged

00:56:44.440 --> 00:56:44.450
that's a product of a privileged
 

00:56:44.450 --> 00:56:46.480
that's a product of a privileged
perspective a perspective they cannot

00:56:46.480 --> 00:56:46.490
perspective a perspective they cannot
 

00:56:46.490 --> 00:56:50.170
perspective a perspective they cannot
imagine the problems of AI affecting us

00:56:50.170 --> 00:56:50.180
imagine the problems of AI affecting us
 

00:56:50.180 --> 00:56:51.580
imagine the problems of AI affecting us
today they have to look to the future to

00:56:51.580 --> 00:56:51.590
today they have to look to the future to
 

00:56:51.590 --> 00:56:54.010
today they have to look to the future to
see to imagine things that go wrong now

00:56:54.010 --> 00:56:54.020
see to imagine things that go wrong now
 

00:56:54.020 --> 00:56:55.990
see to imagine things that go wrong now
that is probably a lazy argument so I

00:56:55.990 --> 00:56:56.000
that is probably a lazy argument so I
 

00:56:56.000 --> 00:56:58.930
that is probably a lazy argument so I
offer a reconciliation so here we have

00:56:58.930 --> 00:56:58.940
offer a reconciliation so here we have
 

00:56:58.940 --> 00:56:59.950
offer a reconciliation so here we have
parking

00:56:59.950 --> 00:56:59.960
parking
 

00:56:59.960 --> 00:57:02.560
parking
along pushing that line of inquiry let

00:57:02.560 --> 00:57:02.570
along pushing that line of inquiry let
 

00:57:02.570 --> 00:57:04.120
along pushing that line of inquiry let
me offer a reconciliation by observing

00:57:04.120 --> 00:57:04.130
me offer a reconciliation by observing
 

00:57:04.130 --> 00:57:07.690
me offer a reconciliation by observing
that questions of AI equity and AI

00:57:07.690 --> 00:57:07.700
that questions of AI equity and AI
 

00:57:07.700 --> 00:57:09.940
that questions of AI equity and AI
safety existential safety concerns they

00:57:09.940 --> 00:57:09.950
safety existential safety concerns they
 

00:57:09.950 --> 00:57:14.050
safety existential safety concerns they
actually share common wheat for example

00:57:14.050 --> 00:57:14.060
actually share common wheat for example
 

00:57:14.060 --> 00:57:16.089
actually share common wheat for example
they are concerned with value alignment

00:57:16.089 --> 00:57:16.099
they are concerned with value alignment
 

00:57:16.099 --> 00:57:18.040
they are concerned with value alignment
by value alignment I mean how do we

00:57:18.040 --> 00:57:18.050
by value alignment I mean how do we
 

00:57:18.050 --> 00:57:20.800
by value alignment I mean how do we
design algorithms that that support

00:57:20.800 --> 00:57:20.810
design algorithms that that support
 

00:57:20.810 --> 00:57:23.980
design algorithms that that support
society defined norms they also share

00:57:23.980 --> 00:57:23.990
society defined norms they also share
 

00:57:23.990 --> 00:57:26.320
society defined norms they also share
the problem of explained ability in the

00:57:26.320 --> 00:57:26.330
the problem of explained ability in the
 

00:57:26.330 --> 00:57:29.500
the problem of explained ability in the
in the interest of X of procedural

00:57:29.500 --> 00:57:29.510
in the interest of X of procedural
 

00:57:29.510 --> 00:57:32.620
in the interest of X of procedural
transparency we would like to be able to

00:57:32.620 --> 00:57:32.630
transparency we would like to be able to
 

00:57:32.630 --> 00:57:35.020
transparency we would like to be able to
derive stable robust meaningful

00:57:35.020 --> 00:57:35.030
derive stable robust meaningful
 

00:57:35.030 --> 00:57:36.760
derive stable robust meaningful
explanations from the algorithmic

00:57:36.760 --> 00:57:36.770
explanations from the algorithmic
 

00:57:36.770 --> 00:57:40.180
explanations from the algorithmic
decision making artifacts but let's

00:57:40.180 --> 00:57:40.190
decision making artifacts but let's
 

00:57:40.190 --> 00:57:42.970
decision making artifacts but let's
focus on equity when you focus on equity

00:57:42.970 --> 00:57:42.980
focus on equity when you focus on equity
 

00:57:42.980 --> 00:57:48.900
focus on equity when you focus on equity
there are there are there are these

00:57:48.900 --> 00:57:48.910
there are there are there are these
 

00:57:48.910 --> 00:57:51.490
there are there are there are these
definitional problems there are these

00:57:51.490 --> 00:57:51.500
definitional problems there are these
 

00:57:51.500 --> 00:57:53.620
definitional problems there are these
issues these are perceived that picture

00:57:53.620 --> 00:57:53.630
issues these are perceived that picture
 

00:57:53.630 --> 00:57:56.260
issues these are perceived that picture
in the definition of equity and it's a

00:57:56.260 --> 00:57:56.270
in the definition of equity and it's a
 

00:57:56.270 --> 00:57:58.150
in the definition of equity and it's a
great thing that we now have examples

00:57:58.150 --> 00:57:58.160
great thing that we now have examples
 

00:57:58.160 --> 00:58:01.060
great thing that we now have examples
that in spite of the arbitrary in spite

00:58:01.060 --> 00:58:01.070
that in spite of the arbitrary in spite
 

00:58:01.070 --> 00:58:02.560
that in spite of the arbitrary in spite
of definition of problems they're able

00:58:02.560 --> 00:58:02.570
of definition of problems they're able
 

00:58:02.570 --> 00:58:05.370
of definition of problems they're able
to demonstrate examples of algorithms

00:58:05.370 --> 00:58:05.380
to demonstrate examples of algorithms
 

00:58:05.380 --> 00:58:08.339
to demonstrate examples of algorithms
violating socially defined equity norms

00:58:08.339 --> 00:58:08.349
violating socially defined equity norms
 

00:58:08.349 --> 00:58:12.359
violating socially defined equity norms
these examples have done as deep and and

00:58:12.359 --> 00:58:12.369
these examples have done as deep and and
 

00:58:12.369 --> 00:58:15.550
these examples have done as deep and and
huge service of kind of sort of

00:58:15.550 --> 00:58:15.560
huge service of kind of sort of
 

00:58:15.560 --> 00:58:18.940
huge service of kind of sort of
challenging that deep slumber of settle

00:58:18.940 --> 00:58:18.950
challenging that deep slumber of settle
 

00:58:18.950 --> 00:58:21.099
challenging that deep slumber of settle
opinion that algorithms are necessarily

00:58:21.099 --> 00:58:21.109
opinion that algorithms are necessarily
 

00:58:21.109 --> 00:58:23.290
opinion that algorithms are necessarily
fair and infallible in fact we have

00:58:23.290 --> 00:58:23.300
fair and infallible in fact we have
 

00:58:23.300 --> 00:58:25.690
fair and infallible in fact we have
researchers now saying that any any

00:58:25.690 --> 00:58:25.700
researchers now saying that any any
 

00:58:25.700 --> 00:58:28.240
researchers now saying that any any
technological design necessarily embed

00:58:28.240 --> 00:58:28.250
technological design necessarily embed
 

00:58:28.250 --> 00:58:30.820
technological design necessarily embed
ethical decisions necessarily embed

00:58:30.820 --> 00:58:30.830
ethical decisions necessarily embed
 

00:58:30.830 --> 00:58:33.270
ethical decisions necessarily embed
values either implicitly or explicitly

00:58:33.270 --> 00:58:33.280
values either implicitly or explicitly
 

00:58:33.280 --> 00:58:36.940
values either implicitly or explicitly
when we talk about AI equity with those

00:58:36.940 --> 00:58:36.950
when we talk about AI equity with those
 

00:58:36.950 --> 00:58:39.130
when we talk about AI equity with those
conversations turn on how those values

00:58:39.130 --> 00:58:39.140
conversations turn on how those values
 

00:58:39.140 --> 00:58:41.770
conversations turn on how those values
align with society defined expectations

00:58:41.770 --> 00:58:41.780
align with society defined expectations
 

00:58:41.780 --> 00:58:47.710
align with society defined expectations
of fairness the first is a street bump

00:58:47.710 --> 00:58:47.720
of fairness the first is a street bump
 

00:58:47.720 --> 00:58:49.390
of fairness the first is a street bump
example the second is a compensation

00:58:49.390 --> 00:58:49.400
example the second is a compensation
 

00:58:49.400 --> 00:58:51.010
example the second is a compensation
division and the third is insurance

00:58:51.010 --> 00:58:51.020
division and the third is insurance
 

00:58:51.020 --> 00:58:53.440
division and the third is insurance
price in the last two examples were sort

00:58:53.440 --> 00:58:53.450
price in the last two examples were sort
 

00:58:53.450 --> 00:58:55.630
price in the last two examples were sort
of where the result of ProPublica is

00:58:55.630 --> 00:58:55.640
of where the result of ProPublica is
 

00:58:55.640 --> 00:58:59.710
of where the result of ProPublica is
explorations let's try to examine how

00:58:59.710 --> 00:58:59.720
explorations let's try to examine how
 

00:58:59.720 --> 00:59:02.140
explorations let's try to examine how
bias is my seep into our supposedly

00:59:02.140 --> 00:59:02.150
bias is my seep into our supposedly
 

00:59:02.150 --> 00:59:06.099
bias is my seep into our supposedly
objective infallible algorithms we are

00:59:06.099 --> 00:59:06.109
objective infallible algorithms we are
 

00:59:06.109 --> 00:59:08.079
objective infallible algorithms we are
probably familiar with how a I systems

00:59:08.079 --> 00:59:08.089
probably familiar with how a I systems
 

00:59:08.089 --> 00:59:10.240
probably familiar with how a I systems
are designed this is really design phase

00:59:10.240 --> 00:59:10.250
are designed this is really design phase
 

00:59:10.250 --> 00:59:11.079
are designed this is really design phase
a programming

00:59:11.079 --> 00:59:11.089
a programming
 

00:59:11.089 --> 00:59:13.209
a programming
is a training phase and a testing phase

00:59:13.209 --> 00:59:13.219
is a training phase and a testing phase
 

00:59:13.219 --> 00:59:15.759
is a training phase and a testing phase
but right here in the design of an

00:59:15.759 --> 00:59:15.769
but right here in the design of an
 

00:59:15.769 --> 00:59:17.769
but right here in the design of an
algorithmic artifact to make a decision

00:59:17.769 --> 00:59:17.779
algorithmic artifact to make a decision
 

00:59:17.779 --> 00:59:19.900
algorithmic artifact to make a decision
we have situations where we have framing

00:59:19.900 --> 00:59:19.910
we have situations where we have framing
 

00:59:19.910 --> 00:59:22.450
we have situations where we have framing
artifacts framing effects when you

00:59:22.450 --> 00:59:22.460
artifacts framing effects when you
 

00:59:22.460 --> 00:59:24.400
artifacts framing effects when you
design a system to make decisions and

00:59:24.400 --> 00:59:24.410
design a system to make decisions and
 

00:59:24.410 --> 00:59:26.019
design a system to make decisions and
you focus any frame it as an

00:59:26.019 --> 00:59:26.029
you focus any frame it as an
 

00:59:26.029 --> 00:59:28.749
you focus any frame it as an
optimization on predictive accuracy it

00:59:28.749 --> 00:59:28.759
optimization on predictive accuracy it
 

00:59:28.759 --> 00:59:30.579
optimization on predictive accuracy it
shouldn't be surprising when you have

00:59:30.579 --> 00:59:30.589
shouldn't be surprising when you have
 

00:59:30.589 --> 00:59:32.650
shouldn't be surprising when you have
fairness fairness problems with that

00:59:32.650 --> 00:59:32.660
fairness fairness problems with that
 

00:59:32.660 --> 00:59:34.499
fairness fairness problems with that
with that design

00:59:34.499 --> 00:59:34.509
with that design
 

00:59:34.509 --> 00:59:37.209
with that design
besides the the actual framing effects

00:59:37.209 --> 00:59:37.219
besides the the actual framing effects
 

00:59:37.219 --> 00:59:39.489
besides the the actual framing effects
in the design you have inputs that are

00:59:39.489 --> 00:59:39.499
in the design you have inputs that are
 

00:59:39.499 --> 00:59:41.890
in the design you have inputs that are
used to design the systems these usually

00:59:41.890 --> 00:59:41.900
used to design the systems these usually
 

00:59:41.900 --> 00:59:44.380
used to design the systems these usually
consist of data and assumptions at least

00:59:44.380 --> 00:59:44.390
consist of data and assumptions at least
 

00:59:44.390 --> 00:59:45.670
consist of data and assumptions at least
in models that is to come machine

00:59:45.670 --> 00:59:45.680
in models that is to come machine
 

00:59:45.680 --> 00:59:48.640
in models that is to come machine
learning models you know all the AI

00:59:48.640 --> 00:59:48.650
learning models you know all the AI
 

00:59:48.650 --> 00:59:50.920
learning models you know all the AI
models they relied more on rules and

00:59:50.920 --> 00:59:50.930
models they relied more on rules and
 

00:59:50.930 --> 00:59:52.719
models they relied more on rules and
assumptions explicit rules and

00:59:52.719 --> 00:59:52.729
assumptions explicit rules and
 

00:59:52.729 --> 00:59:55.569
assumptions explicit rules and
assumptions but even in these inputs you

00:59:55.569 --> 00:59:55.579
assumptions but even in these inputs you
 

00:59:55.579 --> 00:59:57.670
assumptions but even in these inputs you
have this problem and I'm using terms

00:59:57.670 --> 00:59:57.680
have this problem and I'm using terms
 

00:59:57.680 --> 00:59:59.739
have this problem and I'm using terms
from behavioral economics to help us

00:59:59.739 --> 00:59:59.749
from behavioral economics to help us
 

00:59:59.749 --> 01:00:02.019
from behavioral economics to help us
maybe frame this better we have this

01:00:02.019 --> 01:00:02.029
maybe frame this better we have this
 

01:00:02.029 --> 01:00:04.479
maybe frame this better we have this
Pirlo of naive realism where the model

01:00:04.479 --> 01:00:04.489
Pirlo of naive realism where the model
 

01:00:04.489 --> 01:00:07.329
Pirlo of naive realism where the model
assumes that the data it give it's it's

01:00:07.329 --> 01:00:07.339
assumes that the data it give it's it's
 

01:00:07.339 --> 01:00:10.059
assumes that the data it give it's it's
able to see is a complete contextual

01:00:10.059 --> 01:00:10.069
able to see is a complete contextual
 

01:00:10.069 --> 01:00:12.700
able to see is a complete contextual
representation of the real world so you

01:00:12.700 --> 01:00:12.710
representation of the real world so you
 

01:00:12.710 --> 01:00:16.630
representation of the real world so you
can imagine a model trained on a subset

01:00:16.630 --> 01:00:16.640
can imagine a model trained on a subset
 

01:00:16.640 --> 01:00:18.430
can imagine a model trained on a subset
of the population assuming that the

01:00:18.430 --> 01:00:18.440
of the population assuming that the
 

01:00:18.440 --> 01:00:19.749
of the population assuming that the
general population to which to going to

01:00:19.749 --> 01:00:19.759
general population to which to going to
 

01:00:19.759 --> 01:00:21.910
general population to which to going to
be applied looks like that training set

01:00:21.910 --> 01:00:21.920
be applied looks like that training set
 

01:00:21.920 --> 01:00:24.309
be applied looks like that training set
that's a form of naive realism or you

01:00:24.309 --> 01:00:24.319
that's a form of naive realism or you
 

01:00:24.319 --> 01:00:26.920
that's a form of naive realism or you
can imagine an algorithm trying to make

01:00:26.920 --> 01:00:26.930
can imagine an algorithm trying to make
 

01:00:26.930 --> 01:00:29.380
can imagine an algorithm trying to make
inferences about people's networks based

01:00:29.380 --> 01:00:29.390
inferences about people's networks based
 

01:00:29.390 --> 01:00:32.380
inferences about people's networks based
on just social media links assuming that

01:00:32.380 --> 01:00:32.390
on just social media links assuming that
 

01:00:32.390 --> 01:00:34.329
on just social media links assuming that
social media links represent the full

01:00:34.329 --> 01:00:34.339
social media links represent the full
 

01:00:34.339 --> 01:00:35.799
social media links represent the full
state of connections in the real world

01:00:35.799 --> 01:00:35.809
state of connections in the real world
 

01:00:35.809 --> 01:00:38.319
state of connections in the real world
that's a form of naive realism we also

01:00:38.319 --> 01:00:38.329
that's a form of naive realism we also
 

01:00:38.329 --> 01:00:40.109
that's a form of naive realism we also
have this issue of anchoring biases

01:00:40.109 --> 01:00:40.119
have this issue of anchoring biases
 

01:00:40.119 --> 01:00:42.039
have this issue of anchoring biases
statistical models they tend to

01:00:42.039 --> 01:00:42.049
statistical models they tend to
 

01:00:42.049 --> 01:00:44.650
statistical models they tend to
basically anchor on examples they've

01:00:44.650 --> 01:00:44.660
basically anchor on examples they've
 

01:00:44.660 --> 01:00:46.509
basically anchor on examples they've
seen in the past this is an example of a

01:00:46.509 --> 01:00:46.519
seen in the past this is an example of a
 

01:00:46.519 --> 01:00:48.940
seen in the past this is an example of a
generalization error problem and so by

01:00:48.940 --> 01:00:48.950
generalization error problem and so by
 

01:00:48.950 --> 01:00:50.499
generalization error problem and so by
anchoring on those examples in the

01:00:50.499 --> 01:00:50.509
anchoring on those examples in the
 

01:00:50.509 --> 01:00:51.819
anchoring on those examples in the
future when it's presented with things

01:00:51.819 --> 01:00:51.829
future when it's presented with things
 

01:00:51.829 --> 01:00:54.339
future when it's presented with things
it's not familiar with you have these

01:00:54.339 --> 01:00:54.349
it's not familiar with you have these
 

01:00:54.349 --> 01:00:57.099
it's not familiar with you have these
anchors that can sort of help sometimes

01:00:57.099 --> 01:00:57.109
anchors that can sort of help sometimes
 

01:00:57.109 --> 01:00:58.859
anchors that can sort of help sometimes
they help sometimes they're not so good

01:00:58.859 --> 01:00:58.869
they help sometimes they're not so good
 

01:00:58.869 --> 01:01:01.799
they help sometimes they're not so good
the last part where biases can seep in

01:01:01.799 --> 01:01:01.809
the last part where biases can seep in
 

01:01:01.809 --> 01:01:05.380
the last part where biases can seep in
isn't the outcomes so we have this issue

01:01:05.380 --> 01:01:05.390
isn't the outcomes so we have this issue
 

01:01:05.390 --> 01:01:07.930
isn't the outcomes so we have this issue
we might call outcome bias or automation

01:01:07.930 --> 01:01:07.940
we might call outcome bias or automation
 

01:01:07.940 --> 01:01:11.289
we might call outcome bias or automation
bias where we assume that the process

01:01:11.289 --> 01:01:11.299
bias where we assume that the process
 

01:01:11.299 --> 01:01:12.969
bias where we assume that the process
the outcome of our decision-making

01:01:12.969 --> 01:01:12.979
the outcome of our decision-making
 

01:01:12.979 --> 01:01:15.370
the outcome of our decision-making
artifact characterized the quality of

01:01:15.370 --> 01:01:15.380
artifact characterized the quality of
 

01:01:15.380 --> 01:01:17.499
artifact characterized the quality of
the of the decision-making artifact it's

01:01:17.499 --> 01:01:17.509
the of the decision-making artifact it's
 

01:01:17.509 --> 01:01:20.229
the of the decision-making artifact it's
just like the the key example here might

01:01:20.229 --> 01:01:20.239
just like the the key example here might
 

01:01:20.239 --> 01:01:22.190
just like the the key example here might
be saying well a broken

01:01:22.190 --> 01:01:22.200
be saying well a broken
 

01:01:22.200 --> 01:01:23.930
be saying well a broken
clock is correct twice a day if you

01:01:23.930 --> 01:01:23.940
clock is correct twice a day if you
 

01:01:23.940 --> 01:01:25.490
clock is correct twice a day if you
observe it only have those two points of

01:01:25.490 --> 01:01:25.500
observe it only have those two points of
 

01:01:25.500 --> 01:01:26.990
observe it only have those two points of
the day you think it's it's working

01:01:26.990 --> 01:01:27.000
the day you think it's it's working
 

01:01:27.000 --> 01:01:28.670
the day you think it's it's working
perfectly but you're not thinking about

01:01:28.670 --> 01:01:28.680
perfectly but you're not thinking about
 

01:01:28.680 --> 01:01:30.230
perfectly but you're not thinking about
the process by which is getting those

01:01:30.230 --> 01:01:30.240
the process by which is getting those
 

01:01:30.240 --> 01:01:32.210
the process by which is getting those
decisions you're not questioning whether

01:01:32.210 --> 01:01:32.220
decisions you're not questioning whether
 

01:01:32.220 --> 01:01:34.670
decisions you're not questioning whether
that's correct or not so this is just a

01:01:34.670 --> 01:01:34.680
that's correct or not so this is just a
 

01:01:34.680 --> 01:01:36.530
that's correct or not so this is just a
framing I might suggest for how we might

01:01:36.530 --> 01:01:36.540
framing I might suggest for how we might
 

01:01:36.540 --> 01:01:38.870
framing I might suggest for how we might
think about how biases in equity CPM to

01:01:38.870 --> 01:01:38.880
think about how biases in equity CPM to
 

01:01:38.880 --> 01:01:40.460
think about how biases in equity CPM to
grade make decision-making like

01:01:40.460 --> 01:01:40.470
grade make decision-making like
 

01:01:40.470 --> 01:01:45.440
grade make decision-making like
artifacts let's try to be I'm going to

01:01:45.440 --> 01:01:45.450
artifacts let's try to be I'm going to
 

01:01:45.450 --> 01:01:47.120
artifacts let's try to be I'm going to
try to present two examples to case

01:01:47.120 --> 01:01:47.130
try to present two examples to case
 

01:01:47.130 --> 01:01:49.609
try to present two examples to case
studies where we can examine how an

01:01:49.609 --> 01:01:49.619
studies where we can examine how an
 

01:01:49.619 --> 01:01:52.040
studies where we can examine how an
algorithm might have to deal with equity

01:01:52.040 --> 01:01:52.050
algorithm might have to deal with equity
 

01:01:52.050 --> 01:01:53.750
algorithm might have to deal with equity
concerns the first is the use of

01:01:53.750 --> 01:01:53.760
concerns the first is the use of
 

01:01:53.760 --> 01:01:56.420
concerns the first is the use of
algorithms in criminal justice example

01:01:56.420 --> 01:01:56.430
algorithms in criminal justice example
 

01:01:56.430 --> 01:01:58.280
algorithms in criminal justice example
applications of the use of algorithms as

01:01:58.280 --> 01:01:58.290
applications of the use of algorithms as
 

01:01:58.290 --> 01:02:00.470
applications of the use of algorithms as
reflected in a compass report it is the

01:02:00.470 --> 01:02:00.480
reflected in a compass report it is the
 

01:02:00.480 --> 01:02:02.270
reflected in a compass report it is the
use of algorithms for setting bills and

01:02:02.270 --> 01:02:02.280
use of algorithms for setting bills and
 

01:02:02.280 --> 01:02:04.220
use of algorithms for setting bills and
for sentencing it used to be that

01:02:04.220 --> 01:02:04.230
for sentencing it used to be that
 

01:02:04.230 --> 01:02:05.859
for sentencing it used to be that
compass was mainly for setting bail

01:02:05.859 --> 01:02:05.869
compass was mainly for setting bail
 

01:02:05.869 --> 01:02:07.760
compass was mainly for setting bail
increasingly there was a feature creep

01:02:07.760 --> 01:02:07.770
increasingly there was a feature creep
 

01:02:07.770 --> 01:02:08.990
increasingly there was a feature creep
where it was increasingly used for

01:02:08.990 --> 01:02:09.000
where it was increasingly used for
 

01:02:09.000 --> 01:02:11.900
where it was increasingly used for
sentencing now the frame I'm going to

01:02:11.900 --> 01:02:11.910
sentencing now the frame I'm going to
 

01:02:11.910 --> 01:02:13.280
sentencing now the frame I'm going to
use to talk about this case to design

01:02:13.280 --> 01:02:13.290
use to talk about this case to design
 

01:02:13.290 --> 01:02:15.079
use to talk about this case to design
exam is something I'm hoping we can take

01:02:15.079 --> 01:02:15.089
exam is something I'm hoping we can take
 

01:02:15.089 --> 01:02:16.880
exam is something I'm hoping we can take
away from this conversation every time

01:02:16.880 --> 01:02:16.890
away from this conversation every time
 

01:02:16.890 --> 01:02:18.319
away from this conversation every time
you think about algorithms in

01:02:18.319 --> 01:02:18.329
you think about algorithms in
 

01:02:18.329 --> 01:02:19.819
you think about algorithms in
decision-making which you think about

01:02:19.819 --> 01:02:19.829
decision-making which you think about
 

01:02:19.829 --> 01:02:21.500
decision-making which you think about
the problem is trying to solve in this

01:02:21.500 --> 01:02:21.510
the problem is trying to solve in this
 

01:02:21.510 --> 01:02:24.230
the problem is trying to solve in this
case bills bill setting a sentencing but

01:02:24.230 --> 01:02:24.240
case bills bill setting a sentencing but
 

01:02:24.240 --> 01:02:26.540
case bills bill setting a sentencing but
we should also be explicit about the

01:02:26.540 --> 01:02:26.550
we should also be explicit about the
 

01:02:26.550 --> 01:02:29.260
we should also be explicit about the
guiding norms the equity norms that

01:02:29.260 --> 01:02:29.270
guiding norms the equity norms that
 

01:02:29.270 --> 01:02:31.730
guiding norms the equity norms that
decision making artifact is subject to

01:02:31.730 --> 01:02:31.740
decision making artifact is subject to
 

01:02:31.740 --> 01:02:34.010
decision making artifact is subject to
in the context of criminal justice I

01:02:34.010 --> 01:02:34.020
in the context of criminal justice I
 

01:02:34.020 --> 01:02:36.109
in the context of criminal justice I
would argue that we have at least two

01:02:36.109 --> 01:02:36.119
would argue that we have at least two
 

01:02:36.119 --> 01:02:37.609
would argue that we have at least two
norms the equal protection norm

01:02:37.609 --> 01:02:37.619
norms the equal protection norm
 

01:02:37.619 --> 01:02:40.339
norms the equal protection norm
guaranteed by the US Constitution at

01:02:40.339 --> 01:02:40.349
guaranteed by the US Constitution at
 

01:02:40.349 --> 01:02:42.109
guaranteed by the US Constitution at
least in the United States and you might

01:02:42.109 --> 01:02:42.119
least in the United States and you might
 

01:02:42.119 --> 01:02:44.030
least in the United States and you might
also argue for the due process norm

01:02:44.030 --> 01:02:44.040
also argue for the due process norm
 

01:02:44.040 --> 01:02:47.470
also argue for the due process norm
guaranteed by the Fifth Amendment rights

01:02:47.470 --> 01:02:47.480
guaranteed by the Fifth Amendment rights
 

01:02:47.480 --> 01:02:50.210
guaranteed by the Fifth Amendment rights
the second example the second case study

01:02:50.210 --> 01:02:50.220
the second example the second case study
 

01:02:50.220 --> 01:02:52.069
the second example the second case study
the second domain would be algorithms

01:02:52.069 --> 01:02:52.079
the second domain would be algorithms
 

01:02:52.079 --> 01:02:55.430
the second domain would be algorithms
not insurance insurance represents this

01:02:55.430 --> 01:02:55.440
not insurance insurance represents this
 

01:02:55.440 --> 01:02:58.450
not insurance insurance represents this
it solves this social social function of

01:02:58.450 --> 01:02:58.460
it solves this social social function of
 

01:02:58.460 --> 01:03:00.980
it solves this social social function of
safeguarding people from catastrophic

01:03:00.980 --> 01:03:00.990
safeguarding people from catastrophic
 

01:03:00.990 --> 01:03:03.170
safeguarding people from catastrophic
risk and so it's not surprising that

01:03:03.170 --> 01:03:03.180
risk and so it's not surprising that
 

01:03:03.180 --> 01:03:05.030
risk and so it's not surprising that
insurers might want to use algorithms to

01:03:05.030 --> 01:03:05.040
insurers might want to use algorithms to
 

01:03:05.040 --> 01:03:07.220
insurers might want to use algorithms to
help them make their make their

01:03:07.220 --> 01:03:07.230
help them make their make their
 

01:03:07.230 --> 01:03:10.720
help them make their make their
decisions more accurate more fair

01:03:10.720 --> 01:03:10.730
decisions more accurate more fair
 

01:03:10.730 --> 01:03:12.890
decisions more accurate more fair
example applications example problems

01:03:12.890 --> 01:03:12.900
example applications example problems
 

01:03:12.900 --> 01:03:14.690
example applications example problems
here are the problems of rate setting

01:03:14.690 --> 01:03:14.700
here are the problems of rate setting
 

01:03:14.700 --> 01:03:16.819
here are the problems of rate setting
and underwriting rate certain is just

01:03:16.819 --> 01:03:16.829
and underwriting rate certain is just
 

01:03:16.829 --> 01:03:18.620
and underwriting rate certain is just
really about setting the price of an

01:03:18.620 --> 01:03:18.630
really about setting the price of an
 

01:03:18.630 --> 01:03:19.400
really about setting the price of an
insurance premium

01:03:19.400 --> 01:03:19.410
insurance premium
 

01:03:19.410 --> 01:03:21.530
insurance premium
underwriting is kind of sort of gauging

01:03:21.530 --> 01:03:21.540
underwriting is kind of sort of gauging
 

01:03:21.540 --> 01:03:24.650
underwriting is kind of sort of gauging
the risk the risk tranches for people

01:03:24.650 --> 01:03:24.660
the risk the risk tranches for people
 

01:03:24.660 --> 01:03:26.870
the risk the risk tranches for people
who are supposed to be insured now what

01:03:26.870 --> 01:03:26.880
who are supposed to be insured now what
 

01:03:26.880 --> 01:03:29.150
who are supposed to be insured now what
would be the insurance the guiding norm

01:03:29.150 --> 01:03:29.160
would be the insurance the guiding norm
 

01:03:29.160 --> 01:03:32.030
would be the insurance the guiding norm
in the situation this is a bit more

01:03:32.030 --> 01:03:32.040
in the situation this is a bit more
 

01:03:32.040 --> 01:03:35.240
in the situation this is a bit more
complicated and I'd argue that the

01:03:35.240 --> 01:03:35.250
complicated and I'd argue that the
 

01:03:35.250 --> 01:03:36.110
complicated and I'd argue that the
actuarial

01:03:36.110 --> 01:03:36.120
actuarial
 

01:03:36.120 --> 01:03:38.060
actuarial
this body of the United States says well

01:03:38.060 --> 01:03:38.070
this body of the United States says well
 

01:03:38.070 --> 01:03:40.100
this body of the United States says well
your insurance prices should be actually

01:03:40.100 --> 01:03:40.110
your insurance prices should be actually
 

01:03:40.110 --> 01:03:41.630
your insurance prices should be actually
fair ie

01:03:41.630 --> 01:03:41.640
fair ie
 

01:03:41.640 --> 01:03:43.700
fair ie
the cost of an insurance premium should

01:03:43.700 --> 01:03:43.710
the cost of an insurance premium should
 

01:03:43.710 --> 01:03:45.830
the cost of an insurance premium should
reflect on the lying risk of the person

01:03:45.830 --> 01:03:45.840
reflect on the lying risk of the person
 

01:03:45.840 --> 01:03:47.690
reflect on the lying risk of the person
but then we have these issues when we

01:03:47.690 --> 01:03:47.700
but then we have these issues when we
 

01:03:47.700 --> 01:03:49.850
but then we have these issues when we
start asking for universal health care

01:03:49.850 --> 01:03:49.860
start asking for universal health care
 

01:03:49.860 --> 01:03:51.650
start asking for universal health care
mandate universal insurance mandate I

01:03:51.650 --> 01:03:51.660
mandate universal insurance mandate I
 

01:03:51.660 --> 01:03:54.050
mandate universal insurance mandate I
have to worry about affordability being

01:03:54.050 --> 01:03:54.060
have to worry about affordability being
 

01:03:54.060 --> 01:03:57.500
have to worry about affordability being
part of the norms here now in a market

01:03:57.500 --> 01:03:57.510
part of the norms here now in a market
 

01:03:57.510 --> 01:03:59.840
part of the norms here now in a market
setting that might be a problem these

01:03:59.840 --> 01:03:59.850
setting that might be a problem these
 

01:03:59.850 --> 01:04:01.250
setting that might be a problem these
are questions we'll have to tease out as

01:04:01.250 --> 01:04:01.260
are questions we'll have to tease out as
 

01:04:01.260 --> 01:04:02.690
are questions we'll have to tease out as
we as we think about equity going

01:04:02.690 --> 01:04:02.700
we as we think about equity going
 

01:04:02.700 --> 01:04:05.780
we as we think about equity going
forward let's try to use these case

01:04:05.780 --> 01:04:05.790
forward let's try to use these case
 

01:04:05.790 --> 01:04:09.970
forward let's try to use these case
studies to tease out more general ideas

01:04:09.970 --> 01:04:09.980
studies to tease out more general ideas
 

01:04:09.980 --> 01:04:12.590
studies to tease out more general ideas
well the very first idea or the very

01:04:12.590 --> 01:04:12.600
well the very first idea or the very
 

01:04:12.600 --> 01:04:15.620
well the very first idea or the very
first theme is that equity norms and

01:04:15.620 --> 01:04:15.630
first theme is that equity norms and
 

01:04:15.630 --> 01:04:17.930
first theme is that equity norms and
necessarily context-specific that's an

01:04:17.930 --> 01:04:17.940
necessarily context-specific that's an
 

01:04:17.940 --> 01:04:21.170
necessarily context-specific that's an
almost obvious statement but that

01:04:21.170 --> 01:04:21.180
almost obvious statement but that
 

01:04:21.180 --> 01:04:24.310
almost obvious statement but that
obvious statement kind of undermines our

01:04:24.310 --> 01:04:24.320
obvious statement kind of undermines our
 

01:04:24.320 --> 01:04:26.620
obvious statement kind of undermines our
expectation that maybe there is this

01:04:26.620 --> 01:04:26.630
expectation that maybe there is this
 

01:04:26.630 --> 01:04:29.300
expectation that maybe there is this
prescriptive mathematical definition of

01:04:29.300 --> 01:04:29.310
prescriptive mathematical definition of
 

01:04:29.310 --> 01:04:31.820
prescriptive mathematical definition of
equity that we just need to impose on

01:04:31.820 --> 01:04:31.830
equity that we just need to impose on
 

01:04:31.830 --> 01:04:35.420
equity that we just need to impose on
algorithms to make them fair that's

01:04:35.420 --> 01:04:35.430
algorithms to make them fair that's
 

01:04:35.430 --> 01:04:38.330
algorithms to make them fair that's
probably not going to happen and we can

01:04:38.330 --> 01:04:38.340
probably not going to happen and we can
 

01:04:38.340 --> 01:04:40.970
probably not going to happen and we can
actually add more to we can actually add

01:04:40.970 --> 01:04:40.980
actually add more to we can actually add
 

01:04:40.980 --> 01:04:43.250
actually add more to we can actually add
more context more complexity to this to

01:04:43.250 --> 01:04:43.260
more context more complexity to this to
 

01:04:43.260 --> 01:04:45.200
more context more complexity to this to
this issue of the existence of

01:04:45.200 --> 01:04:45.210
this issue of the existence of
 

01:04:45.210 --> 01:04:48.410
this issue of the existence of
prescriptive equity norms work by I hope

01:04:48.410 --> 01:04:48.420
prescriptive equity norms work by I hope
 

01:04:48.420 --> 01:04:49.700
prescriptive equity norms work by I hope
I'm pronouncing that name properly sure

01:04:49.700 --> 01:04:49.710
I'm pronouncing that name properly sure
 

01:04:49.710 --> 01:04:53.060
I'm pronouncing that name properly sure
- over and our crime berg Emily Nathan

01:04:53.060 --> 01:04:53.070
- over and our crime berg Emily Nathan
 

01:04:53.070 --> 01:04:55.220
- over and our crime berg Emily Nathan
they are addressing they are identifying

01:04:55.220 --> 01:04:55.230
they are addressing they are identifying
 

01:04:55.230 --> 01:04:56.870
they are addressing they are identifying
what we might call weak impossibility

01:04:56.870 --> 01:04:56.880
what we might call weak impossibility
 

01:04:56.880 --> 01:05:00.920
what we might call weak impossibility
theorems that if you have a set of a set

01:05:00.920 --> 01:05:00.930
theorems that if you have a set of a set
 

01:05:00.930 --> 01:05:03.980
theorems that if you have a set of a set
of mathematically defined equity norms

01:05:03.980 --> 01:05:03.990
of mathematically defined equity norms
 

01:05:03.990 --> 01:05:05.780
of mathematically defined equity norms
that you would hope your decision-making

01:05:05.780 --> 01:05:05.790
that you would hope your decision-making
 

01:05:05.790 --> 01:05:07.970
that you would hope your decision-making
artifact respect in this case group

01:05:07.970 --> 01:05:07.980
artifact respect in this case group
 

01:05:07.980 --> 01:05:11.150
artifact respect in this case group
calibration equality of negative force

01:05:11.150 --> 01:05:11.160
calibration equality of negative force
 

01:05:11.160 --> 01:05:13.460
calibration equality of negative force
positive or false negative rate equality

01:05:13.460 --> 01:05:13.470
positive or false negative rate equality
 

01:05:13.470 --> 01:05:15.560
positive or false negative rate equality
or false positive rate there is

01:05:15.560 --> 01:05:15.570
or false positive rate there is
 

01:05:15.570 --> 01:05:18.260
or false positive rate there is
something inherently incompatible in

01:05:18.260 --> 01:05:18.270
something inherently incompatible in
 

01:05:18.270 --> 01:05:21.500
something inherently incompatible in
trying to achieve all three equity norms

01:05:21.500 --> 01:05:21.510
trying to achieve all three equity norms
 

01:05:21.510 --> 01:05:24.680
trying to achieve all three equity norms
and conversations a few a few months a

01:05:24.680 --> 01:05:24.690
and conversations a few a few months a
 

01:05:24.690 --> 01:05:26.860
and conversations a few a few months a
few weeks ago at the fat ml conference

01:05:26.860 --> 01:05:26.870
few weeks ago at the fat ml conference
 

01:05:26.870 --> 01:05:30.080
few weeks ago at the fat ml conference
extended this to any collection of

01:05:30.080 --> 01:05:30.090
extended this to any collection of
 

01:05:30.090 --> 01:05:31.700
extended this to any collection of
equity mathematically defined equity

01:05:31.700 --> 01:05:31.710
equity mathematically defined equity
 

01:05:31.710 --> 01:05:33.830
equity mathematically defined equity
norms trying to satisfy more than three

01:05:33.830 --> 01:05:33.840
norms trying to satisfy more than three
 

01:05:33.840 --> 01:05:36.110
norms trying to satisfy more than three
and three or more is income is

01:05:36.110 --> 01:05:36.120
and three or more is income is
 

01:05:36.120 --> 01:05:38.120
and three or more is income is
impossible we can call that on a weak

01:05:38.120 --> 01:05:38.130
impossible we can call that on a weak
 

01:05:38.130 --> 01:05:40.460
impossible we can call that on a weak
impossibility theorem now you might hear

01:05:40.460 --> 01:05:40.470
impossibility theorem now you might hear
 

01:05:40.470 --> 01:05:42.380
impossibility theorem now you might hear
this and think well since I can have it

01:05:42.380 --> 01:05:42.390
this and think well since I can have it
 

01:05:42.390 --> 01:05:42.770
this and think well since I can have it
all

01:05:42.770 --> 01:05:42.780
all
 

01:05:42.780 --> 01:05:45.200
all
let me just have let me have my users

01:05:45.200 --> 01:05:45.210
let me just have let me have my users
 

01:05:45.210 --> 01:05:47.480
let me just have let me have my users
vote to decide which is and which ones

01:05:47.480 --> 01:05:47.490
vote to decide which is and which ones
 

01:05:47.490 --> 01:05:48.090
vote to decide which is and which ones
are important

01:05:48.090 --> 01:05:48.100
are important
 

01:05:48.100 --> 01:05:50.490
are important
but then we run into impossibility

01:05:50.490 --> 01:05:50.500
but then we run into impossibility
 

01:05:50.500 --> 01:05:51.840
but then we run into impossibility
theorem is the stronger impossibility

01:05:51.840 --> 01:05:51.850
theorem is the stronger impossibility
 

01:05:51.850 --> 01:05:54.570
theorem is the stronger impossibility
theorems by Kenneth arrow Bieber dance

01:05:54.570 --> 01:05:54.580
theorems by Kenneth arrow Bieber dance
 

01:05:54.580 --> 01:05:56.640
theorems by Kenneth arrow Bieber dance
after tweet saying essentially it's

01:05:56.640 --> 01:05:56.650
after tweet saying essentially it's
 

01:05:56.650 --> 01:05:59.280
after tweet saying essentially it's
impossible to reasonably aggregate

01:05:59.280 --> 01:05:59.290
impossible to reasonably aggregate
 

01:05:59.290 --> 01:06:03.240
impossible to reasonably aggregate
Collective normative preferences alright

01:06:03.240 --> 01:06:03.250
Collective normative preferences alright
 

01:06:03.250 --> 01:06:06.810
Collective normative preferences alright
so that's the second theme the third

01:06:06.810 --> 01:06:06.820
so that's the second theme the third
 

01:06:06.820 --> 01:06:08.760
so that's the second theme the third
team I'm trying to have been trying to

01:06:08.760 --> 01:06:08.770
team I'm trying to have been trying to
 

01:06:08.770 --> 01:06:10.650
team I'm trying to have been trying to
develop over the past year or so or to

01:06:10.650 --> 01:06:10.660
develop over the past year or so or to
 

01:06:10.660 --> 01:06:12.750
develop over the past year or so or to
thinking about this is this framing of

01:06:12.750 --> 01:06:12.760
thinking about this is this framing of
 

01:06:12.760 --> 01:06:14.490
thinking about this is this framing of
allocation problems distribution

01:06:14.490 --> 01:06:14.500
allocation problems distribution
 

01:06:14.500 --> 01:06:17.370
allocation problems distribution
problems as entitlement programs versus

01:06:17.370 --> 01:06:17.380
problems as entitlement programs versus
 

01:06:17.380 --> 01:06:19.950
problems as entitlement programs versus
market programs we can think about it as

01:06:19.950 --> 01:06:19.960
market programs we can think about it as
 

01:06:19.960 --> 01:06:22.500
market programs we can think about it as
a spectrum entitlement distribution

01:06:22.500 --> 01:06:22.510
a spectrum entitlement distribution
 

01:06:22.510 --> 01:06:25.770
a spectrum entitlement distribution
programs the distribution problems in

01:06:25.770 --> 01:06:25.780
programs the distribution problems in
 

01:06:25.780 --> 01:06:27.870
programs the distribution problems in
which there are clear equity norms

01:06:27.870 --> 01:06:27.880
which there are clear equity norms
 

01:06:27.880 --> 01:06:30.120
which there are clear equity norms
guiding the decision process the

01:06:30.120 --> 01:06:30.130
guiding the decision process the
 

01:06:30.130 --> 01:06:31.710
guiding the decision process the
distribution distributed decision

01:06:31.710 --> 01:06:31.720
distribution distributed decision
 

01:06:31.720 --> 01:06:34.890
distribution distributed decision
process normative clarity usually means

01:06:34.890 --> 01:06:34.900
process normative clarity usually means
 

01:06:34.900 --> 01:06:36.960
process normative clarity usually means
that there is some law some regulation

01:06:36.960 --> 01:06:36.970
that there is some law some regulation
 

01:06:36.970 --> 01:06:39.630
that there is some law some regulation
and everybody accepts that determines

01:06:39.630 --> 01:06:39.640
and everybody accepts that determines
 

01:06:39.640 --> 01:06:43.200
and everybody accepts that determines
how fair our decision is you can think

01:06:43.200 --> 01:06:43.210
how fair our decision is you can think
 

01:06:43.210 --> 01:06:44.880
how fair our decision is you can think
about the example of United States

01:06:44.880 --> 01:06:44.890
about the example of United States
 

01:06:44.890 --> 01:06:47.970
about the example of United States
criminal justice you have the public

01:06:47.970 --> 01:06:47.980
criminal justice you have the public
 

01:06:47.980 --> 01:06:51.060
criminal justice you have the public
good law enforcement that's being

01:06:51.060 --> 01:06:51.070
good law enforcement that's being
 

01:06:51.070 --> 01:06:53.400
good law enforcement that's being
distributed to the public under the

01:06:53.400 --> 01:06:53.410
distributed to the public under the
 

01:06:53.410 --> 01:06:55.890
distributed to the public under the
normative guidelines of equal protection

01:06:55.890 --> 01:06:55.900
normative guidelines of equal protection
 

01:06:55.900 --> 01:07:00.000
normative guidelines of equal protection
and due process but we also have what we

01:07:00.000 --> 01:07:00.010
and due process but we also have what we
 

01:07:00.010 --> 01:07:03.090
and due process but we also have what we
might think of as market programs market

01:07:03.090 --> 01:07:03.100
might think of as market programs market
 

01:07:03.100 --> 01:07:04.410
might think of as market programs market
distribution programs are just

01:07:04.410 --> 01:07:04.420
distribution programs are just
 

01:07:04.420 --> 01:07:06.360
distribution programs are just
distribution programs where equity norms

01:07:06.360 --> 01:07:06.370
distribution programs where equity norms
 

01:07:06.370 --> 01:07:08.100
distribution programs where equity norms
play little or no role and the

01:07:08.100 --> 01:07:08.110
play little or no role and the
 

01:07:08.110 --> 01:07:10.290
play little or no role and the
distributive action is determined

01:07:10.290 --> 01:07:10.300
distributive action is determined
 

01:07:10.300 --> 01:07:12.930
distributive action is determined
largely by the intersection of demand or

01:07:12.930 --> 01:07:12.940
largely by the intersection of demand or
 

01:07:12.940 --> 01:07:17.220
largely by the intersection of demand or
supply now let's try to figure out

01:07:17.220 --> 01:07:17.230
supply now let's try to figure out
 

01:07:17.230 --> 01:07:19.590
supply now let's try to figure out
questions that come up from this in the

01:07:19.590 --> 01:07:19.600
questions that come up from this in the
 

01:07:19.600 --> 01:07:20.850
questions that come up from this in the
entitlement programs you have the

01:07:20.850 --> 01:07:20.860
entitlement programs you have the
 

01:07:20.860 --> 01:07:23.250
entitlement programs you have the
question of are the norms necessarily

01:07:23.250 --> 01:07:23.260
question of are the norms necessarily
 

01:07:23.260 --> 01:07:25.020
question of are the norms necessarily
equitable and if they are not equitable

01:07:25.020 --> 01:07:25.030
equitable and if they are not equitable
 

01:07:25.030 --> 01:07:29.040
equitable and if they are not equitable
are there remedies for it social

01:07:29.040 --> 01:07:29.050
are there remedies for it social
 

01:07:29.050 --> 01:07:30.780
are there remedies for it social
movements like black lives matter and

01:07:30.780 --> 01:07:30.790
movements like black lives matter and
 

01:07:30.790 --> 01:07:32.640
movements like black lives matter and
civil rights movements they are

01:07:32.640 --> 01:07:32.650
civil rights movements they are
 

01:07:32.650 --> 01:07:34.740
civil rights movements they are
essentially challenges to the equity

01:07:34.740 --> 01:07:34.750
essentially challenges to the equity
 

01:07:34.750 --> 01:07:36.210
essentially challenges to the equity
norms they rule in equity norms at the

01:07:36.210 --> 01:07:36.220
norms they rule in equity norms at the
 

01:07:36.220 --> 01:07:38.490
norms they rule in equity norms at the
time and we have the same essentially

01:07:38.490 --> 01:07:38.500
time and we have the same essentially
 

01:07:38.500 --> 01:07:40.410
time and we have the same essentially
that legality is not really a question

01:07:40.410 --> 01:07:40.420
that legality is not really a question
 

01:07:40.420 --> 01:07:44.160
that legality is not really a question
of justice the question of power and we

01:07:44.160 --> 01:07:44.170
of justice the question of power and we
 

01:07:44.170 --> 01:07:45.810
of justice the question of power and we
switch over to the market session

01:07:45.810 --> 01:07:45.820
switch over to the market session
 

01:07:45.820 --> 01:07:47.970
switch over to the market session
you have this issue of there is a

01:07:47.970 --> 01:07:47.980
you have this issue of there is a
 

01:07:47.980 --> 01:07:50.220
you have this issue of there is a
trade-off between the accuracy the

01:07:50.220 --> 01:07:50.230
trade-off between the accuracy the
 

01:07:50.230 --> 01:07:52.320
trade-off between the accuracy the
efficiency of the market and the

01:07:52.320 --> 01:07:52.330
efficiency of the market and the
 

01:07:52.330 --> 01:07:54.930
efficiency of the market and the
imposition of equity norms if we try to

01:07:54.930 --> 01:07:54.940
imposition of equity norms if we try to
 

01:07:54.940 --> 01:07:56.970
imposition of equity norms if we try to
make a market fair accordance on the

01:07:56.970 --> 01:07:56.980
make a market fair accordance on the
 

01:07:56.980 --> 01:07:59.180
make a market fair accordance on the
equity norms you can often impose a

01:07:59.180 --> 01:07:59.190
equity norms you can often impose a
 

01:07:59.190 --> 01:08:01.670
equity norms you can often impose a
deadweight cost what is essentially dead

01:08:01.670 --> 01:08:01.680
deadweight cost what is essentially dead
 

01:08:01.680 --> 01:08:02.330
deadweight cost what is essentially dead
wood cost

01:08:02.330 --> 01:08:02.340
wood cost
 

01:08:02.340 --> 01:08:04.400
wood cost
the question becomes who gets to bear

01:08:04.400 --> 01:08:04.410
the question becomes who gets to bear
 

01:08:04.410 --> 01:08:09.130
the question becomes who gets to bear
that burden and how big is that burden

01:08:09.130 --> 01:08:09.140
that burden and how big is that burden
 

01:08:09.140 --> 01:08:12.650
that burden and how big is that burden
that sounds theoretical highfalutin in

01:08:12.650 --> 01:08:12.660
that sounds theoretical highfalutin in
 

01:08:12.660 --> 01:08:14.570
that sounds theoretical highfalutin in
concrete terms you can think about it in

01:08:14.570 --> 01:08:14.580
concrete terms you can think about it in
 

01:08:14.580 --> 01:08:16.070
concrete terms you can think about it in
terms of the mark the insurance market

01:08:16.070 --> 01:08:16.080
terms of the mark the insurance market
 

01:08:16.080 --> 01:08:17.810
terms of the mark the insurance market
in health insurance if you have

01:08:17.810 --> 01:08:17.820
in health insurance if you have
 

01:08:17.820 --> 01:08:20.510
in health insurance if you have
universal mandate who gets to bear the

01:08:20.510 --> 01:08:20.520
universal mandate who gets to bear the
 

01:08:20.520 --> 01:08:23.120
universal mandate who gets to bear the
cost of that increasing in risk just

01:08:23.120 --> 01:08:23.130
cost of that increasing in risk just
 

01:08:23.130 --> 01:08:25.070
cost of that increasing in risk just
because you have universal mandate less

01:08:25.070 --> 01:08:25.080
because you have universal mandate less
 

01:08:25.080 --> 01:08:26.570
because you have universal mandate less
controversially initially in auto

01:08:26.570 --> 01:08:26.580
controversially initially in auto
 

01:08:26.580 --> 01:08:28.550
controversially initially in auto
insurance market who gets to bear that

01:08:28.550 --> 01:08:28.560
insurance market who gets to bear that
 

01:08:28.560 --> 01:08:30.560
insurance market who gets to bear that
cost those are questions we have to

01:08:30.560 --> 01:08:30.570
cost those are questions we have to
 

01:08:30.570 --> 01:08:32.330
cost those are questions we have to
tackle with we have to grapple with if

01:08:32.330 --> 01:08:32.340
tackle with we have to grapple with if
 

01:08:32.340 --> 01:08:35.480
tackle with we have to grapple with if
we really care about equity in the

01:08:35.480 --> 01:08:35.490
we really care about equity in the
 

01:08:35.490 --> 01:08:38.750
we really care about equity in the
decision-making systems all right I feel

01:08:38.750 --> 01:08:38.760
decision-making systems all right I feel
 

01:08:38.760 --> 01:08:42.110
decision-making systems all right I feel
like I've been a bit negative about the

01:08:42.110 --> 01:08:42.120
like I've been a bit negative about the
 

01:08:42.120 --> 01:08:45.680
like I've been a bit negative about the
prospect of algorithmic equity let's try

01:08:45.680 --> 01:08:45.690
prospect of algorithmic equity let's try
 

01:08:45.690 --> 01:08:48.740
prospect of algorithmic equity let's try
to see if I can pull out more positive

01:08:48.740 --> 01:08:48.750
to see if I can pull out more positive
 

01:08:48.750 --> 01:08:53.560
to see if I can pull out more positive
vision from the rubble

01:08:53.560 --> 01:08:53.570
 

01:08:53.570 --> 01:08:56.570
we've I've been poking holes in the idea

01:08:56.570 --> 01:08:56.580
we've I've been poking holes in the idea
 

01:08:56.580 --> 01:08:58.160
we've I've been poking holes in the idea
that you can actually define press

01:08:58.160 --> 01:08:58.170
that you can actually define press
 

01:08:58.170 --> 01:09:00.080
that you can actually define press
creative equity enormous mathematically

01:09:00.080 --> 01:09:00.090
creative equity enormous mathematically
 

01:09:00.090 --> 01:09:02.420
creative equity enormous mathematically
and impose them on your algorithms and

01:09:02.420 --> 01:09:02.430
and impose them on your algorithms and
 

01:09:02.430 --> 01:09:04.730
and impose them on your algorithms and
even if such a thing existed you will

01:09:04.730 --> 01:09:04.740
even if such a thing existed you will
 

01:09:04.740 --> 01:09:06.560
even if such a thing existed you will
still need a buy-in of your subjects of

01:09:06.560 --> 01:09:06.570
still need a buy-in of your subjects of
 

01:09:06.570 --> 01:09:09.830
still need a buy-in of your subjects of
your users that is not easy to to to

01:09:09.830 --> 01:09:09.840
your users that is not easy to to to
 

01:09:09.840 --> 01:09:12.250
your users that is not easy to to to
maintain to receive in a dictatorial

01:09:12.250 --> 01:09:12.260
maintain to receive in a dictatorial
 

01:09:12.260 --> 01:09:15.500
maintain to receive in a dictatorial
technocracy so this suggests that maybe

01:09:15.500 --> 01:09:15.510
technocracy so this suggests that maybe
 

01:09:15.510 --> 01:09:17.030
technocracy so this suggests that maybe
we should be thinking more about

01:09:17.030 --> 01:09:17.040
we should be thinking more about
 

01:09:17.040 --> 01:09:19.430
we should be thinking more about
participate reversions participatory

01:09:19.430 --> 01:09:19.440
participate reversions participatory
 

01:09:19.440 --> 01:09:24.350
participate reversions participatory
models of AI equity basically these will

01:09:24.350 --> 01:09:24.360
models of AI equity basically these will
 

01:09:24.360 --> 01:09:26.900
models of AI equity basically these will
be models in which your users which

01:09:26.900 --> 01:09:26.910
be models in which your users which
 

01:09:26.910 --> 01:09:27.730
be models in which your users which
constantly

01:09:27.730 --> 01:09:27.740
constantly
 

01:09:27.740 --> 01:09:31.450
constantly
query your users for equity concerns

01:09:31.450 --> 01:09:31.460
query your users for equity concerns
 

01:09:31.460 --> 01:09:34.310
query your users for equity concerns
when we talk about diversity in the rank

01:09:34.310 --> 01:09:34.320
when we talk about diversity in the rank
 

01:09:34.320 --> 01:09:36.590
when we talk about diversity in the rank
this is the diversity point you knew it

01:09:36.590 --> 01:09:36.600
this is the diversity point you knew it
 

01:09:36.600 --> 01:09:38.690
this is the diversity point you knew it
was coming so whatever when we talk

01:09:38.690 --> 01:09:38.700
was coming so whatever when we talk
 

01:09:38.700 --> 01:09:40.160
was coming so whatever when we talk
about diversity in the ranks of

01:09:40.160 --> 01:09:40.170
about diversity in the ranks of
 

01:09:40.170 --> 01:09:42.590
about diversity in the ranks of
algorithmic designers we are not just

01:09:42.590 --> 01:09:42.600
algorithmic designers we are not just
 

01:09:42.600 --> 01:09:45.350
algorithmic designers we are not just
pleading special circumstances it's

01:09:45.350 --> 01:09:45.360
pleading special circumstances it's
 

01:09:45.360 --> 01:09:47.510
pleading special circumstances it's
precisely because by having more design

01:09:47.510 --> 01:09:47.520
precisely because by having more design
 

01:09:47.520 --> 01:09:50.060
precisely because by having more design
more diversity in the rank of designers

01:09:50.060 --> 01:09:50.070
more diversity in the rank of designers
 

01:09:50.070 --> 01:09:51.560
more diversity in the rank of designers
you're pulling in more social

01:09:51.560 --> 01:09:51.570
you're pulling in more social
 

01:09:51.570 --> 01:09:53.600
you're pulling in more social
perspectives to inform the design

01:09:53.600 --> 01:09:53.610
perspectives to inform the design
 

01:09:53.610 --> 01:09:57.770
perspectives to inform the design
process but even diversity diversify is

01:09:57.770 --> 01:09:57.780
process but even diversity diversify is
 

01:09:57.780 --> 01:10:00.200
process but even diversity diversify is
not enough because if you think about it

01:10:00.200 --> 01:10:00.210
not enough because if you think about it
 

01:10:00.210 --> 01:10:02.450
not enough because if you think about it
algorithms are always necessarily going

01:10:02.450 --> 01:10:02.460
algorithms are always necessarily going
 

01:10:02.460 --> 01:10:05.300
algorithms are always necessarily going
to act well generally going to affect

01:10:05.300 --> 01:10:05.310
to act well generally going to affect
 

01:10:05.310 --> 01:10:07.910
to act well generally going to affect
more people than are being represented

01:10:07.910 --> 01:10:07.920
more people than are being represented
 

01:10:07.920 --> 01:10:11.100
more people than are being represented
in the designer class

01:10:11.100 --> 01:10:11.110
 

01:10:11.110 --> 01:10:13.050
so what you're going to need is

01:10:13.050 --> 01:10:13.060
so what you're going to need is
 

01:10:13.060 --> 01:10:16.050
so what you're going to need is
something that allows for for more

01:10:16.050 --> 01:10:16.060
something that allows for for more
 

01:10:16.060 --> 01:10:18.420
something that allows for for more
participation from wider groups and we

01:10:18.420 --> 01:10:18.430
participation from wider groups and we
 

01:10:18.430 --> 01:10:20.870
participation from wider groups and we
can sharpen this point by observing that

01:10:20.870 --> 01:10:20.880
can sharpen this point by observing that
 

01:10:20.880 --> 01:10:23.430
can sharpen this point by observing that
equity challenges equity concerns

01:10:23.430 --> 01:10:23.440
equity challenges equity concerns
 

01:10:23.440 --> 01:10:26.220
equity challenges equity concerns
they're distributed form of knowledge

01:10:26.220 --> 01:10:26.230
they're distributed form of knowledge
 

01:10:26.230 --> 01:10:28.770
they're distributed form of knowledge
there is no single controlling group

01:10:28.770 --> 01:10:28.780
there is no single controlling group
 

01:10:28.780 --> 01:10:30.150
there is no single controlling group
that's ever going to be able to

01:10:30.150 --> 01:10:30.160
that's ever going to be able to
 

01:10:30.160 --> 01:10:32.790
that's ever going to be able to
coordinate the transmission and

01:10:32.790 --> 01:10:32.800
coordinate the transmission and
 

01:10:32.800 --> 01:10:35.130
coordinate the transmission and
accommodation of equity challenges so we

01:10:35.130 --> 01:10:35.140
accommodation of equity challenges so we
 

01:10:35.140 --> 01:10:37.080
accommodation of equity challenges so we
need more participation basically we

01:10:37.080 --> 01:10:37.090
need more participation basically we
 

01:10:37.090 --> 01:10:39.960
need more participation basically we
need more more most Socrates and fewer

01:10:39.960 --> 01:10:39.970
need more more most Socrates and fewer
 

01:10:39.970 --> 01:10:43.500
need more more most Socrates and fewer
Plato's designing from from isolation we

01:10:43.500 --> 01:10:43.510
Plato's designing from from isolation we
 

01:10:43.510 --> 01:10:45.420
Plato's designing from from isolation we
need people who are going to go out

01:10:45.420 --> 01:10:45.430
need people who are going to go out
 

01:10:45.430 --> 01:10:47.190
need people who are going to go out
there and do the messy work of engaging

01:10:47.190 --> 01:10:47.200
there and do the messy work of engaging
 

01:10:47.200 --> 01:10:48.900
there and do the messy work of engaging
with the subject and this is where I'm

01:10:48.900 --> 01:10:48.910
with the subject and this is where I'm
 

01:10:48.910 --> 01:10:50.430
with the subject and this is where I'm
reminded again of shoulder shoulders

01:10:50.430 --> 01:10:50.440
reminded again of shoulder shoulders
 

01:10:50.440 --> 01:10:52.770
reminded again of shoulder shoulders
work on the Allegheny County child

01:10:52.770 --> 01:10:52.780
work on the Allegheny County child
 

01:10:52.780 --> 01:10:55.470
work on the Allegheny County child
welfare system it's really good work

01:10:55.470 --> 01:10:55.480
welfare system it's really good work
 

01:10:55.480 --> 01:11:00.290
welfare system it's really good work
which is why I'm talking about it now

01:11:00.290 --> 01:11:00.300
 

01:11:00.300 --> 01:11:03.690
besides the participatory model we might

01:11:03.690 --> 01:11:03.700
besides the participatory model we might
 

01:11:03.700 --> 01:11:05.490
besides the participatory model we might
want to think about instead of

01:11:05.490 --> 01:11:05.500
want to think about instead of
 

01:11:05.500 --> 01:11:07.650
want to think about instead of
prescriptive norms we might want to

01:11:07.650 --> 01:11:07.660
prescriptive norms we might want to
 

01:11:07.660 --> 01:11:11.220
prescriptive norms we might want to
think about infrastructures for imposing

01:11:11.220 --> 01:11:11.230
think about infrastructures for imposing
 

01:11:11.230 --> 01:11:14.370
think about infrastructures for imposing
for enabling and designing equity into

01:11:14.370 --> 01:11:14.380
for enabling and designing equity into
 

01:11:14.380 --> 01:11:16.800
for enabling and designing equity into
algorithmic decision-making systems this

01:11:16.800 --> 01:11:16.810
algorithmic decision-making systems this
 

01:11:16.810 --> 01:11:18.750
algorithmic decision-making systems this
is kind of sort of the gdpr route

01:11:18.750 --> 01:11:18.760
is kind of sort of the gdpr route
 

01:11:18.760 --> 01:11:21.240
is kind of sort of the gdpr route
instead of trying to impose norm as you

01:11:21.240 --> 01:11:21.250
instead of trying to impose norm as you
 

01:11:21.250 --> 01:11:23.040
instead of trying to impose norm as you
impose it like you define an

01:11:23.040 --> 01:11:23.050
impose it like you define an
 

01:11:23.050 --> 01:11:24.810
impose it like you define an
infrastructure that enables good

01:11:24.810 --> 01:11:24.820
infrastructure that enables good
 

01:11:24.820 --> 01:11:28.230
infrastructure that enables good
behavior to to happen now you might want

01:11:28.230 --> 01:11:28.240
behavior to to happen now you might want
 

01:11:28.240 --> 01:11:30.330
behavior to to happen now you might want
to ask ourselves what are the key the

01:11:30.330 --> 01:11:30.340
to ask ourselves what are the key the
 

01:11:30.340 --> 01:11:32.220
to ask ourselves what are the key the
minimal key components of such an

01:11:32.220 --> 01:11:32.230
minimal key components of such an
 

01:11:32.230 --> 01:11:34.860
minimal key components of such an
infrastructure I want to argue for three

01:11:34.860 --> 01:11:34.870
infrastructure I want to argue for three
 

01:11:34.870 --> 01:11:39.510
infrastructure I want to argue for three
or three or four fluency we need more

01:11:39.510 --> 01:11:39.520
or three or four fluency we need more
 

01:11:39.520 --> 01:11:44.790
or three or four fluency we need more
fluency in the principles of equity so

01:11:44.790 --> 01:11:44.800
fluency in the principles of equity so
 

01:11:44.800 --> 01:11:46.920
fluency in the principles of equity so
the economist Peyton Jung put it this

01:11:46.920 --> 01:11:46.930
the economist Peyton Jung put it this
 

01:11:46.930 --> 01:11:47.400
the economist Peyton Jung put it this
way

01:11:47.400 --> 01:11:47.410
way
 

01:11:47.410 --> 01:11:49.860
way
equity principles these norms I've been

01:11:49.860 --> 01:11:49.870
equity principles these norms I've been
 

01:11:49.870 --> 01:11:51.870
equity principles these norms I've been
going on about and the mathematical

01:11:51.870 --> 01:11:51.880
going on about and the mathematical
 

01:11:51.880 --> 01:11:53.490
going on about and the mathematical
definitions of these norms they are

01:11:53.490 --> 01:11:53.500
definitions of these norms they are
 

01:11:53.500 --> 01:11:55.500
definitions of these norms they are
really just instruments they're not end

01:11:55.500 --> 01:11:55.510
really just instruments they're not end
 

01:11:55.510 --> 01:11:57.600
really just instruments they're not end
in themselves they're just imposed in an

01:11:57.600 --> 01:11:57.610
in themselves they're just imposed in an
 

01:11:57.610 --> 01:11:59.550
in themselves they're just imposed in an
algorithm they are really instrument by

01:11:59.550 --> 01:11:59.560
algorithm they are really instrument by
 

01:11:59.560 --> 01:12:02.810
algorithm they are really instrument by
which society kind of sort of

01:12:02.810 --> 01:12:02.820
which society kind of sort of
 

01:12:02.820 --> 01:12:05.760
which society kind of sort of
adjudicates equity problems distributive

01:12:05.760 --> 01:12:05.770
adjudicates equity problems distributive
 

01:12:05.770 --> 01:12:08.070
adjudicates equity problems distributive
problems especially when efficient in

01:12:08.070 --> 01:12:08.080
problems especially when efficient in
 

01:12:08.080 --> 01:12:09.870
problems especially when efficient in
market efficiency fails as a as a

01:12:09.870 --> 01:12:09.880
market efficiency fails as a as a
 

01:12:09.880 --> 01:12:12.900
market efficiency fails as a as a
criterion now if we are not fluent in

01:12:12.900 --> 01:12:12.910
criterion now if we are not fluent in
 

01:12:12.910 --> 01:12:14.520
criterion now if we are not fluent in
the language of equity principles

01:12:14.520 --> 01:12:14.530
the language of equity principles
 

01:12:14.530 --> 01:12:16.680
the language of equity principles
they're going to have problems properly

01:12:16.680 --> 01:12:16.690
they're going to have problems properly
 

01:12:16.690 --> 01:12:18.870
they're going to have problems properly
adjudicated and responding to equity

01:12:18.870 --> 01:12:18.880
adjudicated and responding to equity
 

01:12:18.880 --> 01:12:21.219
adjudicated and responding to equity
concerns

01:12:21.219 --> 01:12:21.229
concerns
 

01:12:21.229 --> 01:12:23.199
concerns
the second one under which I'll combine

01:12:23.199 --> 01:12:23.209
the second one under which I'll combine
 

01:12:23.209 --> 01:12:25.419
the second one under which I'll combine
it to transparency and dissent I think

01:12:25.419 --> 01:12:25.429
it to transparency and dissent I think
 

01:12:25.429 --> 01:12:27.669
it to transparency and dissent I think
primarily it's about dissent you need

01:12:27.669 --> 01:12:27.679
primarily it's about dissent you need
 

01:12:27.679 --> 01:12:30.009
primarily it's about dissent you need
any implementation any algorithmic

01:12:30.009 --> 01:12:30.019
any implementation any algorithmic
 

01:12:30.019 --> 01:12:34.989
any implementation any algorithmic
implementation to have basically settled

01:12:34.989 --> 01:12:34.999
implementation to have basically settled
 

01:12:34.999 --> 01:12:37.120
implementation to have basically settled
avenues for for enabling dissent for

01:12:37.120 --> 01:12:37.130
avenues for for enabling dissent for
 

01:12:37.130 --> 01:12:40.719
avenues for for enabling dissent for
collecting dissent from the users part

01:12:40.719 --> 01:12:40.729
collecting dissent from the users part
 

01:12:40.729 --> 01:12:42.699
collecting dissent from the users part
of that dissent procedure will probably

01:12:42.699 --> 01:12:42.709
of that dissent procedure will probably
 

01:12:42.709 --> 01:12:44.649
of that dissent procedure will probably
require transparency at all the norms

01:12:44.649 --> 01:12:44.659
require transparency at all the norms
 

01:12:44.659 --> 01:12:45.910
require transparency at all the norms
governing the area throughout the domain

01:12:45.910 --> 01:12:45.920
governing the area throughout the domain
 

01:12:45.920 --> 01:12:48.669
governing the area throughout the domain
and thirdly I think this is the part

01:12:48.669 --> 01:12:48.679
and thirdly I think this is the part
 

01:12:48.679 --> 01:12:50.140
and thirdly I think this is the part
that feels especially in privacy

01:12:50.140 --> 01:12:50.150
that feels especially in privacy
 

01:12:50.150 --> 01:12:52.419
that feels especially in privacy
conversations you need methods for

01:12:52.419 --> 01:12:52.429
conversations you need methods for
 

01:12:52.429 --> 01:12:54.549
conversations you need methods for
accountable redress you need to be able

01:12:54.549 --> 01:12:54.559
accountable redress you need to be able
 

01:12:54.559 --> 01:12:57.870
accountable redress you need to be able
to have institutions be able to make and

01:12:57.870 --> 01:12:57.880
to have institutions be able to make and
 

01:12:57.880 --> 01:13:00.250
to have institutions be able to make and
keep promises you need incentives to

01:13:00.250 --> 01:13:00.260
keep promises you need incentives to
 

01:13:00.260 --> 01:13:02.589
keep promises you need incentives to
allow that to happen usually at least

01:13:02.589 --> 01:13:02.599
allow that to happen usually at least
 

01:13:02.599 --> 01:13:05.439
allow that to happen usually at least
according to GDP GDP GDP are basically

01:13:05.439 --> 01:13:05.449
according to GDP GDP GDP are basically
 

01:13:05.449 --> 01:13:07.779
according to GDP GDP GDP are basically
says or the way to do that is by having

01:13:07.779 --> 01:13:07.789
says or the way to do that is by having
 

01:13:07.789 --> 01:13:10.209
says or the way to do that is by having
these steep fines and I'm inclined to

01:13:10.209 --> 01:13:10.219
these steep fines and I'm inclined to
 

01:13:10.219 --> 01:13:12.729
these steep fines and I'm inclined to
agree because we have a history of data

01:13:12.729 --> 01:13:12.739
agree because we have a history of data
 

01:13:12.739 --> 01:13:14.919
agree because we have a history of data
breaches that have that has not abated

01:13:14.919 --> 01:13:14.929
breaches that have that has not abated
 

01:13:14.929 --> 01:13:17.020
breaches that have that has not abated
in spite of the small fines we've been

01:13:17.020 --> 01:13:17.030
in spite of the small fines we've been
 

01:13:17.030 --> 01:13:20.169
in spite of the small fines we've been
having so far these are the ideas I have

01:13:20.169 --> 01:13:20.179
having so far these are the ideas I have
 

01:13:20.179 --> 01:13:23.529
having so far these are the ideas I have
out there I I will admit that my

01:13:23.529 --> 01:13:23.539
out there I I will admit that my
 

01:13:23.539 --> 01:13:25.949
out there I I will admit that my
conversation has been from a very

01:13:25.949 --> 01:13:25.959
conversation has been from a very
 

01:13:25.959 --> 01:13:29.649
conversation has been from a very
allocated distribution perspective I I

01:13:29.649 --> 01:13:29.659
allocated distribution perspective I I
 

01:13:29.659 --> 01:13:31.500
allocated distribution perspective I I
want to acknowledge Kate Crawford

01:13:31.500 --> 01:13:31.510
want to acknowledge Kate Crawford
 

01:13:31.510 --> 01:13:34.299
want to acknowledge Kate Crawford
perspective that besides allocated

01:13:34.299 --> 01:13:34.309
perspective that besides allocated
 

01:13:34.309 --> 01:13:36.580
perspective that besides allocated
problems you have representational harms

01:13:36.580 --> 01:13:36.590
problems you have representational harms
 

01:13:36.590 --> 01:13:38.709
problems you have representational harms
that come from using algorithms in

01:13:38.709 --> 01:13:38.719
that come from using algorithms in
 

01:13:38.719 --> 01:13:42.459
that come from using algorithms in
society windows allocate renders when

01:13:42.459 --> 01:13:42.469
society windows allocate renders when
 

01:13:42.469 --> 01:13:45.520
society windows allocate renders when
algorithms that have that will present

01:13:45.520 --> 01:13:45.530
algorithms that have that will present
 

01:13:45.530 --> 01:13:47.979
algorithms that have that will present
the proper distribution and society this

01:13:47.979 --> 01:13:47.989
the proper distribution and society this
 

01:13:47.989 --> 01:13:49.689
the proper distribution and society this
disparate distributions of society are

01:13:49.689 --> 01:13:49.699
disparate distributions of society are
 

01:13:49.699 --> 01:13:52.270
disparate distributions of society are
now responsible for shaping our

01:13:52.270 --> 01:13:52.280
now responsible for shaping our
 

01:13:52.280 --> 01:13:53.890
now responsible for shaping our
preferences what does that mean long

01:13:53.890 --> 01:13:53.900
preferences what does that mean long
 

01:13:53.900 --> 01:13:58.470
preferences what does that mean long
term that's all I have for you

01:13:58.470 --> 01:13:58.480
term that's all I have for you
 

01:13:58.480 --> 01:14:04.220
term that's all I have for you
[Applause]

01:14:04.220 --> 01:14:04.230
[Applause]
 

01:14:04.230 --> 01:14:05.609
[Applause]
thank you

01:14:05.609 --> 01:14:05.619
thank you
 

01:14:05.619 --> 01:14:09.299
thank you
now we get to relax and talk take any

01:14:09.299 --> 01:14:09.309
now we get to relax and talk take any
 

01:14:09.309 --> 01:14:11.959
now we get to relax and talk take any
chair as long just one in the middle -

01:14:11.959 --> 01:14:11.969
chair as long just one in the middle -
 

01:14:11.969 --> 01:14:15.080
chair as long just one in the middle -
that's like a fair allocation problem

01:14:15.080 --> 01:14:15.090
that's like a fair allocation problem
 

01:14:15.090 --> 01:14:19.169
that's like a fair allocation problem
that's right so fascinating talk we have

01:14:19.169 --> 01:14:19.179
that's right so fascinating talk we have
 

01:14:19.179 --> 01:14:20.129
that's right so fascinating talk we have
a few questions we're gonna ask

01:14:20.129 --> 01:14:20.139
a few questions we're gonna ask
 

01:14:20.139 --> 01:14:21.540
a few questions we're gonna ask
everybody but I have to drill into what

01:14:21.540 --> 01:14:21.550
everybody but I have to drill into what
 

01:14:21.550 --> 01:14:24.510
everybody but I have to drill into what
you said first okay first of all you

01:14:24.510 --> 01:14:24.520
you said first okay first of all you
 

01:14:24.520 --> 01:14:25.560
you said first okay first of all you
know when you talked about weak and

01:14:25.560 --> 01:14:25.570
know when you talked about weak and
 

01:14:25.570 --> 01:14:27.330
know when you talked about weak and
strong impossibility there was a point

01:14:27.330 --> 01:14:27.340
strong impossibility there was a point
 

01:14:27.340 --> 01:14:28.680
strong impossibility there was a point
there that I think it's important to

01:14:28.680 --> 01:14:28.690
there that I think it's important to
 

01:14:28.690 --> 01:14:30.899
there that I think it's important to
reiterate for the audience so I want to

01:14:30.899 --> 01:14:30.909
reiterate for the audience so I want to
 

01:14:30.909 --> 01:14:31.890
reiterate for the audience so I want to
ask you to build out a little bit

01:14:31.890 --> 01:14:31.900
ask you to build out a little bit
 

01:14:31.900 --> 01:14:36.049
ask you to build out a little bit
because this idea that mathematically

01:14:36.049 --> 01:14:36.059
because this idea that mathematically
 

01:14:36.059 --> 01:14:38.580
because this idea that mathematically
achieving a computational way of

01:14:38.580 --> 01:14:38.590
achieving a computational way of
 

01:14:38.590 --> 01:14:40.260
achieving a computational way of
including the norms of equity such that

01:14:40.260 --> 01:14:40.270
including the norms of equity such that
 

01:14:40.270 --> 01:14:42.240
including the norms of equity such that
the computer system itself the robot the

01:14:42.240 --> 01:14:42.250
the computer system itself the robot the
 

01:14:42.250 --> 01:14:44.819
the computer system itself the robot the
AI system can execute and guarantee that

01:14:44.819 --> 01:14:44.829
AI system can execute and guarantee that
 

01:14:44.829 --> 01:14:46.500
AI system can execute and guarantee that
mm-hmm what I heard you say is that

01:14:46.500 --> 01:14:46.510
mm-hmm what I heard you say is that
 

01:14:46.510 --> 01:14:50.390
mm-hmm what I heard you say is that
that's essentially impossible yeah I

01:14:50.390 --> 01:14:50.400
that's essentially impossible yeah I
 

01:14:50.400 --> 01:14:52.740
that's essentially impossible yeah I
mean I mean we can all we can all talk

01:14:52.740 --> 01:14:52.750
mean I mean we can all we can all talk
 

01:14:52.750 --> 01:14:55.049
mean I mean we can all we can all talk
about okay we want equal we can oh wait

01:14:55.049 --> 01:14:55.059
about okay we want equal we can oh wait
 

01:14:55.059 --> 01:14:56.910
about okay we want equal we can oh wait
we can make arguments for certain equity

01:14:56.910 --> 01:14:56.920
we can make arguments for certain equity
 

01:14:56.920 --> 01:14:58.560
we can make arguments for certain equity
principles to be enforced in algorithms

01:14:58.560 --> 01:14:58.570
principles to be enforced in algorithms
 

01:14:58.570 --> 01:15:02.399
principles to be enforced in algorithms
but at some point a group is going to

01:15:02.399 --> 01:15:02.409
but at some point a group is going to
 

01:15:02.409 --> 01:15:04.799
but at some point a group is going to
argue and see actually the outcome on

01:15:04.799 --> 01:15:04.809
argue and see actually the outcome on
 

01:15:04.809 --> 01:15:06.899
argue and see actually the outcome on
our end is is violate what we think is

01:15:06.899 --> 01:15:06.909
our end is is violate what we think is
 

01:15:06.909 --> 01:15:08.430
our end is is violate what we think is
fair you're going to have to accommodate

01:15:08.430 --> 01:15:08.440
fair you're going to have to accommodate
 

01:15:08.440 --> 01:15:11.640
fair you're going to have to accommodate
that non theoretical that very practical

01:15:11.640 --> 01:15:11.650
that non theoretical that very practical
 

01:15:11.650 --> 01:15:13.919
that non theoretical that very practical
messy equity concern and that probably

01:15:13.919 --> 01:15:13.929
messy equity concern and that probably
 

01:15:13.929 --> 01:15:16.140
messy equity concern and that probably
means moving away from mathematically

01:15:16.140 --> 01:15:16.150
means moving away from mathematically
 

01:15:16.150 --> 01:15:18.479
means moving away from mathematically
precise definitions of equity and trying

01:15:18.479 --> 01:15:18.489
precise definitions of equity and trying
 

01:15:18.489 --> 01:15:22.140
precise definitions of equity and trying
to kind of engage with them I I think

01:15:22.140 --> 01:15:22.150
to kind of engage with them I I think
 

01:15:22.150 --> 01:15:25.500
to kind of engage with them I I think
the judgment hand he put it this way he

01:15:25.500 --> 01:15:25.510
the judgment hand he put it this way he
 

01:15:25.510 --> 01:15:26.850
the judgment hand he put it this way he
said equity concerns

01:15:26.850 --> 01:15:26.860
said equity concerns
 

01:15:26.860 --> 01:15:30.899
said equity concerns
well equity is really just about the the

01:15:30.899 --> 01:15:30.909
well equity is really just about the the
 

01:15:30.909 --> 01:15:33.379
well equity is really just about the the
the uncomfortable accommodation of

01:15:33.379 --> 01:15:33.389
the uncomfortable accommodation of
 

01:15:33.389 --> 01:15:35.939
the uncomfortable accommodation of
conflict to the society trying to do

01:15:35.939 --> 01:15:35.949
conflict to the society trying to do
 

01:15:35.949 --> 01:15:38.580
conflict to the society trying to do
that in a precise mathematical way might

01:15:38.580 --> 01:15:38.590
that in a precise mathematical way might
 

01:15:38.590 --> 01:15:41.280
that in a precise mathematical way might
not be a robust solution long term this

01:15:41.280 --> 01:15:41.290
not be a robust solution long term this
 

01:15:41.290 --> 01:15:43.709
not be a robust solution long term this
reminds me of talking to urban designers

01:15:43.709 --> 01:15:43.719
reminds me of talking to urban designers
 

01:15:43.719 --> 01:15:45.600
reminds me of talking to urban designers
who say there's this thing called the

01:15:45.600 --> 01:15:45.610
who say there's this thing called the
 

01:15:45.610 --> 01:15:47.790
who say there's this thing called the
wicked problem you can't optimize

01:15:47.790 --> 01:15:47.800
wicked problem you can't optimize
 

01:15:47.800 --> 01:15:49.890
wicked problem you can't optimize
because this is a social challenge

01:15:49.890 --> 01:15:49.900
because this is a social challenge
 

01:15:49.900 --> 01:15:52.709
because this is a social challenge
exactly and this also reminds me of the

01:15:52.709 --> 01:15:52.719
exactly and this also reminds me of the
 

01:15:52.719 --> 01:15:54.180
exactly and this also reminds me of the
defense argument this says we'll make

01:15:54.180 --> 01:15:54.190
defense argument this says we'll make
 

01:15:54.190 --> 01:15:56.280
defense argument this says we'll make
the perfect robot soldier argument

01:15:56.280 --> 01:15:56.290
the perfect robot soldier argument
 

01:15:56.290 --> 01:15:58.799
the perfect robot soldier argument
against which is no actually even war is

01:15:58.799 --> 01:15:58.809
against which is no actually even war is
 

01:15:58.809 --> 01:16:00.510
against which is no actually even war is
social we don't optimize it

01:16:00.510 --> 01:16:00.520
social we don't optimize it
 

01:16:00.520 --> 01:16:03.000
social we don't optimize it
quantitatively yep yep well so this is

01:16:03.000 --> 01:16:03.010
quantitatively yep yep well so this is
 

01:16:03.010 --> 01:16:04.890
quantitatively yep yep well so this is
interesting if it's the case that we

01:16:04.890 --> 01:16:04.900
interesting if it's the case that we
 

01:16:04.900 --> 01:16:07.439
interesting if it's the case that we
can't actually hope for an AI system

01:16:07.439 --> 01:16:07.449
can't actually hope for an AI system
 

01:16:07.449 --> 01:16:08.939
can't actually hope for an AI system
that will make the world equitable by

01:16:08.939 --> 01:16:08.949
that will make the world equitable by
 

01:16:08.949 --> 01:16:10.200
that will make the world equitable by
the very nature of how its programmed

01:16:10.200 --> 01:16:10.210
the very nature of how its programmed
 

01:16:10.210 --> 01:16:12.030
the very nature of how its programmed
then you talked about participatory

01:16:12.030 --> 01:16:12.040
then you talked about participatory
 

01:16:12.040 --> 01:16:15.299
then you talked about participatory
equity which I love except when I look

01:16:15.299 --> 01:16:15.309
equity which I love except when I look
 

01:16:15.309 --> 01:16:15.709
equity which I love except when I look
at this

01:16:15.709 --> 01:16:15.719
at this
 

01:16:15.719 --> 01:16:17.899
at this
like Facebook er AdWords on Google what

01:16:17.899 --> 01:16:17.909
like Facebook er AdWords on Google what
 

01:16:17.909 --> 01:16:20.779
like Facebook er AdWords on Google what
I see is participatory systems and what

01:16:20.779 --> 01:16:20.789
I see is participatory systems and what
 

01:16:20.789 --> 01:16:23.299
I see is participatory systems and what
I see in those systems is since no

01:16:23.299 --> 01:16:23.309
I see in those systems is since no
 

01:16:23.309 --> 01:16:25.520
I see in those systems is since no
system is equitably perfect so to speak

01:16:25.520 --> 01:16:25.530
system is equitably perfect so to speak
 

01:16:25.530 --> 01:16:28.250
system is equitably perfect so to speak
people find a way to hack them yep and

01:16:28.250 --> 01:16:28.260
people find a way to hack them yep and
 

01:16:28.260 --> 01:16:30.529
people find a way to hack them yep and
then use their own asymmetric levers of

01:16:30.529 --> 01:16:30.539
then use their own asymmetric levers of
 

01:16:30.539 --> 01:16:32.120
then use their own asymmetric levers of
power there to hack the very

01:16:32.120 --> 01:16:32.130
power there to hack the very
 

01:16:32.130 --> 01:16:33.649
power there to hack the very
participatory system you created to do

01:16:33.649 --> 01:16:33.659
participatory system you created to do
 

01:16:33.659 --> 01:16:35.750
participatory system you created to do
things like throw say I don't know an

01:16:35.750 --> 01:16:35.760
things like throw say I don't know an
 

01:16:35.760 --> 01:16:39.819
things like throw say I don't know an
election so that's not funny but ability

01:16:39.819 --> 01:16:39.829
election so that's not funny but ability
 

01:16:39.829 --> 01:16:44.810
election so that's not funny but ability
so I would argue that maybe the the the

01:16:44.810 --> 01:16:44.820
so I would argue that maybe the the the
 

01:16:44.820 --> 01:16:47.089
so I would argue that maybe the the the
issue is probably mechanism design

01:16:47.089 --> 01:16:47.099
issue is probably mechanism design
 

01:16:47.099 --> 01:16:50.209
issue is probably mechanism design
learning to to put in incentives and in

01:16:50.209 --> 01:16:50.219
learning to to put in incentives and in
 

01:16:50.219 --> 01:16:52.910
learning to to put in incentives and in
platforms that allow that allow for this

01:16:52.910 --> 01:16:52.920
platforms that allow that allow for this
 

01:16:52.920 --> 01:16:54.709
platforms that allow that allow for this
type of for correcting for these types

01:16:54.709 --> 01:16:54.719
type of for correcting for these types
 

01:16:54.719 --> 01:16:57.200
type of for correcting for these types
of evolutionary behaviors over time

01:16:57.200 --> 01:16:57.210
of evolutionary behaviors over time
 

01:16:57.210 --> 01:17:00.830
of evolutionary behaviors over time
trying to a priority design a perfect

01:17:00.830 --> 01:17:00.840
trying to a priority design a perfect
 

01:17:00.840 --> 01:17:03.020
trying to a priority design a perfect
system that's like you're saying it's

01:17:03.020 --> 01:17:03.030
system that's like you're saying it's
 

01:17:03.030 --> 01:17:04.850
system that's like you're saying it's
not going to happen so it's about insane

01:17:04.850 --> 01:17:04.860
not going to happen so it's about insane
 

01:17:04.860 --> 01:17:06.680
not going to happen so it's about insane
structural incentives creating an

01:17:06.680 --> 01:17:06.690
structural incentives creating an
 

01:17:06.690 --> 01:17:09.169
structural incentives creating an
infrastructure for for for modifying

01:17:09.169 --> 01:17:09.179
infrastructure for for for modifying
 

01:17:09.179 --> 01:17:13.009
infrastructure for for for modifying
them over time so then here's my fear I

01:17:13.009 --> 01:17:13.019
them over time so then here's my fear I
 

01:17:13.019 --> 01:17:14.450
them over time so then here's my fear I
totally get that we're gonna make

01:17:14.450 --> 01:17:14.460
totally get that we're gonna make
 

01:17:14.460 --> 01:17:16.129
totally get that we're gonna make
systems we'll make them as as good as we

01:17:16.129 --> 01:17:16.139
systems we'll make them as as good as we
 

01:17:16.139 --> 01:17:16.520
systems we'll make them as as good as we
can

01:17:16.520 --> 01:17:16.530
can
 

01:17:16.530 --> 01:17:18.200
can
people will misuse them they're only bad

01:17:18.200 --> 01:17:18.210
people will misuse them they're only bad
 

01:17:18.210 --> 01:17:20.330
people will misuse them they're only bad
actors and we're going to respond we're

01:17:20.330 --> 01:17:20.340
actors and we're going to respond we're
 

01:17:20.340 --> 01:17:22.790
actors and we're going to respond we're
gonna have a rapid reaction to that but

01:17:22.790 --> 01:17:22.800
gonna have a rapid reaction to that but
 

01:17:22.800 --> 01:17:25.310
gonna have a rapid reaction to that but
here's my concern the AI systems we're

01:17:25.310 --> 01:17:25.320
here's my concern the AI systems we're
 

01:17:25.320 --> 01:17:26.299
here's my concern the AI systems we're
talking about are inherently

01:17:26.299 --> 01:17:26.309
talking about are inherently
 

01:17:26.309 --> 01:17:29.029
talking about are inherently
accumulating and concentrating power and

01:17:29.029 --> 01:17:29.039
accumulating and concentrating power and
 

01:17:29.039 --> 01:17:32.120
accumulating and concentrating power and
knowledge which means the bad actors

01:17:32.120 --> 01:17:32.130
knowledge which means the bad actors
 

01:17:32.130 --> 01:17:35.629
knowledge which means the bad actors
ability to negatively influence reality

01:17:35.629 --> 01:17:35.639
ability to negatively influence reality
 

01:17:35.639 --> 01:17:39.290
ability to negatively influence reality
it's big and only bigger next year and

01:17:39.290 --> 01:17:39.300
it's big and only bigger next year and
 

01:17:39.300 --> 01:17:41.629
it's big and only bigger next year and
the year after here so it seems to me by

01:17:41.629 --> 01:17:41.639
the year after here so it seems to me by
 

01:17:41.639 --> 01:17:43.669
the year after here so it seems to me by
the time we catch them the crimes get

01:17:43.669 --> 01:17:43.679
the time we catch them the crimes get
 

01:17:43.679 --> 01:17:46.189
the time we catch them the crimes get
worse before we do the redress or the

01:17:46.189 --> 01:17:46.199
worse before we do the redress or the
 

01:17:46.199 --> 01:17:48.049
worse before we do the redress or the
things so don't we have this weird

01:17:48.049 --> 01:17:48.059
things so don't we have this weird
 

01:17:48.059 --> 01:17:50.919
things so don't we have this weird
runaway problem yes is it so my so I

01:17:50.919 --> 01:17:50.929
runaway problem yes is it so my so I
 

01:17:50.929 --> 01:17:53.479
runaway problem yes is it so my so I
this these are problems I think about

01:17:53.479 --> 01:17:53.489
this these are problems I think about
 

01:17:53.489 --> 01:17:55.850
this these are problems I think about
every day and I talk about with my with

01:17:55.850 --> 01:17:55.860
every day and I talk about with my with
 

01:17:55.860 --> 01:17:57.830
every day and I talk about with my with
fellow researchers at Rand and one of my

01:17:57.830 --> 01:17:57.840
fellow researchers at Rand and one of my
 

01:17:57.840 --> 01:17:59.330
fellow researchers at Rand and one of my
fellow research because I dislike the

01:17:59.330 --> 01:17:59.340
fellow research because I dislike the
 

01:17:59.340 --> 01:18:01.189
fellow research because I dislike the
solution right I don't see a way around

01:18:01.189 --> 01:18:01.199
solution right I don't see a way around
 

01:18:01.199 --> 01:18:03.229
solution right I don't see a way around
it one of my fellow researchers rant

01:18:03.229 --> 01:18:03.239
it one of my fellow researchers rant
 

01:18:03.239 --> 01:18:05.120
it one of my fellow researchers rant
Walsman he argues that the way to

01:18:05.120 --> 01:18:05.130
Walsman he argues that the way to
 

01:18:05.130 --> 01:18:09.200
Walsman he argues that the way to
address this is to have adversarial

01:18:09.200 --> 01:18:09.210
address this is to have adversarial
 

01:18:09.210 --> 01:18:12.080
address this is to have adversarial
reaction so the first person the bad

01:18:12.080 --> 01:18:12.090
reaction so the first person the bad
 

01:18:12.090 --> 01:18:14.419
reaction so the first person the bad
actors do something like flooding for

01:18:14.419 --> 01:18:14.429
actors do something like flooding for
 

01:18:14.429 --> 01:18:17.149
actors do something like flooding for
the net with with Reznick fake news and

01:18:17.149 --> 01:18:17.159
the net with with Reznick fake news and
 

01:18:17.159 --> 01:18:19.339
the net with with Reznick fake news and
the solution is for the good actors to

01:18:19.339 --> 01:18:19.349
the solution is for the good actors to
 

01:18:19.349 --> 01:18:22.779
the solution is for the good actors to
flood the net with resonant true-true

01:18:22.779 --> 01:18:22.789
flood the net with resonant true-true
 

01:18:22.789 --> 01:18:25.870
flood the net with resonant true-true
News and the idea is how to is basically

01:18:25.870 --> 01:18:25.880
News and the idea is how to is basically
 

01:18:25.880 --> 01:18:27.609
News and the idea is how to is basically
this back and forth between between

01:18:27.609 --> 01:18:27.619
this back and forth between between
 

01:18:27.619 --> 01:18:30.160
this back and forth between between
actors I see that as a runaway situation

01:18:30.160 --> 01:18:30.170
actors I see that as a runaway situation
 

01:18:30.170 --> 01:18:35.009
actors I see that as a runaway situation
yeah but thinking of solutions is hard

01:18:35.009 --> 01:18:35.019
yeah but thinking of solutions is hard
 

01:18:35.019 --> 01:18:37.479
yeah but thinking of solutions is hard
so that's really scares me that's a good

01:18:37.479 --> 01:18:37.489
so that's really scares me that's a good
 

01:18:37.489 --> 01:18:38.589
so that's really scares me that's a good
way to start the conference because

01:18:38.589 --> 01:18:38.599
way to start the conference because
 

01:18:38.599 --> 01:18:40.629
way to start the conference because
we're clearly into the conference so let

01:18:40.629 --> 01:18:40.639
we're clearly into the conference so let
 

01:18:40.639 --> 01:18:43.509
we're clearly into the conference so let
me jump on my extreme level of fear now

01:18:43.509 --> 01:18:43.519
me jump on my extreme level of fear now
 

01:18:43.519 --> 01:18:45.339
me jump on my extreme level of fear now
that I have to the three questions we're

01:18:45.339 --> 01:18:45.349
that I have to the three questions we're
 

01:18:45.349 --> 01:18:47.229
that I have to the three questions we're
hoping to ask everybody because this

01:18:47.229 --> 01:18:47.239
hoping to ask everybody because this
 

01:18:47.239 --> 01:18:49.000
hoping to ask everybody because this
gives us a neat thread to push

01:18:49.000 --> 01:18:49.010
gives us a neat thread to push
 

01:18:49.010 --> 01:18:50.890
gives us a neat thread to push
throughout the whole conference I think

01:18:50.890 --> 01:18:50.900
throughout the whole conference I think
 

01:18:50.900 --> 01:18:52.330
throughout the whole conference I think
you answered one of them already but I'm

01:18:52.330 --> 01:18:52.340
you answered one of them already but I'm
 

01:18:52.340 --> 01:18:53.859
you answered one of them already but I'm
getting ahead of myself

01:18:53.859 --> 01:18:53.869
getting ahead of myself
 

01:18:53.869 --> 01:18:55.899
getting ahead of myself
the first question I really want to

01:18:55.899 --> 01:18:55.909
the first question I really want to
 

01:18:55.909 --> 01:18:58.419
the first question I really want to
narrow your view and unfortunately cause

01:18:58.419 --> 01:18:58.429
narrow your view and unfortunately cause
 

01:18:58.429 --> 01:18:59.709
narrow your view and unfortunately cause
you to prognosticate a little bit into

01:18:59.709 --> 01:18:59.719
you to prognosticate a little bit into
 

01:18:59.719 --> 01:19:01.240
you to prognosticate a little bit into
the future I want to look at the next

01:19:01.240 --> 01:19:01.250
the future I want to look at the next
 

01:19:01.250 --> 01:19:03.129
the future I want to look at the next
ten years hmm and I want to ask the

01:19:03.129 --> 01:19:03.139
ten years hmm and I want to ask the
 

01:19:03.139 --> 01:19:04.779
ten years hmm and I want to ask the
question what is the thing you're most

01:19:04.779 --> 01:19:04.789
question what is the thing you're most
 

01:19:04.789 --> 01:19:07.299
question what is the thing you're most
excited about visa Vai and equity what

01:19:07.299 --> 01:19:07.309
excited about visa Vai and equity what
 

01:19:07.309 --> 01:19:08.740
excited about visa Vai and equity what
is it what is something you can imagine

01:19:08.740 --> 01:19:08.750
is it what is something you can imagine
 

01:19:08.750 --> 01:19:10.660
is it what is something you can imagine
that that's actually doable the next ten

01:19:10.660 --> 01:19:10.670
that that's actually doable the next ten
 

01:19:10.670 --> 01:19:13.270
that that's actually doable the next ten
years where AI can actually help with

01:19:13.270 --> 01:19:13.280
years where AI can actually help with
 

01:19:13.280 --> 01:19:18.490
years where AI can actually help with
questions of equity was a good one we

01:19:18.490 --> 01:19:18.500
questions of equity was a good one we
 

01:19:18.500 --> 01:19:19.600
questions of equity was a good one we
didn't share these with the speakers

01:19:19.600 --> 01:19:19.610
didn't share these with the speakers
 

01:19:19.610 --> 01:19:28.330
didn't share these with the speakers
ahead of time so I think we already seen

01:19:28.330 --> 01:19:28.340
ahead of time so I think we already seen
 

01:19:28.340 --> 01:19:30.040
ahead of time so I think we already seen
this happening maybe ten years is too

01:19:30.040 --> 01:19:30.050
this happening maybe ten years is too
 

01:19:30.050 --> 01:19:32.290
this happening maybe ten years is too
far out the issue of credit allocation

01:19:32.290 --> 01:19:32.300
far out the issue of credit allocation
 

01:19:32.300 --> 01:19:35.620
far out the issue of credit allocation
credit scoring for thin file for thin

01:19:35.620 --> 01:19:35.630
credit scoring for thin file for thin
 

01:19:35.630 --> 01:19:37.410
credit scoring for thin file for thin
flour subjects people who don't have

01:19:37.410 --> 01:19:37.420
flour subjects people who don't have
 

01:19:37.420 --> 01:19:40.660
flour subjects people who don't have
large records in large formal records in

01:19:40.660 --> 01:19:40.670
large records in large formal records in
 

01:19:40.670 --> 01:19:42.339
large records in large formal records in
the economic sectors so this would be

01:19:42.339 --> 01:19:42.349
the economic sectors so this would be
 

01:19:42.349 --> 01:19:44.169
the economic sectors so this would be
like people in developing countries poor

01:19:44.169 --> 01:19:44.179
like people in developing countries poor
 

01:19:44.179 --> 01:19:47.109
like people in developing countries poor
people the ability to use artificial

01:19:47.109 --> 01:19:47.119
people the ability to use artificial
 

01:19:47.119 --> 01:19:50.879
people the ability to use artificial
intelligence to I tried to use the term

01:19:50.879 --> 01:19:50.889
intelligence to I tried to use the term
 

01:19:50.889 --> 01:19:52.870
intelligence to I tried to use the term
algorithm intelligence as often as I can

01:19:52.870 --> 01:19:52.880
algorithm intelligence as often as I can
 

01:19:52.880 --> 01:19:54.490
algorithm intelligence as often as I can
the ability to use machine learning

01:19:54.490 --> 01:19:54.500
the ability to use machine learning
 

01:19:54.500 --> 01:19:56.830
the ability to use machine learning
methods to learn about credit score

01:19:56.830 --> 01:19:56.840
methods to learn about credit score
 

01:19:56.840 --> 01:19:58.930
methods to learn about credit score
about credit worthiness from all the

01:19:58.930 --> 01:19:58.940
about credit worthiness from all the
 

01:19:58.940 --> 01:20:02.049
about credit worthiness from all the
sources of data seems to me extremely

01:20:02.049 --> 01:20:02.059
sources of data seems to me extremely
 

01:20:02.059 --> 01:20:04.359
sources of data seems to me extremely
powerful well if if properly applied and

01:20:04.359 --> 01:20:04.369
powerful well if if properly applied and
 

01:20:04.369 --> 01:20:07.120
powerful well if if properly applied and
properly supervised because you can also

01:20:07.120 --> 01:20:07.130
properly supervised because you can also
 

01:20:07.130 --> 01:20:09.160
properly supervised because you can also
use the type of scoring mechanism for

01:20:09.160 --> 01:20:09.170
use the type of scoring mechanism for
 

01:20:09.170 --> 01:20:11.169
use the type of scoring mechanism for
things like the China the Chinese

01:20:11.169 --> 01:20:11.179
things like the China the Chinese
 

01:20:11.179 --> 01:20:13.870
things like the China the Chinese
approach of social scoring but just

01:20:13.870 --> 01:20:13.880
approach of social scoring but just
 

01:20:13.880 --> 01:20:16.660
approach of social scoring but just
enabling better credit to people in

01:20:16.660 --> 01:20:16.670
enabling better credit to people in
 

01:20:16.670 --> 01:20:19.240
enabling better credit to people in
poorer situations he's a huge

01:20:19.240 --> 01:20:19.250
poorer situations he's a huge
 

01:20:19.250 --> 01:20:22.029
poorer situations he's a huge
transformative fit I think and I love

01:20:22.029 --> 01:20:22.039
transformative fit I think and I love
 

01:20:22.039 --> 01:20:23.379
transformative fit I think and I love
that answer because it also brings into

01:20:23.379 --> 01:20:23.389
that answer because it also brings into
 

01:20:23.389 --> 01:20:24.729
that answer because it also brings into
the play the whole question of

01:20:24.729 --> 01:20:24.739
the play the whole question of
 

01:20:24.739 --> 01:20:26.109
the play the whole question of
developing world and ways in which

01:20:26.109 --> 01:20:26.119
developing world and ways in which
 

01:20:26.119 --> 01:20:27.700
developing world and ways in which
developing world may access what we're

01:20:27.700 --> 01:20:27.710
developing world may access what we're
 

01:20:27.710 --> 01:20:29.859
developing world may access what we're
talking about so the second question is

01:20:29.859 --> 01:20:29.869
talking about so the second question is
 

01:20:29.869 --> 01:20:31.959
talking about so the second question is
the exact opposite again limiting

01:20:31.959 --> 01:20:31.969
the exact opposite again limiting
 

01:20:31.969 --> 01:20:34.540
the exact opposite again limiting
ourselves to ten years or so what is it

01:20:34.540 --> 01:20:34.550
ourselves to ten years or so what is it
 

01:20:34.550 --> 01:20:35.780
ourselves to ten years or so what is it
that most

01:20:35.780 --> 01:20:35.790
that most
 

01:20:35.790 --> 01:20:37.730
that most
keeps you up at night in terms of equity

01:20:37.730 --> 01:20:37.740
keeps you up at night in terms of equity
 

01:20:37.740 --> 01:20:42.530
keeps you up at night in terms of equity
and AI mmm mine don't sleep regularly so

01:20:42.530 --> 01:20:42.540
and AI mmm mine don't sleep regularly so
 

01:20:42.540 --> 01:20:49.340
and AI mmm mine don't sleep regularly so
you know I tend to avoid projects

01:20:49.340 --> 01:20:49.350
you know I tend to avoid projects
 

01:20:49.350 --> 01:20:52.790
you know I tend to avoid projects
involve in heart defense because it

01:20:52.790 --> 01:20:52.800
involve in heart defense because it
 

01:20:52.800 --> 01:20:56.000
involve in heart defense because it
makes me really anxious and the question

01:20:56.000 --> 01:20:56.010
makes me really anxious and the question
 

01:20:56.010 --> 01:20:58.490
makes me really anxious and the question
of deterrence in a world in which we

01:20:58.490 --> 01:20:58.500
of deterrence in a world in which we
 

01:20:58.500 --> 01:21:02.840
of deterrence in a world in which we
have a I enabled weapons makes me

01:21:02.840 --> 01:21:02.850
have a I enabled weapons makes me
 

01:21:02.850 --> 01:21:06.380
have a I enabled weapons makes me
worried and worse than that it's not

01:21:06.380 --> 01:21:06.390
worried and worse than that it's not
 

01:21:06.390 --> 01:21:07.940
worried and worse than that it's not
just the existence of the weapons that

01:21:07.940 --> 01:21:07.950
just the existence of the weapons that
 

01:21:07.950 --> 01:21:09.680
just the existence of the weapons that
makes me worried its take is the

01:21:09.680 --> 01:21:09.690
makes me worried its take is the
 

01:21:09.690 --> 01:21:12.710
makes me worried its take is the
differences in the culture the cultures

01:21:12.710 --> 01:21:12.720
differences in the culture the cultures
 

01:21:12.720 --> 01:21:14.930
differences in the culture the cultures
deploying those weapons if you think

01:21:14.930 --> 01:21:14.940
deploying those weapons if you think
 

01:21:14.940 --> 01:21:16.910
deploying those weapons if you think
about the Chinese the Chinese our

01:21:16.910 --> 01:21:16.920
about the Chinese the Chinese our
 

01:21:16.920 --> 01:21:18.500
about the Chinese the Chinese our
approach to defense in the American

01:21:18.500 --> 01:21:18.510
approach to defense in the American
 

01:21:18.510 --> 01:21:20.900
approach to defense in the American
approach to defense you have basically

01:21:20.900 --> 01:21:20.910
approach to defense you have basically
 

01:21:20.910 --> 01:21:24.530
approach to defense you have basically
China seeming to be more trusting of

01:21:24.530 --> 01:21:24.540
China seeming to be more trusting of
 

01:21:24.540 --> 01:21:28.100
China seeming to be more trusting of
just wholesale fully automated weapons

01:21:28.100 --> 01:21:28.110
just wholesale fully automated weapons
 

01:21:28.110 --> 01:21:31.340
just wholesale fully automated weapons
and you have United States doctrine say

01:21:31.340 --> 01:21:31.350
and you have United States doctrine say
 

01:21:31.350 --> 01:21:32.870
and you have United States doctrine say
something to the effect that will always

01:21:32.870 --> 01:21:32.880
something to the effect that will always
 

01:21:32.880 --> 01:21:35.390
something to the effect that will always
have humans in the loop that that that

01:21:35.390 --> 01:21:35.400
have humans in the loop that that that
 

01:21:35.400 --> 01:21:37.640
have humans in the loop that that that
makes sense from that seems comfortable

01:21:37.640 --> 01:21:37.650
makes sense from that seems comfortable
 

01:21:37.650 --> 01:21:40.100
makes sense from that seems comfortable
comforting from a unilateral perspective

01:21:40.100 --> 01:21:40.110
comforting from a unilateral perspective
 

01:21:40.110 --> 01:21:42.800
comforting from a unilateral perspective
but imagine a situation in which a

01:21:42.800 --> 01:21:42.810
but imagine a situation in which a
 

01:21:42.810 --> 01:21:45.230
but imagine a situation in which a
country has human loop for all the

01:21:45.230 --> 01:21:45.240
country has human loop for all the
 

01:21:45.240 --> 01:21:47.960
country has human loop for all the
autonomous systems and the other country

01:21:47.960 --> 01:21:47.970
autonomous systems and the other country
 

01:21:47.970 --> 01:21:51.050
autonomous systems and the other country
just doesn't care that seems to have

01:21:51.050 --> 01:21:51.060
just doesn't care that seems to have
 

01:21:51.060 --> 01:21:53.060
just doesn't care that seems to have
deterrent implications that are

01:21:53.060 --> 01:21:53.070
deterrent implications that are
 

01:21:53.070 --> 01:21:54.830
deterrent implications that are
difficult to think about so lack of

01:21:54.830 --> 01:21:54.840
difficult to think about so lack of
 

01:21:54.840 --> 01:21:57.260
difficult to think about so lack of
parity may actually force a good actor

01:21:57.260 --> 01:21:57.270
parity may actually force a good actor
 

01:21:57.270 --> 01:21:59.990
parity may actually force a good actor
to become less good yep indeed last

01:21:59.990 --> 01:22:00.000
to become less good yep indeed last
 

01:22:00.000 --> 01:22:02.330
to become less good yep indeed last
question for you is an interesting

01:22:02.330 --> 01:22:02.340
question for you is an interesting
 

01:22:02.340 --> 01:22:03.740
question for you is an interesting
future in question that's kind of

01:22:03.740 --> 01:22:03.750
future in question that's kind of
 

01:22:03.750 --> 01:22:05.870
future in question that's kind of
navel-gazing about this conference so

01:22:05.870 --> 01:22:05.880
navel-gazing about this conference so
 

01:22:05.880 --> 01:22:07.580
navel-gazing about this conference so
imagine we're here 15 years from now

01:22:07.580 --> 01:22:07.590
imagine we're here 15 years from now
 

01:22:07.590 --> 01:22:09.560
imagine we're here 15 years from now
having this conference mmm how will it

01:22:09.560 --> 01:22:09.570
having this conference mmm how will it
 

01:22:09.570 --> 01:22:13.750
having this conference mmm how will it
look or feel different because of AI I

01:22:13.750 --> 01:22:13.760
look or feel different because of AI I
 

01:22:13.760 --> 01:22:15.890
look or feel different because of AI I
assume most people would not actually be

01:22:15.890 --> 01:22:15.900
assume most people would not actually be
 

01:22:15.900 --> 01:22:18.350
assume most people would not actually be
here would have bots in telepresence

01:22:18.350 --> 01:22:18.360
here would have bots in telepresence
 

01:22:18.360 --> 01:22:20.180
here would have bots in telepresence
always be talking to a set of screens

01:22:20.180 --> 01:22:20.190
always be talking to a set of screens
 

01:22:20.190 --> 01:22:25.160
always be talking to a set of screens
here yeah III like this idea and maybe I

01:22:25.160 --> 01:22:25.170
here yeah III like this idea and maybe I
 

01:22:25.170 --> 01:22:26.510
here yeah III like this idea and maybe I
mean I'm trying to imagine how that

01:22:26.510 --> 01:22:26.520
mean I'm trying to imagine how that
 

01:22:26.520 --> 01:22:28.100
mean I'm trying to imagine how that
would apply in a conference setting I

01:22:28.100 --> 01:22:28.110
would apply in a conference setting I
 

01:22:28.110 --> 01:22:30.290
would apply in a conference setting I
like this idea of our clock on traumas

01:22:30.290 --> 01:22:30.300
like this idea of our clock on traumas
 

01:22:30.300 --> 01:22:32.270
like this idea of our clock on traumas
extended mind I feel like most of us

01:22:32.270 --> 01:22:32.280
extended mind I feel like most of us
 

01:22:32.280 --> 01:22:34.430
extended mind I feel like most of us
will be more invested in the tools that

01:22:34.430 --> 01:22:34.440
will be more invested in the tools that
 

01:22:34.440 --> 01:22:36.350
will be more invested in the tools that
enable us to engage with the world and

01:22:36.350 --> 01:22:36.360
enable us to engage with the world and
 

01:22:36.360 --> 01:22:39.620
enable us to engage with the world and
so maybe we'll have augmented brains of

01:22:39.620 --> 01:22:39.630
so maybe we'll have augmented brains of
 

01:22:39.630 --> 01:22:42.350
so maybe we'll have augmented brains of
some sort that make us faster thinkers I

01:22:42.350 --> 01:22:42.360
some sort that make us faster thinkers I
 

01:22:42.360 --> 01:22:44.000
some sort that make us faster thinkers I
always think about how to think faster

01:22:44.000 --> 01:22:44.010
always think about how to think faster
 

01:22:44.010 --> 01:22:45.530
always think about how to think faster
how to think more creatively and I

01:22:45.530 --> 01:22:45.540
how to think more creatively and I
 

01:22:45.540 --> 01:22:47.000
how to think more creatively and I
imagine that artificial intelligence

01:22:47.000 --> 01:22:47.010
imagine that artificial intelligence
 

01:22:47.010 --> 01:22:47.959
imagine that artificial intelligence
will probably enabled

01:22:47.959 --> 01:22:47.969
will probably enabled
 

01:22:47.969 --> 01:22:49.700
will probably enabled
the future so cognitive orthotics for us

01:22:49.700 --> 01:22:49.710
the future so cognitive orthotics for us
 

01:22:49.710 --> 01:22:51.800
the future so cognitive orthotics for us
all maybe yeah excellent well thank you

01:22:51.800 --> 01:22:51.810
all maybe yeah excellent well thank you
 

01:22:51.810 --> 01:23:00.850
all maybe yeah excellent well thank you
so much for joining us wonderful speech

01:23:00.850 --> 01:23:00.860
 

01:23:00.860 --> 01:23:03.260
we're gonna be heading now to a panel

01:23:03.260 --> 01:23:03.270
we're gonna be heading now to a panel
 

01:23:03.270 --> 01:23:05.600
we're gonna be heading now to a panel
discussion that lets us actually relax

01:23:05.600 --> 01:23:05.610
discussion that lets us actually relax
 

01:23:05.610 --> 01:23:07.040
discussion that lets us actually relax
and hear more on the issue of equity

01:23:07.040 --> 01:23:07.050
and hear more on the issue of equity
 

01:23:07.050 --> 01:23:10.370
and hear more on the issue of equity
with a wonderful panel of guests but can

01:23:10.370 --> 01:23:10.380
with a wonderful panel of guests but can
 

01:23:10.380 --> 01:23:11.060
with a wonderful panel of guests but can
I bring up the slide

01:23:11.060 --> 01:23:11.070
I bring up the slide
 

01:23:11.070 --> 01:23:13.160
I bring up the slide
Oh slides so we can let folks know how

01:23:13.160 --> 01:23:13.170
Oh slides so we can let folks know how
 

01:23:13.170 --> 01:23:16.160
Oh slides so we can let folks know how
they're gonna be using slide oh if I'm

01:23:16.160 --> 01:23:16.170
they're gonna be using slide oh if I'm
 

01:23:16.170 --> 01:23:18.620
they're gonna be using slide oh if I'm
even saying its name correctly slider is

01:23:18.620 --> 01:23:18.630
even saying its name correctly slider is
 

01:23:18.630 --> 01:23:20.720
even saying its name correctly slider is
a technology that's going to allow you

01:23:20.720 --> 01:23:20.730
a technology that's going to allow you
 

01:23:20.730 --> 01:23:23.689
a technology that's going to allow you
to ask questions our journalist who is

01:23:23.689 --> 01:23:23.699
to ask questions our journalist who is
 

01:23:23.699 --> 01:23:25.040
to ask questions our journalist who is
the moderator for this panel will see

01:23:25.040 --> 01:23:25.050
the moderator for this panel will see
 

01:23:25.050 --> 01:23:27.380
the moderator for this panel will see
your questions on her screen and so she

01:23:27.380 --> 01:23:27.390
your questions on her screen and so she
 

01:23:27.390 --> 01:23:28.700
your questions on her screen and so she
can pick and choose from amongst the

01:23:28.700 --> 01:23:28.710
can pick and choose from amongst the
 

01:23:28.710 --> 01:23:30.620
can pick and choose from amongst the
common features of the questions to

01:23:30.620 --> 01:23:30.630
common features of the questions to
 

01:23:30.630 --> 01:23:32.630
common features of the questions to
bring your comments and questions into

01:23:32.630 --> 01:23:32.640
bring your comments and questions into
 

01:23:32.640 --> 01:23:33.920
bring your comments and questions into
the conversation she's having with the

01:23:33.920 --> 01:23:33.930
the conversation she's having with the
 

01:23:33.930 --> 01:23:36.770
the conversation she's having with the
panel so what you're seeing is the

01:23:36.770 --> 01:23:36.780
panel so what you're seeing is the
 

01:23:36.780 --> 01:23:38.360
panel so what you're seeing is the
website for slide oh you can go to that

01:23:38.360 --> 01:23:38.370
website for slide oh you can go to that
 

01:23:38.370 --> 01:23:40.520
website for slide oh you can go to that
with any mobile device once you get

01:23:40.520 --> 01:23:40.530
with any mobile device once you get
 

01:23:40.530 --> 01:23:42.920
with any mobile device once you get
there you simply enter that hashtag and

01:23:42.920 --> 01:23:42.930
there you simply enter that hashtag and
 

01:23:42.930 --> 01:23:44.900
there you simply enter that hashtag and
when you do so then all the questions

01:23:44.900 --> 01:23:44.910
when you do so then all the questions
 

01:23:44.910 --> 01:23:46.580
when you do so then all the questions
you ask will show up on the handy-dandy

01:23:46.580 --> 01:23:46.590
you ask will show up on the handy-dandy
 

01:23:46.590 --> 01:23:48.920
you ask will show up on the handy-dandy
tablet that we're providing to our

01:23:48.920 --> 01:23:48.930
tablet that we're providing to our
 

01:23:48.930 --> 01:23:51.110
tablet that we're providing to our
moderator so without further ado I'll be

01:23:51.110 --> 01:23:51.120
moderator so without further ado I'll be
 

01:23:51.120 --> 01:23:52.850
moderator so without further ado I'll be
introducing the moderator who in turn

01:23:52.850 --> 01:23:52.860
introducing the moderator who in turn
 

01:23:52.860 --> 01:23:55.970
introducing the moderator who in turn
will introduce the panel as a whole I'm

01:23:55.970 --> 01:23:55.980
will introduce the panel as a whole I'm
 

01:23:55.980 --> 01:23:57.439
will introduce the panel as a whole I'm
really pleased to introduce my precious

01:23:57.439 --> 01:23:57.449
really pleased to introduce my precious
 

01:23:57.449 --> 01:23:59.120
really pleased to introduce my precious
singer because I have been reading

01:23:59.120 --> 01:23:59.130
singer because I have been reading
 

01:23:59.130 --> 01:24:00.890
singer because I have been reading
natashia singers articles for years

01:24:00.890 --> 01:24:00.900
natashia singers articles for years
 

01:24:00.900 --> 01:24:02.750
natashia singers articles for years
she's a technology reporter for the New

01:24:02.750 --> 01:24:02.760
she's a technology reporter for the New
 

01:24:02.760 --> 01:24:04.459
she's a technology reporter for the New
York Times and she covers health

01:24:04.459 --> 01:24:04.469
York Times and she covers health
 

01:24:04.469 --> 01:24:06.320
York Times and she covers health
technology education technology and

01:24:06.320 --> 01:24:06.330
technology education technology and
 

01:24:06.330 --> 01:24:08.390
technology education technology and
consumer privacy she's reporting on how

01:24:08.390 --> 01:24:08.400
consumer privacy she's reporting on how
 

01:24:08.400 --> 01:24:09.680
consumer privacy she's reporting on how
tech companies are developing digital

01:24:09.680 --> 01:24:09.690
tech companies are developing digital
 

01:24:09.690 --> 01:24:11.060
tech companies are developing digital
health tools and the implications

01:24:11.060 --> 01:24:11.070
health tools and the implications
 

01:24:11.070 --> 01:24:13.250
health tools and the implications
dead-house for healthcare industry and

01:24:13.250 --> 01:24:13.260
dead-house for healthcare industry and
 

01:24:13.260 --> 01:24:15.709
dead-house for healthcare industry and
for privacy for medical practice and for

01:24:15.709 --> 01:24:15.719
for privacy for medical practice and for
 

01:24:15.719 --> 01:24:18.229
for privacy for medical practice and for
consumers and I believe she's a teacher

01:24:18.229 --> 01:24:18.239
consumers and I believe she's a teacher
 

01:24:18.239 --> 01:24:19.939
consumers and I believe she's a teacher
too so she's one of us fellow professors

01:24:19.939 --> 01:24:19.949
too so she's one of us fellow professors
 

01:24:19.949 --> 01:24:21.410
too so she's one of us fellow professors
she has taught even a high school course

01:24:21.410 --> 01:24:21.420
she has taught even a high school course
 

01:24:21.420 --> 01:24:23.570
she has taught even a high school course
on technology innovation ethics at the

01:24:23.570 --> 01:24:23.580
on technology innovation ethics at the
 

01:24:23.580 --> 01:24:25.610
on technology innovation ethics at the
school of the New York Times pre-college

01:24:25.610 --> 01:24:25.620
school of the New York Times pre-college
 

01:24:25.620 --> 01:24:27.709
school of the New York Times pre-college
program the New York Times runs so thank

01:24:27.709 --> 01:24:27.719
program the New York Times runs so thank
 

01:24:27.719 --> 01:24:29.390
program the New York Times runs so thank
you for joining us Natasha thank you for

01:24:29.390 --> 01:24:29.400
you for joining us Natasha thank you for
 

01:24:29.400 --> 01:24:36.930
you for joining us Natasha thank you for
having me

01:24:36.930 --> 01:24:36.940
 

01:24:36.940 --> 01:24:39.310
so I'm just gonna briefly announce the

01:24:39.310 --> 01:24:39.320
so I'm just gonna briefly announce the
 

01:24:39.320 --> 01:24:41.110
so I'm just gonna briefly announce the
panelists and then I'm gonna invite them

01:24:41.110 --> 01:24:41.120
panelists and then I'm gonna invite them
 

01:24:41.120 --> 01:24:43.120
panelists and then I'm gonna invite them
to come up and talk for a minute about

01:24:43.120 --> 01:24:43.130
to come up and talk for a minute about
 

01:24:43.130 --> 01:24:46.200
to come up and talk for a minute about
their work and how it relates to AI and

01:24:46.200 --> 01:24:46.210
their work and how it relates to AI and
 

01:24:46.210 --> 01:24:49.480
their work and how it relates to AI and
I'm delighted that we have kind of such

01:24:49.480 --> 01:24:49.490
I'm delighted that we have kind of such
 

01:24:49.490 --> 01:24:53.980
I'm delighted that we have kind of such
a range of views so first is Michael

01:24:53.980 --> 01:24:53.990
a range of views so first is Michael
 

01:24:53.990 --> 01:24:57.520
a range of views so first is Michael
scorpion who is the co-founder of

01:24:57.520 --> 01:24:57.530
scorpion who is the co-founder of
 

01:24:57.530 --> 01:24:59.710
scorpion who is the co-founder of
community forge which is a nonprofit

01:24:59.710 --> 01:24:59.720
community forge which is a nonprofit
 

01:24:59.720 --> 01:25:03.040
community forge which is a nonprofit
that has bought a former school and is

01:25:03.040 --> 01:25:03.050
that has bought a former school and is
 

01:25:03.050 --> 01:25:04.930
that has bought a former school and is
revamping it into a kind of incredible

01:25:04.930 --> 01:25:04.940
revamping it into a kind of incredible
 

01:25:04.940 --> 01:25:07.990
revamping it into a kind of incredible
multi-use community center and we have

01:25:07.990 --> 01:25:08.000
multi-use community center and we have
 

01:25:08.000 --> 01:25:10.750
multi-use community center and we have
Rajesh ratha who is the chief innovation

01:25:10.750 --> 01:25:10.760
Rajesh ratha who is the chief innovation
 

01:25:10.760 --> 01:25:13.750
Rajesh ratha who is the chief innovation
officer of the University of Pittsburgh

01:25:13.750 --> 01:25:13.760
officer of the University of Pittsburgh
 

01:25:13.760 --> 01:25:17.050
officer of the University of Pittsburgh
Medical Center and we have Alexandra

01:25:17.050 --> 01:25:17.060
Medical Center and we have Alexandra
 

01:25:17.060 --> 01:25:18.790
Medical Center and we have Alexandra
Childers OVA who is an assistant

01:25:18.790 --> 01:25:18.800
Childers OVA who is an assistant
 

01:25:18.800 --> 01:25:21.010
Childers OVA who is an assistant
professor of statistics and public

01:25:21.010 --> 01:25:21.020
professor of statistics and public
 

01:25:21.020 --> 01:25:24.070
professor of statistics and public
policy here at CMU so why don't you guys

01:25:24.070 --> 01:25:24.080
policy here at CMU so why don't you guys
 

01:25:24.080 --> 01:25:29.500
policy here at CMU so why don't you guys
come up and then Moll

01:25:29.500 --> 01:25:29.510
 

01:25:29.510 --> 01:25:50.669
[Applause]

01:25:50.669 --> 01:25:50.679
 

01:25:50.679 --> 01:25:56.189
so Alex do you want to start just one

01:25:56.189 --> 01:25:56.199
so Alex do you want to start just one
 

01:25:56.199 --> 01:25:57.899
so Alex do you want to start just one
minute about each of yourselves and then

01:25:57.899 --> 01:25:57.909
minute about each of yourselves and then
 

01:25:57.909 --> 01:25:59.820
minute about each of yourselves and then
we'll get started with hashing out these

01:25:59.820 --> 01:25:59.830
we'll get started with hashing out these
 

01:25:59.830 --> 01:26:02.280
we'll get started with hashing out these
big questions introduce lesson starting

01:26:02.280 --> 01:26:02.290
big questions introduce lesson starting
 

01:26:02.290 --> 01:26:04.080
big questions introduce lesson starting
first I think everyone can hear me in

01:26:04.080 --> 01:26:04.090
first I think everyone can hear me in
 

01:26:04.090 --> 01:26:06.930
first I think everyone can hear me in
the room is that correct great closer to

01:26:06.930 --> 01:26:06.940
the room is that correct great closer to
 

01:26:06.940 --> 01:26:08.970
the room is that correct great closer to
the mic excellent

01:26:08.970 --> 01:26:08.980
the mic excellent
 

01:26:08.980 --> 01:26:11.880
the mic excellent
so I'm Alex shoulder shava as Natasha

01:26:11.880 --> 01:26:11.890
so I'm Alex shoulder shava as Natasha
 

01:26:11.890 --> 01:26:13.770
so I'm Alex shoulder shava as Natasha
said I'm an assistant professor here at

01:26:13.770 --> 01:26:13.780
said I'm an assistant professor here at
 

01:26:13.780 --> 01:26:16.350
said I'm an assistant professor here at
Carnegie Mellon University and I'm a

01:26:16.350 --> 01:26:16.360
Carnegie Mellon University and I'm a
 

01:26:16.360 --> 01:26:19.229
Carnegie Mellon University and I'm a
statistician by training most of my work

01:26:19.229 --> 01:26:19.239
statistician by training most of my work
 

01:26:19.239 --> 01:26:22.350
statistician by training most of my work
these days concerns the use of AI or

01:26:22.350 --> 01:26:22.360
these days concerns the use of AI or
 

01:26:22.360 --> 01:26:24.270
these days concerns the use of AI or
really risk assessment tools in the

01:26:24.270 --> 01:26:24.280
really risk assessment tools in the
 

01:26:24.280 --> 01:26:26.149
really risk assessment tools in the
context of government operations

01:26:26.149 --> 01:26:26.159
context of government operations
 

01:26:26.159 --> 01:26:29.220
context of government operations
specifically I look at how these tools

01:26:29.220 --> 01:26:29.230
specifically I look at how these tools
 

01:26:29.230 --> 01:26:30.750
specifically I look at how these tools
are being used in high-stakes

01:26:30.750 --> 01:26:30.760
are being used in high-stakes
 

01:26:30.760 --> 01:26:32.939
are being used in high-stakes
applications such as criminal justice

01:26:32.939 --> 01:26:32.949
applications such as criminal justice
 

01:26:32.949 --> 01:26:35.640
applications such as criminal justice
and human services as a previous speaker

01:26:35.640 --> 01:26:35.650
and human services as a previous speaker
 

01:26:35.650 --> 01:26:38.399
and human services as a previous speaker
mentioned I've done some work motivated

01:26:38.399 --> 01:26:38.409
mentioned I've done some work motivated
 

01:26:38.409 --> 01:26:41.700
mentioned I've done some work motivated
by some of the public analyses and one

01:26:41.700 --> 01:26:41.710
by some of the public analyses and one
 

01:26:41.710 --> 01:26:43.200
by some of the public analyses and one
of the things that I try to challenge as

01:26:43.200 --> 01:26:43.210
of the things that I try to challenge as
 

01:26:43.210 --> 01:26:46.260
of the things that I try to challenge as
a statistician is this adage that if you

01:26:46.260 --> 01:26:46.270
a statistician is this adage that if you
 

01:26:46.270 --> 01:26:48.990
a statistician is this adage that if you
have bias in you have bias out which has

01:26:48.990 --> 01:26:49.000
have bias in you have bias out which has
 

01:26:49.000 --> 01:26:51.300
have bias in you have bias out which has
come out of this idea of garbage in

01:26:51.300 --> 01:26:51.310
come out of this idea of garbage in
 

01:26:51.310 --> 01:26:53.970
come out of this idea of garbage in
garbage out and the way I think that we

01:26:53.970 --> 01:26:53.980
garbage out and the way I think that we
 

01:26:53.980 --> 01:26:55.620
garbage out and the way I think that we
can challenge this is that to the extent

01:26:55.620 --> 01:26:55.630
can challenge this is that to the extent
 

01:26:55.630 --> 01:26:58.020
can challenge this is that to the extent
that we really understand how is it that

01:26:58.020 --> 01:26:58.030
that we really understand how is it that
 

01:26:58.030 --> 01:27:00.419
that we really understand how is it that
the data that we're using as the basis

01:27:00.419 --> 01:27:00.429
the data that we're using as the basis
 

01:27:00.429 --> 01:27:02.340
the data that we're using as the basis
for our systems how did that become

01:27:02.340 --> 01:27:02.350
for our systems how did that become
 

01:27:02.350 --> 01:27:05.250
for our systems how did that become
biased we can make some Corrections and

01:27:05.250 --> 01:27:05.260
biased we can make some Corrections and
 

01:27:05.260 --> 01:27:08.459
biased we can make some Corrections and
really move well well extract the

01:27:08.459 --> 01:27:08.469
really move well well extract the
 

01:27:08.469 --> 01:27:10.350
really move well well extract the
information that we need and move

01:27:10.350 --> 01:27:10.360
information that we need and move
 

01:27:10.360 --> 01:27:15.030
information that we need and move
towards more equitable solutions hi

01:27:15.030 --> 01:27:15.040
towards more equitable solutions hi
 

01:27:15.040 --> 01:27:16.979
towards more equitable solutions hi
everyone my name is Russ Resta I'm

01:27:16.979 --> 01:27:16.989
everyone my name is Russ Resta I'm
 

01:27:16.989 --> 01:27:19.169
everyone my name is Russ Resta I'm
actually a radiologist by background so

01:27:19.169 --> 01:27:19.179
actually a radiologist by background so
 

01:27:19.179 --> 01:27:21.750
actually a radiologist by background so
when you hear all the hype around run

01:27:21.750 --> 01:27:21.760
when you hear all the hype around run
 

01:27:21.760 --> 01:27:22.380
when you hear all the hype around run
for the hills

01:27:22.380 --> 01:27:22.390
for the hills
 

01:27:22.390 --> 01:27:25.640
for the hills
AI is coming the machines are after us

01:27:25.640 --> 01:27:25.650
AI is coming the machines are after us
 

01:27:25.650 --> 01:27:28.770
AI is coming the machines are after us
and the radiologists are the first to be

01:27:28.770 --> 01:27:28.780
and the radiologists are the first to be
 

01:27:28.780 --> 01:27:31.220
and the radiologists are the first to be
eliminated Here I am

01:27:31.220 --> 01:27:31.230
eliminated Here I am
 

01:27:31.230 --> 01:27:35.040
eliminated Here I am
so as the radiologist by background I

01:27:35.040 --> 01:27:35.050
so as the radiologist by background I
 

01:27:35.050 --> 01:27:38.189
so as the radiologist by background I
have some views on AI but what I do at

01:27:38.189 --> 01:27:38.199
have some views on AI but what I do at
 

01:27:38.199 --> 01:27:40.320
have some views on AI but what I do at
UPMC really is i'm the chief innovation

01:27:40.320 --> 01:27:40.330
UPMC really is i'm the chief innovation
 

01:27:40.330 --> 01:27:44.610
UPMC really is i'm the chief innovation
officer UPMC us some of you or many of

01:27:44.610 --> 01:27:44.620
officer UPMC us some of you or many of
 

01:27:44.620 --> 01:27:46.110
officer UPMC us some of you or many of
you might know is a large healthcare

01:27:46.110 --> 01:27:46.120
you might know is a large healthcare
 

01:27:46.120 --> 01:27:47.640
you might know is a large healthcare
organization we're a payer and a

01:27:47.640 --> 01:27:47.650
organization we're a payer and a
 

01:27:47.650 --> 01:27:50.010
organization we're a payer and a
provider organization and when it comes

01:27:50.010 --> 01:27:50.020
provider organization and when it comes
 

01:27:50.020 --> 01:27:53.130
provider organization and when it comes
to the topic of ethics and AI and equity

01:27:53.130 --> 01:27:53.140
to the topic of ethics and AI and equity
 

01:27:53.140 --> 01:27:55.590
to the topic of ethics and AI and equity
we really believe that it's important

01:27:55.590 --> 01:27:55.600
we really believe that it's important
 

01:27:55.600 --> 01:27:59.430
we really believe that it's important
for us to see healthcare as something

01:27:59.430 --> 01:27:59.440
for us to see healthcare as something
 

01:27:59.440 --> 01:28:04.290
for us to see healthcare as something
that is very human so my role at UPMC

01:28:04.290 --> 01:28:04.300
that is very human so my role at UPMC
 

01:28:04.300 --> 01:28:06.090
that is very human so my role at UPMC
to take all of the data that we have

01:28:06.090 --> 01:28:06.100
to take all of the data that we have
 

01:28:06.100 --> 01:28:08.700
to take all of the data that we have
around these human beings all of the

01:28:08.700 --> 01:28:08.710
around these human beings all of the
 

01:28:08.710 --> 01:28:10.740
around these human beings all of the
data that we have around the medicine

01:28:10.740 --> 01:28:10.750
data that we have around the medicine
 

01:28:10.750 --> 01:28:13.170
data that we have around the medicine
and the science of really pushing health

01:28:13.170 --> 01:28:13.180
and the science of really pushing health
 

01:28:13.180 --> 01:28:15.080
and the science of really pushing health
and healthcare to the next level and

01:28:15.080 --> 01:28:15.090
and healthcare to the next level and
 

01:28:15.090 --> 01:28:17.550
and healthcare to the next level and
algorithm eyes these to innovate around

01:28:17.550 --> 01:28:17.560
algorithm eyes these to innovate around
 

01:28:17.560 --> 01:28:19.230
algorithm eyes these to innovate around
these to think outside the box and

01:28:19.230 --> 01:28:19.240
these to think outside the box and
 

01:28:19.240 --> 01:28:22.260
these to think outside the box and
really question the status quo right so

01:28:22.260 --> 01:28:22.270
really question the status quo right so
 

01:28:22.270 --> 01:28:25.110
really question the status quo right so
I in addition to being the chief

01:28:25.110 --> 01:28:25.120
I in addition to being the chief
 

01:28:25.120 --> 01:28:27.720
I in addition to being the chief
innovation officer UPMC I'm also really

01:28:27.720 --> 01:28:27.730
innovation officer UPMC I'm also really
 

01:28:27.730 --> 01:28:29.250
innovation officer UPMC I'm also really
privileged to be part of you PMC

01:28:29.250 --> 01:28:29.260
privileged to be part of you PMC
 

01:28:29.260 --> 01:28:30.930
privileged to be part of you PMC
enterprises which is our innovation and

01:28:30.930 --> 01:28:30.940
enterprises which is our innovation and
 

01:28:30.940 --> 01:28:33.060
enterprises which is our innovation and
entrepreneurship arm and working with

01:28:33.060 --> 01:28:33.070
entrepreneurship arm and working with
 

01:28:33.070 --> 01:28:35.190
entrepreneurship arm and working with
the best and brightest minds here in the

01:28:35.190 --> 01:28:35.200
the best and brightest minds here in the
 

01:28:35.200 --> 01:28:36.990
the best and brightest minds here in the
Pittsburgh area especially here at CMU

01:28:36.990 --> 01:28:37.000
Pittsburgh area especially here at CMU
 

01:28:37.000 --> 01:28:38.820
Pittsburgh area especially here at CMU
and the University of Pittsburgh we're

01:28:38.820 --> 01:28:38.830
and the University of Pittsburgh we're
 

01:28:38.830 --> 01:28:40.800
and the University of Pittsburgh we're
really pushing the boundaries of how we

01:28:40.800 --> 01:28:40.810
really pushing the boundaries of how we
 

01:28:40.810 --> 01:28:45.000
really pushing the boundaries of how we
can look at data to reinvent the future

01:28:45.000 --> 01:28:45.010
can look at data to reinvent the future
 

01:28:45.010 --> 01:28:47.790
can look at data to reinvent the future
of healthcare reinvent the future of

01:28:47.790 --> 01:28:47.800
of healthcare reinvent the future of
 

01:28:47.800 --> 01:28:50.370
of healthcare reinvent the future of
what health will look like not just 10

01:28:50.370 --> 01:28:50.380
what health will look like not just 10
 

01:28:50.380 --> 01:28:52.320
what health will look like not just 10
years into the future but further down

01:28:52.320 --> 01:28:52.330
years into the future but further down
 

01:28:52.330 --> 01:28:54.600
years into the future but further down
line at the downstream as well so really

01:28:54.600 --> 01:28:54.610
line at the downstream as well so really
 

01:28:54.610 --> 01:28:56.010
line at the downstream as well so really
excited about the conversation that

01:28:56.010 --> 01:28:56.020
excited about the conversation that
 

01:28:56.020 --> 01:28:58.080
excited about the conversation that
we're having today because it's not

01:28:58.080 --> 01:28:58.090
we're having today because it's not
 

01:28:58.090 --> 01:29:00.390
we're having today because it's not
about the hype it's about the reality of

01:29:00.390 --> 01:29:00.400
about the hype it's about the reality of
 

01:29:00.400 --> 01:29:02.940
about the hype it's about the reality of
what it means to take these algorithms

01:29:02.940 --> 01:29:02.950
what it means to take these algorithms
 

01:29:02.950 --> 01:29:04.620
what it means to take these algorithms
and make a difference in the lives of

01:29:04.620 --> 01:29:04.630
and make a difference in the lives of
 

01:29:04.630 --> 01:29:06.600
and make a difference in the lives of
these human beings that were trying to

01:29:06.600 --> 01:29:06.610
these human beings that were trying to
 

01:29:06.610 --> 01:29:11.460
these human beings that were trying to
impact day in and day out thank you hi

01:29:11.460 --> 01:29:11.470
impact day in and day out thank you hi
 

01:29:11.470 --> 01:29:14.850
impact day in and day out thank you hi
and I'm Michael Skupin so my background

01:29:14.850 --> 01:29:14.860
and I'm Michael Skupin so my background
 

01:29:14.860 --> 01:29:17.670
and I'm Michael Skupin so my background
is actually in this tough translation

01:29:17.670 --> 01:29:17.680
is actually in this tough translation
 

01:29:17.680 --> 01:29:19.440
is actually in this tough translation
problem I think about which our session

01:29:19.440 --> 01:29:19.450
problem I think about which our session
 

01:29:19.450 --> 01:29:21.180
problem I think about which our session
speaker talked about which is how do we

01:29:21.180 --> 01:29:21.190
speaker talked about which is how do we
 

01:29:21.190 --> 01:29:23.610
speaker talked about which is how do we
actually articulate our social norms in

01:29:23.610 --> 01:29:23.620
actually articulate our social norms in
 

01:29:23.620 --> 01:29:25.470
actually articulate our social norms in
terms of technical decisions and

01:29:25.470 --> 01:29:25.480
terms of technical decisions and
 

01:29:25.480 --> 01:29:27.420
terms of technical decisions and
technical processes that we're trying to

01:29:27.420 --> 01:29:27.430
technical processes that we're trying to
 

01:29:27.430 --> 01:29:29.910
technical processes that we're trying to
champion so a lot of what I've done is

01:29:29.910 --> 01:29:29.920
champion so a lot of what I've done is
 

01:29:29.920 --> 01:29:32.340
champion so a lot of what I've done is
worked with students so on pedagogical

01:29:32.340 --> 01:29:32.350
worked with students so on pedagogical
 

01:29:32.350 --> 01:29:33.930
worked with students so on pedagogical
practices for engineers who are in

01:29:33.930 --> 01:29:33.940
practices for engineers who are in
 

01:29:33.940 --> 01:29:36.330
practices for engineers who are in
training but also on professionals who

01:29:36.330 --> 01:29:36.340
training but also on professionals who
 

01:29:36.340 --> 01:29:38.130
training but also on professionals who
are designing technical system

01:29:38.130 --> 01:29:38.140
are designing technical system
 

01:29:38.140 --> 01:29:39.720
are designing technical system
specifically machine learning systems

01:29:39.720 --> 01:29:39.730
specifically machine learning systems
 

01:29:39.730 --> 01:29:41.550
specifically machine learning systems
and choosing what data mining operation

01:29:41.550 --> 01:29:41.560
and choosing what data mining operation
 

01:29:41.560 --> 01:29:43.380
and choosing what data mining operation
they're going to have or what training

01:29:43.380 --> 01:29:43.390
they're going to have or what training
 

01:29:43.390 --> 01:29:44.460
they're going to have or what training
procedure they're going to use and

01:29:44.460 --> 01:29:44.470
procedure they're going to use and
 

01:29:44.470 --> 01:29:46.530
procedure they're going to use and
actually trying to get them to project a

01:29:46.530 --> 01:29:46.540
actually trying to get them to project a
 

01:29:46.540 --> 01:29:49.230
actually trying to get them to project a
future a little bit about well what does

01:29:49.230 --> 01:29:49.240
future a little bit about well what does
 

01:29:49.240 --> 01:29:51.000
future a little bit about well what does
this small technical decision I'm making

01:29:51.000 --> 01:29:51.010
this small technical decision I'm making
 

01:29:51.010 --> 01:29:53.130
this small technical decision I'm making
here of choosing this sorting algorithm

01:29:53.130 --> 01:29:53.140
here of choosing this sorting algorithm
 

01:29:53.140 --> 01:29:54.780
here of choosing this sorting algorithm
on this interface or potential using

01:29:54.780 --> 01:29:54.790
on this interface or potential using
 

01:29:54.790 --> 01:29:56.640
on this interface or potential using
this data set to try to train on this

01:29:56.640 --> 01:29:56.650
this data set to try to train on this
 

01:29:56.650 --> 01:29:58.740
this data set to try to train on this
against this objective how do I get

01:29:58.740 --> 01:29:58.750
against this objective how do I get
 

01:29:58.750 --> 01:30:00.180
against this objective how do I get
people to actually think a little bit

01:30:00.180 --> 01:30:00.190
people to actually think a little bit
 

01:30:00.190 --> 01:30:01.350
people to actually think a little bit
about what's going to be the social

01:30:01.350 --> 01:30:01.360
about what's going to be the social
 

01:30:01.360 --> 01:30:03.420
about what's going to be the social
impact of this choice and what are some

01:30:03.420 --> 01:30:03.430
impact of this choice and what are some
 

01:30:03.430 --> 01:30:05.340
impact of this choice and what are some
of my options and how do they relate to

01:30:05.340 --> 01:30:05.350
of my options and how do they relate to
 

01:30:05.350 --> 01:30:07.770
of my options and how do they relate to
that social impact and so that is

01:30:07.770 --> 01:30:07.780
that social impact and so that is
 

01:30:07.780 --> 01:30:09.330
that social impact and so that is
actually a very difficult translation

01:30:09.330 --> 01:30:09.340
actually a very difficult translation
 

01:30:09.340 --> 01:30:11.340
actually a very difficult translation
problem and that's what I spent most of

01:30:11.340 --> 01:30:11.350
problem and that's what I spent most of
 

01:30:11.350 --> 01:30:12.459
problem and that's what I spent most of
my active

01:30:12.459 --> 01:30:12.469
my active
 

01:30:12.469 --> 01:30:14.680
my active
make time viewing but now I'm the

01:30:14.680 --> 01:30:14.690
make time viewing but now I'm the
 

01:30:14.690 --> 01:30:16.089
make time viewing but now I'm the
director of a community center which is

01:30:16.089 --> 01:30:16.099
director of a community center which is
 

01:30:16.099 --> 01:30:17.830
director of a community center which is
in Wilkinsburg Pennsylvania which is a

01:30:17.830 --> 01:30:17.840
in Wilkinsburg Pennsylvania which is a
 

01:30:17.840 --> 01:30:19.120
in Wilkinsburg Pennsylvania which is a
neighbouring borough right here in

01:30:19.120 --> 01:30:19.130
neighbouring borough right here in
 

01:30:19.130 --> 01:30:22.149
neighbouring borough right here in
Pittsburgh and so what we're trying to

01:30:22.149 --> 01:30:22.159
Pittsburgh and so what we're trying to
 

01:30:22.159 --> 01:30:24.790
Pittsburgh and so what we're trying to
do there is actually also a little bit

01:30:24.790 --> 01:30:24.800
do there is actually also a little bit
 

01:30:24.800 --> 01:30:26.410
do there is actually also a little bit
to our session speaker also he was

01:30:26.410 --> 01:30:26.420
to our session speaker also he was
 

01:30:26.420 --> 01:30:30.009
to our session speaker also he was
talking about is be deeply engaged in a

01:30:30.009 --> 01:30:30.019
talking about is be deeply engaged in a
 

01:30:30.019 --> 01:30:32.500
talking about is be deeply engaged in a
community setting as a group our leaders

01:30:32.500 --> 01:30:32.510
community setting as a group our leaders
 

01:30:32.510 --> 01:30:33.850
community setting as a group our leaders
range from across the board at the

01:30:33.850 --> 01:30:33.860
range from across the board at the
 

01:30:33.860 --> 01:30:35.589
range from across the board at the
community center from technologists like

01:30:35.589 --> 01:30:35.599
community center from technologists like
 

01:30:35.599 --> 01:30:37.600
community center from technologists like
myself and a few others but also to

01:30:37.600 --> 01:30:37.610
myself and a few others but also to
 

01:30:37.610 --> 01:30:40.089
myself and a few others but also to
educators and community servants who are

01:30:40.089 --> 01:30:40.099
educators and community servants who are
 

01:30:40.099 --> 01:30:42.850
educators and community servants who are
already working in the region and our

01:30:42.850 --> 01:30:42.860
already working in the region and our
 

01:30:42.860 --> 01:30:45.279
already working in the region and our
goal is to be deeply connected with that

01:30:45.279 --> 01:30:45.289
goal is to be deeply connected with that
 

01:30:45.289 --> 01:30:46.899
goal is to be deeply connected with that
community and be sure that we're

01:30:46.899 --> 01:30:46.909
community and be sure that we're
 

01:30:46.909 --> 01:30:48.879
community and be sure that we're
creating an inner community space where

01:30:48.879 --> 01:30:48.889
creating an inner community space where
 

01:30:48.889 --> 01:30:50.589
creating an inner community space where
people can learn from each other and

01:30:50.589 --> 01:30:50.599
people can learn from each other and
 

01:30:50.599 --> 01:30:52.719
people can learn from each other and
work with one another and as our session

01:30:52.719 --> 01:30:52.729
work with one another and as our session
 

01:30:52.729 --> 01:30:54.640
work with one another and as our session
speakers said actually learn from folks

01:30:54.640 --> 01:30:54.650
speakers said actually learn from folks
 

01:30:54.650 --> 01:30:56.680
speakers said actually learn from folks
who understand problems on the ground

01:30:56.680 --> 01:30:56.690
who understand problems on the ground
 

01:30:56.690 --> 01:30:58.390
who understand problems on the ground
better than folks who are in the

01:30:58.390 --> 01:30:58.400
better than folks who are in the
 

01:30:58.400 --> 01:31:00.129
better than folks who are in the
research lab or who are actually working

01:31:00.129 --> 01:31:00.139
research lab or who are actually working
 

01:31:00.139 --> 01:31:01.870
research lab or who are actually working
on these problems in an academic setting

01:31:01.870 --> 01:31:01.880
on these problems in an academic setting
 

01:31:01.880 --> 01:31:04.120
on these problems in an academic setting
and be able to actually help people

01:31:04.120 --> 01:31:04.130
and be able to actually help people
 

01:31:04.130 --> 01:31:06.399
and be able to actually help people
articulate their problems to us so that

01:31:06.399 --> 01:31:06.409
articulate their problems to us so that
 

01:31:06.409 --> 01:31:08.709
articulate their problems to us so that
we can then actually correctly have the

01:31:08.709 --> 01:31:08.719
we can then actually correctly have the
 

01:31:08.719 --> 01:31:11.080
we can then actually correctly have the
right head on our shoulders around what

01:31:11.080 --> 01:31:11.090
right head on our shoulders around what
 

01:31:11.090 --> 01:31:12.629
right head on our shoulders around what
we're trying to do rather than

01:31:12.629 --> 01:31:12.639
we're trying to do rather than
 

01:31:12.639 --> 01:31:15.069
we're trying to do rather than
potentially project what we think should

01:31:15.069 --> 01:31:15.079
potentially project what we think should
 

01:31:15.079 --> 01:31:17.560
potentially project what we think should
be happening and so our community center

01:31:17.560 --> 01:31:17.570
be happening and so our community center
 

01:31:17.570 --> 01:31:19.839
be happening and so our community center
is a very very I would say like scrappy

01:31:19.839 --> 01:31:19.849
is a very very I would say like scrappy
 

01:31:19.849 --> 01:31:21.850
is a very very I would say like scrappy
effort to to try to make sure that that

01:31:21.850 --> 01:31:21.860
effort to to try to make sure that that
 

01:31:21.860 --> 01:31:23.529
effort to to try to make sure that that
is happening and we do things such as

01:31:23.529 --> 01:31:23.539
is happening and we do things such as
 

01:31:23.539 --> 01:31:25.689
is happening and we do things such as
education services getting kids into

01:31:25.689 --> 01:31:25.699
education services getting kids into
 

01:31:25.699 --> 01:31:27.549
education services getting kids into
pathways around technology who might not

01:31:27.549 --> 01:31:27.559
pathways around technology who might not
 

01:31:27.559 --> 01:31:29.379
pathways around technology who might not
otherwise have those opportunities but

01:31:29.379 --> 01:31:29.389
otherwise have those opportunities but
 

01:31:29.389 --> 01:31:33.640
otherwise have those opportunities but
also as a civics and community service

01:31:33.640 --> 01:31:33.650
also as a civics and community service
 

01:31:33.650 --> 01:31:35.319
also as a civics and community service
project to really just make sure that

01:31:35.319 --> 01:31:35.329
project to really just make sure that
 

01:31:35.329 --> 01:31:36.850
project to really just make sure that
those conversations are happening and

01:31:36.850 --> 01:31:36.860
those conversations are happening and
 

01:31:36.860 --> 01:31:38.439
those conversations are happening and
that there's a space for them and so

01:31:38.439 --> 01:31:38.449
that there's a space for them and so
 

01:31:38.449 --> 01:31:39.580
that there's a space for them and so
that's a lot what the community center

01:31:39.580 --> 01:31:39.590
that's a lot what the community center
 

01:31:39.590 --> 01:31:41.830
that's a lot what the community center
about which is a you know output of some

01:31:41.830 --> 01:31:41.840
about which is a you know output of some
 

01:31:41.840 --> 01:31:43.209
about which is a you know output of some
of the things I think about an academic

01:31:43.209 --> 01:31:43.219
of the things I think about an academic
 

01:31:43.219 --> 01:31:46.870
of the things I think about an academic
setting so I was really excited to come

01:31:46.870 --> 01:31:46.880
setting so I was really excited to come
 

01:31:46.880 --> 01:31:49.209
setting so I was really excited to come
and moderate this panel because as a

01:31:49.209 --> 01:31:49.219
and moderate this panel because as a
 

01:31:49.219 --> 01:31:51.700
and moderate this panel because as a
journalist the question of access and

01:31:51.700 --> 01:31:51.710
journalist the question of access and
 

01:31:51.710 --> 01:31:54.009
journalist the question of access and
impact of technology or things I deal

01:31:54.009 --> 01:31:54.019
impact of technology or things I deal
 

01:31:54.019 --> 01:31:57.250
impact of technology or things I deal
with every day but also as a journalist

01:31:57.250 --> 01:31:57.260
with every day but also as a journalist
 

01:31:57.260 --> 01:31:59.890
with every day but also as a journalist
right my training is to be skeptical and

01:31:59.890 --> 01:31:59.900
right my training is to be skeptical and
 

01:31:59.900 --> 01:32:02.020
right my training is to be skeptical and
question assumptions so when I was

01:32:02.020 --> 01:32:02.030
question assumptions so when I was
 

01:32:02.030 --> 01:32:04.120
question assumptions so when I was
invited to moderate this the first thing

01:32:04.120 --> 01:32:04.130
invited to moderate this the first thing
 

01:32:04.130 --> 01:32:05.859
invited to moderate this the first thing
I did was have this knee-jerk reaction

01:32:05.859 --> 01:32:05.869
I did was have this knee-jerk reaction
 

01:32:05.869 --> 01:32:08.589
I did was have this knee-jerk reaction
and call up and say you know why are we

01:32:08.589 --> 01:32:08.599
and call up and say you know why are we
 

01:32:08.599 --> 01:32:10.959
and call up and say you know why are we
focusing on equity of access and equity

01:32:10.959 --> 01:32:10.969
focusing on equity of access and equity
 

01:32:10.969 --> 01:32:14.560
focusing on equity of access and equity
of impact because that really makes an

01:32:14.560 --> 01:32:14.570
of impact because that really makes an
 

01:32:14.570 --> 01:32:17.859
of impact because that really makes an
assumption that a AI is an inevitable

01:32:17.859 --> 01:32:17.869
assumption that a AI is an inevitable
 

01:32:17.869 --> 01:32:20.739
assumption that a AI is an inevitable
and B it's a known good that the good

01:32:20.739 --> 01:32:20.749
and B it's a known good that the good
 

01:32:20.749 --> 01:32:22.870
and B it's a known good that the good
outweighs the horn and we don't know

01:32:22.870 --> 01:32:22.880
outweighs the horn and we don't know
 

01:32:22.880 --> 01:32:23.729
outweighs the horn and we don't know
that yet

01:32:23.729 --> 01:32:23.739
that yet
 

01:32:23.739 --> 01:32:28.580
that yet
and then I said you know also I think

01:32:28.580 --> 01:32:28.590
and then I said you know also I think
 

01:32:28.590 --> 01:32:31.080
and then I said you know also I think
it's really important to focus on biased

01:32:31.080 --> 01:32:31.090
it's really important to focus on biased
 

01:32:31.090 --> 01:32:34.470
it's really important to focus on biased
datasets and bias algorithms and

01:32:34.470 --> 01:32:34.480
datasets and bias algorithms and
 

01:32:34.480 --> 01:32:37.290
datasets and bias algorithms and
disparate impact but this obsession we

01:32:37.290 --> 01:32:37.300
disparate impact but this obsession we
 

01:32:37.300 --> 01:32:39.690
disparate impact but this obsession we
have with computational fairness seems

01:32:39.690 --> 01:32:39.700
have with computational fairness seems
 

01:32:39.700 --> 01:32:41.400
have with computational fairness seems
to distract also from some of the big

01:32:41.400 --> 01:32:41.410
to distract also from some of the big
 

01:32:41.410 --> 01:32:44.130
to distract also from some of the big
problems and so I called my panelists

01:32:44.130 --> 01:32:44.140
problems and so I called my panelists
 

01:32:44.140 --> 01:32:46.380
problems and so I called my panelists
and discovered that you know everybody

01:32:46.380 --> 01:32:46.390
and discovered that you know everybody
 

01:32:46.390 --> 01:32:48.990
and discovered that you know everybody
also is a bit of an insurrectionist so

01:32:48.990 --> 01:32:49.000
also is a bit of an insurrectionist so
 

01:32:49.000 --> 01:32:51.209
also is a bit of an insurrectionist so
we're gonna talk about some of the

01:32:51.209 --> 01:32:51.219
we're gonna talk about some of the
 

01:32:51.219 --> 01:32:52.920
we're gonna talk about some of the
expected questions but we're also going

01:32:52.920 --> 01:32:52.930
expected questions but we're also going
 

01:32:52.930 --> 01:32:54.810
expected questions but we're also going
to talk about some of the fundamental

01:32:54.810 --> 01:32:54.820
to talk about some of the fundamental
 

01:32:54.820 --> 01:32:57.990
to talk about some of the fundamental
questions and so you know the first

01:32:57.990 --> 01:32:58.000
questions and so you know the first
 

01:32:58.000 --> 01:32:59.430
questions and so you know the first
question is just an establishing

01:32:59.430 --> 01:32:59.440
question is just an establishing
 

01:32:59.440 --> 01:33:01.260
question is just an establishing
question Alex maybe you want to start

01:33:01.260 --> 01:33:01.270
question Alex maybe you want to start
 

01:33:01.270 --> 01:33:03.870
question Alex maybe you want to start
question is when we talk about AI and we

01:33:03.870 --> 01:33:03.880
question is when we talk about AI and we
 

01:33:03.880 --> 01:33:06.180
question is when we talk about AI and we
talk about equity what is it we're

01:33:06.180 --> 01:33:06.190
talk about equity what is it we're
 

01:33:06.190 --> 01:33:10.140
talk about equity what is it we're
actually talking about I think in many

01:33:10.140 --> 01:33:10.150
actually talking about I think in many
 

01:33:10.150 --> 01:33:11.910
actually talking about I think in many
ways we're talking about the very same

01:33:11.910 --> 01:33:11.920
ways we're talking about the very same
 

01:33:11.920 --> 01:33:13.890
ways we're talking about the very same
things that we're referring to and we

01:33:13.890 --> 01:33:13.900
things that we're referring to and we
 

01:33:13.900 --> 01:33:16.560
things that we're referring to and we
speak about equity of any process so in

01:33:16.560 --> 01:33:16.570
speak about equity of any process so in
 

01:33:16.570 --> 01:33:18.390
speak about equity of any process so in
my experience a lot of the concerns that

01:33:18.390 --> 01:33:18.400
my experience a lot of the concerns that
 

01:33:18.400 --> 01:33:21.209
my experience a lot of the concerns that
we have in let's say criminal justice is

01:33:21.209 --> 01:33:21.219
we have in let's say criminal justice is
 

01:33:21.219 --> 01:33:23.790
we have in let's say criminal justice is
a risk of just perpetuating the very

01:33:23.790 --> 01:33:23.800
a risk of just perpetuating the very
 

01:33:23.800 --> 01:33:25.940
a risk of just perpetuating the very
same disparities that the system already

01:33:25.940 --> 01:33:25.950
same disparities that the system already
 

01:33:25.950 --> 01:33:29.100
same disparities that the system already
absorbedly suffers from so I think

01:33:29.100 --> 01:33:29.110
absorbedly suffers from so I think
 

01:33:29.110 --> 01:33:30.360
absorbedly suffers from so I think
what's really important to keep in mind

01:33:30.360 --> 01:33:30.370
what's really important to keep in mind
 

01:33:30.370 --> 01:33:32.250
what's really important to keep in mind
when we think about these equity issues

01:33:32.250 --> 01:33:32.260
when we think about these equity issues
 

01:33:32.260 --> 01:33:34.350
when we think about these equity issues
is compared to what baseline that

01:33:34.350 --> 01:33:34.360
is compared to what baseline that
 

01:33:34.360 --> 01:33:36.420
is compared to what baseline that
there's certainly a risk that as we

01:33:36.420 --> 01:33:36.430
there's certainly a risk that as we
 

01:33:36.430 --> 01:33:39.180
there's certainly a risk that as we
switch to more algorithmic or at least

01:33:39.180 --> 01:33:39.190
switch to more algorithmic or at least
 

01:33:39.190 --> 01:33:41.250
switch to more algorithmic or at least
algorithmically supported decision

01:33:41.250 --> 01:33:41.260
algorithmically supported decision
 

01:33:41.260 --> 01:33:43.290
algorithmically supported decision
systems that we could exacerbate those

01:33:43.290 --> 01:33:43.300
systems that we could exacerbate those
 

01:33:43.300 --> 01:33:45.450
systems that we could exacerbate those
same problems or we could address them

01:33:45.450 --> 01:33:45.460
same problems or we could address them
 

01:33:45.460 --> 01:33:48.180
same problems or we could address them
and I think that's really at the core of

01:33:48.180 --> 01:33:48.190
and I think that's really at the core of
 

01:33:48.190 --> 01:33:49.860
and I think that's really at the core of
equity that this is not a new problem

01:33:49.860 --> 01:33:49.870
equity that this is not a new problem
 

01:33:49.870 --> 01:33:52.320
equity that this is not a new problem
that AI has brought about it's something

01:33:52.320 --> 01:33:52.330
that AI has brought about it's something
 

01:33:52.330 --> 01:33:53.910
that AI has brought about it's something
old that we're now trying to address

01:33:53.910 --> 01:33:53.920
old that we're now trying to address
 

01:33:53.920 --> 01:34:02.070
old that we're now trying to address
perhaps with some new tools yeah so I

01:34:02.070 --> 01:34:02.080
perhaps with some new tools yeah so I
 

01:34:02.080 --> 01:34:05.010
perhaps with some new tools yeah so I
think when we're talking about equity as

01:34:05.010 --> 01:34:05.020
think when we're talking about equity as
 

01:34:05.020 --> 01:34:08.100
think when we're talking about equity as
as Alexandra said we're often thinking

01:34:08.100 --> 01:34:08.110
as Alexandra said we're often thinking
 

01:34:08.110 --> 01:34:10.620
as Alexandra said we're often thinking
about inequities that already exist and

01:34:10.620 --> 01:34:10.630
about inequities that already exist and
 

01:34:10.630 --> 01:34:14.970
about inequities that already exist and
then could be compounded by AI solutions

01:34:14.970 --> 01:34:14.980
then could be compounded by AI solutions
 

01:34:14.980 --> 01:34:17.010
then could be compounded by AI solutions
and the approaches that we take so so

01:34:17.010 --> 01:34:17.020
and the approaches that we take so so
 

01:34:17.020 --> 01:34:19.110
and the approaches that we take so so
I'm very interested in and I'm when I

01:34:19.110 --> 01:34:19.120
I'm very interested in and I'm when I
 

01:34:19.120 --> 01:34:21.030
I'm very interested in and I'm when I
think about equity and I hear about it I

01:34:21.030 --> 01:34:21.040
think about equity and I hear about it I
 

01:34:21.040 --> 01:34:22.950
think about equity and I hear about it I
do have this kind of two sides of the

01:34:22.950 --> 01:34:22.960
do have this kind of two sides of the
 

01:34:22.960 --> 01:34:25.080
do have this kind of two sides of the
coin of is the AI system already out

01:34:25.080 --> 01:34:25.090
coin of is the AI system already out
 

01:34:25.090 --> 01:34:25.620
coin of is the AI system already out
there

01:34:25.620 --> 01:34:25.630
there
 

01:34:25.630 --> 01:34:28.080
there
did we make something and then is it

01:34:28.080 --> 01:34:28.090
did we make something and then is it
 

01:34:28.090 --> 01:34:30.420
did we make something and then is it
actually creating something an inequity

01:34:30.420 --> 01:34:30.430
actually creating something an inequity
 

01:34:30.430 --> 01:34:31.890
actually creating something an inequity
that we maybe already understand and

01:34:31.890 --> 01:34:31.900
that we maybe already understand and
 

01:34:31.900 --> 01:34:32.850
that we maybe already understand and
knowbut maybe it's already being

01:34:32.850 --> 01:34:32.860
knowbut maybe it's already being
 

01:34:32.860 --> 01:34:35.040
knowbut maybe it's already being
produced in another way and now we're

01:34:35.040 --> 01:34:35.050
produced in another way and now we're
 

01:34:35.050 --> 01:34:37.050
produced in another way and now we're
actually replicating it or accelerating

01:34:37.050 --> 01:34:37.060
actually replicating it or accelerating
 

01:34:37.060 --> 01:34:37.670
actually replicating it or accelerating
it or maybe we're

01:34:37.670 --> 01:34:37.680
it or maybe we're
 

01:34:37.680 --> 01:34:40.040
it or maybe we're
amazing it but we are missing some edge

01:34:40.040 --> 01:34:40.050
amazing it but we are missing some edge
 

01:34:40.050 --> 01:34:42.740
amazing it but we are missing some edge
cases still but then there's also the

01:34:42.740 --> 01:34:42.750
cases still but then there's also the
 

01:34:42.750 --> 01:34:46.370
cases still but then there's also the
side of how is inequity just exacerbated

01:34:46.370 --> 01:34:46.380
side of how is inequity just exacerbated
 

01:34:46.380 --> 01:34:48.800
side of how is inequity just exacerbated
by the push of this industry thinking

01:34:48.800 --> 01:34:48.810
by the push of this industry thinking
 

01:34:48.810 --> 01:34:50.780
by the push of this industry thinking
about how where is investment coming

01:34:50.780 --> 01:34:50.790
about how where is investment coming
 

01:34:50.790 --> 01:34:52.220
about how where is investment coming
from who are the people who get the

01:34:52.220 --> 01:34:52.230
from who are the people who get the
 

01:34:52.230 --> 01:34:54.080
from who are the people who get the
chance to decide what the objectives are

01:34:54.080 --> 01:34:54.090
chance to decide what the objectives are
 

01:34:54.090 --> 01:34:55.520
chance to decide what the objectives are
as many of you probably know when you're

01:34:55.520 --> 01:34:55.530
as many of you probably know when you're
 

01:34:55.530 --> 01:34:57.470
as many of you probably know when you're
designing an AI system it's always

01:34:57.470 --> 01:34:57.480
designing an AI system it's always
 

01:34:57.480 --> 01:34:59.690
designing an AI system it's always
towards an objective towards a task that

01:34:59.690 --> 01:34:59.700
towards an objective towards a task that
 

01:34:59.700 --> 01:35:01.220
towards an objective towards a task that
you're trying to develop a machine

01:35:01.220 --> 01:35:01.230
you're trying to develop a machine
 

01:35:01.230 --> 01:35:03.470
you're trying to develop a machine
learning algorithm for and who gets to

01:35:03.470 --> 01:35:03.480
learning algorithm for and who gets to
 

01:35:03.480 --> 01:35:06.680
learning algorithm for and who gets to
decide what is the relevant approach to

01:35:06.680 --> 01:35:06.690
decide what is the relevant approach to
 

01:35:06.690 --> 01:35:07.820
decide what is the relevant approach to
that so when you think about say a

01:35:07.820 --> 01:35:07.830
that so when you think about say a
 

01:35:07.830 --> 01:35:10.370
that so when you think about say a
recidivism score is there also an

01:35:10.370 --> 01:35:10.380
recidivism score is there also an
 

01:35:10.380 --> 01:35:13.070
recidivism score is there also an
objective or we say does this prisoner

01:35:13.070 --> 01:35:13.080
objective or we say does this prisoner
 

01:35:13.080 --> 01:35:15.590
objective or we say does this prisoner
have the social support network to go

01:35:15.590 --> 01:35:15.600
have the social support network to go
 

01:35:15.600 --> 01:35:17.870
have the social support network to go
out and actually get a job or to

01:35:17.870 --> 01:35:17.880
out and actually get a job or to
 

01:35:17.880 --> 01:35:21.350
out and actually get a job or to
potentially go on a pathway out of you

01:35:21.350 --> 01:35:21.360
potentially go on a pathway out of you
 

01:35:21.360 --> 01:35:22.820
potentially go on a pathway out of you
know involved in involvement in crime

01:35:22.820 --> 01:35:22.830
know involved in involvement in crime
 

01:35:22.830 --> 01:35:25.790
know involved in involvement in crime
and is there a way to use AI to predict

01:35:25.790 --> 01:35:25.800
and is there a way to use AI to predict
 

01:35:25.800 --> 01:35:27.650
and is there a way to use AI to predict
whether this person's social system is

01:35:27.650 --> 01:35:27.660
whether this person's social system is
 

01:35:27.660 --> 01:35:29.810
whether this person's social system is
just not gonna be you know supportive

01:35:29.810 --> 01:35:29.820
just not gonna be you know supportive
 

01:35:29.820 --> 01:35:31.520
just not gonna be you know supportive
enough to be able to get them back on

01:35:31.520 --> 01:35:31.530
enough to be able to get them back on
 

01:35:31.530 --> 01:35:32.930
enough to be able to get them back on
their feet is that an alternative look

01:35:32.930 --> 01:35:32.940
their feet is that an alternative look
 

01:35:32.940 --> 01:35:34.670
their feet is that an alternative look
at a recidivism score what a different

01:35:34.670 --> 01:35:34.680
at a recidivism score what a different
 

01:35:34.680 --> 01:35:35.930
at a recidivism score what a different
person when thinking about that problem

01:35:35.930 --> 01:35:35.940
person when thinking about that problem
 

01:35:35.940 --> 01:35:38.630
person when thinking about that problem
see it that way and so yeah so that's

01:35:38.630 --> 01:35:38.640
see it that way and so yeah so that's
 

01:35:38.640 --> 01:35:40.250
see it that way and so yeah so that's
what I'm kind of thinking about is where

01:35:40.250 --> 01:35:40.260
what I'm kind of thinking about is where
 

01:35:40.260 --> 01:35:42.050
what I'm kind of thinking about is where
does it come from when we first start

01:35:42.050 --> 01:35:42.060
does it come from when we first start
 

01:35:42.060 --> 01:35:44.600
does it come from when we first start
thinking about AI and then also thinking

01:35:44.600 --> 01:35:44.610
thinking about AI and then also thinking
 

01:35:44.610 --> 01:35:46.640
thinking about AI and then also thinking
well now we have any AI solution is is

01:35:46.640 --> 01:35:46.650
well now we have any AI solution is is
 

01:35:46.650 --> 01:35:49.400
well now we have any AI solution is is
it being equitable in practice and how

01:35:49.400 --> 01:35:49.410
it being equitable in practice and how
 

01:35:49.410 --> 01:35:51.470
it being equitable in practice and how
do you think about it in terms of equity

01:35:51.470 --> 01:35:51.480
do you think about it in terms of equity
 

01:35:51.480 --> 01:35:54.980
do you think about it in terms of equity
and health care yeah so it's really

01:35:54.980 --> 01:35:54.990
and health care yeah so it's really
 

01:35:54.990 --> 01:35:56.300
and health care yeah so it's really
interesting when you think about health

01:35:56.300 --> 01:35:56.310
interesting when you think about health
 

01:35:56.310 --> 01:35:57.980
interesting when you think about health
care as I mentioned earlier there's

01:35:57.980 --> 01:35:57.990
care as I mentioned earlier there's
 

01:35:57.990 --> 01:36:02.450
care as I mentioned earlier there's
nothing really more humane or human than

01:36:02.450 --> 01:36:02.460
nothing really more humane or human than
 

01:36:02.460 --> 01:36:05.030
nothing really more humane or human than
this pursuit of excellence in providing

01:36:05.030 --> 01:36:05.040
this pursuit of excellence in providing
 

01:36:05.040 --> 01:36:07.700
this pursuit of excellence in providing
best quality care that you can to fellow

01:36:07.700 --> 01:36:07.710
best quality care that you can to fellow
 

01:36:07.710 --> 01:36:09.380
best quality care that you can to fellow
human beings now when you talk about

01:36:09.380 --> 01:36:09.390
human beings now when you talk about
 

01:36:09.390 --> 01:36:11.530
human beings now when you talk about
equity in health care it really is about

01:36:11.530 --> 01:36:11.540
equity in health care it really is about
 

01:36:11.540 --> 01:36:14.630
equity in health care it really is about
making sure that you have access to care

01:36:14.630 --> 01:36:14.640
making sure that you have access to care
 

01:36:14.640 --> 01:36:17.240
making sure that you have access to care
and making sure that you're fair about

01:36:17.240 --> 01:36:17.250
and making sure that you're fair about
 

01:36:17.250 --> 01:36:20.780
and making sure that you're fair about
who has access and what level or what

01:36:20.780 --> 01:36:20.790
who has access and what level or what
 

01:36:20.790 --> 01:36:23.120
who has access and what level or what
quality of access these human beings

01:36:23.120 --> 01:36:23.130
quality of access these human beings
 

01:36:23.130 --> 01:36:27.110
quality of access these human beings
have to those care processes so when you

01:36:27.110 --> 01:36:27.120
have to those care processes so when you
 

01:36:27.120 --> 01:36:29.030
have to those care processes so when you
really think about the three-legged

01:36:29.030 --> 01:36:29.040
really think about the three-legged
 

01:36:29.040 --> 01:36:31.700
really think about the three-legged
stool of health care the three point

01:36:31.700 --> 01:36:31.710
stool of health care the three point
 

01:36:31.710 --> 01:36:33.260
stool of health care the three point
five trillion dollar industry that's

01:36:33.260 --> 01:36:33.270
five trillion dollar industry that's
 

01:36:33.270 --> 01:36:35.420
five trillion dollar industry that's
health care today it's cost quality and

01:36:35.420 --> 01:36:35.430
health care today it's cost quality and
 

01:36:35.430 --> 01:36:37.520
health care today it's cost quality and
access right and you know some would add

01:36:37.520 --> 01:36:37.530
access right and you know some would add
 

01:36:37.530 --> 01:36:40.220
access right and you know some would add
a fourth stool there around experience

01:36:40.220 --> 01:36:40.230
a fourth stool there around experience
 

01:36:40.230 --> 01:36:42.710
a fourth stool there around experience
but when you think about cost quality

01:36:42.710 --> 01:36:42.720
but when you think about cost quality
 

01:36:42.720 --> 01:36:45.170
but when you think about cost quality
and access cost continues to accelerate

01:36:45.170 --> 01:36:45.180
and access cost continues to accelerate
 

01:36:45.180 --> 01:36:47.290
and access cost continues to accelerate
it continues to go up

01:36:47.290 --> 01:36:47.300
it continues to go up
 

01:36:47.300 --> 01:36:49.640
it continues to go up
quality is somewhat still questionable

01:36:49.640 --> 01:36:49.650
quality is somewhat still questionable
 

01:36:49.650 --> 01:36:50.810
quality is somewhat still questionable
in terms of

01:36:50.810 --> 01:36:50.820
in terms of
 

01:36:50.820 --> 01:36:53.480
in terms of
what it is that we're really going for

01:36:53.480 --> 01:36:53.490
what it is that we're really going for
 

01:36:53.490 --> 01:36:55.910
what it is that we're really going for
around creating those the outcomes that

01:36:55.910 --> 01:36:55.920
around creating those the outcomes that
 

01:36:55.920 --> 01:36:58.430
around creating those the outcomes that
we desire around the health processes

01:36:58.430 --> 01:36:58.440
we desire around the health processes
 

01:36:58.440 --> 01:37:00.350
we desire around the health processes
that we're putting in place and then

01:37:00.350 --> 01:37:00.360
that we're putting in place and then
 

01:37:00.360 --> 01:37:03.190
that we're putting in place and then
access to care I think is getting better

01:37:03.190 --> 01:37:03.200
access to care I think is getting better
 

01:37:03.200 --> 01:37:06.470
access to care I think is getting better
but having said that the topic that

01:37:06.470 --> 01:37:06.480
but having said that the topic that
 

01:37:06.480 --> 01:37:09.230
but having said that the topic that
we're talking about today i I think

01:37:09.230 --> 01:37:09.240
we're talking about today i I think
 

01:37:09.240 --> 01:37:12.830
we're talking about today i I think
potentially has the potential to level

01:37:12.830 --> 01:37:12.840
potentially has the potential to level
 

01:37:12.840 --> 01:37:15.110
potentially has the potential to level
the playing field in ways that we've

01:37:15.110 --> 01:37:15.120
the playing field in ways that we've
 

01:37:15.120 --> 01:37:17.860
the playing field in ways that we've
just not been able to do in healthcare

01:37:17.860 --> 01:37:17.870
just not been able to do in healthcare
 

01:37:17.870 --> 01:37:20.870
just not been able to do in healthcare
it has the potential for example in

01:37:20.870 --> 01:37:20.880
it has the potential for example in
 

01:37:20.880 --> 01:37:24.520
it has the potential for example in
being able to bring to the common man

01:37:24.520 --> 01:37:24.530
being able to bring to the common man
 

01:37:24.530 --> 01:37:27.790
being able to bring to the common man
algorithms that perhaps the richest and

01:37:27.790 --> 01:37:27.800
algorithms that perhaps the richest and
 

01:37:27.800 --> 01:37:31.910
algorithms that perhaps the richest and
those that had that were privileged only

01:37:31.910 --> 01:37:31.920
those that had that were privileged only
 

01:37:31.920 --> 01:37:35.380
those that had that were privileged only
had in example treating cancer right

01:37:35.380 --> 01:37:35.390
had in example treating cancer right
 

01:37:35.390 --> 01:37:39.440
had in example treating cancer right
specific types of protocols that we were

01:37:39.440 --> 01:37:39.450
specific types of protocols that we were
 

01:37:39.450 --> 01:37:41.570
specific types of protocols that we were
able to really push forward in only

01:37:41.570 --> 01:37:41.580
able to really push forward in only
 

01:37:41.580 --> 01:37:44.390
able to really push forward in only
perhaps you know that the wealthiest of

01:37:44.390 --> 01:37:44.400
perhaps you know that the wealthiest of
 

01:37:44.400 --> 01:37:47.240
perhaps you know that the wealthiest of
nations or the wealthiest of areas and

01:37:47.240 --> 01:37:47.250
nations or the wealthiest of areas and
 

01:37:47.250 --> 01:37:49.640
nations or the wealthiest of areas and
now we're able to really reproduce these

01:37:49.640 --> 01:37:49.650
now we're able to really reproduce these
 

01:37:49.650 --> 01:37:52.720
now we're able to really reproduce these
at scale we're able to look at data at

01:37:52.720 --> 01:37:52.730
at scale we're able to look at data at
 

01:37:52.730 --> 01:37:55.100
at scale we're able to look at data at
scales that would just not been able to

01:37:55.100 --> 01:37:55.110
scales that would just not been able to
 

01:37:55.110 --> 01:37:57.200
scales that would just not been able to
do before and be able to then impart

01:37:57.200 --> 01:37:57.210
do before and be able to then impart
 

01:37:57.210 --> 01:37:59.540
do before and be able to then impart
these in in larger populations of

01:37:59.540 --> 01:37:59.550
these in in larger populations of
 

01:37:59.550 --> 01:38:01.490
these in in larger populations of
patients as we absolutely need to be

01:38:01.490 --> 01:38:01.500
patients as we absolutely need to be
 

01:38:01.500 --> 01:38:04.670
patients as we absolutely need to be
doing so like cancer genomics cancer

01:38:04.670 --> 01:38:04.680
doing so like cancer genomics cancer
 

01:38:04.680 --> 01:38:06.560
doing so like cancer genomics cancer
genomics is one specific area right

01:38:06.560 --> 01:38:06.570
genomics is one specific area right
 

01:38:06.570 --> 01:38:09.770
genomics is one specific area right
where we have petabytes worth of data

01:38:09.770 --> 01:38:09.780
where we have petabytes worth of data
 

01:38:09.780 --> 01:38:13.040
where we have petabytes worth of data
that we have access to where the notion

01:38:13.040 --> 01:38:13.050
that we have access to where the notion
 

01:38:13.050 --> 01:38:15.380
that we have access to where the notion
of machine learning and artificial

01:38:15.380 --> 01:38:15.390
of machine learning and artificial
 

01:38:15.390 --> 01:38:19.340
of machine learning and artificial
intelligence becomes actually an enabler

01:38:19.340 --> 01:38:19.350
intelligence becomes actually an enabler
 

01:38:19.350 --> 01:38:23.210
intelligence becomes actually an enabler
of better care so the notion here yes is

01:38:23.210 --> 01:38:23.220
of better care so the notion here yes is
 

01:38:23.220 --> 01:38:26.570
of better care so the notion here yes is
of taking these algorithms and in many

01:38:26.570 --> 01:38:26.580
of taking these algorithms and in many
 

01:38:26.580 --> 01:38:28.340
of taking these algorithms and in many
ways trying to democratize the process

01:38:28.340 --> 01:38:28.350
ways trying to democratize the process
 

01:38:28.350 --> 01:38:32.810
ways trying to democratize the process
of providing excellent care for not as

01:38:32.810 --> 01:38:32.820
of providing excellent care for not as
 

01:38:32.820 --> 01:38:34.880
of providing excellent care for not as
populations of patients that in this

01:38:34.880 --> 01:38:34.890
populations of patients that in this
 

01:38:34.890 --> 01:38:37.310
populations of patients that in this
case may have a certain type of a tumor

01:38:37.310 --> 01:38:37.320
case may have a certain type of a tumor
 

01:38:37.320 --> 01:38:40.790
case may have a certain type of a tumor
but also around the notion of precision

01:38:40.790 --> 01:38:40.800
but also around the notion of precision
 

01:38:40.800 --> 01:38:43.490
but also around the notion of precision
medicine which is really looking at not

01:38:43.490 --> 01:38:43.500
medicine which is really looking at not
 

01:38:43.500 --> 01:38:45.650
medicine which is really looking at not
just the N of many but the N of one

01:38:45.650 --> 01:38:45.660
just the N of many but the N of one
 

01:38:45.660 --> 01:38:48.080
just the N of many but the N of one
right so one individual that may have

01:38:48.080 --> 01:38:48.090
right so one individual that may have
 

01:38:48.090 --> 01:38:51.260
right so one individual that may have
specific types of bio receptors certain

01:38:51.260 --> 01:38:51.270
specific types of bio receptors certain
 

01:38:51.270 --> 01:38:54.440
specific types of bio receptors certain
types of perhaps even preferences in

01:38:54.440 --> 01:38:54.450
types of perhaps even preferences in
 

01:38:54.450 --> 01:38:55.970
types of perhaps even preferences in
terms of how he or she may want to be

01:38:55.970 --> 01:38:55.980
terms of how he or she may want to be
 

01:38:55.980 --> 01:38:56.570
terms of how he or she may want to be
treated

01:38:56.570 --> 01:38:56.580
treated
 

01:38:56.580 --> 01:38:59.390
treated
zip code that that person that patient

01:38:59.390 --> 01:38:59.400
zip code that that person that patient
 

01:38:59.400 --> 01:39:01.910
zip code that that person that patient
may be living in and other types of

01:39:01.910 --> 01:39:01.920
may be living in and other types of
 

01:39:01.920 --> 01:39:03.200
may be living in and other types of
social determinants

01:39:03.200 --> 01:39:03.210
social determinants
 

01:39:03.210 --> 01:39:05.330
social determinants
factors that might influence his or her

01:39:05.330 --> 01:39:05.340
factors that might influence his or her
 

01:39:05.340 --> 01:39:08.060
factors that might influence his or her
behaviors or specific outcomes so really

01:39:08.060 --> 01:39:08.070
behaviors or specific outcomes so really
 

01:39:08.070 --> 01:39:12.050
behaviors or specific outcomes so really
trying to personalize the care to that

01:39:12.050 --> 01:39:12.060
trying to personalize the care to that
 

01:39:12.060 --> 01:39:14.570
trying to personalize the care to that
nf1 I think is the new reality of what

01:39:14.570 --> 01:39:14.580
nf1 I think is the new reality of what
 

01:39:14.580 --> 01:39:17.360
nf1 I think is the new reality of what
we might be able to put in place so it's

01:39:17.360 --> 01:39:17.370
we might be able to put in place so it's
 

01:39:17.370 --> 01:39:19.640
we might be able to put in place so it's
interesting cuz you both mentioned power

01:39:19.640 --> 01:39:19.650
interesting cuz you both mentioned power
 

01:39:19.650 --> 01:39:22.310
interesting cuz you both mentioned power
asymmetries Mike in the fact that if you

01:39:22.310 --> 01:39:22.320
asymmetries Mike in the fact that if you
 

01:39:22.320 --> 01:39:24.530
asymmetries Mike in the fact that if you
have AI in the hands of a few you're

01:39:24.530 --> 01:39:24.540
have AI in the hands of a few you're
 

01:39:24.540 --> 01:39:26.150
have AI in the hands of a few you're
gonna have a few powerful entities

01:39:26.150 --> 01:39:26.160
gonna have a few powerful entities
 

01:39:26.160 --> 01:39:28.610
gonna have a few powerful entities
choosing what the problems are to solve

01:39:28.610 --> 01:39:28.620
choosing what the problems are to solve
 

01:39:28.620 --> 01:39:30.320
choosing what the problems are to solve
and how to solve them and you've

01:39:30.320 --> 01:39:30.330
and how to solve them and you've
 

01:39:30.330 --> 01:39:33.310
and how to solve them and you've
mentioned a i's possible way of

01:39:33.310 --> 01:39:33.320
mentioned a i's possible way of
 

01:39:33.320 --> 01:39:35.720
mentioned a i's possible way of
democratizing access to higher-end

01:39:35.720 --> 01:39:35.730
democratizing access to higher-end
 

01:39:35.730 --> 01:39:37.729
democratizing access to higher-end
healthcare or more innovative healthcare

01:39:37.729 --> 01:39:37.739
healthcare or more innovative healthcare
 

01:39:37.739 --> 01:39:39.380
healthcare or more innovative healthcare
but I want to go back to the power

01:39:39.380 --> 01:39:39.390
but I want to go back to the power
 

01:39:39.390 --> 01:39:42.440
but I want to go back to the power
question because it's a foundational

01:39:42.440 --> 01:39:42.450
question because it's a foundational
 

01:39:42.450 --> 01:39:45.650
question because it's a foundational
moment and we see that a handful of

01:39:45.650 --> 01:39:45.660
moment and we see that a handful of
 

01:39:45.660 --> 01:39:47.330
moment and we see that a handful of
really big players the googles and the

01:39:47.330 --> 01:39:47.340
really big players the googles and the
 

01:39:47.340 --> 01:39:50.870
really big players the googles and the
Facebook's and the ubers have a

01:39:50.870 --> 01:39:50.880
Facebook's and the ubers have a
 

01:39:50.880 --> 01:39:53.390
Facebook's and the ubers have a
concentration of power knowledge and

01:39:53.390 --> 01:39:53.400
concentration of power knowledge and
 

01:39:53.400 --> 01:39:56.120
concentration of power knowledge and
financial gain and what do we do about

01:39:56.120 --> 01:39:56.130
financial gain and what do we do about
 

01:39:56.130 --> 01:40:00.020
financial gain and what do we do about
that in the context of AI and also some

01:40:00.020 --> 01:40:00.030
that in the context of AI and also some
 

01:40:00.030 --> 01:40:03.020
that in the context of AI and also some
of those companies have huge databases

01:40:03.020 --> 01:40:03.030
of those companies have huge databases
 

01:40:03.030 --> 01:40:05.840
of those companies have huge databases
of consumer data right but those

01:40:05.840 --> 01:40:05.850
of consumer data right but those
 

01:40:05.850 --> 01:40:07.700
of consumer data right but those
consumers aren't being compensated for

01:40:07.700 --> 01:40:07.710
consumers aren't being compensated for
 

01:40:07.710 --> 01:40:09.650
consumers aren't being compensated for
their data yet it's going to cause

01:40:09.650 --> 01:40:09.660
their data yet it's going to cause
 

01:40:09.660 --> 01:40:12.290
their data yet it's going to cause
financial gain so how do we do deal with

01:40:12.290 --> 01:40:12.300
financial gain so how do we do deal with
 

01:40:12.300 --> 01:40:15.400
financial gain so how do we do deal with
like equity of distribution of benefits

01:40:15.400 --> 01:40:15.410
like equity of distribution of benefits
 

01:40:15.410 --> 01:40:19.700
like equity of distribution of benefits
Alex do you wanna start I think in many

01:40:19.700 --> 01:40:19.710
Alex do you wanna start I think in many
 

01:40:19.710 --> 01:40:21.890
Alex do you wanna start I think in many
ways there's strong parallels to the way

01:40:21.890 --> 01:40:21.900
ways there's strong parallels to the way
 

01:40:21.900 --> 01:40:23.330
ways there's strong parallels to the way
we think about experimentation in

01:40:23.330 --> 01:40:23.340
we think about experimentation in
 

01:40:23.340 --> 01:40:26.120
we think about experimentation in
medical contexts or in research contexts

01:40:26.120 --> 01:40:26.130
medical contexts or in research contexts
 

01:40:26.130 --> 01:40:28.670
medical contexts or in research contexts
if you think about the steps that an

01:40:28.670 --> 01:40:28.680
if you think about the steps that an
 

01:40:28.680 --> 01:40:30.740
if you think about the steps that an
academic researcher has to go through in

01:40:30.740 --> 01:40:30.750
academic researcher has to go through in
 

01:40:30.750 --> 01:40:33.170
academic researcher has to go through in
order to collect specific data on human

01:40:33.170 --> 01:40:33.180
order to collect specific data on human
 

01:40:33.180 --> 01:40:36.229
order to collect specific data on human
subjects or to perform particular

01:40:36.229 --> 01:40:36.239
subjects or to perform particular
 

01:40:36.239 --> 01:40:38.350
subjects or to perform particular
experiments that affect human subjects

01:40:38.350 --> 01:40:38.360
experiments that affect human subjects
 

01:40:38.360 --> 01:40:40.910
experiments that affect human subjects
these are governed by internal review

01:40:40.910 --> 01:40:40.920
these are governed by internal review
 

01:40:40.920 --> 01:40:44.570
these are governed by internal review
boards and codes of ethics and research

01:40:44.570 --> 01:40:44.580
boards and codes of ethics and research
 

01:40:44.580 --> 01:40:46.310
boards and codes of ethics and research
standards that have evolved over decades

01:40:46.310 --> 01:40:46.320
standards that have evolved over decades
 

01:40:46.320 --> 01:40:48.200
standards that have evolved over decades
and with good reason because without

01:40:48.200 --> 01:40:48.210
and with good reason because without
 

01:40:48.210 --> 01:40:50.570
and with good reason because without
them there were considerable human

01:40:50.570 --> 01:40:50.580
them there were considerable human
 

01:40:50.580 --> 01:40:53.840
them there were considerable human
rights abuses taking place as we were

01:40:53.840 --> 01:40:53.850
rights abuses taking place as we were
 

01:40:53.850 --> 01:40:56.300
rights abuses taking place as we were
pursuing knowledge for in general the

01:40:56.300 --> 01:40:56.310
pursuing knowledge for in general the
 

01:40:56.310 --> 01:40:59.930
pursuing knowledge for in general the
greater good in the corporate context I

01:40:59.930 --> 01:40:59.940
greater good in the corporate context I
 

01:40:59.940 --> 01:41:02.870
greater good in the corporate context I
think and I mean they're not the first

01:41:02.870 --> 01:41:02.880
think and I mean they're not the first
 

01:41:02.880 --> 01:41:05.540
think and I mean they're not the first
person to propose this there has been a

01:41:05.540 --> 01:41:05.550
person to propose this there has been a
 

01:41:05.550 --> 01:41:08.420
person to propose this there has been a
push to consider what does human

01:41:08.420 --> 01:41:08.430
push to consider what does human
 

01:41:08.430 --> 01:41:10.580
push to consider what does human
subjects research actually look like in

01:41:10.580 --> 01:41:10.590
subjects research actually look like in
 

01:41:10.590 --> 01:41:12.290
subjects research actually look like in
that context so where are the human

01:41:12.290 --> 01:41:12.300
that context so where are the human
 

01:41:12.300 --> 01:41:14.750
that context so where are the human
subjects and experimentation and why is

01:41:14.750 --> 01:41:14.760
subjects and experimentation and why is
 

01:41:14.760 --> 01:41:16.580
subjects and experimentation and why is
this a problem outside of

01:41:16.580 --> 01:41:16.590
this a problem outside of
 

01:41:16.590 --> 01:41:19.310
this a problem outside of
the research community well if you think

01:41:19.310 --> 01:41:19.320
the research community well if you think
 

01:41:19.320 --> 01:41:22.280
the research community well if you think
about what's going on in the large

01:41:22.280 --> 01:41:22.290
about what's going on in the large
 

01:41:22.290 --> 01:41:24.500
about what's going on in the large
technology companies they run millionths

01:41:24.500 --> 01:41:24.510
technology companies they run millionths
 

01:41:24.510 --> 01:41:26.839
technology companies they run millionths
of a beat us running experiments on us

01:41:26.839 --> 01:41:26.849
of a beat us running experiments on us
 

01:41:26.849 --> 01:41:27.500
of a beat us running experiments on us
every day

01:41:27.500 --> 01:41:27.510
every day
 

01:41:27.510 --> 01:41:30.229
every day
many of them are benign but some of them

01:41:30.229 --> 01:41:30.239
many of them are benign but some of them
 

01:41:30.239 --> 01:41:32.870
many of them are benign but some of them
actually carry a risk of harm and as

01:41:32.870 --> 01:41:32.880
actually carry a risk of harm and as
 

01:41:32.880 --> 01:41:35.390
actually carry a risk of harm and as
soon as whatever it is that you're

01:41:35.390 --> 01:41:35.400
soon as whatever it is that you're
 

01:41:35.400 --> 01:41:37.640
soon as whatever it is that you're
trying to toggle carries a risk of harm

01:41:37.640 --> 01:41:37.650
trying to toggle carries a risk of harm
 

01:41:37.650 --> 01:41:40.520
trying to toggle carries a risk of harm
to individuals then there is a question

01:41:40.520 --> 01:41:40.530
to individuals then there is a question
 

01:41:40.530 --> 01:41:43.129
to individuals then there is a question
of who are you experimenting on have

01:41:43.129 --> 01:41:43.139
of who are you experimenting on have
 

01:41:43.139 --> 01:41:44.750
of who are you experimenting on have
they provided meaningful informed

01:41:44.750 --> 01:41:44.760
they provided meaningful informed
 

01:41:44.760 --> 01:41:47.419
they provided meaningful informed
consent and how do we even ensure that

01:41:47.419 --> 01:41:47.429
consent and how do we even ensure that
 

01:41:47.429 --> 01:41:49.729
consent and how do we even ensure that
we can collect such a thing in the

01:41:49.729 --> 01:41:49.739
we can collect such a thing in the
 

01:41:49.739 --> 01:41:51.500
we can collect such a thing in the
context of let's say the terms of use

01:41:51.500 --> 01:41:51.510
context of let's say the terms of use
 

01:41:51.510 --> 01:41:53.870
context of let's say the terms of use
that we sign and I think we need to

01:41:53.870 --> 01:41:53.880
that we sign and I think we need to
 

01:41:53.880 --> 01:41:58.450
that we sign and I think we need to
rethink kind of the liberties that

01:41:58.450 --> 01:41:58.460
rethink kind of the liberties that
 

01:41:58.460 --> 01:42:00.470
rethink kind of the liberties that
corporations can take or private

01:42:00.470 --> 01:42:00.480
corporations can take or private
 

01:42:00.480 --> 01:42:02.149
corporations can take or private
entities can take and experimenting on

01:42:02.149 --> 01:42:02.159
entities can take and experimenting on
 

01:42:02.159 --> 01:42:05.209
entities can take and experimenting on
their users because there is potentially

01:42:05.209 --> 01:42:05.219
their users because there is potentially
 

01:42:05.219 --> 01:42:06.770
their users because there is potentially
a real risk of harm in some of these

01:42:06.770 --> 01:42:06.780
a real risk of harm in some of these
 

01:42:06.780 --> 01:42:08.959
a real risk of harm in some of these
instances depending on precisely what it

01:42:08.959 --> 01:42:08.969
instances depending on precisely what it
 

01:42:08.969 --> 01:42:10.729
instances depending on precisely what it
is they're doing so one example that

01:42:10.729 --> 01:42:10.739
is they're doing so one example that
 

01:42:10.739 --> 01:42:12.589
is they're doing so one example that
comes up is for instance rideshare

01:42:12.589 --> 01:42:12.599
comes up is for instance rideshare
 

01:42:12.599 --> 01:42:15.229
comes up is for instance rideshare
companies experiment on routes and whose

01:42:15.229 --> 01:42:15.239
companies experiment on routes and whose
 

01:42:15.239 --> 01:42:16.580
companies experiment on routes and whose
routes are going to be experimented on

01:42:16.580 --> 01:42:16.590
routes are going to be experimented on
 

01:42:16.590 --> 01:42:19.609
routes are going to be experimented on
if I'm moving around the main body of

01:42:19.609 --> 01:42:19.619
if I'm moving around the main body of
 

01:42:19.619 --> 01:42:21.649
if I'm moving around the main body of
Manhattan no one's gonna experiment on

01:42:21.649 --> 01:42:21.659
Manhattan no one's gonna experiment on
 

01:42:21.659 --> 01:42:23.810
Manhattan no one's gonna experiment on
me if I'm in some of the lower

01:42:23.810 --> 01:42:23.820
me if I'm in some of the lower
 

01:42:23.820 --> 01:42:26.510
me if I'm in some of the lower
socioeconomic neighborhoods certainly

01:42:26.510 --> 01:42:26.520
socioeconomic neighborhoods certainly
 

01:42:26.520 --> 01:42:27.350
socioeconomic neighborhoods certainly
there would be considerable

01:42:27.350 --> 01:42:27.360
there would be considerable
 

01:42:27.360 --> 01:42:29.390
there would be considerable
experimentation and that may lead to

01:42:29.390 --> 01:42:29.400
experimentation and that may lead to
 

01:42:29.400 --> 01:42:31.790
experimentation and that may lead to
measurable harm and if you think about

01:42:31.790 --> 01:42:31.800
measurable harm and if you think about
 

01:42:31.800 --> 01:42:35.180
measurable harm and if you think about
it sure I sign or I click agree on a

01:42:35.180 --> 01:42:35.190
it sure I sign or I click agree on a
 

01:42:35.190 --> 01:42:37.189
it sure I sign or I click agree on a
term of use but when we go into a

01:42:37.189 --> 01:42:37.199
term of use but when we go into a
 

01:42:37.199 --> 01:42:39.500
term of use but when we go into a
doctor's office whatever forms we saw

01:42:39.500 --> 01:42:39.510
doctor's office whatever forms we saw
 

01:42:39.510 --> 01:42:41.270
doctor's office whatever forms we saw
and it doesn't just allow the doctor to

01:42:41.270 --> 01:42:41.280
and it doesn't just allow the doctor to
 

01:42:41.280 --> 01:42:43.520
and it doesn't just allow the doctor to
experiment on us however it is that they

01:42:43.520 --> 01:42:43.530
experiment on us however it is that they
 

01:42:43.530 --> 01:42:46.640
experiment on us however it is that they
want to improve their medical service so

01:42:46.640 --> 01:42:46.650
want to improve their medical service so
 

01:42:46.650 --> 01:42:48.500
want to improve their medical service so
at any juncture where there's a

01:42:48.500 --> 01:42:48.510
at any juncture where there's a
 

01:42:48.510 --> 01:42:50.270
at any juncture where there's a
meaningful risk of harm I think we need

01:42:50.270 --> 01:42:50.280
meaningful risk of harm I think we need
 

01:42:50.280 --> 01:42:52.370
meaningful risk of harm I think we need
to reassess this issue of informed

01:42:52.370 --> 01:42:52.380
to reassess this issue of informed
 

01:42:52.380 --> 01:42:56.750
to reassess this issue of informed
consent and actually that involves

01:42:56.750 --> 01:42:56.760
consent and actually that involves
 

01:42:56.760 --> 01:42:58.760
consent and actually that involves
thinking a little bit ahead as to what

01:42:58.760 --> 01:42:58.770
thinking a little bit ahead as to what
 

01:42:58.770 --> 01:43:00.320
thinking a little bit ahead as to what
are the potential harms that might

01:43:00.320 --> 01:43:00.330
are the potential harms that might
 

01:43:00.330 --> 01:43:03.109
are the potential harms that might
result so that leads me to my fellow

01:43:03.109 --> 01:43:03.119
result so that leads me to my fellow
 

01:43:03.119 --> 01:43:05.540
result so that leads me to my fellow
question because I've covered this a

01:43:05.540 --> 01:43:05.550
question because I've covered this a
 

01:43:05.550 --> 01:43:07.700
question because I've covered this a
little bit and we're seeing as I'm sure

01:43:07.700 --> 01:43:07.710
little bit and we're seeing as I'm sure
 

01:43:07.710 --> 01:43:09.709
little bit and we're seeing as I'm sure
you all know that companies are making

01:43:09.709 --> 01:43:09.719
you all know that companies are making
 

01:43:09.719 --> 01:43:11.930
you all know that companies are making
inferences about our health from the way

01:43:11.930 --> 01:43:11.940
inferences about our health from the way
 

01:43:11.940 --> 01:43:14.180
inferences about our health from the way
we use technology and I've been struck

01:43:14.180 --> 01:43:14.190
we use technology and I've been struck
 

01:43:14.190 --> 01:43:15.620
we use technology and I've been struck
by the differences and approaches

01:43:15.620 --> 01:43:15.630
by the differences and approaches
 

01:43:15.630 --> 01:43:19.010
by the differences and approaches
between Microsoft and Facebook so a few

01:43:19.010 --> 01:43:19.020
between Microsoft and Facebook so a few
 

01:43:19.020 --> 01:43:20.810
between Microsoft and Facebook so a few
years ago Microsoft did this study on

01:43:20.810 --> 01:43:20.820
years ago Microsoft did this study on
 

01:43:20.820 --> 01:43:24.350
years ago Microsoft did this study on
depression on Twitter and they got 171

01:43:24.350 --> 01:43:24.360
depression on Twitter and they got 171
 

01:43:24.360 --> 01:43:26.390
depression on Twitter and they got 171
people to volunteer and give Microsoft

01:43:26.390 --> 01:43:26.400
people to volunteer and give Microsoft
 

01:43:26.400 --> 01:43:27.360
people to volunteer and give Microsoft
one-time acts

01:43:27.360 --> 01:43:27.370
one-time acts
 

01:43:27.370 --> 01:43:30.630
one-time acts
to their Twitter feed and so Microsoft

01:43:30.630 --> 01:43:30.640
to their Twitter feed and so Microsoft
 

01:43:30.640 --> 01:43:32.370
to their Twitter feed and so Microsoft
developed algorithms that went back and

01:43:32.370 --> 01:43:32.380
developed algorithms that went back and
 

01:43:32.380 --> 01:43:34.080
developed algorithms that went back and
looked over time you know they knew when

01:43:34.080 --> 01:43:34.090
looked over time you know they knew when
 

01:43:34.090 --> 01:43:36.390
looked over time you know they knew when
the onset of depression was and what

01:43:36.390 --> 01:43:36.400
the onset of depression was and what
 

01:43:36.400 --> 01:43:38.220
the onset of depression was and what
were the tweets like before and after

01:43:38.220 --> 01:43:38.230
were the tweets like before and after
 

01:43:38.230 --> 01:43:40.710
were the tweets like before and after
and looking at the number of words in

01:43:40.710 --> 01:43:40.720
and looking at the number of words in
 

01:43:40.720 --> 01:43:42.660
and looking at the number of words in
the tweets the time of the tweets the

01:43:42.660 --> 01:43:42.670
the tweets the time of the tweets the
 

01:43:42.670 --> 01:43:45.090
the tweets the time of the tweets the
kind of words used in the tweets and

01:43:45.090 --> 01:43:45.100
kind of words used in the tweets and
 

01:43:45.100 --> 01:43:47.520
kind of words used in the tweets and
they discovered that you know when they

01:43:47.520 --> 01:43:47.530
they discovered that you know when they
 

01:43:47.530 --> 01:43:49.470
they discovered that you know when they
tested the algorithm on the group they

01:43:49.470 --> 01:43:49.480
tested the algorithm on the group they
 

01:43:49.480 --> 01:43:52.680
tested the algorithm on the group they
developed it on it was 70% accurate now

01:43:52.680 --> 01:43:52.690
developed it on it was 70% accurate now
 

01:43:52.690 --> 01:43:55.440
developed it on it was 70% accurate now
I know that there were 171 volunteers

01:43:55.440 --> 01:43:55.450
I know that there were 171 volunteers
 

01:43:55.450 --> 01:43:56.970
I know that there were 171 volunteers
and it was 70% accurate because

01:43:56.970 --> 01:43:56.980
and it was 70% accurate because
 

01:43:56.980 --> 01:43:59.790
and it was 70% accurate because
Microsoft published this but Facebook

01:43:59.790 --> 01:43:59.800
Microsoft published this but Facebook
 

01:43:59.800 --> 01:44:02.520
Microsoft published this but Facebook
recently deployed this suicide detection

01:44:02.520 --> 01:44:02.530
recently deployed this suicide detection
 

01:44:02.530 --> 01:44:06.270
recently deployed this suicide detection
algorithm and it is scanning every

01:44:06.270 --> 01:44:06.280
algorithm and it is scanning every
 

01:44:06.280 --> 01:44:08.970
algorithm and it is scanning every
single post every single day on Facebook

01:44:08.970 --> 01:44:08.980
single post every single day on Facebook
 

01:44:08.980 --> 01:44:11.700
single post every single day on Facebook
and also every video without really

01:44:11.700 --> 01:44:11.710
and also every video without really
 

01:44:11.710 --> 01:44:13.710
and also every video without really
telling anybody and when I call to say

01:44:13.710 --> 01:44:13.720
telling anybody and when I call to say
 

01:44:13.720 --> 01:44:17.820
telling anybody and when I call to say
hey you know who did you consult and are

01:44:17.820 --> 01:44:17.830
hey you know who did you consult and are
 

01:44:17.830 --> 01:44:20.270
hey you know who did you consult and are
you gonna publish the data and you know

01:44:20.270 --> 01:44:20.280
you gonna publish the data and you know
 

01:44:20.280 --> 01:44:24.180
you gonna publish the data and you know
do you have kind of informed consent and

01:44:24.180 --> 01:44:24.190
do you have kind of informed consent and
 

01:44:24.190 --> 01:44:27.360
do you have kind of informed consent and
kind of consumers opt-out and how do you

01:44:27.360 --> 01:44:27.370
kind of consumers opt-out and how do you
 

01:44:27.370 --> 01:44:29.460
kind of consumers opt-out and how do you
measure harm there really weren't any

01:44:29.460 --> 01:44:29.470
measure harm there really weren't any
 

01:44:29.470 --> 01:44:31.980
measure harm there really weren't any
answers and so I want to ask you rather

01:44:31.980 --> 01:44:31.990
answers and so I want to ask you rather
 

01:44:31.990 --> 01:44:33.240
answers and so I want to ask you rather
like what do we do about that

01:44:33.240 --> 01:44:33.250
like what do we do about that
 

01:44:33.250 --> 01:44:36.690
like what do we do about that
if you throw I'd this experiment at UPMC

01:44:36.690 --> 01:44:36.700
if you throw I'd this experiment at UPMC
 

01:44:36.700 --> 01:44:38.250
if you throw I'd this experiment at UPMC
you would need an internal review board

01:44:38.250 --> 01:44:38.260
you would need an internal review board
 

01:44:38.260 --> 01:44:41.250
you would need an internal review board
so so if it's even a specific health

01:44:41.250 --> 01:44:41.260
so so if it's even a specific health
 

01:44:41.260 --> 01:44:42.930
so so if it's even a specific health
thing that tech companies are looking

01:44:42.930 --> 01:44:42.940
thing that tech companies are looking
 

01:44:42.940 --> 01:44:46.320
thing that tech companies are looking
for what should we do so I think

01:44:46.320 --> 01:44:46.330
for what should we do so I think
 

01:44:46.330 --> 01:44:49.770
for what should we do so I think
especially in this time and in this age

01:44:49.770 --> 01:44:49.780
especially in this time and in this age
 

01:44:49.780 --> 01:44:53.850
especially in this time and in this age
where we have you know set of dire needs

01:44:53.850 --> 01:44:53.860
where we have you know set of dire needs
 

01:44:53.860 --> 01:44:56.070
where we have you know set of dire needs
to really push technologies like AI

01:44:56.070 --> 01:44:56.080
to really push technologies like AI
 

01:44:56.080 --> 01:44:58.560
to really push technologies like AI
forward and you know like I said earlier

01:44:58.560 --> 01:44:58.570
forward and you know like I said earlier
 

01:44:58.570 --> 01:45:00.810
forward and you know like I said earlier
really try to bring to scale the

01:45:00.810 --> 01:45:00.820
really try to bring to scale the
 

01:45:00.820 --> 01:45:02.490
really try to bring to scale the
capabilities that we've we've been

01:45:02.490 --> 01:45:02.500
capabilities that we've we've been
 

01:45:02.500 --> 01:45:04.950
capabilities that we've we've been
really working on in the pursuit of say

01:45:04.950 --> 01:45:04.960
really working on in the pursuit of say
 

01:45:04.960 --> 01:45:06.630
really working on in the pursuit of say
excellence in health care which is my

01:45:06.630 --> 01:45:06.640
excellence in health care which is my
 

01:45:06.640 --> 01:45:10.620
excellence in health care which is my
space I think the need the impetus for

01:45:10.620 --> 01:45:10.630
space I think the need the impetus for
 

01:45:10.630 --> 01:45:14.390
space I think the need the impetus for
us to double down perhaps on ethics is

01:45:14.390 --> 01:45:14.400
us to double down perhaps on ethics is
 

01:45:14.400 --> 01:45:18.090
us to double down perhaps on ethics is
all the more important it is such a

01:45:18.090 --> 01:45:18.100
all the more important it is such a
 

01:45:18.100 --> 01:45:19.980
all the more important it is such a
slippery slope it can be a very very

01:45:19.980 --> 01:45:19.990
slippery slope it can be a very very
 

01:45:19.990 --> 01:45:22.710
slippery slope it can be a very very
slippery slope for us not to do that for

01:45:22.710 --> 01:45:22.720
slippery slope for us not to do that for
 

01:45:22.720 --> 01:45:24.540
slippery slope for us not to do that for
us to say hey but you know it's

01:45:24.540 --> 01:45:24.550
us to say hey but you know it's
 

01:45:24.550 --> 01:45:27.150
us to say hey but you know it's
information that's out there and we're

01:45:27.150 --> 01:45:27.160
information that's out there and we're
 

01:45:27.160 --> 01:45:28.980
information that's out there and we're
trying to do this and the greater good

01:45:28.980 --> 01:45:28.990
trying to do this and the greater good
 

01:45:28.990 --> 01:45:32.550
trying to do this and the greater good
of humanity anyway yes we are but at the

01:45:32.550 --> 01:45:32.560
of humanity anyway yes we are but at the
 

01:45:32.560 --> 01:45:34.410
of humanity anyway yes we are but at the
same time we're talking about human

01:45:34.410 --> 01:45:34.420
same time we're talking about human
 

01:45:34.420 --> 01:45:36.300
same time we're talking about human
beings when it comes to human subjects

01:45:36.300 --> 01:45:36.310
beings when it comes to human subjects
 

01:45:36.310 --> 01:45:39.480
beings when it comes to human subjects
there are specific IRB type protocols as

01:45:39.480 --> 01:45:39.490
there are specific IRB type protocols as
 

01:45:39.490 --> 01:45:40.160
there are specific IRB type protocols as
Alex Minh

01:45:40.160 --> 01:45:40.170
Alex Minh
 

01:45:40.170 --> 01:45:43.160
Alex Minh
that are there for a reason right so

01:45:43.160 --> 01:45:43.170
that are there for a reason right so
 

01:45:43.170 --> 01:45:44.300
that are there for a reason right so
there's a right way to go about doing

01:45:44.300 --> 01:45:44.310
there's a right way to go about doing
 

01:45:44.310 --> 01:45:45.860
there's a right way to go about doing
this and a wrong way to go about doing

01:45:45.860 --> 01:45:45.870
this and a wrong way to go about doing
 

01:45:45.870 --> 01:45:48.590
this and a wrong way to go about doing
this and we believe that that's part of

01:45:48.590 --> 01:45:48.600
this and we believe that that's part of
 

01:45:48.600 --> 01:45:51.740
this and we believe that that's part of
the reason why UPMC and UPMC enterprises

01:45:51.740 --> 01:45:51.750
the reason why UPMC and UPMC enterprises
 

01:45:51.750 --> 01:45:54.830
the reason why UPMC and UPMC enterprises
exists right so we're working with you

01:45:54.830 --> 01:45:54.840
exists right so we're working with you
 

01:45:54.840 --> 01:45:56.720
exists right so we're working with you
know large companies like Microsoft for

01:45:56.720 --> 01:45:56.730
know large companies like Microsoft for
 

01:45:56.730 --> 01:45:59.990
know large companies like Microsoft for
example and taking large troves of data

01:45:59.990 --> 01:46:00.000
example and taking large troves of data
 

01:46:00.000 --> 01:46:02.290
example and taking large troves of data
we have three point

01:46:02.290 --> 01:46:02.300
we have three point
 

01:46:02.300 --> 01:46:05.510
we have three point
sorry 227 petabytes worth of data at

01:46:05.510 --> 01:46:05.520
sorry 227 petabytes worth of data at
 

01:46:05.520 --> 01:46:07.760
sorry 227 petabytes worth of data at
UPMC its doubling every 18 months it

01:46:07.760 --> 01:46:07.770
UPMC its doubling every 18 months it
 

01:46:07.770 --> 01:46:10.610
UPMC its doubling every 18 months it
continues to increase exponentially but

01:46:10.610 --> 01:46:10.620
continues to increase exponentially but
 

01:46:10.620 --> 01:46:12.170
continues to increase exponentially but
we're working with machine learning

01:46:12.170 --> 01:46:12.180
we're working with machine learning
 

01:46:12.180 --> 01:46:14.450
we're working with machine learning
experts in AI experts and companies like

01:46:14.450 --> 01:46:14.460
experts in AI experts and companies like
 

01:46:14.460 --> 01:46:17.140
experts in AI experts and companies like
Microsoft and entrepreneurs and startups

01:46:17.140 --> 01:46:17.150
Microsoft and entrepreneurs and startups
 

01:46:17.150 --> 01:46:22.040
Microsoft and entrepreneurs and startups
but with a ethical pursuit taking into

01:46:22.040 --> 01:46:22.050
but with a ethical pursuit taking into
 

01:46:22.050 --> 01:46:24.200
but with a ethical pursuit taking into
account all of the rules and regulations

01:46:24.200 --> 01:46:24.210
account all of the rules and regulations
 

01:46:24.210 --> 01:46:26.630
account all of the rules and regulations
that IR bees have right so we're not

01:46:26.630 --> 01:46:26.640
that IR bees have right so we're not
 

01:46:26.640 --> 01:46:28.130
that IR bees have right so we're not
trying to shortchange those we're

01:46:28.130 --> 01:46:28.140
trying to shortchange those we're
 

01:46:28.140 --> 01:46:30.050
trying to shortchange those we're
working within the constraints but we're

01:46:30.050 --> 01:46:30.060
working within the constraints but we're
 

01:46:30.060 --> 01:46:32.990
working within the constraints but we're
doing this at scale leveraging these

01:46:32.990 --> 01:46:33.000
doing this at scale leveraging these
 

01:46:33.000 --> 01:46:35.330
doing this at scale leveraging these
technologies and capabilities so you

01:46:35.330 --> 01:46:35.340
technologies and capabilities so you
 

01:46:35.340 --> 01:46:36.650
technologies and capabilities so you
know I think it's time for us to double

01:46:36.650 --> 01:46:36.660
know I think it's time for us to double
 

01:46:36.660 --> 01:46:39.080
know I think it's time for us to double
down on these capabilities and making

01:46:39.080 --> 01:46:39.090
down on these capabilities and making
 

01:46:39.090 --> 01:46:41.300
down on these capabilities and making
sure that we're able to do this in the

01:46:41.300 --> 01:46:41.310
sure that we're able to do this in the
 

01:46:41.310 --> 01:46:44.990
sure that we're able to do this in the
most ethical manner possible and Mike

01:46:44.990 --> 01:46:45.000
most ethical manner possible and Mike
 

01:46:45.000 --> 01:46:47.510
most ethical manner possible and Mike
who do we think should regulate this

01:46:47.510 --> 01:46:47.520
who do we think should regulate this
 

01:46:47.520 --> 01:46:50.480
who do we think should regulate this
when we talk about equity of access and

01:46:50.480 --> 01:46:50.490
when we talk about equity of access and
 

01:46:50.490 --> 01:46:52.550
when we talk about equity of access and
equity of impact is it up to researchers

01:46:52.550 --> 01:46:52.560
equity of impact is it up to researchers
 

01:46:52.560 --> 01:46:55.780
equity of impact is it up to researchers
to be accountable for their own

01:46:55.780 --> 01:46:55.790
to be accountable for their own
 

01:46:55.790 --> 01:46:59.060
to be accountable for their own
innovations should the government come

01:46:59.060 --> 01:46:59.070
innovations should the government come
 

01:46:59.070 --> 01:47:01.970
innovations should the government come
in and regulate or legislate AI and make

01:47:01.970 --> 01:47:01.980
in and regulate or legislate AI and make
 

01:47:01.980 --> 01:47:03.890
in and regulate or legislate AI and make
sure it's equitable and then we have the

01:47:03.890 --> 01:47:03.900
sure it's equitable and then we have the
 

01:47:03.900 --> 01:47:06.290
sure it's equitable and then we have the
problem with you know there's a minority

01:47:06.290 --> 01:47:06.300
problem with you know there's a minority
 

01:47:06.300 --> 01:47:08.480
problem with you know there's a minority
of politicians who actually understand

01:47:08.480 --> 01:47:08.490
of politicians who actually understand
 

01:47:08.490 --> 01:47:10.400
of politicians who actually understand
how AI works so are they the right

01:47:10.400 --> 01:47:10.410
how AI works so are they the right
 

01:47:10.410 --> 01:47:13.520
how AI works so are they the right
audience yeah I mean I think it's gonna

01:47:13.520 --> 01:47:13.530
audience yeah I mean I think it's gonna
 

01:47:13.530 --> 01:47:16.070
audience yeah I mean I think it's gonna
have to be a mixed bag to say that

01:47:16.070 --> 01:47:16.080
have to be a mixed bag to say that
 

01:47:16.080 --> 01:47:18.640
have to be a mixed bag to say that
there's some kind of you know simple one

01:47:18.640 --> 01:47:18.650
there's some kind of you know simple one
 

01:47:18.650 --> 01:47:22.670
there's some kind of you know simple one
unilateral way in which we can kind of

01:47:22.670 --> 01:47:22.680
unilateral way in which we can kind of
 

01:47:22.680 --> 01:47:24.950
unilateral way in which we can kind of
ensure that this trust and

01:47:24.950 --> 01:47:24.960
ensure that this trust and
 

01:47:24.960 --> 01:47:26.450
ensure that this trust and
accountability is instilled in the

01:47:26.450 --> 01:47:26.460
accountability is instilled in the
 

01:47:26.460 --> 01:47:27.920
accountability is instilled in the
system I think that would be crazy so

01:47:27.920 --> 01:47:27.930
system I think that would be crazy so
 

01:47:27.930 --> 01:47:30.650
system I think that would be crazy so
it's gonna have to come in part from I

01:47:30.650 --> 01:47:30.660
it's gonna have to come in part from I
 

01:47:30.660 --> 01:47:33.080
it's gonna have to come in part from I
do think some amount of legislation or

01:47:33.080 --> 01:47:33.090
do think some amount of legislation or
 

01:47:33.090 --> 01:47:35.270
do think some amount of legislation or
regulation because even talking about

01:47:35.270 --> 01:47:35.280
regulation because even talking about
 

01:47:35.280 --> 01:47:37.490
regulation because even talking about
the things that Ross is talking about

01:47:37.490 --> 01:47:37.500
the things that Ross is talking about
 

01:47:37.500 --> 01:47:38.930
the things that Ross is talking about
and some of the exciting things you

01:47:38.930 --> 01:47:38.940
and some of the exciting things you
 

01:47:38.940 --> 01:47:40.970
and some of the exciting things you
think about well if we think that the

01:47:40.970 --> 01:47:40.980
think about well if we think that the
 

01:47:40.980 --> 01:47:42.830
think about well if we think that the
landscape of a certain industry say the

01:47:42.830 --> 01:47:42.840
landscape of a certain industry say the
 

01:47:42.840 --> 01:47:44.600
landscape of a certain industry say the
healthcare industry already has

01:47:44.600 --> 01:47:44.610
healthcare industry already has
 

01:47:44.610 --> 01:47:47.510
healthcare industry already has
inequities in it so the soil for the

01:47:47.510 --> 01:47:47.520
inequities in it so the soil for the
 

01:47:47.520 --> 01:47:49.790
inequities in it so the soil for the
seed to be planted in is already maybe

01:47:49.790 --> 01:47:49.800
seed to be planted in is already maybe
 

01:47:49.800 --> 01:47:51.920
seed to be planted in is already maybe
not actually the most fertile for

01:47:51.920 --> 01:47:51.930
not actually the most fertile for
 

01:47:51.930 --> 01:47:54.439
not actually the most fertile for
equality you probably are going to need

01:47:54.439 --> 01:47:54.449
equality you probably are going to need
 

01:47:54.449 --> 01:47:56.569
equality you probably are going to need
some kind of regulatory mechanism to

01:47:56.569 --> 01:47:56.579
some kind of regulatory mechanism to
 

01:47:56.579 --> 01:47:57.979
some kind of regulatory mechanism to
make sure the development of these of

01:47:57.979 --> 01:47:57.989
make sure the development of these of
 

01:47:57.989 --> 01:47:59.390
make sure the development of these of
these technologies is fair and that

01:47:59.390 --> 01:47:59.400
these technologies is fair and that
 

01:47:59.400 --> 01:48:02.359
these technologies is fair and that
might be pushing on antitrust law and

01:48:02.359 --> 01:48:02.369
might be pushing on antitrust law and
 

01:48:02.369 --> 01:48:04.279
might be pushing on antitrust law and
actually saying that what we cannot have

01:48:04.279 --> 01:48:04.289
actually saying that what we cannot have
 

01:48:04.289 --> 01:48:06.049
actually saying that what we cannot have
constant mergers and acquisitions so

01:48:06.049 --> 01:48:06.059
constant mergers and acquisitions so
 

01:48:06.059 --> 01:48:07.879
constant mergers and acquisitions so
that a few companies who can kind of

01:48:07.879 --> 01:48:07.889
that a few companies who can kind of
 

01:48:07.889 --> 01:48:09.469
that a few companies who can kind of
lobby for a position that's really

01:48:09.469 --> 01:48:09.479
lobby for a position that's really
 

01:48:09.479 --> 01:48:11.299
lobby for a position that's really
really good for them to deploy these

01:48:11.299 --> 01:48:11.309
really good for them to deploy these
 

01:48:11.309 --> 01:48:13.399
really good for them to deploy these
technologies rapidly without IRB or

01:48:13.399 --> 01:48:13.409
technologies rapidly without IRB or
 

01:48:13.409 --> 01:48:15.770
technologies rapidly without IRB or
without really good informed consent we

01:48:15.770 --> 01:48:15.780
without really good informed consent we
 

01:48:15.780 --> 01:48:17.479
without really good informed consent we
might need a way to prevent that at the

01:48:17.479 --> 01:48:17.489
might need a way to prevent that at the
 

01:48:17.489 --> 01:48:19.189
might need a way to prevent that at the
level of business but then we might also

01:48:19.189 --> 01:48:19.199
level of business but then we might also
 

01:48:19.199 --> 01:48:21.140
level of business but then we might also
have to say that okay there's gonna be

01:48:21.140 --> 01:48:21.150
have to say that okay there's gonna be
 

01:48:21.150 --> 01:48:23.419
have to say that okay there's gonna be
minimum standards for what data use or

01:48:23.419 --> 01:48:23.429
minimum standards for what data use or
 

01:48:23.429 --> 01:48:25.189
minimum standards for what data use or
data practices come with an informed

01:48:25.189 --> 01:48:25.199
data practices come with an informed
 

01:48:25.199 --> 01:48:27.199
data practices come with an informed
consent mechanism because as as

01:48:27.199 --> 01:48:27.209
consent mechanism because as as
 

01:48:27.209 --> 01:48:29.899
consent mechanism because as as
Alexander is talking about an IRB you're

01:48:29.899 --> 01:48:29.909
Alexander is talking about an IRB you're
 

01:48:29.909 --> 01:48:31.790
Alexander is talking about an IRB you're
you're actually agreeing to a very

01:48:31.790 --> 01:48:31.800
you're actually agreeing to a very
 

01:48:31.800 --> 01:48:33.649
you're actually agreeing to a very
specific study which you understand the

01:48:33.649 --> 01:48:33.659
specific study which you understand the
 

01:48:33.659 --> 01:48:36.109
specific study which you understand the
purpose of whenever I agree to a Terms

01:48:36.109 --> 01:48:36.119
purpose of whenever I agree to a Terms
 

01:48:36.119 --> 01:48:38.870
purpose of whenever I agree to a Terms
of Service on Facebook I broadly agree

01:48:38.870 --> 01:48:38.880
of Service on Facebook I broadly agree
 

01:48:38.880 --> 01:48:41.660
of Service on Facebook I broadly agree
to marketing or something product

01:48:41.660 --> 01:48:41.670
to marketing or something product
 

01:48:41.670 --> 01:48:44.600
to marketing or something product
development and to my partners using

01:48:44.600 --> 01:48:44.610
development and to my partners using
 

01:48:44.610 --> 01:48:47.060
development and to my partners using
this data and I do think that that is

01:48:47.060 --> 01:48:47.070
this data and I do think that that is
 

01:48:47.070 --> 01:48:49.310
this data and I do think that that is
gonna take then business leaders who say

01:48:49.310 --> 01:48:49.320
gonna take then business leaders who say
 

01:48:49.320 --> 01:48:52.129
gonna take then business leaders who say
okay actually enough with these you know

01:48:52.129 --> 01:48:52.139
okay actually enough with these you know
 

01:48:52.139 --> 01:48:53.719
okay actually enough with these you know
broad blanket provisions and they're

01:48:53.719 --> 01:48:53.729
broad blanket provisions and they're
 

01:48:53.729 --> 01:48:54.979
broad blanket provisions and they're
gonna have to have some potential

01:48:54.979 --> 01:48:54.989
gonna have to have some potential
 

01:48:54.989 --> 01:48:57.229
gonna have to have some potential
pressure from maybe somebody on the

01:48:57.229 --> 01:48:57.239
pressure from maybe somebody on the
 

01:48:57.239 --> 01:48:59.779
pressure from maybe somebody on the
litigation side or regulators saying

01:48:59.779 --> 01:48:59.789
litigation side or regulators saying
 

01:48:59.789 --> 01:49:01.399
litigation side or regulators saying
that hey if things don't clean up we're

01:49:01.399 --> 01:49:01.409
that hey if things don't clean up we're
 

01:49:01.409 --> 01:49:03.290
that hey if things don't clean up we're
actually going to have to change the way

01:49:03.290 --> 01:49:03.300
actually going to have to change the way
 

01:49:03.300 --> 01:49:04.910
actually going to have to change the way
that consent works and then maybe they

01:49:04.910 --> 01:49:04.920
that consent works and then maybe they
 

01:49:04.920 --> 01:49:06.109
that consent works and then maybe they
should say well we have to be more

01:49:06.109 --> 01:49:06.119
should say well we have to be more
 

01:49:06.119 --> 01:49:08.449
should say well we have to be more
specific named who our partners are or

01:49:08.449 --> 01:49:08.459
specific named who our partners are or
 

01:49:08.459 --> 01:49:10.819
specific named who our partners are or
named what exact things were actually

01:49:10.819 --> 01:49:10.829
named what exact things were actually
 

01:49:10.829 --> 01:49:12.770
named what exact things were actually
going to do with your data so people

01:49:12.770 --> 01:49:12.780
going to do with your data so people
 

01:49:12.780 --> 01:49:14.569
going to do with your data so people
have a way to to meaningfully choose

01:49:14.569 --> 01:49:14.579
have a way to to meaningfully choose
 

01:49:14.579 --> 01:49:15.919
have a way to to meaningfully choose
because right now I wouldn't I would say

01:49:15.919 --> 01:49:15.929
because right now I wouldn't I would say
 

01:49:15.929 --> 01:49:17.719
because right now I wouldn't I would say
that there's a consent procedure but the

01:49:17.719 --> 01:49:17.729
that there's a consent procedure but the
 

01:49:17.729 --> 01:49:19.580
that there's a consent procedure but the
informed parts been slashed for the most

01:49:19.580 --> 01:49:19.590
informed parts been slashed for the most
 

01:49:19.590 --> 01:49:21.080
informed parts been slashed for the most
part but then there's also

01:49:21.080 --> 01:49:21.090
part but then there's also
 

01:49:21.090 --> 01:49:23.540
part but then there's also
responsibility I think on on us as a

01:49:23.540 --> 01:49:23.550
responsibility I think on on us as a
 

01:49:23.550 --> 01:49:25.819
responsibility I think on on us as a
public which maybe is an educational

01:49:25.819 --> 01:49:25.829
public which maybe is an educational
 

01:49:25.829 --> 01:49:27.609
public which maybe is an educational
responsibility to become literate and

01:49:27.609 --> 01:49:27.619
responsibility to become literate and
 

01:49:27.619 --> 01:49:30.890
responsibility to become literate and
data literate and understanding things

01:49:30.890 --> 01:49:30.900
data literate and understanding things
 

01:49:30.900 --> 01:49:32.600
data literate and understanding things
that are really complicated right now

01:49:32.600 --> 01:49:32.610
that are really complicated right now
 

01:49:32.610 --> 01:49:34.310
that are really complicated right now
and I don't want to put too much of an

01:49:34.310 --> 01:49:34.320
and I don't want to put too much of an
 

01:49:34.320 --> 01:49:35.870
and I don't want to put too much of an
onus on people that you have to go out

01:49:35.870 --> 01:49:35.880
onus on people that you have to go out
 

01:49:35.880 --> 01:49:37.339
onus on people that you have to go out
and start reading up on machine learning

01:49:37.339 --> 01:49:37.349
and start reading up on machine learning
 

01:49:37.349 --> 01:49:40.069
and start reading up on machine learning
books but I do think people developing a

01:49:40.069 --> 01:49:40.079
books but I do think people developing a
 

01:49:40.079 --> 01:49:41.750
books but I do think people developing a
consciousness that when you put a photo

01:49:41.750 --> 01:49:41.760
consciousness that when you put a photo
 

01:49:41.760 --> 01:49:44.689
consciousness that when you put a photo
out there that there might actually be a

01:49:44.689 --> 01:49:44.699
out there that there might actually be a
 

01:49:44.699 --> 01:49:46.819
out there that there might actually be a
way to make an inference about medical

01:49:46.819 --> 01:49:46.829
way to make an inference about medical
 

01:49:46.829 --> 01:49:49.160
way to make an inference about medical
information from a photo I think a lot

01:49:49.160 --> 01:49:49.170
information from a photo I think a lot
 

01:49:49.170 --> 01:49:51.259
information from a photo I think a lot
of people don't even really know that or

01:49:51.259 --> 01:49:51.269
of people don't even really know that or
 

01:49:51.269 --> 01:49:52.969
of people don't even really know that or
can't connect those dots and so I do

01:49:52.969 --> 01:49:52.979
can't connect those dots and so I do
 

01:49:52.979 --> 01:49:55.640
can't connect those dots and so I do
think that's where I think academia and

01:49:55.640 --> 01:49:55.650
think that's where I think academia and
 

01:49:55.650 --> 01:49:57.439
think that's where I think academia and
educators probably have a responsibility

01:49:57.439 --> 01:49:57.449
educators probably have a responsibility
 

01:49:57.449 --> 01:49:59.719
educators probably have a responsibility
to their public to making sure as we're

01:49:59.719 --> 01:49:59.729
to their public to making sure as we're
 

01:49:59.729 --> 01:50:01.489
to their public to making sure as we're
developing these new techniques are we

01:50:01.489 --> 01:50:01.499
developing these new techniques are we
 

01:50:01.499 --> 01:50:03.439
developing these new techniques are we
actually doing outreach that helps

01:50:03.439 --> 01:50:03.449
actually doing outreach that helps
 

01:50:03.449 --> 01:50:05.239
actually doing outreach that helps
people understand the capabilities of

01:50:05.239 --> 01:50:05.249
people understand the capabilities of
 

01:50:05.249 --> 01:50:06.670
people understand the capabilities of
what their society around

01:50:06.670 --> 01:50:06.680
what their society around
 

01:50:06.680 --> 01:50:10.390
what their society around
is becoming and so and so and I think of

01:50:10.390 --> 01:50:10.400
is becoming and so and so and I think of
 

01:50:10.400 --> 01:50:12.010
is becoming and so and so and I think of
course the the last pieces with

01:50:12.010 --> 01:50:12.020
course the the last pieces with
 

01:50:12.020 --> 01:50:14.800
course the the last pieces with
academics and both Rossi and Alex are

01:50:14.800 --> 01:50:14.810
academics and both Rossi and Alex are
 

01:50:14.810 --> 01:50:17.770
academics and both Rossi and Alex are
great examples of this but academics who

01:50:17.770 --> 01:50:17.780
great examples of this but academics who
 

01:50:17.780 --> 01:50:19.570
great examples of this but academics who
also continually to challenge those

01:50:19.570 --> 01:50:19.580
also continually to challenge those
 

01:50:19.580 --> 01:50:21.100
also continually to challenge those
questions as they're doing the research

01:50:21.100 --> 01:50:21.110
questions as they're doing the research
 

01:50:21.110 --> 01:50:24.300
questions as they're doing the research
and put into place early on a priori to

01:50:24.300 --> 01:50:24.310
and put into place early on a priori to
 

01:50:24.310 --> 01:50:26.710
and put into place early on a priori to
offering up a paper of a new technique

01:50:26.710 --> 01:50:26.720
offering up a paper of a new technique
 

01:50:26.720 --> 01:50:28.930
offering up a paper of a new technique
also putting in here's the questions you

01:50:28.930 --> 01:50:28.940
also putting in here's the questions you
 

01:50:28.940 --> 01:50:30.460
also putting in here's the questions you
should interrogate of this technique

01:50:30.460 --> 01:50:30.470
should interrogate of this technique
 

01:50:30.470 --> 01:50:32.470
should interrogate of this technique
before it's commercialized before it's

01:50:32.470 --> 01:50:32.480
before it's commercialized before it's
 

01:50:32.480 --> 01:50:34.330
before it's commercialized before it's
done at scale and having those questions

01:50:34.330 --> 01:50:34.340
done at scale and having those questions
 

01:50:34.340 --> 01:50:35.830
done at scale and having those questions
up front so that somebody doesn't have

01:50:35.830 --> 01:50:35.840
up front so that somebody doesn't have
 

01:50:35.840 --> 01:50:37.870
up front so that somebody doesn't have
an excuse later to go back and say well

01:50:37.870 --> 01:50:37.880
an excuse later to go back and say well
 

01:50:37.880 --> 01:50:39.370
an excuse later to go back and say well
oh I had no idea should ask that

01:50:39.370 --> 01:50:39.380
oh I had no idea should ask that
 

01:50:39.380 --> 01:50:40.840
oh I had no idea should ask that
question which I think a lot is what

01:50:40.840 --> 01:50:40.850
question which I think a lot is what
 

01:50:40.850 --> 01:50:43.240
question which I think a lot is what
happens now people go wrap it forward

01:50:43.240 --> 01:50:43.250
happens now people go wrap it forward
 

01:50:43.250 --> 01:50:45.010
happens now people go wrap it forward
and then once the question arises here

01:50:45.010 --> 01:50:45.020
and then once the question arises here
 

01:50:45.020 --> 01:50:46.510
and then once the question arises here
becomes obvious it's like oh man I

01:50:46.510 --> 01:50:46.520
becomes obvious it's like oh man I
 

01:50:46.520 --> 01:50:48.220
becomes obvious it's like oh man I
should and I should have known or

01:50:48.220 --> 01:50:48.230
should and I should have known or
 

01:50:48.230 --> 01:50:49.870
should and I should have known or
something but we could potentially

01:50:49.870 --> 01:50:49.880
something but we could potentially
 

01:50:49.880 --> 01:50:51.640
something but we could potentially
preempt that a bit by making that part

01:50:51.640 --> 01:50:51.650
preempt that a bit by making that part
 

01:50:51.650 --> 01:50:54.160
preempt that a bit by making that part
of the research process so we talked

01:50:54.160 --> 01:50:54.170
of the research process so we talked
 

01:50:54.170 --> 01:50:55.810
of the research process so we talked
about all the possible things that could

01:50:55.810 --> 01:50:55.820
about all the possible things that could
 

01:50:55.820 --> 01:50:58.900
about all the possible things that could
go wrong and Alex I was hoping you would

01:50:58.900 --> 01:50:58.910
go wrong and Alex I was hoping you would
 

01:50:58.910 --> 01:51:00.370
go wrong and Alex I was hoping you would
talk about you know how do we measure

01:51:00.370 --> 01:51:00.380
talk about you know how do we measure
 

01:51:00.380 --> 01:51:04.360
talk about you know how do we measure
fairness and what examples do we have of

01:51:04.360 --> 01:51:04.370
fairness and what examples do we have of
 

01:51:04.370 --> 01:51:06.460
fairness and what examples do we have of
algorithms that are actually improving

01:51:06.460 --> 01:51:06.470
algorithms that are actually improving
 

01:51:06.470 --> 01:51:09.220
algorithms that are actually improving
impact and and can you talk a little

01:51:09.220 --> 01:51:09.230
impact and and can you talk a little
 

01:51:09.230 --> 01:51:11.080
impact and and can you talk a little
about your work with the Allegheny

01:51:11.080 --> 01:51:11.090
about your work with the Allegheny
 

01:51:11.090 --> 01:51:14.920
about your work with the Allegheny
children's welfare system sure so this

01:51:14.920 --> 01:51:14.930
children's welfare system sure so this
 

01:51:14.930 --> 01:51:16.420
children's welfare system sure so this
really is a place where I think we have

01:51:16.420 --> 01:51:16.430
really is a place where I think we have
 

01:51:16.430 --> 01:51:19.300
really is a place where I think we have
cause for optimism because in many

01:51:19.300 --> 01:51:19.310
cause for optimism because in many
 

01:51:19.310 --> 01:51:21.370
cause for optimism because in many
instances as I mentioned in my opening

01:51:21.370 --> 01:51:21.380
instances as I mentioned in my opening
 

01:51:21.380 --> 01:51:23.110
instances as I mentioned in my opening
remarks the baseline that we're

01:51:23.110 --> 01:51:23.120
remarks the baseline that we're
 

01:51:23.120 --> 01:51:27.120
remarks the baseline that we're
comparing to is very uninspiring and

01:51:27.120 --> 01:51:27.130
comparing to is very uninspiring and
 

01:51:27.130 --> 01:51:29.980
comparing to is very uninspiring and
this actually relates to what a lot of

01:51:29.980 --> 01:51:29.990
this actually relates to what a lot of
 

01:51:29.990 --> 01:51:31.780
this actually relates to what a lot of
people think of as a fundamental problem

01:51:31.780 --> 01:51:31.790
people think of as a fundamental problem
 

01:51:31.790 --> 01:51:34.360
people think of as a fundamental problem
in training fair algorithms which is the

01:51:34.360 --> 01:51:34.370
in training fair algorithms which is the
 

01:51:34.370 --> 01:51:36.700
in training fair algorithms which is the
bias and the original data but what does

01:51:36.700 --> 01:51:36.710
bias and the original data but what does
 

01:51:36.710 --> 01:51:39.100
bias and the original data but what does
that refer to it may refer to for

01:51:39.100 --> 01:51:39.110
that refer to it may refer to for
 

01:51:39.110 --> 01:51:40.630
that refer to it may refer to for
instance in the child welfare context

01:51:40.630 --> 01:51:40.640
instance in the child welfare context
 

01:51:40.640 --> 01:51:42.880
instance in the child welfare context
that were systematically screening and

01:51:42.880 --> 01:51:42.890
that were systematically screening and
 

01:51:42.890 --> 01:51:46.330
that were systematically screening and
for investigation families from

01:51:46.330 --> 01:51:46.340
for investigation families from
 

01:51:46.340 --> 01:51:48.730
for investigation families from
communities of color at higher rates

01:51:48.730 --> 01:51:48.740
communities of color at higher rates
 

01:51:48.740 --> 01:51:53.890
communities of color at higher rates
than white families and if you don't

01:51:53.890 --> 01:51:53.900
than white families and if you don't
 

01:51:53.900 --> 01:51:55.840
than white families and if you don't
look at the data if you never collect it

01:51:55.840 --> 01:51:55.850
look at the data if you never collect it
 

01:51:55.850 --> 01:51:57.400
look at the data if you never collect it
you'll never realize that this is an

01:51:57.400 --> 01:51:57.410
you'll never realize that this is an
 

01:51:57.410 --> 01:51:59.560
you'll never realize that this is an
issue with your existing process so in

01:51:59.560 --> 01:51:59.570
issue with your existing process so in
 

01:51:59.570 --> 01:52:01.240
issue with your existing process so in
part realizing that the data we might

01:52:01.240 --> 01:52:01.250
part realizing that the data we might
 

01:52:01.250 --> 01:52:02.710
part realizing that the data we might
want to use for some kind of downstream

01:52:02.710 --> 01:52:02.720
want to use for some kind of downstream
 

01:52:02.720 --> 01:52:05.470
want to use for some kind of downstream
purposes bias is suggesting to us that

01:52:05.470 --> 01:52:05.480
purposes bias is suggesting to us that
 

01:52:05.480 --> 01:52:07.690
purposes bias is suggesting to us that
there may be inequities in our existing

01:52:07.690 --> 01:52:07.700
there may be inequities in our existing
 

01:52:07.700 --> 01:52:09.340
there may be inequities in our existing
process that we might want to address

01:52:09.340 --> 01:52:09.350
process that we might want to address
 

01:52:09.350 --> 01:52:12.880
process that we might want to address
and correct for quantifying fairness is

01:52:12.880 --> 01:52:12.890
and correct for quantifying fairness is
 

01:52:12.890 --> 01:52:15.110
and correct for quantifying fairness is
extremely difficult and

01:52:15.110 --> 01:52:15.120
extremely difficult and
 

01:52:15.120 --> 01:52:17.780
extremely difficult and
the session speaker pointed out we do

01:52:17.780 --> 01:52:17.790
the session speaker pointed out we do
 

01:52:17.790 --> 01:52:19.940
the session speaker pointed out we do
have impossibility results that say that

01:52:19.940 --> 01:52:19.950
have impossibility results that say that
 

01:52:19.950 --> 01:52:21.890
have impossibility results that say that
you may have certain quantifiable

01:52:21.890 --> 01:52:21.900
you may have certain quantifiable
 

01:52:21.900 --> 01:52:24.500
you may have certain quantifiable
desiderata that you want to satisfy and

01:52:24.500 --> 01:52:24.510
desiderata that you want to satisfy and
 

01:52:24.510 --> 01:52:26.510
desiderata that you want to satisfy and
you just can't have it all fundamentally

01:52:26.510 --> 01:52:26.520
you just can't have it all fundamentally
 

01:52:26.520 --> 01:52:30.560
you just can't have it all fundamentally
but at the same time what we can do is

01:52:30.560 --> 01:52:30.570
but at the same time what we can do is
 

01:52:30.570 --> 01:52:32.960
but at the same time what we can do is
we can look at the system we can collect

01:52:32.960 --> 01:52:32.970
we can look at the system we can collect
 

01:52:32.970 --> 01:52:35.480
we can look at the system we can collect
data on the process we can see how well

01:52:35.480 --> 01:52:35.490
data on the process we can see how well
 

01:52:35.490 --> 01:52:37.430
data on the process we can see how well
it's currently performing and we can

01:52:37.430 --> 01:52:37.440
it's currently performing and we can
 

01:52:37.440 --> 01:52:39.440
it's currently performing and we can
think about what are the metrics that

01:52:39.440 --> 01:52:39.450
think about what are the metrics that
 

01:52:39.450 --> 01:52:42.440
think about what are the metrics that
matter most to us so maybe it's really

01:52:42.440 --> 01:52:42.450
matter most to us so maybe it's really
 

01:52:42.450 --> 01:52:44.870
matter most to us so maybe it's really
making sure that to the extent that we

01:52:44.870 --> 01:52:44.880
making sure that to the extent that we
 

01:52:44.880 --> 01:52:48.860
making sure that to the extent that we
can assess need or we can assess a kind

01:52:48.860 --> 01:52:48.870
can assess need or we can assess a kind
 

01:52:48.870 --> 01:52:52.010
can assess need or we can assess a kind
of dessert that the resources which are

01:52:52.010 --> 01:52:52.020
of dessert that the resources which are
 

01:52:52.020 --> 01:52:54.530
of dessert that the resources which are
inherently limited are being devoted to

01:52:54.530 --> 01:52:54.540
inherently limited are being devoted to
 

01:52:54.540 --> 01:52:56.570
inherently limited are being devoted to
individuals who are highest need or

01:52:56.570 --> 01:52:56.580
individuals who are highest need or
 

01:52:56.580 --> 01:52:59.810
individuals who are highest need or
highest risk and this is fundamentally

01:52:59.810 --> 01:52:59.820
highest risk and this is fundamentally
 

01:52:59.820 --> 01:53:02.090
highest risk and this is fundamentally
the primary question a lot of the

01:53:02.090 --> 01:53:02.100
the primary question a lot of the
 

01:53:02.100 --> 01:53:04.940
the primary question a lot of the
government uses of AI systems we want to

01:53:04.940 --> 01:53:04.950
government uses of AI systems we want to
 

01:53:04.950 --> 01:53:06.980
government uses of AI systems we want to
make use of data to identify families

01:53:06.980 --> 01:53:06.990
make use of data to identify families
 

01:53:06.990 --> 01:53:08.840
make use of data to identify families
that would benefit most from services or

01:53:08.840 --> 01:53:08.850
that would benefit most from services or
 

01:53:08.850 --> 01:53:13.460
that would benefit most from services or
most needy we want to identify criminal

01:53:13.460 --> 01:53:13.470
most needy we want to identify criminal
 

01:53:13.470 --> 01:53:16.340
most needy we want to identify criminal
defendants in the bail context who are

01:53:16.340 --> 01:53:16.350
defendants in the bail context who are
 

01:53:16.350 --> 01:53:18.830
defendants in the bail context who are
currently likely being detained on

01:53:18.830 --> 01:53:18.840
currently likely being detained on
 

01:53:18.840 --> 01:53:22.660
currently likely being detained on
account of being unable to pay bond and

01:53:22.660 --> 01:53:22.670
account of being unable to pay bond and
 

01:53:22.670 --> 01:53:25.820
account of being unable to pay bond and
replace that monetary bond system as

01:53:25.820 --> 01:53:25.830
replace that monetary bond system as
 

01:53:25.830 --> 01:53:27.470
replace that monetary bond system as
it's already been replaced in many other

01:53:27.470 --> 01:53:27.480
it's already been replaced in many other
 

01:53:27.480 --> 01:53:29.930
it's already been replaced in many other
countries with something else and

01:53:29.930 --> 01:53:29.940
countries with something else and
 

01:53:29.940 --> 01:53:32.750
countries with something else and
honestly anything else and this is where

01:53:32.750 --> 01:53:32.760
honestly anything else and this is where
 

01:53:32.760 --> 01:53:34.520
honestly anything else and this is where
for instance risk assessment presents a

01:53:34.520 --> 01:53:34.530
for instance risk assessment presents a
 

01:53:34.530 --> 01:53:36.440
for instance risk assessment presents a
nice balance satisfying some of these

01:53:36.440 --> 01:53:36.450
nice balance satisfying some of these
 

01:53:36.450 --> 01:53:38.330
nice balance satisfying some of these
safety concerns on the part of

01:53:38.330 --> 01:53:38.340
safety concerns on the part of
 

01:53:38.340 --> 01:53:40.760
safety concerns on the part of
prosecutors and judges while also

01:53:40.760 --> 01:53:40.770
prosecutors and judges while also
 

01:53:40.770 --> 01:53:43.550
prosecutors and judges while also
meeting this progressive reform goal at

01:53:43.550 --> 01:53:43.560
meeting this progressive reform goal at
 

01:53:43.560 --> 01:53:44.990
meeting this progressive reform goal at
least to some extent on the part of

01:53:44.990 --> 01:53:45.000
least to some extent on the part of
 

01:53:45.000 --> 01:53:48.080
least to some extent on the part of
defenders and the individuals who are

01:53:48.080 --> 01:53:48.090
defenders and the individuals who are
 

01:53:48.090 --> 01:53:49.940
defenders and the individuals who are
being detained by these systems so if

01:53:49.940 --> 01:53:49.950
being detained by these systems so if
 

01:53:49.950 --> 01:53:51.650
being detained by these systems so if
you look at some of the early results

01:53:51.650 --> 01:53:51.660
you look at some of the early results
 

01:53:51.660 --> 01:53:53.570
you look at some of the early results
from New Jersey who recently replaced

01:53:53.570 --> 01:53:53.580
from New Jersey who recently replaced
 

01:53:53.580 --> 01:53:57.260
from New Jersey who recently replaced
their monetary bond system with risk

01:53:57.260 --> 01:53:57.270
their monetary bond system with risk
 

01:53:57.270 --> 01:53:59.630
their monetary bond system with risk
assessment this evolved a lot more

01:53:59.630 --> 01:53:59.640
assessment this evolved a lot more
 

01:53:59.640 --> 01:54:01.730
assessment this evolved a lot more
process reform than simply putting in a

01:54:01.730 --> 01:54:01.740
process reform than simply putting in a
 

01:54:01.740 --> 01:54:03.440
process reform than simply putting in a
risk assessment score and saying go

01:54:03.440 --> 01:54:03.450
risk assessment score and saying go
 

01:54:03.450 --> 01:54:05.420
risk assessment score and saying go
everything's fixed but at the same time

01:54:05.420 --> 01:54:05.430
everything's fixed but at the same time
 

01:54:05.430 --> 01:54:07.360
everything's fixed but at the same time
this is something where we're seeing

01:54:07.360 --> 01:54:07.370
this is something where we're seeing
 

01:54:07.370 --> 01:54:10.340
this is something where we're seeing
we're replacing a root cause of

01:54:10.340 --> 01:54:10.350
we're replacing a root cause of
 

01:54:10.350 --> 01:54:12.380
we're replacing a root cause of
tremendous amount of new Finnick wa t

01:54:12.380 --> 01:54:12.390
tremendous amount of new Finnick wa t
 

01:54:12.390 --> 01:54:14.600
tremendous amount of new Finnick wa t
which is the monetary bond system with

01:54:14.600 --> 01:54:14.610
which is the monetary bond system with
 

01:54:14.610 --> 01:54:18.320
which is the monetary bond system with
something else so I have one more

01:54:18.320 --> 01:54:18.330
something else so I have one more
 

01:54:18.330 --> 01:54:19.940
something else so I have one more
question and then we're gonna open that

01:54:19.940 --> 01:54:19.950
question and then we're gonna open that
 

01:54:19.950 --> 01:54:23.180
question and then we're gonna open that
up for audience questions and so my

01:54:23.180 --> 01:54:23.190
up for audience questions and so my
 

01:54:23.190 --> 01:54:24.840
up for audience questions and so my
question is

01:54:24.840 --> 01:54:24.850
question is
 

01:54:24.850 --> 01:54:28.240
question is
the European Union is about to put in

01:54:28.240 --> 01:54:28.250
the European Union is about to put in
 

01:54:28.250 --> 01:54:30.280
the European Union is about to put in
this new data protection law the general

01:54:30.280 --> 01:54:30.290
this new data protection law the general
 

01:54:30.290 --> 01:54:32.800
this new data protection law the general
data protection regulation and one of

01:54:32.800 --> 01:54:32.810
data protection regulation and one of
 

01:54:32.810 --> 01:54:35.080
data protection regulation and one of
the stipulations is that citizens have

01:54:35.080 --> 01:54:35.090
the stipulations is that citizens have
 

01:54:35.090 --> 01:54:37.510
the stipulations is that citizens have
the right to not to be subject to

01:54:37.510 --> 01:54:37.520
the right to not to be subject to
 

01:54:37.520 --> 01:54:40.300
the right to not to be subject to
algorithmic processing if it will

01:54:40.300 --> 01:54:40.310
algorithmic processing if it will
 

01:54:40.310 --> 01:54:42.729
algorithmic processing if it will
significantly impact them and they also

01:54:42.729 --> 01:54:42.739
significantly impact them and they also
 

01:54:42.739 --> 01:54:45.430
significantly impact them and they also
have the right to explanation about what

01:54:45.430 --> 01:54:45.440
have the right to explanation about what
 

01:54:45.440 --> 01:54:47.170
have the right to explanation about what
criteria the algorithm used to make a

01:54:47.170 --> 01:54:47.180
criteria the algorithm used to make a
 

01:54:47.180 --> 01:54:49.390
criteria the algorithm used to make a
decision about them and I'm interested

01:54:49.390 --> 01:54:49.400
decision about them and I'm interested
 

01:54:49.400 --> 01:54:51.100
decision about them and I'm interested
in what you think about that and what

01:54:51.100 --> 01:54:51.110
in what you think about that and what
 

01:54:51.110 --> 01:54:52.570
in what you think about that and what
you think the impact of that's gonna be

01:54:52.570 --> 01:54:52.580
you think the impact of that's gonna be
 

01:54:52.580 --> 01:54:54.670
you think the impact of that's gonna be
and whether you think we should have

01:54:54.670 --> 01:54:54.680
and whether you think we should have
 

01:54:54.680 --> 01:55:04.620
and whether you think we should have
that in the United States so I think any

01:55:04.620 --> 01:55:04.630
that in the United States so I think any
 

01:55:04.630 --> 01:55:07.720
that in the United States so I think any
movement that really looks at the rights

01:55:07.720 --> 01:55:07.730
movement that really looks at the rights
 

01:55:07.730 --> 01:55:11.260
movement that really looks at the rights
of human beings and in the case of

01:55:11.260 --> 01:55:11.270
of human beings and in the case of
 

01:55:11.270 --> 01:55:12.760
of human beings and in the case of
health care the rights of patients and

01:55:12.760 --> 01:55:12.770
health care the rights of patients and
 

01:55:12.770 --> 01:55:15.250
health care the rights of patients and
consumers is a good good movement it's

01:55:15.250 --> 01:55:15.260
consumers is a good good movement it's
 

01:55:15.260 --> 01:55:17.830
consumers is a good good movement it's
good to ask questions right it'll be

01:55:17.830 --> 01:55:17.840
good to ask questions right it'll be
 

01:55:17.840 --> 01:55:19.270
good to ask questions right it'll be
interesting to see how it plays out in

01:55:19.270 --> 01:55:19.280
interesting to see how it plays out in
 

01:55:19.280 --> 01:55:21.570
interesting to see how it plays out in
in the European Union to be very honest

01:55:21.570 --> 01:55:21.580
in the European Union to be very honest
 

01:55:21.580 --> 01:55:23.860
in the European Union to be very honest
you know perhaps we have the luxury of

01:55:23.860 --> 01:55:23.870
you know perhaps we have the luxury of
 

01:55:23.870 --> 01:55:25.750
you know perhaps we have the luxury of
standing back and really looking at

01:55:25.750 --> 01:55:25.760
standing back and really looking at
 

01:55:25.760 --> 01:55:27.040
standing back and really looking at
what's worked and what's not worked

01:55:27.040 --> 01:55:27.050
what's worked and what's not worked
 

01:55:27.050 --> 01:55:28.960
what's worked and what's not worked
maybe taking some of the best aspects of

01:55:28.960 --> 01:55:28.970
maybe taking some of the best aspects of
 

01:55:28.970 --> 01:55:30.550
maybe taking some of the best aspects of
that maybe modifying it for the United

01:55:30.550 --> 01:55:30.560
that maybe modifying it for the United
 

01:55:30.560 --> 01:55:33.550
that maybe modifying it for the United
States the notion in healthcare in

01:55:33.550 --> 01:55:33.560
States the notion in healthcare in
 

01:55:33.560 --> 01:55:36.070
States the notion in healthcare in
general you know those reference earlier

01:55:36.070 --> 01:55:36.080
general you know those reference earlier
 

01:55:36.080 --> 01:55:38.250
general you know those reference earlier
this earlier speaker mentioned

01:55:38.250 --> 01:55:38.260
this earlier speaker mentioned
 

01:55:38.260 --> 01:55:42.729
this earlier speaker mentioned
participatory and and in and in making

01:55:42.729 --> 01:55:42.739
participatory and and in and in making
 

01:55:42.739 --> 01:55:44.560
participatory and and in and in making
the process more participatory I think

01:55:44.560 --> 01:55:44.570
the process more participatory I think
 

01:55:44.570 --> 01:55:46.750
the process more participatory I think
that's a good notion in healthcare in

01:55:46.750 --> 01:55:46.760
that's a good notion in healthcare in
 

01:55:46.760 --> 01:55:49.360
that's a good notion in healthcare in
particular healthcare traditionally has

01:55:49.360 --> 01:55:49.370
particular healthcare traditionally has
 

01:55:49.370 --> 01:55:52.570
particular healthcare traditionally has
been very paternalistic right you go to

01:55:52.570 --> 01:55:52.580
been very paternalistic right you go to
 

01:55:52.580 --> 01:55:54.670
been very paternalistic right you go to
a doctor's office and you expect for

01:55:54.670 --> 01:55:54.680
a doctor's office and you expect for
 

01:55:54.680 --> 01:55:56.020
a doctor's office and you expect for
care to be provided to you there's

01:55:56.020 --> 01:55:56.030
care to be provided to you there's
 

01:55:56.030 --> 01:55:59.560
care to be provided to you there's
almost a god complex of sorts between

01:55:59.560 --> 01:55:59.570
almost a god complex of sorts between
 

01:55:59.570 --> 01:56:02.290
almost a god complex of sorts between
the physician and the patient and also

01:56:02.290 --> 01:56:02.300
the physician and the patient and also
 

01:56:02.300 --> 01:56:03.670
the physician and the patient and also
between the patient and the physician it

01:56:03.670 --> 01:56:03.680
between the patient and the physician it
 

01:56:03.680 --> 01:56:06.370
between the patient and the physician it
goes both ways and it's changing right

01:56:06.370 --> 01:56:06.380
goes both ways and it's changing right
 

01:56:06.380 --> 01:56:06.610
goes both ways and it's changing right
now

01:56:06.610 --> 01:56:06.620
now
 

01:56:06.620 --> 01:56:08.680
now
you know consumers they go out and see

01:56:08.680 --> 01:56:08.690
you know consumers they go out and see
 

01:56:08.690 --> 01:56:10.300
you know consumers they go out and see
doctor Google before they go see dr.

01:56:10.300 --> 01:56:10.310
doctor Google before they go see dr.
 

01:56:10.310 --> 01:56:13.150
doctor Google before they go see dr.
Shrestha or my peers and that's a good

01:56:13.150 --> 01:56:13.160
Shrestha or my peers and that's a good
 

01:56:13.160 --> 01:56:14.890
Shrestha or my peers and that's a good
thing right we're more connected we're

01:56:14.890 --> 01:56:14.900
thing right we're more connected we're
 

01:56:14.900 --> 01:56:16.840
thing right we're more connected we're
more leaning in more more curious we're

01:56:16.840 --> 01:56:16.850
more leaning in more more curious we're
 

01:56:16.850 --> 01:56:19.120
more leaning in more more curious we're
more participatory than we've ever been

01:56:19.120 --> 01:56:19.130
more participatory than we've ever been
 

01:56:19.130 --> 01:56:19.750
more participatory than we've ever been
before

01:56:19.750 --> 01:56:19.760
before
 

01:56:19.760 --> 01:56:22.240
before
and that's that I think a good thing I

01:56:22.240 --> 01:56:22.250
and that's that I think a good thing I
 

01:56:22.250 --> 01:56:24.520
and that's that I think a good thing I
think what we can do however is we can

01:56:24.520 --> 01:56:24.530
think what we can do however is we can
 

01:56:24.530 --> 01:56:26.650
think what we can do however is we can
take the power of these algorithms to

01:56:26.650 --> 01:56:26.660
take the power of these algorithms to
 

01:56:26.660 --> 01:56:29.080
take the power of these algorithms to
further inform us because the notion of

01:56:29.080 --> 01:56:29.090
further inform us because the notion of
 

01:56:29.090 --> 01:56:32.260
further inform us because the notion of
equity is not just one of providing

01:56:32.260 --> 01:56:32.270
equity is not just one of providing
 

01:56:32.270 --> 01:56:34.450
equity is not just one of providing
access it's also one of empowering

01:56:34.450 --> 01:56:34.460
access it's also one of empowering
 

01:56:34.460 --> 01:56:37.560
access it's also one of empowering
consumers with tools incapable

01:56:37.560 --> 01:56:37.570
consumers with tools incapable
 

01:56:37.570 --> 01:56:39.260
consumers with tools incapable
to make better and more informed

01:56:39.260 --> 01:56:39.270
to make better and more informed
 

01:56:39.270 --> 01:56:41.250
to make better and more informed
decisions about themselves and their

01:56:41.250 --> 01:56:41.260
decisions about themselves and their
 

01:56:41.260 --> 01:56:43.260
decisions about themselves and their
loved ones and that's really the pursuit

01:56:43.260 --> 01:56:43.270
loved ones and that's really the pursuit
 

01:56:43.270 --> 01:56:45.240
loved ones and that's really the pursuit
that we're pushing forward with in terms

01:56:45.240 --> 01:56:45.250
that we're pushing forward with in terms
 

01:56:45.250 --> 01:56:47.069
that we're pushing forward with in terms
of innovation in healthcare so overall

01:56:47.069 --> 01:56:47.079
of innovation in healthcare so overall
 

01:56:47.079 --> 01:56:48.810
of innovation in healthcare so overall
it's a good thing you know we'll watch

01:56:48.810 --> 01:56:48.820
it's a good thing you know we'll watch
 

01:56:48.820 --> 01:56:51.240
it's a good thing you know we'll watch
and see what the Europeans do but to

01:56:51.240 --> 01:56:51.250
and see what the Europeans do but to
 

01:56:51.250 --> 01:56:52.649
and see what the Europeans do but to
question this to make sure that we're

01:56:52.649 --> 01:56:52.659
question this to make sure that we're
 

01:56:52.659 --> 01:56:54.660
question this to make sure that we're
able to you know speak to the specific

01:56:54.660 --> 01:56:54.670
able to you know speak to the specific
 

01:56:54.670 --> 01:56:57.479
able to you know speak to the specific
rights of the individuals it's it's it's

01:56:57.479 --> 01:56:57.489
rights of the individuals it's it's it's
 

01:56:57.489 --> 01:57:00.030
rights of the individuals it's it's it's
a good thing I'm fascinated by this

01:57:00.030 --> 01:57:00.040
a good thing I'm fascinated by this
 

01:57:00.040 --> 01:57:01.160
a good thing I'm fascinated by this
because I'm interested whether

01:57:01.160 --> 01:57:01.170
because I'm interested whether
 

01:57:01.170 --> 01:57:03.180
because I'm interested whether
individual Europeans are gonna challenge

01:57:03.180 --> 01:57:03.190
individual Europeans are gonna challenge
 

01:57:03.190 --> 01:57:05.729
individual Europeans are gonna challenge
you know algorithmic voter profiling and

01:57:05.729 --> 01:57:05.739
you know algorithmic voter profiling and
 

01:57:05.739 --> 01:57:09.209
you know algorithmic voter profiling and
advertising for example yeah I think

01:57:09.209 --> 01:57:09.219
advertising for example yeah I think
 

01:57:09.219 --> 01:57:11.669
advertising for example yeah I think
that that's the right to the explanation

01:57:11.669 --> 01:57:11.679
that that's the right to the explanation
 

01:57:11.679 --> 01:57:13.169
that that's the right to the explanation
explanation is one of those interesting

01:57:13.169 --> 01:57:13.179
explanation is one of those interesting
 

01:57:13.179 --> 01:57:15.870
explanation is one of those interesting
parts because right now you know say in

01:57:15.870 --> 01:57:15.880
parts because right now you know say in
 

01:57:15.880 --> 01:57:18.270
parts because right now you know say in
credit reporting we actually do have

01:57:18.270 --> 01:57:18.280
credit reporting we actually do have
 

01:57:18.280 --> 01:57:20.490
credit reporting we actually do have
auditing procedures that you have to go

01:57:20.490 --> 01:57:20.500
auditing procedures that you have to go
 

01:57:20.500 --> 01:57:21.810
auditing procedures that you have to go
through to show that you're doing fair

01:57:21.810 --> 01:57:21.820
through to show that you're doing fair
 

01:57:21.820 --> 01:57:23.780
through to show that you're doing fair
credit reporting and similarly with like

01:57:23.780 --> 01:57:23.790
credit reporting and similarly with like
 

01:57:23.790 --> 01:57:26.910
credit reporting and similarly with like
investment transactions through the SCC

01:57:26.910 --> 01:57:26.920
investment transactions through the SCC
 

01:57:26.920 --> 01:57:29.189
investment transactions through the SCC
although I've heard a lot of bad things

01:57:29.189 --> 01:57:29.199
although I've heard a lot of bad things
 

01:57:29.199 --> 01:57:31.319
although I've heard a lot of bad things
about those audits I do believe it's

01:57:31.319 --> 01:57:31.329
about those audits I do believe it's
 

01:57:31.329 --> 01:57:32.879
about those audits I do believe it's
good that the audits are in place and

01:57:32.879 --> 01:57:32.889
good that the audits are in place and
 

01:57:32.889 --> 01:57:34.890
good that the audits are in place and
yet there's no real audits for how we

01:57:34.890 --> 01:57:34.900
yet there's no real audits for how we
 

01:57:34.900 --> 01:57:36.810
yet there's no real audits for how we
train AI systems or anything and I think

01:57:36.810 --> 01:57:36.820
train AI systems or anything and I think
 

01:57:36.820 --> 01:57:38.399
train AI systems or anything and I think
the right to an explanation is going to

01:57:38.399 --> 01:57:38.409
the right to an explanation is going to
 

01:57:38.409 --> 01:57:41.160
the right to an explanation is going to
be good insofar as I'm not sure how much

01:57:41.160 --> 01:57:41.170
be good insofar as I'm not sure how much
 

01:57:41.170 --> 01:57:42.930
be good insofar as I'm not sure how much
teeth that's gonna end up having we'll

01:57:42.930 --> 01:57:42.940
teeth that's gonna end up having we'll
 

01:57:42.940 --> 01:57:45.030
teeth that's gonna end up having we'll
see but I do think it will at least get

01:57:45.030 --> 01:57:45.040
see but I do think it will at least get
 

01:57:45.040 --> 01:57:46.649
see but I do think it will at least get
people starting to figure out well how

01:57:46.649 --> 01:57:46.659
people starting to figure out well how
 

01:57:46.659 --> 01:57:48.300
people starting to figure out well how
do we even generate an explanation

01:57:48.300 --> 01:57:48.310
do we even generate an explanation
 

01:57:48.310 --> 01:57:51.089
do we even generate an explanation
what does useful audit actually look

01:57:51.089 --> 01:57:51.099
what does useful audit actually look
 

01:57:51.099 --> 01:57:52.919
what does useful audit actually look
like what information should be in place

01:57:52.919 --> 01:57:52.929
like what information should be in place
 

01:57:52.929 --> 01:57:55.919
like what information should be in place
for a court or for a judge to be able to

01:57:55.919 --> 01:57:55.929
for a court or for a judge to be able to
 

01:57:55.929 --> 01:57:57.750
for a court or for a judge to be able to
actually think meaningfully about what

01:57:57.750 --> 01:57:57.760
actually think meaningfully about what
 

01:57:57.760 --> 01:57:59.970
actually think meaningfully about what
happened but the piece that I'm most

01:57:59.970 --> 01:57:59.980
happened but the piece that I'm most
 

01:57:59.980 --> 01:58:01.770
happened but the piece that I'm most
interested in actually is actually the

01:58:01.770 --> 01:58:01.780
interested in actually is actually the
 

01:58:01.780 --> 01:58:05.160
interested in actually is actually the
data Portability piece of this and

01:58:05.160 --> 01:58:05.170
data Portability piece of this and
 

01:58:05.170 --> 01:58:07.890
data Portability piece of this and
that's because right now obviously once

01:58:07.890 --> 01:58:07.900
that's because right now obviously once
 

01:58:07.900 --> 01:58:11.700
that's because right now obviously once
you grow a huge user base even if you

01:58:11.700 --> 01:58:11.710
you grow a huge user base even if you
 

01:58:11.710 --> 01:58:13.169
you grow a huge user base even if you
find out that the company who has that

01:58:13.169 --> 01:58:13.179
find out that the company who has that
 

01:58:13.179 --> 01:58:14.700
find out that the company who has that
user base isn't really doing the best

01:58:14.700 --> 01:58:14.710
user base isn't really doing the best
 

01:58:14.710 --> 01:58:16.830
user base isn't really doing the best
things they have all the data and the

01:58:16.830 --> 01:58:16.840
things they have all the data and the
 

01:58:16.840 --> 01:58:19.050
things they have all the data and the
data is the power in the case of AI and

01:58:19.050 --> 01:58:19.060
data is the power in the case of AI and
 

01:58:19.060 --> 01:58:20.939
data is the power in the case of AI and
I think data Portability is a very

01:58:20.939 --> 01:58:20.949
I think data Portability is a very
 

01:58:20.949 --> 01:58:22.530
I think data Portability is a very
interesting part of this of whether

01:58:22.530 --> 01:58:22.540
interesting part of this of whether
 

01:58:22.540 --> 01:58:24.359
interesting part of this of whether
really it's going to come down that

01:58:24.359 --> 01:58:24.369
really it's going to come down that
 

01:58:24.369 --> 01:58:27.000
really it's going to come down that
people have to allow you to export and

01:58:27.000 --> 01:58:27.010
people have to allow you to export and
 

01:58:27.010 --> 01:58:28.950
people have to allow you to export and
move your data over into other services

01:58:28.950 --> 01:58:28.960
move your data over into other services
 

01:58:28.960 --> 01:58:31.950
move your data over into other services
because this would allow actually a real

01:58:31.950 --> 01:58:31.960
because this would allow actually a real
 

01:58:31.960 --> 01:58:33.870
because this would allow actually a real
competitive force potentially to take

01:58:33.870 --> 01:58:33.880
competitive force potentially to take
 

01:58:33.880 --> 01:58:35.490
competitive force potentially to take
place when Facebook suddenly does

01:58:35.490 --> 01:58:35.500
place when Facebook suddenly does
 

01:58:35.500 --> 01:58:37.229
place when Facebook suddenly does
something that people aren't so happy

01:58:37.229 --> 01:58:37.239
something that people aren't so happy
 

01:58:37.239 --> 01:58:39.600
something that people aren't so happy
about if Facebook is forced through the

01:58:39.600 --> 01:58:39.610
about if Facebook is forced through the
 

01:58:39.610 --> 01:58:42.149
about if Facebook is forced through the
gdpr to actually have a way to export

01:58:42.149 --> 01:58:42.159
gdpr to actually have a way to export
 

01:58:42.159 --> 01:58:43.979
gdpr to actually have a way to export
their data into a new social media

01:58:43.979 --> 01:58:43.989
their data into a new social media
 

01:58:43.989 --> 01:58:46.350
their data into a new social media
platform suddenly you could have smaller

01:58:46.350 --> 01:58:46.360
platform suddenly you could have smaller
 

01:58:46.360 --> 01:58:48.030
platform suddenly you could have smaller
groups show up and say well we're gonna

01:58:48.030 --> 01:58:48.040
groups show up and say well we're gonna
 

01:58:48.040 --> 01:58:49.979
groups show up and say well we're gonna
make a social media platform that's just

01:58:49.979 --> 01:58:49.989
make a social media platform that's just
 

01:58:49.989 --> 01:58:50.760
make a social media platform that's just
for Pittsburgh

01:58:50.760 --> 01:58:50.770
for Pittsburgh
 

01:58:50.770 --> 01:58:52.950
for Pittsburgh
oh and we can bring all your photo and

01:58:52.950 --> 01:58:52.960
oh and we can bring all your photo and
 

01:58:52.960 --> 01:58:54.420
oh and we can bring all your photo and
all your like history and all your

01:58:54.420 --> 01:58:54.430
all your like history and all your
 

01:58:54.430 --> 01:58:56.130
all your like history and all your
friend and everything you want over into

01:58:56.130 --> 01:58:56.140
friend and everything you want over into
 

01:58:56.140 --> 01:58:58.800
friend and everything you want over into
our platform like that so just to chime

01:58:58.800 --> 01:58:58.810
our platform like that so just to chime
 

01:58:58.810 --> 01:59:00.720
our platform like that so just to chime
in here um they've set up a system where

01:59:00.720 --> 01:59:00.730
in here um they've set up a system where
 

01:59:00.730 --> 01:59:02.430
in here um they've set up a system where
you can export your photos and your

01:59:02.430 --> 01:59:02.440
you can export your photos and your
 

01:59:02.440 --> 01:59:06.060
you can export your photos and your
posts right um and also you know your

01:59:06.060 --> 01:59:06.070
posts right um and also you know your
 

01:59:06.070 --> 01:59:08.160
posts right um and also you know your
logins and various other things but

01:59:08.160 --> 01:59:08.170
logins and various other things but
 

01:59:08.170 --> 01:59:09.840
logins and various other things but
under gdpr they're gonna also have to

01:59:09.840 --> 01:59:09.850
under gdpr they're gonna also have to
 

01:59:09.850 --> 01:59:12.090
under gdpr they're gonna also have to
give you if you ask your algorithmic

01:59:12.090 --> 01:59:12.100
give you if you ask your algorithmic
 

01:59:12.100 --> 01:59:14.040
give you if you ask your algorithmic
scores and any inferences they've made

01:59:14.040 --> 01:59:14.050
scores and any inferences they've made
 

01:59:14.050 --> 01:59:16.050
scores and any inferences they've made
about you and so far I haven't seen that

01:59:16.050 --> 01:59:16.060
about you and so far I haven't seen that
 

01:59:16.060 --> 01:59:17.640
about you and so far I haven't seen that
being portable so we're gonna have a

01:59:17.640 --> 01:59:17.650
being portable so we're gonna have a
 

01:59:17.650 --> 01:59:19.110
being portable so we're gonna have a
lightning round where I ask a few

01:59:19.110 --> 01:59:19.120
lightning round where I ask a few
 

01:59:19.120 --> 01:59:21.120
lightning round where I ask a few
questions and you can like take 30

01:59:21.120 --> 01:59:21.130
questions and you can like take 30
 

01:59:21.130 --> 01:59:23.520
questions and you can like take 30
seconds to answer so one of the

01:59:23.520 --> 01:59:23.530
seconds to answer so one of the
 

01:59:23.530 --> 01:59:26.850
seconds to answer so one of the
questions is is there a sense that we

01:59:26.850 --> 01:59:26.860
questions is is there a sense that we
 

01:59:26.860 --> 01:59:29.130
questions is is there a sense that we
might seriously need equity of being

01:59:29.130 --> 01:59:29.140
might seriously need equity of being
 

01:59:29.140 --> 01:59:32.640
might seriously need equity of being
able to avoid a in certain circumstances

01:59:32.640 --> 01:59:32.650
able to avoid a in certain circumstances
 

01:59:32.650 --> 01:59:34.830
able to avoid a in certain circumstances
so are there any circumstances in which

01:59:34.830 --> 01:59:34.840
so are there any circumstances in which
 

01:59:34.840 --> 01:59:37.310
so are there any circumstances in which
we should prohibit the use of AI or

01:59:37.310 --> 01:59:37.320
we should prohibit the use of AI or
 

01:59:37.320 --> 01:59:39.570
we should prohibit the use of AI or
where we don't want to develop a life

01:59:39.570 --> 01:59:39.580
where we don't want to develop a life
 

01:59:39.580 --> 01:59:45.240
where we don't want to develop a life
for that purpose I might steal Michaels

01:59:45.240 --> 01:59:45.250
for that purpose I might steal Michaels
 

01:59:45.250 --> 01:59:46.650
for that purpose I might steal Michaels
response here because we were just

01:59:46.650 --> 01:59:46.660
response here because we were just
 

01:59:46.660 --> 01:59:48.930
response here because we were just
talking about this question earlier but

01:59:48.930 --> 01:59:48.940
talking about this question earlier but
 

01:59:48.940 --> 01:59:51.630
talking about this question earlier but
my view is that there are certain times

01:59:51.630 --> 01:59:51.640
my view is that there are certain times
 

01:59:51.640 --> 01:59:56.190
my view is that there are certain times
when it's difficult to put all of our

01:59:56.190 --> 01:59:56.200
when it's difficult to put all of our
 

01:59:56.200 --> 01:59:58.650
when it's difficult to put all of our
values into a single objective that the

01:59:58.650 --> 01:59:58.660
values into a single objective that the
 

01:59:58.660 --> 02:00:01.170
values into a single objective that the
ASIS the AI system would then optimize

02:00:01.170 --> 02:00:01.180
ASIS the AI system would then optimize
 

02:00:01.180 --> 02:00:03.540
ASIS the AI system would then optimize
for and so to the extent that we need

02:00:03.540 --> 02:00:03.550
for and so to the extent that we need
 

02:00:03.550 --> 02:00:05.730
for and so to the extent that we need
additional controls over and above the

02:00:05.730 --> 02:00:05.740
additional controls over and above the
 

02:00:05.740 --> 02:00:07.410
additional controls over and above the
specific objective that the AI is

02:00:07.410 --> 02:00:07.420
specific objective that the AI is
 

02:00:07.420 --> 02:00:09.210
specific objective that the AI is
optimizing I think that's a place where

02:00:09.210 --> 02:00:09.220
optimizing I think that's a place where
 

02:00:09.220 --> 02:00:11.610
optimizing I think that's a place where
we should either not rely on it at all

02:00:11.610 --> 02:00:11.620
we should either not rely on it at all
 

02:00:11.620 --> 02:00:13.680
we should either not rely on it at all
if no part of our objective can be

02:00:13.680 --> 02:00:13.690
if no part of our objective can be
 

02:00:13.690 --> 02:00:16.170
if no part of our objective can be
formalized in this way or rely on it

02:00:16.170 --> 02:00:16.180
formalized in this way or rely on it
 

02:00:16.180 --> 02:00:17.790
formalized in this way or rely on it
with human guidance if there are certain

02:00:17.790 --> 02:00:17.800
with human guidance if there are certain
 

02:00:17.800 --> 02:00:20.630
with human guidance if there are certain
aspects that still require human control

02:00:20.630 --> 02:00:20.640
aspects that still require human control
 

02:00:20.640 --> 02:00:25.380
aspects that still require human control
questions for you it says if a is judged

02:00:25.380 --> 02:00:25.390
questions for you it says if a is judged
 

02:00:25.390 --> 02:00:27.990
questions for you it says if a is judged
as a tool rather than an experimental

02:00:27.990 --> 02:00:28.000
as a tool rather than an experimental
 

02:00:28.000 --> 02:00:30.120
as a tool rather than an experimental
treatment an IRB is not relevant

02:00:30.120 --> 02:00:30.130
treatment an IRB is not relevant
 

02:00:30.130 --> 02:00:32.220
treatment an IRB is not relevant
so what internal policy should

02:00:32.220 --> 02:00:32.230
so what internal policy should
 

02:00:32.230 --> 02:00:36.080
so what internal policy should
healthcare providers have in place I

02:00:36.080 --> 02:00:36.090
 

02:00:36.090 --> 02:00:38.760
think the specific guideline that we

02:00:38.760 --> 02:00:38.770
think the specific guideline that we
 

02:00:38.770 --> 02:00:41.640
think the specific guideline that we
should have in place is one of our moral

02:00:41.640 --> 02:00:41.650
should have in place is one of our moral
 

02:00:41.650 --> 02:00:44.970
should have in place is one of our moral
objectives right so why is it that we're

02:00:44.970 --> 02:00:44.980
objectives right so why is it that we're
 

02:00:44.980 --> 02:00:46.670
objectives right so why is it that we're
doing what it is that we're doing and

02:00:46.670 --> 02:00:46.680
doing what it is that we're doing and
 

02:00:46.680 --> 02:00:48.900
doing what it is that we're doing and
when you when you think about it in the

02:00:48.900 --> 02:00:48.910
when you when you think about it in the
 

02:00:48.910 --> 02:00:50.610
when you when you think about it in the
context of what I just mentioned there

02:00:50.610 --> 02:00:50.620
context of what I just mentioned there
 

02:00:50.620 --> 02:00:52.800
context of what I just mentioned there
then it becomes clearer for us because

02:00:52.800 --> 02:00:52.810
then it becomes clearer for us because
 

02:00:52.810 --> 02:00:55.740
then it becomes clearer for us because
what we're talking about here is to

02:00:55.740 --> 02:00:55.750
what we're talking about here is to
 

02:00:55.750 --> 02:00:59.940
what we're talking about here is to
really push not artificial intelligence

02:00:59.940 --> 02:00:59.950
really push not artificial intelligence
 

02:00:59.950 --> 02:01:02.610
really push not artificial intelligence
and these algorithms that you know

02:01:02.610 --> 02:01:02.620
and these algorithms that you know
 

02:01:02.620 --> 02:01:04.770
and these algorithms that you know
would look at all of the data elements

02:01:04.770 --> 02:01:04.780
would look at all of the data elements
 

02:01:04.780 --> 02:01:05.910
would look at all of the data elements
and say alright you know here's what

02:01:05.910 --> 02:01:05.920
and say alright you know here's what
 

02:01:05.920 --> 02:01:08.490
and say alright you know here's what
must be done for this human subject but

02:01:08.490 --> 02:01:08.500
must be done for this human subject but
 

02:01:08.500 --> 02:01:10.710
must be done for this human subject but
the specifics of what's in the best

02:01:10.710 --> 02:01:10.720
the specifics of what's in the best
 

02:01:10.720 --> 02:01:13.320
the specifics of what's in the best
interests of this human being that's at

02:01:13.320 --> 02:01:13.330
interests of this human being that's at
 

02:01:13.330 --> 02:01:17.910
interests of this human being that's at
the other end so I say this because when

02:01:17.910 --> 02:01:17.920
the other end so I say this because when
 

02:01:17.920 --> 02:01:20.280
the other end so I say this because when
we look at artificial intelligence I

02:01:20.280 --> 02:01:20.290
we look at artificial intelligence I
 

02:01:20.290 --> 02:01:23.670
we look at artificial intelligence I
look at artificial intelligence as less

02:01:23.670 --> 02:01:23.680
look at artificial intelligence as less
 

02:01:23.680 --> 02:01:26.010
look at artificial intelligence as less
about pushing the artificial mess of

02:01:26.010 --> 02:01:26.020
about pushing the artificial mess of
 

02:01:26.020 --> 02:01:28.350
about pushing the artificial mess of
care forward but really augmenting the

02:01:28.350 --> 02:01:28.360
care forward but really augmenting the
 

02:01:28.360 --> 02:01:31.800
care forward but really augmenting the
care process to Alex's earlier point in

02:01:31.800 --> 02:01:31.810
care process to Alex's earlier point in
 

02:01:31.810 --> 02:01:33.060
care process to Alex's earlier point in
one of the things that we need to be

02:01:33.060 --> 02:01:33.070
one of the things that we need to be
 

02:01:33.070 --> 02:01:35.520
one of the things that we need to be
doing is to making sure to be to be sure

02:01:35.520 --> 02:01:35.530
doing is to making sure to be to be sure
 

02:01:35.530 --> 02:01:38.340
doing is to making sure to be to be sure
that we're accounting for all of the the

02:01:38.340 --> 02:01:38.350
that we're accounting for all of the the
 

02:01:38.350 --> 02:01:42.180
that we're accounting for all of the the
human aspect of what it is that we're

02:01:42.180 --> 02:01:42.190
human aspect of what it is that we're
 

02:01:42.190 --> 02:01:44.550
human aspect of what it is that we're
doing so in providing health care so for

02:01:44.550 --> 02:01:44.560
doing so in providing health care so for
 

02:01:44.560 --> 02:01:46.710
doing so in providing health care so for
example in end-of-life decisions right

02:01:46.710 --> 02:01:46.720
example in end-of-life decisions right
 

02:01:46.720 --> 02:01:48.690
example in end-of-life decisions right
there are specific decisions that we can

02:01:48.690 --> 02:01:48.700
there are specific decisions that we can
 

02:01:48.700 --> 02:01:50.490
there are specific decisions that we can
algorithm eyes you know best practices

02:01:50.490 --> 02:01:50.500
algorithm eyes you know best practices
 

02:01:50.500 --> 02:01:52.890
algorithm eyes you know best practices
clinical protocols evidence-based

02:01:52.890 --> 02:01:52.900
clinical protocols evidence-based
 

02:01:52.900 --> 02:01:54.690
clinical protocols evidence-based
guidelines and we can Al Gore demise

02:01:54.690 --> 02:01:54.700
guidelines and we can Al Gore demise
 

02:01:54.700 --> 02:01:57.720
guidelines and we can Al Gore demise
these two to no end but there are also

02:01:57.720 --> 02:01:57.730
these two to no end but there are also
 

02:01:57.730 --> 02:02:00.300
these two to no end but there are also
specific preferences patients

02:02:00.300 --> 02:02:00.310
specific preferences patients
 

02:02:00.310 --> 02:02:02.460
specific preferences patients
preferences religious constraints that

02:02:02.460 --> 02:02:02.470
preferences religious constraints that
 

02:02:02.470 --> 02:02:06.060
preferences religious constraints that
that are in place and other things that

02:02:06.060 --> 02:02:06.070
that are in place and other things that
 

02:02:06.070 --> 02:02:08.010
that are in place and other things that
we absolutely have to look at it from a

02:02:08.010 --> 02:02:08.020
we absolutely have to look at it from a
 

02:02:08.020 --> 02:02:10.830
we absolutely have to look at it from a
very human perspective so there's things

02:02:10.830 --> 02:02:10.840
very human perspective so there's things
 

02:02:10.840 --> 02:02:12.780
very human perspective so there's things
that you know the IRB protocols will

02:02:12.780 --> 02:02:12.790
that you know the IRB protocols will
 

02:02:12.790 --> 02:02:15.420
that you know the IRB protocols will
obviously you know help you navigate

02:02:15.420 --> 02:02:15.430
obviously you know help you navigate
 

02:02:15.430 --> 02:02:18.420
obviously you know help you navigate
better at the same time we really need

02:02:18.420 --> 02:02:18.430
better at the same time we really need
 

02:02:18.430 --> 02:02:20.970
better at the same time we really need
to look at this with the lens of why it

02:02:20.970 --> 02:02:20.980
to look at this with the lens of why it
 

02:02:20.980 --> 02:02:22.410
to look at this with the lens of why it
is that we're pushing these algorithms

02:02:22.410 --> 02:02:22.420
is that we're pushing these algorithms
 

02:02:22.420 --> 02:02:24.360
is that we're pushing these algorithms
forward and what it is that we're we're

02:02:24.360 --> 02:02:24.370
forward and what it is that we're we're
 

02:02:24.370 --> 02:02:26.490
forward and what it is that we're we're
actually desiring in terms of the moral

02:02:26.490 --> 02:02:26.500
actually desiring in terms of the moral
 

02:02:26.500 --> 02:02:29.460
actually desiring in terms of the moral
objectives so two more questions when

02:02:29.460 --> 02:02:29.470
objectives so two more questions when
 

02:02:29.470 --> 02:02:31.140
objectives so two more questions when
Alex this is for you what are the

02:02:31.140 --> 02:02:31.150
Alex this is for you what are the
 

02:02:31.150 --> 02:02:33.720
Alex this is for you what are the
techniques used to remove biasness from

02:02:33.720 --> 02:02:33.730
techniques used to remove biasness from
 

02:02:33.730 --> 02:02:38.220
techniques used to remove biasness from
AI systems knowledge of how the bias got

02:02:38.220 --> 02:02:38.230
AI systems knowledge of how the bias got
 

02:02:38.230 --> 02:02:40.140
AI systems knowledge of how the bias got
there in the first place so it's

02:02:40.140 --> 02:02:40.150
there in the first place so it's
 

02:02:40.150 --> 02:02:42.330
there in the first place so it's
oftentimes extremely easy to write down

02:02:42.330 --> 02:02:42.340
oftentimes extremely easy to write down
 

02:02:42.340 --> 02:02:44.340
oftentimes extremely easy to write down
the mathematical correction formula and

02:02:44.340 --> 02:02:44.350
the mathematical correction formula and
 

02:02:44.350 --> 02:02:46.170
the mathematical correction formula and
it's extremely difficult to estimate all

02:02:46.170 --> 02:02:46.180
it's extremely difficult to estimate all
 

02:02:46.180 --> 02:02:48.090
it's extremely difficult to estimate all
the terms involved so without getting

02:02:48.090 --> 02:02:48.100
the terms involved so without getting
 

02:02:48.100 --> 02:02:49.890
the terms involved so without getting
too technical there is no

02:02:49.890 --> 02:02:49.900
too technical there is no
 

02:02:49.900 --> 02:02:51.930
too technical there is no
one-size-fits-all solution you really

02:02:51.930 --> 02:02:51.940
one-size-fits-all solution you really
 

02:02:51.940 --> 02:02:53.310
one-size-fits-all solution you really
need to understand what is the mechanism

02:02:53.310 --> 02:02:53.320
need to understand what is the mechanism
 

02:02:53.320 --> 02:02:56.250
need to understand what is the mechanism
by which that bias got into there and

02:02:56.250 --> 02:02:56.260
by which that bias got into there and
 

02:02:56.260 --> 02:02:59.330
by which that bias got into there and
that's in part why this area of kind of

02:02:59.330 --> 02:02:59.340
that's in part why this area of kind of
 

02:02:59.340 --> 02:03:01.380
that's in part why this area of kind of
ethics and algorithms is so

02:03:01.380 --> 02:03:01.390
ethics and algorithms is so
 

02:03:01.390 --> 02:03:03.060
ethics and algorithms is so
interdisciplinary that we have a

02:03:03.060 --> 02:03:03.070
interdisciplinary that we have a
 

02:03:03.070 --> 02:03:04.890
interdisciplinary that we have a
tremendous amount of literature from the

02:03:04.890 --> 02:03:04.900
tremendous amount of literature from the
 

02:03:04.900 --> 02:03:07.110
tremendous amount of literature from the
humanities and Social Sciences to rely

02:03:07.110 --> 02:03:07.120
humanities and Social Sciences to rely
 

02:03:07.120 --> 02:03:09.240
humanities and Social Sciences to rely
on and figuring out the how did it get

02:03:09.240 --> 02:03:09.250
on and figuring out the how did it get
 

02:03:09.250 --> 02:03:12.460
on and figuring out the how did it get
there on our way to correction

02:03:12.460 --> 02:03:12.470
there on our way to correction
 

02:03:12.470 --> 02:03:17.380
there on our way to correction
and here's another question how will we

02:03:17.380 --> 02:03:17.390
and here's another question how will we
 

02:03:17.390 --> 02:03:19.990
and here's another question how will we
create safeguards against possible AI

02:03:19.990 --> 02:03:20.000
create safeguards against possible AI
 

02:03:20.000 --> 02:03:22.390
create safeguards against possible AI
misbehavior and a totally a I run

02:03:22.390 --> 02:03:22.400
misbehavior and a totally a I run
 

02:03:22.400 --> 02:03:24.820
misbehavior and a totally a I run
society will will be able to pull the

02:03:24.820 --> 02:03:24.830
society will will be able to pull the
 

02:03:24.830 --> 02:03:26.920
society will will be able to pull the
plug so I just want to add something to

02:03:26.920 --> 02:03:26.930
plug so I just want to add something to
 

02:03:26.930 --> 02:03:28.180
plug so I just want to add something to
that because Mike and I were talking

02:03:28.180 --> 02:03:28.190
that because Mike and I were talking
 

02:03:28.190 --> 02:03:31.150
that because Mike and I were talking
about if we play the idea of autonomous

02:03:31.150 --> 02:03:31.160
about if we play the idea of autonomous
 

02:03:31.160 --> 02:03:35.680
about if we play the idea of autonomous
cars out to you know the end solution we

02:03:35.680 --> 02:03:35.690
cars out to you know the end solution we
 

02:03:35.690 --> 02:03:39.520
cars out to you know the end solution we
have nobody owning a car everybody

02:03:39.520 --> 02:03:39.530
have nobody owning a car everybody
 

02:03:39.530 --> 02:03:41.530
have nobody owning a car everybody
having a nap to call a driverless car

02:03:41.530 --> 02:03:41.540
having a nap to call a driverless car
 

02:03:41.540 --> 02:03:44.140
having a nap to call a driverless car
and the cars all owned by Google Google

02:03:44.140 --> 02:03:44.150
and the cars all owned by Google Google
 

02:03:44.150 --> 02:03:46.330
and the cars all owned by Google Google
and uber and a few other companies and

02:03:46.330 --> 02:03:46.340
and uber and a few other companies and
 

02:03:46.340 --> 02:03:49.240
and uber and a few other companies and
so what do we do if you're in China and

02:03:49.240 --> 02:03:49.250
so what do we do if you're in China and
 

02:03:49.250 --> 02:03:51.220
so what do we do if you're in China and
you use your app to call a driverless

02:03:51.220 --> 02:03:51.230
you use your app to call a driverless
 

02:03:51.230 --> 02:03:53.160
you use your app to call a driverless
car and it won't take you to a protest

02:03:53.160 --> 02:03:53.170
car and it won't take you to a protest
 

02:03:53.170 --> 02:03:55.450
car and it won't take you to a protest
you know what do we do like are we gonna

02:03:55.450 --> 02:03:55.460
you know what do we do like are we gonna
 

02:03:55.460 --> 02:03:57.190
you know what do we do like are we gonna
change the sharing economy to the

02:03:57.190 --> 02:03:57.200
change the sharing economy to the
 

02:03:57.200 --> 02:03:59.860
change the sharing economy to the
renting economy should driverless cars

02:03:59.860 --> 02:03:59.870
renting economy should driverless cars
 

02:03:59.870 --> 02:04:03.670
renting economy should driverless cars
be a kind of public utility now so sort

02:04:03.670 --> 02:04:03.680
be a kind of public utility now so sort
 

02:04:03.680 --> 02:04:08.080
be a kind of public utility now so sort
of that's my add to that question yeah I

02:04:08.080 --> 02:04:08.090
of that's my add to that question yeah I
 

02:04:08.090 --> 02:04:10.690
of that's my add to that question yeah I
mean I think that it's this is a huge

02:04:10.690 --> 02:04:10.700
mean I think that it's this is a huge
 

02:04:10.700 --> 02:04:12.400
mean I think that it's this is a huge
worry as the privatization of all of

02:04:12.400 --> 02:04:12.410
worry as the privatization of all of
 

02:04:12.410 --> 02:04:14.500
worry as the privatization of all of
this and and what it the consequences of

02:04:14.500 --> 02:04:14.510
this and and what it the consequences of
 

02:04:14.510 --> 02:04:15.910
this and and what it the consequences of
that are and I think we have to be

02:04:15.910 --> 02:04:15.920
that are and I think we have to be
 

02:04:15.920 --> 02:04:17.860
that are and I think we have to be
serious about making sure things that

02:04:17.860 --> 02:04:17.870
serious about making sure things that
 

02:04:17.870 --> 02:04:21.370
serious about making sure things that
are public goods or that are going to be

02:04:21.370 --> 02:04:21.380
are public goods or that are going to be
 

02:04:21.380 --> 02:04:23.590
are public goods or that are going to be
at scale affecting many many people of a

02:04:23.590 --> 02:04:23.600
at scale affecting many many people of a
 

02:04:23.600 --> 02:04:26.320
at scale affecting many many people of a
citizenry we have to have public options

02:04:26.320 --> 02:04:26.330
citizenry we have to have public options
 

02:04:26.330 --> 02:04:28.690
citizenry we have to have public options
that are have some oversight and that

02:04:28.690 --> 02:04:28.700
that are have some oversight and that
 

02:04:28.700 --> 02:04:30.640
that are have some oversight and that
have some transparency and some

02:04:30.640 --> 02:04:30.650
have some transparency and some
 

02:04:30.650 --> 02:04:32.850
have some transparency and some
back-and-forth of being able to have

02:04:32.850 --> 02:04:32.860
back-and-forth of being able to have
 

02:04:32.860 --> 02:04:35.290
back-and-forth of being able to have
effective dissidents against it and I

02:04:35.290 --> 02:04:35.300
effective dissidents against it and I
 

02:04:35.300 --> 02:04:37.150
effective dissidents against it and I
our speaker brought this up if we have

02:04:37.150 --> 02:04:37.160
our speaker brought this up if we have
 

02:04:37.160 --> 02:04:39.280
our speaker brought this up if we have
to have a way to have redress and have

02:04:39.280 --> 02:04:39.290
to have a way to have redress and have
 

02:04:39.290 --> 02:04:40.690
to have a way to have redress and have
to have a way to have a voice and say

02:04:40.690 --> 02:04:40.700
to have a way to have a voice and say
 

02:04:40.700 --> 02:04:42.430
to have a way to have a voice and say
this isn't actually working and right

02:04:42.430 --> 02:04:42.440
this isn't actually working and right
 

02:04:42.440 --> 02:04:44.110
this isn't actually working and right
now in the context of private companies

02:04:44.110 --> 02:04:44.120
now in the context of private companies
 

02:04:44.120 --> 02:04:46.930
now in the context of private companies
you don't really have that so I do think

02:04:46.930 --> 02:04:46.940
you don't really have that so I do think
 

02:04:46.940 --> 02:04:48.580
you don't really have that so I do think
that making sure we we deploy systems

02:04:48.580 --> 02:04:48.590
that making sure we we deploy systems
 

02:04:48.590 --> 02:04:50.830
that making sure we we deploy systems
with that in mind having public

02:04:50.830 --> 02:04:50.840
with that in mind having public
 

02:04:50.840 --> 02:04:52.570
with that in mind having public
officials who when they think about

02:04:52.570 --> 02:04:52.580
officials who when they think about
 

02:04:52.580 --> 02:04:54.040
officials who when they think about
what's the future of transportation

02:04:54.040 --> 02:04:54.050
what's the future of transportation
 

02:04:54.050 --> 02:04:55.780
what's the future of transportation
don't just say oh I can't wait for

02:04:55.780 --> 02:04:55.790
don't just say oh I can't wait for
 

02:04:55.790 --> 02:04:57.070
don't just say oh I can't wait for
Amazon and nuber to fix our

02:04:57.070 --> 02:04:57.080
Amazon and nuber to fix our
 

02:04:57.080 --> 02:04:58.960
Amazon and nuber to fix our
transportation system but that are also

02:04:58.960 --> 02:04:58.970
transportation system but that are also
 

02:04:58.970 --> 02:05:01.450
transportation system but that are also
kind of thinking hey I probably should

02:05:01.450 --> 02:05:01.460
kind of thinking hey I probably should
 

02:05:01.460 --> 02:05:03.100
kind of thinking hey I probably should
be thinking about how to expand our

02:05:03.100 --> 02:05:03.110
be thinking about how to expand our
 

02:05:03.110 --> 02:05:05.230
be thinking about how to expand our
public bus system at the same time so

02:05:05.230 --> 02:05:05.240
public bus system at the same time so
 

02:05:05.240 --> 02:05:06.940
public bus system at the same time so
that there's still options out there we

02:05:06.940 --> 02:05:06.950
that there's still options out there we
 

02:05:06.950 --> 02:05:08.440
that there's still options out there we
don't end up going to this like

02:05:08.440 --> 02:05:08.450
don't end up going to this like
 

02:05:08.450 --> 02:05:11.760
don't end up going to this like
asymptote of only uber or only Google

02:05:11.760 --> 02:05:11.770
asymptote of only uber or only Google
 

02:05:11.770 --> 02:05:13.930
asymptote of only uber or only Google
well thank you everybody thank you

02:05:13.930 --> 02:05:13.940
well thank you everybody thank you
 

02:05:13.940 --> 02:05:16.500
well thank you everybody thank you
audience

02:05:16.500 --> 02:05:16.510
 

02:05:16.510 --> 02:05:26.459
[Applause]

02:05:26.459 --> 02:05:26.469
 

02:05:26.469 --> 02:05:28.629
well thanks again to the wonderful panel

02:05:28.629 --> 02:05:28.639
well thanks again to the wonderful panel
 

02:05:28.639 --> 02:05:30.039
well thanks again to the wonderful panel
that was an excellent way to start this

02:05:30.039 --> 02:05:30.049
that was an excellent way to start this
 

02:05:30.049 --> 02:05:32.049
that was an excellent way to start this
program what I really enjoy is that

02:05:32.049 --> 02:05:32.059
program what I really enjoy is that
 

02:05:32.059 --> 02:05:34.239
program what I really enjoy is that
we're going deep and not at all being

02:05:34.239 --> 02:05:34.249
we're going deep and not at all being
 

02:05:34.249 --> 02:05:36.489
we're going deep and not at all being
superficial in the in the way we treat

02:05:36.489 --> 02:05:36.499
superficial in the in the way we treat
 

02:05:36.499 --> 02:05:37.949
superficial in the in the way we treat
these subject areas and that's fantastic

02:05:37.949 --> 02:05:37.959
these subject areas and that's fantastic
 

02:05:37.959 --> 02:05:41.409
these subject areas and that's fantastic
now one quick interlude and then you

02:05:41.409 --> 02:05:41.419
now one quick interlude and then you
 

02:05:41.419 --> 02:05:43.839
now one quick interlude and then you
will get to your copy break the

02:05:43.839 --> 02:05:43.849
will get to your copy break the
 

02:05:43.849 --> 02:05:45.579
will get to your copy break the
interlude is to first of all introduce

02:05:45.579 --> 02:05:45.589
interlude is to first of all introduce
 

02:05:45.589 --> 02:05:47.529
interlude is to first of all introduce
the K&amp;L gates presidential fellowship

02:05:47.529 --> 02:05:47.539
the K&amp;L gates presidential fellowship
 

02:05:47.539 --> 02:05:49.359
the K&amp;L gates presidential fellowship
program one of the wonderful things that

02:05:49.359 --> 02:05:49.369
program one of the wonderful things that
 

02:05:49.369 --> 02:05:51.429
program one of the wonderful things that
Colonel gates did is they did not create

02:05:51.429 --> 02:05:51.439
Colonel gates did is they did not create
 

02:05:51.439 --> 02:05:53.439
Colonel gates did is they did not create
one giant pot of money and say do good

02:05:53.439 --> 02:05:53.449
one giant pot of money and say do good
 

02:05:53.449 --> 02:05:56.079
one giant pot of money and say do good
they actually created multiple specific

02:05:56.079 --> 02:05:56.089
they actually created multiple specific
 

02:05:56.089 --> 02:05:57.969
they actually created multiple specific
sub endowments that allow us to do many

02:05:57.969 --> 02:05:57.979
sub endowments that allow us to do many
 

02:05:57.979 --> 02:06:00.669
sub endowments that allow us to do many
things well and one of those is a

02:06:00.669 --> 02:06:00.679
things well and one of those is a
 

02:06:00.679 --> 02:06:03.250
things well and one of those is a
special pot of money for four fellows

02:06:03.250 --> 02:06:03.260
special pot of money for four fellows
 

02:06:03.260 --> 02:06:05.289
special pot of money for four fellows
those are graduate students we're doing

02:06:05.289 --> 02:06:05.299
those are graduate students we're doing
 

02:06:05.299 --> 02:06:06.969
those are graduate students we're doing
outstanding research in the area of

02:06:06.969 --> 02:06:06.979
outstanding research in the area of
 

02:06:06.979 --> 02:06:08.289
outstanding research in the area of
ethics and computational technology

02:06:08.289 --> 02:06:08.299
ethics and computational technology
 

02:06:08.299 --> 02:06:10.869
ethics and computational technology
there's a group that's selecting those

02:06:10.869 --> 02:06:10.879
there's a group that's selecting those
 

02:06:10.879 --> 02:06:13.599
there's a group that's selecting those
four fellows every year and those

02:06:13.599 --> 02:06:13.609
four fellows every year and those
 

02:06:13.609 --> 02:06:15.309
four fellows every year and those
fellows in turn provided the financial

02:06:15.309 --> 02:06:15.319
fellows in turn provided the financial
 

02:06:15.319 --> 02:06:17.139
fellows in turn provided the financial
support to allow those students to do

02:06:17.139 --> 02:06:17.149
support to allow those students to do
 

02:06:17.149 --> 02:06:20.229
support to allow those students to do
that work for that year so we're gonna

02:06:20.229 --> 02:06:20.239
that work for that year so we're gonna
 

02:06:20.239 --> 02:06:21.879
that work for that year so we're gonna
be meeting those four fellows one at a

02:06:21.879 --> 02:06:21.889
be meeting those four fellows one at a
 

02:06:21.889 --> 02:06:24.819
be meeting those four fellows one at a
time across today and tomorrow and so

02:06:24.819 --> 02:06:24.829
time across today and tomorrow and so
 

02:06:24.829 --> 02:06:26.199
time across today and tomorrow and so
I'm very pleased to introduce you to the

02:06:26.199 --> 02:06:26.209
I'm very pleased to introduce you to the
 

02:06:26.209 --> 02:06:28.479
I'm very pleased to introduce you to the
first one except that I'm introducing

02:06:28.479 --> 02:06:28.489
first one except that I'm introducing
 

02:06:28.489 --> 02:06:30.399
first one except that I'm introducing
you to the one who is not here in person

02:06:30.399 --> 02:06:30.409
you to the one who is not here in person
 

02:06:30.409 --> 02:06:32.199
you to the one who is not here in person
because she's on a trip so you're gonna

02:06:32.199 --> 02:06:32.209
because she's on a trip so you're gonna
 

02:06:32.209 --> 02:06:34.239
because she's on a trip so you're gonna
have a video introduction to her so let

02:06:34.239 --> 02:06:34.249
have a video introduction to her so let
 

02:06:34.249 --> 02:06:36.339
have a video introduction to her so let
me first tell you a little bit about her

02:06:36.339 --> 02:06:36.349
me first tell you a little bit about her
 

02:06:36.349 --> 02:06:38.619
me first tell you a little bit about her
and then we'll watch her video

02:06:38.619 --> 02:06:38.629
and then we'll watch her video
 

02:06:38.629 --> 02:06:40.750
and then we'll watch her video
the Karen legates Presidential Inaugural

02:06:40.750 --> 02:06:40.760
the Karen legates Presidential Inaugural
 

02:06:40.760 --> 02:06:42.969
the Karen legates Presidential Inaugural
Fellow in this program is Veronica

02:06:42.969 --> 02:06:42.979
Fellow in this program is Veronica
 

02:06:42.979 --> 02:06:45.639
Fellow in this program is Veronica
Mariota and Veronica completed a

02:06:45.639 --> 02:06:45.649
Mariota and Veronica completed a
 

02:06:45.649 --> 02:06:47.949
Mariota and Veronica completed a
bachelor's degree in economics and a

02:06:47.949 --> 02:06:47.959
bachelor's degree in economics and a
 

02:06:47.959 --> 02:06:49.959
bachelor's degree in economics and a
master's degree in science and economics

02:06:49.959 --> 02:06:49.969
master's degree in science and economics
 

02:06:49.969 --> 02:06:52.359
master's degree in science and economics
and business law at the University of

02:06:52.359 --> 02:06:52.369
and business law at the University of
 

02:06:52.369 --> 02:06:53.949
and business law at the University of
Rome at Toro variegata

02:06:53.949 --> 02:06:53.959
Rome at Toro variegata
 

02:06:53.959 --> 02:06:56.079
Rome at Toro variegata
she then earned a master's degree in

02:06:56.079 --> 02:06:56.089
she then earned a master's degree in
 

02:06:56.089 --> 02:06:58.059
she then earned a master's degree in
economics and mathematical economics at

02:06:58.059 --> 02:06:58.069
economics and mathematical economics at
 

02:06:58.069 --> 02:06:59.739
economics and mathematical economics at
Tilburg University School of Economics

02:06:59.739 --> 02:06:59.749
Tilburg University School of Economics
 

02:06:59.749 --> 02:07:02.770
Tilburg University School of Economics
and management now her focus area is

02:07:02.770 --> 02:07:02.780
and management now her focus area is
 

02:07:02.780 --> 02:07:04.479
and management now her focus area is
fantastic for this conference because

02:07:04.479 --> 02:07:04.489
fantastic for this conference because
 

02:07:04.489 --> 02:07:07.059
fantastic for this conference because
your focus area is on the societal

02:07:07.059 --> 02:07:07.069
your focus area is on the societal
 

02:07:07.069 --> 02:07:09.339
your focus area is on the societal
issues related to information usage and

02:07:09.339 --> 02:07:09.349
issues related to information usage and
 

02:07:09.349 --> 02:07:11.559
issues related to information usage and
specifically the collection of consumer

02:07:11.559 --> 02:07:11.569
specifically the collection of consumer
 

02:07:11.569 --> 02:07:15.159
specifically the collection of consumer
data and particularly the collection of

02:07:15.159 --> 02:07:15.169
data and particularly the collection of
 

02:07:15.169 --> 02:07:15.909
data and particularly the collection of
unlighted

02:07:15.909 --> 02:07:15.919
unlighted
 

02:07:15.919 --> 02:07:18.849
unlighted
on sorry I said that wrong online

02:07:18.849 --> 02:07:18.859
on sorry I said that wrong online
 

02:07:18.859 --> 02:07:21.159
on sorry I said that wrong online
targeted behavioral data that allows

02:07:21.159 --> 02:07:21.169
targeted behavioral data that allows
 

02:07:21.169 --> 02:07:24.009
targeted behavioral data that allows
specific bespoke marketing to happen so

02:07:24.009 --> 02:07:24.019
specific bespoke marketing to happen so
 

02:07:24.019 --> 02:07:26.619
specific bespoke marketing to happen so
her whole work area has focused on a

02:07:26.619 --> 02:07:26.629
her whole work area has focused on a
 

02:07:26.629 --> 02:07:28.839
her whole work area has focused on a
question of targeted advertising and the

02:07:28.839 --> 02:07:28.849
question of targeted advertising and the
 

02:07:28.849 --> 02:07:31.059
question of targeted advertising and the
ways in which targeted advertisements

02:07:31.059 --> 02:07:31.069
ways in which targeted advertisements
 

02:07:31.069 --> 02:07:34.179
ways in which targeted advertisements
impact socially on consumers and on

02:07:34.179 --> 02:07:34.189
impact socially on consumers and on
 

02:07:34.189 --> 02:07:35.199
impact socially on consumers and on
publishers

02:07:35.199 --> 02:07:35.209
publishers
 

02:07:35.209 --> 02:07:37.149
publishers
so she has an analytical model that

02:07:37.149 --> 02:07:37.159
so she has an analytical model that
 

02:07:37.159 --> 02:07:39.040
so she has an analytical model that
analyzes the impact of targeted

02:07:39.040 --> 02:07:39.050
analyzes the impact of targeted
 

02:07:39.050 --> 02:07:41.770
analyzes the impact of targeted
advertisements on consumer welfare and

02:07:41.770 --> 02:07:41.780
advertisements on consumer welfare and
 

02:07:41.780 --> 02:07:43.870
advertisements on consumer welfare and
I'd love to see that numerical model for

02:07:43.870 --> 02:07:43.880
I'd love to see that numerical model for
 

02:07:43.880 --> 02:07:45.069
I'd love to see that numerical model for
dealing with welfare because that's an

02:07:45.069 --> 02:07:45.079
dealing with welfare because that's an
 

02:07:45.079 --> 02:07:47.290
dealing with welfare because that's an
important topic so let's please roll the

02:07:47.290 --> 02:07:47.300
important topic so let's please roll the
 

02:07:47.300 --> 02:07:54.429
important topic so let's please roll the
video for Veronica in one of my projects

02:07:54.429 --> 02:07:54.439
video for Veronica in one of my projects
 

02:07:54.439 --> 02:07:57.160
video for Veronica in one of my projects
I developed a framework to understand

02:07:57.160 --> 02:07:57.170
I developed a framework to understand
 

02:07:57.170 --> 02:07:59.350
I developed a framework to understand
the role of personal data in the context

02:07:59.350 --> 02:07:59.360
the role of personal data in the context
 

02:07:59.360 --> 02:08:01.509
the role of personal data in the context
of online targeted advertising and

02:08:01.509 --> 02:08:01.519
of online targeted advertising and
 

02:08:01.519 --> 02:08:04.629
of online targeted advertising and
within this context I analyzed how their

02:08:04.629 --> 02:08:04.639
within this context I analyzed how their
 

02:08:04.639 --> 02:08:07.569
within this context I analyzed how their
location of benefits among consumers and

02:08:07.569 --> 02:08:07.579
location of benefits among consumers and
 

02:08:07.579 --> 02:08:10.209
location of benefits among consumers and
industry players would change when

02:08:10.209 --> 02:08:10.219
industry players would change when
 

02:08:10.219 --> 02:08:13.419
industry players would change when
different policy interventions that can

02:08:13.419 --> 02:08:13.429
different policy interventions that can
 

02:08:13.429 --> 02:08:17.140
different policy interventions that can
restrict how many and which consumer

02:08:17.140 --> 02:08:17.150
restrict how many and which consumer
 

02:08:17.150 --> 02:08:19.449
restrict how many and which consumer
data can be collected and used are

02:08:19.449 --> 02:08:19.459
data can be collected and used are
 

02:08:19.459 --> 02:08:21.819
data can be collected and used are
actually implemented in the market and

02:08:21.819 --> 02:08:21.829
actually implemented in the market and
 

02:08:21.829 --> 02:08:24.160
actually implemented in the market and
the results of the analysis highlights

02:08:24.160 --> 02:08:24.170
the results of the analysis highlights
 

02:08:24.170 --> 02:08:26.009
the results of the analysis highlights
the existence of an incentive

02:08:26.009 --> 02:08:26.019
the existence of an incentive
 

02:08:26.019 --> 02:08:29.259
the existence of an incentive
misalignments among the players in that

02:08:29.259 --> 02:08:29.269
misalignments among the players in that
 

02:08:29.269 --> 02:08:31.299
misalignments among the players in that
the policy intervention that who

02:08:31.299 --> 02:08:31.309
the policy intervention that who
 

02:08:31.309 --> 02:08:33.429
the policy intervention that who
maximized the industry profit for

02:08:33.429 --> 02:08:33.439
maximized the industry profit for
 

02:08:33.439 --> 02:08:36.100
maximized the industry profit for
example can vastly differ from the

02:08:36.100 --> 02:08:36.110
example can vastly differ from the
 

02:08:36.110 --> 02:08:37.770
example can vastly differ from the
policy intervention that will maximize

02:08:37.770 --> 02:08:37.780
policy intervention that will maximize
 

02:08:37.780 --> 02:08:40.839
policy intervention that will maximize
consumers benefits and so this creates a

02:08:40.839 --> 02:08:40.849
consumers benefits and so this creates a
 

02:08:40.849 --> 02:08:43.149
consumers benefits and so this creates a
tensions among these two set of players

02:08:43.149 --> 02:08:43.159
tensions among these two set of players
 

02:08:43.159 --> 02:08:45.279
tensions among these two set of players
because depending on the underlying

02:08:45.279 --> 02:08:45.289
because depending on the underlying
 

02:08:45.289 --> 02:08:47.410
because depending on the underlying
market structure different police

02:08:47.410 --> 02:08:47.420
market structure different police
 

02:08:47.420 --> 02:08:50.350
market structure different police
interventions are going to lead to very

02:08:50.350 --> 02:08:50.360
interventions are going to lead to very
 

02:08:50.360 --> 02:08:52.660
interventions are going to lead to very
different allocations of benefits the

02:08:52.660 --> 02:08:52.670
different allocations of benefits the
 

02:08:52.670 --> 02:08:54.359
different allocations of benefits the
existence of this trade-off between

02:08:54.359 --> 02:08:54.369
existence of this trade-off between
 

02:08:54.369 --> 02:08:57.189
existence of this trade-off between
consumers interest and the industry

02:08:57.189 --> 02:08:57.199
consumers interest and the industry
 

02:08:57.199 --> 02:09:00.370
consumers interest and the industry
interest under certain circumstances but

02:09:00.370 --> 02:09:00.380
interest under certain circumstances but
 

02:09:00.380 --> 02:09:03.669
interest under certain circumstances but
not in all the cases calls for policy

02:09:03.669 --> 02:09:03.679
not in all the cases calls for policy
 

02:09:03.679 --> 02:09:05.919
not in all the cases calls for policy
interventions that are tailored rather

02:09:05.919 --> 02:09:05.929
interventions that are tailored rather
 

02:09:05.929 --> 02:09:09.069
interventions that are tailored rather
than a uniform or Omni comprehensive so

02:09:09.069 --> 02:09:09.079
than a uniform or Omni comprehensive so
 

02:09:09.079 --> 02:09:11.709
than a uniform or Omni comprehensive so
for example under certain conditions the

02:09:11.709 --> 02:09:11.719
for example under certain conditions the
 

02:09:11.719 --> 02:09:14.080
for example under certain conditions the
market by itself can lead to an outcome

02:09:14.080 --> 02:09:14.090
market by itself can lead to an outcome
 

02:09:14.090 --> 02:09:16.660
market by itself can lead to an outcome
that is optimal for most of the players

02:09:16.660 --> 02:09:16.670
that is optimal for most of the players
 

02:09:16.670 --> 02:09:20.109
that is optimal for most of the players
if not for all of them but in other

02:09:20.109 --> 02:09:20.119
if not for all of them but in other
 

02:09:20.119 --> 02:09:22.270
if not for all of them but in other
situations when there is a complete miss

02:09:22.270 --> 02:09:22.280
situations when there is a complete miss
 

02:09:22.280 --> 02:09:24.219
situations when there is a complete miss
alignments of interest among the players

02:09:24.219 --> 02:09:24.229
alignments of interest among the players
 

02:09:24.229 --> 02:09:26.739
alignments of interest among the players
then there may be the need for a

02:09:26.739 --> 02:09:26.749
then there may be the need for a
 

02:09:26.749 --> 02:09:29.049
then there may be the need for a
policymaker to intervene either to

02:09:29.049 --> 02:09:29.059
policymaker to intervene either to
 

02:09:29.059 --> 02:09:31.419
policymaker to intervene either to
protect the consumers or to protect the

02:09:31.419 --> 02:09:31.429
protect the consumers or to protect the
 

02:09:31.429 --> 02:09:34.390
protect the consumers or to protect the
industry depending also on the objective

02:09:34.390 --> 02:09:34.400
industry depending also on the objective
 

02:09:34.400 --> 02:09:36.640
industry depending also on the objective
of the policy in another of my projects

02:09:36.640 --> 02:09:36.650
of the policy in another of my projects
 

02:09:36.650 --> 02:09:39.219
of the policy in another of my projects
I designed an experiment to investigate

02:09:39.219 --> 02:09:39.229
I designed an experiment to investigate
 

02:09:39.229 --> 02:09:41.259
I designed an experiment to investigate
the impact of blocking social media

02:09:41.259 --> 02:09:41.269
the impact of blocking social media
 

02:09:41.269 --> 02:09:43.870
the impact of blocking social media
interruptions on individuals performance

02:09:43.870 --> 02:09:43.880
interruptions on individuals performance
 

02:09:43.880 --> 02:09:46.810
interruptions on individuals performance
and the experiment was implemented by

02:09:46.810 --> 02:09:46.820
and the experiment was implemented by
 

02:09:46.820 --> 02:09:47.340
and the experiment was implemented by
using

02:09:47.340 --> 02:09:47.350
using
 

02:09:47.350 --> 02:09:49.500
using
Digital application so an app that

02:09:49.500 --> 02:09:49.510
Digital application so an app that
 

02:09:49.510 --> 02:09:52.500
Digital application so an app that
allowed us to block the access to

02:09:52.500 --> 02:09:52.510
allowed us to block the access to
 

02:09:52.510 --> 02:09:55.020
allowed us to block the access to
specific social media websites for a

02:09:55.020 --> 02:09:55.030
specific social media websites for a
 

02:09:55.030 --> 02:09:57.300
specific social media websites for a
group of individuals and the results of

02:09:57.300 --> 02:09:57.310
group of individuals and the results of
 

02:09:57.310 --> 02:09:58.470
group of individuals and the results of
the experiment

02:09:58.470 --> 02:09:58.480
the experiment
 

02:09:58.480 --> 02:10:00.960
the experiment
indeed suggest that the average social

02:10:00.960 --> 02:10:00.970
indeed suggest that the average social
 

02:10:00.970 --> 02:10:02.660
indeed suggest that the average social
media user could benefit from

02:10:02.660 --> 02:10:02.670
media user could benefit from
 

02:10:02.670 --> 02:10:04.680
media user could benefit from
technologies to limit

02:10:04.680 --> 02:10:04.690
technologies to limit
 

02:10:04.690 --> 02:10:07.500
technologies to limit
such social media interruptions in that

02:10:07.500 --> 02:10:07.510
such social media interruptions in that
 

02:10:07.510 --> 02:10:10.220
such social media interruptions in that
in our experiment participants

02:10:10.220 --> 02:10:10.230
in our experiment participants
 

02:10:10.230 --> 02:10:12.990
in our experiment participants
experienced a significant improvement in

02:10:12.990 --> 02:10:13.000
experienced a significant improvement in
 

02:10:13.000 --> 02:10:15.630
experienced a significant improvement in
their performance so for example they

02:10:15.630 --> 02:10:15.640
their performance so for example they
 

02:10:15.640 --> 02:10:17.610
their performance so for example they
completed a significantly higher number

02:10:17.610 --> 02:10:17.620
completed a significantly higher number
 

02:10:17.620 --> 02:10:20.790
completed a significantly higher number
of working tasks per hour compared to

02:10:20.790 --> 02:10:20.800
of working tasks per hour compared to
 

02:10:20.800 --> 02:10:22.950
of working tasks per hour compared to
individuals for whom we didn't block

02:10:22.950 --> 02:10:22.960
individuals for whom we didn't block
 

02:10:22.960 --> 02:10:25.200
individuals for whom we didn't block
social media interactions so in the same

02:10:25.200 --> 02:10:25.210
social media interactions so in the same
 

02:10:25.210 --> 02:10:27.720
social media interactions so in the same
experiment we also add a second group of

02:10:27.720 --> 02:10:27.730
experiment we also add a second group of
 

02:10:27.730 --> 02:10:30.390
experiment we also add a second group of
individuals that was given the

02:10:30.390 --> 02:10:30.400
individuals that was given the
 

02:10:30.400 --> 02:10:33.360
individuals that was given the
application and left free to use it so

02:10:33.360 --> 02:10:33.370
application and left free to use it so
 

02:10:33.370 --> 02:10:35.640
application and left free to use it so
choose which social media websites to

02:10:35.640 --> 02:10:35.650
choose which social media websites to
 

02:10:35.650 --> 02:10:37.860
choose which social media websites to
block him for how long and the objective

02:10:37.860 --> 02:10:37.870
block him for how long and the objective
 

02:10:37.870 --> 02:10:39.890
block him for how long and the objective
was exactly try to investigate whether

02:10:39.890 --> 02:10:39.900
was exactly try to investigate whether
 

02:10:39.900 --> 02:10:42.870
was exactly try to investigate whether
individuals when letting control can

02:10:42.870 --> 02:10:42.880
individuals when letting control can
 

02:10:42.880 --> 02:10:44.670
individuals when letting control can
impose restrictions on their home

02:10:44.670 --> 02:10:44.680
impose restrictions on their home
 

02:10:44.680 --> 02:10:46.500
impose restrictions on their home
behavior and the result of the

02:10:46.500 --> 02:10:46.510
behavior and the result of the
 

02:10:46.510 --> 02:10:48.840
behavior and the result of the
experiment suggests that individuals may

02:10:48.840 --> 02:10:48.850
experiment suggests that individuals may
 

02:10:48.850 --> 02:10:51.180
experiment suggests that individuals may
not be able to do so either for a lack

02:10:51.180 --> 02:10:51.190
not be able to do so either for a lack
 

02:10:51.190 --> 02:10:53.550
not be able to do so either for a lack
of motivation in using such technologies

02:10:53.550 --> 02:10:53.560
of motivation in using such technologies
 

02:10:53.560 --> 02:10:57.180
of motivation in using such technologies
or for a lack of self control that may

02:10:57.180 --> 02:10:57.190
or for a lack of self control that may
 

02:10:57.190 --> 02:10:59.820
or for a lack of self control that may
somewhat limit their ability to impose

02:10:59.820 --> 02:10:59.830
somewhat limit their ability to impose
 

02:10:59.830 --> 02:11:01.680
somewhat limit their ability to impose
restrictions on their own browsing

02:11:01.680 --> 02:11:01.690
restrictions on their own browsing
 

02:11:01.690 --> 02:11:06.540
restrictions on their own browsing
behavior I think it's completely

02:11:06.540 --> 02:11:06.550
behavior I think it's completely
 

02:11:06.550 --> 02:11:10.070
behavior I think it's completely
fascinating that access can disempower

02:11:10.070 --> 02:11:10.080
fascinating that access can disempower
 

02:11:10.080 --> 02:11:12.090
fascinating that access can disempower
because that's not usually how we think

02:11:12.090 --> 02:11:12.100
because that's not usually how we think
 

02:11:12.100 --> 02:11:13.980
because that's not usually how we think
and Veronica's research is outstanding

02:11:13.980 --> 02:11:13.990
and Veronica's research is outstanding
 

02:11:13.990 --> 02:11:16.620
and Veronica's research is outstanding
at showing that nuanced so I'd love to

02:11:16.620 --> 02:11:16.630
at showing that nuanced so I'd love to
 

02:11:16.630 --> 02:11:17.700
at showing that nuanced so I'd love to
sit with her now and have a little

02:11:17.700 --> 02:11:17.710
sit with her now and have a little
 

02:11:17.710 --> 02:11:20.010
sit with her now and have a little
discussion that's not gonna go well all

02:11:20.010 --> 02:11:20.020
discussion that's not gonna go well all
 

02:11:20.020 --> 02:11:22.290
discussion that's not gonna go well all
I can do is play video again so instead

02:11:22.290 --> 02:11:22.300
I can do is play video again so instead
 

02:11:22.300 --> 02:11:25.080
I can do is play video again so instead
let's have a coffee break and at 4:30

02:11:25.080 --> 02:11:25.090
let's have a coffee break and at 4:30
 

02:11:25.090 --> 02:11:26.940
let's have a coffee break and at 4:30
sharp we're gonna begin again and we

02:11:26.940 --> 02:11:26.950
sharp we're gonna begin again and we
 

02:11:26.950 --> 02:11:28.680
sharp we're gonna begin again and we
will have our introduction in keynote

02:11:28.680 --> 02:11:28.690
will have our introduction in keynote
 

02:11:28.690 --> 02:11:30.600
will have our introduction in keynote
speech by Eric Horvitz head of Microsoft

02:11:30.600 --> 02:11:30.610
speech by Eric Horvitz head of Microsoft
 

02:11:30.610 --> 02:11:32.220
speech by Eric Horvitz head of Microsoft
Research so we look forward to seeing

02:11:32.220 --> 02:11:32.230
Research so we look forward to seeing
 

02:11:32.230 --> 02:11:34.920
Research so we look forward to seeing
you then the coffee is in the back just

02:11:34.920 --> 02:11:34.930
you then the coffee is in the back just
 

02:11:34.930 --> 02:11:38.350
you then the coffee is in the back just
behind the gap in the wall see you soon

02:11:38.350 --> 02:11:38.360
behind the gap in the wall see you soon
 

02:11:38.360 --> 02:11:46.620
behind the gap in the wall see you soon
[Music]

02:11:46.620 --> 02:11:46.630
 

02:11:46.630 --> 02:11:48.690
you

