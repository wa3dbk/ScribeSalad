WEBVTT
Kind: captions
Language: en

00:00:02.980 --> 00:00:06.000
Hi, I’m Carrie Anne and welcome to CrashCourse
Computer Science!

00:00:06.000 --> 00:00:09.710
As we’ve discussed throughout the series,
computers have come a long way from mechanical

00:00:09.710 --> 00:00:15.200
devices capable of maybe one calculation per second, to CPUs running at kilohertz and megahertz speeds.

00:00:15.360 --> 00:00:19.220
The device you’re watching this video on
right now is almost certainly running at Gigahertz

00:00:19.220 --> 00:00:22.610
speeds - that’s billions of instructions
executed every second.

00:00:22.610 --> 00:00:24.980
Which, trust me, is a lot of computation!

00:00:24.980 --> 00:00:28.970
In the early days of electronic computing,
processors were typically made faster by improving

00:00:28.970 --> 00:00:33.140
the switching time of the transistors inside
the chip - the ones that make up all the logic

00:00:33.140 --> 00:00:36.440
gates, ALUs and other stuff we’ve talked
about over the past few episodes.

00:00:36.440 --> 00:00:41.079
But just making transistors faster and more
efficient only went so far, so processor designers

00:00:41.079 --> 00:00:45.540
have developed various techniques to boost
performance allowing not only simple instructions

00:00:45.540 --> 00:00:49.300
to run fast, but also performing much more
sophisticated operations.

00:00:49.300 --> 00:00:58.480
INTRO

00:00:58.480 --> 00:01:03.680
Last episode, we created a small program for
our CPU that allowed us to divide two numbers.

00:01:03.690 --> 00:01:08.240
We did this by doing many subtractions in
a row... so, for example, 16 divided by 4

00:01:08.250 --> 00:01:13.770
could be broken down into the smaller problem
of 16 minus 4, minus 4, minus 4, minus 4.

00:01:13.770 --> 00:01:17.319
When we hit zero, or a negative number, we
knew that we we’re done.

00:01:17.319 --> 00:01:20.869
But this approach gobbles up a lot of clock
cycles, and isn’t particularly efficient.

00:01:20.869 --> 00:01:25.020
So most computer processors today have divide
as one of the instructions that the ALU can

00:01:25.020 --> 00:01:26.319
perform in hardware.

00:01:26.320 --> 00:01:30.740
Of course, this extra circuitry makes the
ALU bigger and more complicated to design,

00:01:30.740 --> 00:01:35.420
but also more capable - a complexity-for-speed
tradeoff that has been made many times in

00:01:35.420 --> 00:01:36.420
computing history.

00:01:36.420 --> 00:01:40.689
For instance, modern computer processors now
have special circuits for things like graphics

00:01:40.689 --> 00:01:45.189
operations, decoding compressed video, and
encrypting files - all of which are operations

00:01:45.189 --> 00:01:48.799
that would take many many many clock cycles
to perform with standard operations.

00:01:48.800 --> 00:01:53.420
You may have even heard of processors with
MMX, 3DNow!, or SSE.

00:01:53.420 --> 00:01:57.660
These are processors with additional, fancy
circuits that allow them to execute additional,

00:01:57.670 --> 00:02:00.800
fancy instructions - for things like gaming
and encryption.

00:02:00.810 --> 00:02:04.409
These extensions to the instruction set have
grown, and grown over time, and once people

00:02:04.409 --> 00:02:07.859
have written programs to take advantage of
them, it’s hard to remove them.

00:02:07.859 --> 00:02:12.230
So instruction sets tend to keep getting larger
and larger keeping all the old opcodes around

00:02:12.230 --> 00:02:13.650
for backwards compatibility.

00:02:13.650 --> 00:02:19.730
The Intel 4004, the first truly integrated
CPU, had 46 instructions - which was enough

00:02:19.730 --> 00:02:21.590
to build a fully functional computer.

00:02:21.590 --> 00:02:26.110
But a modern computer processor has thousands
of different instructions, which utilize all

00:02:26.110 --> 00:02:28.549
sorts of clever and complex internal circuitry.

00:02:28.549 --> 00:02:33.709
Now, high clock speeds and fancy instruction
sets lead to another problem - getting data

00:02:33.709 --> 00:02:35.560
in and out of the CPU quickly enough.

00:02:35.560 --> 00:02:40.519
It’s like having a powerful steam locomotive,
but no way to shovel in coal fast enough.

00:02:40.519 --> 00:02:42.319
In this case, the bottleneck is RAM.

00:02:42.319 --> 00:02:45.689
RAM is typically a memory module that lies
outside the CPU.

00:02:45.689 --> 00:02:49.609
This means that data has to be transmitted
to and from RAM along sets of data wires,

00:02:49.609 --> 00:02:50.950
called a bus.

00:02:50.950 --> 00:02:54.450
This bus might only be a few centimeters long,
and remember those electrical signals are

00:02:54.450 --> 00:02:58.330
traveling near the speed of light, but when
you are operating at gigahertz speeds – that’s

00:02:58.330 --> 00:03:02.900
billionths of a second – even this small
delay starts to become problematic.

00:03:02.900 --> 00:03:07.030
It also takes time for RAM itself to lookup
the address, retrieve the data, and configure

00:03:07.030 --> 00:03:08.440
itself for output.

00:03:08.460 --> 00:03:12.510
So a “load from RAM” instruction might take
dozens of clock cycles to complete, and during

00:03:12.510 --> 00:03:16.040
this time the processor is just sitting there
idly waiting for the data.

00:03:16.040 --> 00:03:20.310
One solution is to put a little piece of RAM
right on the CPU -- called a cache.

00:03:20.310 --> 00:03:23.890
There isn’t a lot of space on a processor’s
chip, so most caches are just kilobytes or

00:03:23.890 --> 00:03:27.300
maybe megabytes in size, where RAM is usually
gigabytes.

00:03:27.300 --> 00:03:29.520
Having a cache speeds things up in a clever
way.

00:03:29.530 --> 00:03:34.480
When the CPU requests a memory location from
RAM, the RAM can transmit not just one single

00:03:34.499 --> 00:03:36.569
value, but a whole block of data.

00:03:36.569 --> 00:03:38.659
This takes only a little bit more time than
transmitting a single value, but it allows

00:03:38.659 --> 00:03:41.180
this data block to be saved into the cache.

00:03:41.180 --> 00:03:45.920
This tends to be really useful because computer
data is often arranged and processed sequentially.

00:03:45.920 --> 00:03:50.239
For example, let say the processor is totalling
up daily sales for a restaurant.

00:03:50.239 --> 00:03:54.200
It starts by fetching the first transaction
from RAM at memory location 100.

00:03:54.200 --> 00:03:58.879
The RAM, instead of sending back just that
one value, sends a block of data, from memory

00:03:58.879 --> 00:04:02.840
location 100 through 200, which are then all
copied into the cache.

00:04:02.840 --> 00:04:06.651
Now, when the processor requests the next
transaction to add to its running total, the

00:04:06.651 --> 00:04:10.859
value at address 101, the cache will say “Oh,
I’ve already got that value right here,

00:04:10.859 --> 00:04:12.239
so I can give it to you right away!”

00:04:12.239 --> 00:04:14.730
And there’s no need to go all the way to
RAM.

00:04:14.730 --> 00:04:18.489
Because the cache is so close to the processor,
it can typically provide the data in a single

00:04:18.489 --> 00:04:21.039
clock cycle -- no waiting required.

00:04:21.039 --> 00:04:24.990
This speeds things up tremendously over having
to go back and forth to RAM every single time.

00:04:24.990 --> 00:04:28.440
When data requested in RAM is already stored
in the cache like this it’s called a cache

00:04:28.440 --> 00:04:29.440
hit,

00:04:29.440 --> 00:04:32.970
and if the data requested isn’t in the cache,
so you have to go to RAM, it’s a called

00:04:32.970 --> 00:04:34.090
a cache miss.

00:04:34.090 --> 00:04:38.419
The cache can also be used like a scratch
space, storing intermediate values when performing

00:04:38.420 --> 00:04:41.120
a longer, or more complicated calculation.

00:04:41.120 --> 00:04:44.580
Continuing our restaurant example, let’s
say the processor has finished totalling up

00:04:44.590 --> 00:04:48.990
all of the sales for the day, and wants to
store the result in memory address 150.

00:04:48.990 --> 00:04:53.319
Like before, instead of going back all the
way to RAM to save that value, it can be stored

00:04:53.319 --> 00:04:59.039
in cached copy, which is faster to save to,
and also faster to access later if more calculations

00:04:59.040 --> 00:04:59.720
are needed.

00:04:59.720 --> 00:05:04.000
But this introduces an interesting problem
-- the cache’s copy of the data is now different

00:05:04.009 --> 00:05:05.900
to the real version stored in RAM.

00:05:05.900 --> 00:05:09.870
This mismatch has to be recorded, so that
at some point everything can get synced up.

00:05:09.870 --> 00:05:13.960
For this purpose, the cache has a special
flag for each block of memory it stores, called

00:05:13.960 --> 00:05:18.820
the dirty bit -- which might just be the best
term computer scientists have ever invented.

00:05:18.820 --> 00:05:23.060
Most often this synchronization happens when
the cache is full, but a new block of memory

00:05:23.060 --> 00:05:24.960
is being requested by the processor.

00:05:24.960 --> 00:05:29.220
Before the cache erases the old block to free
up space, it checks its dirty bit, and if

00:05:29.220 --> 00:05:33.260
it’s dirty, the old block of data is written
back to RAM before loading in the new block.

00:05:33.260 --> 00:05:37.160
Another trick to boost cpu performance is
called instruction pipelining.

00:05:37.169 --> 00:05:40.440
Imagine you have to wash an entire hotel’s
worth of sheets, but you’ve only got one

00:05:40.440 --> 00:05:42.020
washing machine and one dryer.

00:05:42.020 --> 00:05:45.790
One option is to do it all sequentially: put
a batch of sheets in the washer and wait 30

00:05:45.790 --> 00:05:46.940
minutes for it to finish.

00:05:46.940 --> 00:05:50.180
Then take the wet sheets out and put them
in the dryer and wait another 30 minutes for

00:05:50.180 --> 00:05:51.310
that to finish.

00:05:51.310 --> 00:05:53.710
This allows you to do one batch of sheets
every hour.

00:05:53.710 --> 00:05:57.509
Side note: if you have a dryer that can dry
a load of laundry in 30 minutes, please tell

00:05:57.509 --> 00:06:01.500
me the brand and model in the comments, because
I’m living with 90 minute dry times, minimum.

00:06:01.500 --> 00:06:06.260
But, even with this magic clothes dryer, you
can speed things up even more if you parallelize

00:06:06.260 --> 00:06:07.270
your operation.

00:06:07.270 --> 00:06:10.430
As before, you start off putting one batch
of sheets in the washer.

00:06:10.430 --> 00:06:12.259
You wait 30 minutes for it to finish.

00:06:12.259 --> 00:06:14.830
Then you take the wet sheets out and put them
in the dryer.

00:06:14.830 --> 00:06:19.039
But this time, instead of just waiting 30
minutes for the dryer to finish, you simultaneously

00:06:19.040 --> 00:06:21.280
start another load in the washing machine.

00:06:21.280 --> 00:06:23.540
Now you’ve got both machines going at once.

00:06:23.540 --> 00:06:27.659
Wait 30 minutes, and one batch is now done,
one batch is half done, and another is ready

00:06:27.659 --> 00:06:28.659
to go in.

00:06:28.660 --> 00:06:30.180
This effectively doubles your throughput.

00:06:30.220 --> 00:06:32.380
Processor designs can apply the same idea.

00:06:32.380 --> 00:06:37.550
In episode 7, our example processor performed
the fetch-decode-execute cycle sequentially

00:06:37.550 --> 00:06:41.720
and in a continuous loop: Fetch-decode-execute,
fetch-decode-execute, fetch-decode-execute,

00:06:41.720 --> 00:06:43.250
and so on.

00:06:43.250 --> 00:06:46.050
This meant our design required three clock
cycles to execute one instruction.

00:06:46.050 --> 00:06:50.240
But each of these stages uses a different
part of the CPU, meaning there is an opportunity

00:06:50.240 --> 00:06:51.240
to parallelize!

00:06:51.240 --> 00:06:55.300
While one instruction is getting executed,
the next instruction could be getting decoded,

00:06:55.300 --> 00:06:57.840
and the instruction beyond that fetched from
memory.

00:06:57.840 --> 00:07:01.629
All of these separate processes can overlap
so that all parts of the CPU are active at

00:07:01.629 --> 00:07:02.939
any given time.

00:07:02.939 --> 00:07:07.460
In this pipelined design, an instruction is
executed every single clock cycle which triples

00:07:07.460 --> 00:07:08.460
the throughput.

00:07:08.460 --> 00:07:11.060
But just like with caching this can lead to
some tricky problems.

00:07:11.060 --> 00:07:14.110
A big hazard is a dependency in the instructions.

00:07:14.110 --> 00:07:17.759
For example, you might fetch something that
the currently executing instruction is just

00:07:17.759 --> 00:07:21.190
about to modify, which means you’ll end
up with the old value in the pipeline.

00:07:21.190 --> 00:07:25.490
To compensate for this, pipelined processors
have to look ahead for data dependencies,

00:07:25.490 --> 00:07:28.340
and if necessary, stall their pipelines to
avoid problems.

00:07:28.340 --> 00:07:32.720
High end processors, like those found in laptops
and smartphones, go one step further and can

00:07:32.720 --> 00:07:37.840
dynamically reorder instructions with dependencies
in order to minimize stalls and keep the pipeline

00:07:37.840 --> 00:07:40.419
moving, which is called out-of-order execution.

00:07:40.420 --> 00:07:44.240
As you might imagine, the circuits that figure
this all out are incredibly complicated.

00:07:44.240 --> 00:07:49.300
Nonetheless, pipelining is tremendously effective
and almost all processors implement it today.

00:07:49.300 --> 00:07:54.260
Another big hazard are conditional jump instructions
-- we talked about one example, a JUMP NEGATIVE,

00:07:54.270 --> 00:07:55.260
last episode.

00:07:55.270 --> 00:07:59.250
These instructions can change the execution
flow of a program depending on a value.

00:07:59.250 --> 00:08:03.500
A simple pipelined processor will perform
a long stall when it sees a jump instruction,

00:08:03.500 --> 00:08:05.520
waiting for the value to be finalized.

00:08:05.530 --> 00:08:09.600
Only once the jump outcome is known, does
the processor start refilling its pipeline.

00:08:09.610 --> 00:08:13.930
But, this can produce long delays, so high-end
processors have some tricks to deal with this

00:08:13.930 --> 00:08:14.930
problem too.

00:08:14.930 --> 00:08:18.530
Imagine an upcoming jump instruction as a
fork in a road - a branch.

00:08:18.530 --> 00:08:21.960
Advanced CPUs guess which way they are going
to go, and start filling their pipeline with

00:08:21.980 --> 00:08:27.220
instructions based off that guess – a technique
called speculative execution.

00:08:27.220 --> 00:08:31.490
When the jump instruction is finally resolved,
if the CPU guessed correctly, then the pipeline

00:08:31.490 --> 00:08:35.080
is already full of the correct instructions
and it can motor along without delay.

00:08:35.080 --> 00:08:38.790
However, if the CPU guessed wrong, it has
to discard all its speculative results and

00:08:38.790 --> 00:08:42.860
perform a pipeline flush - sort of like when
you miss a turn and have to do a u-turn to

00:08:42.860 --> 00:08:46.280
get back on route, and stop your GPS’s insistent
shouting.

00:08:46.280 --> 00:08:50.770
To minimize the effects of these flushes,
CPU manufacturers have developed sophisticated

00:08:50.770 --> 00:08:54.200
ways to guess which way branches will go,
called branch prediction.

00:08:54.200 --> 00:08:59.420
Instead of being a 50/50 guess, today’s
processors can often guess with over 90% accuracy!

00:08:59.420 --> 00:09:03.430
In an ideal case, pipelining lets you complete
one instruction every single clock cycle,

00:09:03.430 --> 00:09:08.090
but then superscalar processors came along
which can execute more than one instruction

00:09:08.090 --> 00:09:09.080
per clock cycle.

00:09:09.080 --> 00:09:13.140
During the execute phase even in a pipelined design, whole areas of the processor might

00:09:13.140 --> 00:09:14.640
be totally idle.

00:09:14.640 --> 00:09:18.960
For example, while executing an instruction
that fetches a value from memory, the ALU

00:09:18.960 --> 00:09:21.650
is just going to be sitting there, not doing
a thing.

00:09:21.650 --> 00:09:26.390
So why not fetch-and-decode several instructions
at once, and whenever possible, execute instructions

00:09:26.390 --> 00:09:29.640
that require different parts of the CPU all
at the same time!?

00:09:29.640 --> 00:09:32.860
But we can take this
one step further and add duplicate circuitry

00:09:32.860 --> 00:09:34.520
for popular instructions.

00:09:34.520 --> 00:09:39.090
For example, many processors will have four,
eight or more identical ALUs, so they can

00:09:39.090 --> 00:09:42.180
execute many mathematical instructions all
in parallel!

00:09:42.180 --> 00:09:46.220
Ok, the techniques we’ve discussed so far
primarily optimize the execution throughput

00:09:46.220 --> 00:09:50.690
of a single stream of instructions, but another
way to increase performance is to run several

00:09:50.690 --> 00:09:54.170
streams of instructions at once with multi-core
processors.

00:09:54.170 --> 00:09:57.080
You might have heard of dual core or quad
core processors.

00:09:57.080 --> 00:10:01.680
This means there are multiple independent
processing units inside of a single CPU chip.

00:10:01.680 --> 00:10:05.770
In many ways, this is very much like having
multiple separate CPUs, but because they’re

00:10:05.770 --> 00:10:10.600
tightly integrated, they can share some resources,
like cache, allowing the cores to work together

00:10:10.600 --> 00:10:11.890
on shared computations.

00:10:11.890 --> 00:10:16.320
But, when more cores just isn’t enough,
you can build computers with multiple independent

00:10:16.320 --> 00:10:17.320
CPUs!

00:10:17.320 --> 00:10:21.630
High end computers, like the servers streaming
this video from YouTube’s datacenter, often

00:10:21.630 --> 00:10:25.420
need the extra horsepower to keep it silky
smooth for the hundreds of people watching

00:10:25.420 --> 00:10:26.420
simultaneously.

00:10:26.420 --> 00:10:30.000
Two- and four-processor configuration are
the most common right now, but every now and

00:10:30.000 --> 00:10:32.700
again even that much processing power isn’t
enough.

00:10:32.700 --> 00:10:36.940
So we humans get extra ambitious and build
ourselves a supercomputer!

00:10:36.940 --> 00:10:40.420
If you’re looking to do some really monster
calculations – like simulating the formation

00:10:40.420 --> 00:10:43.940
of the universe - you’ll need some pretty
serious compute power.

00:10:43.940 --> 00:10:47.150
A few extra processors in a desktop computer
just isn’t going to cut it.

00:10:47.150 --> 00:10:49.680
You’re going to need a lot of processors.

00:10:49.680 --> 00:10:51.920
No.. no... even more than that.

00:10:51.920 --> 00:10:52.920
A lot more!

00:10:52.920 --> 00:10:56.760
When this video was made, the world’s fastest
computer was located in The National Supercomputing

00:10:56.760 --> 00:10:58.120
Center in Wuxi, China.

00:10:58.120 --> 00:11:06.590
The Sunway TaihuLight contains a brain-melting
40,960 CPUs, each with 256 cores!

00:11:06.590 --> 00:11:11.620
Thats over ten million cores in total... and
each one of those cores runs at 1.45 gigahertz.

00:11:11.620 --> 00:11:17.090
In total, this machine can process 93 Quadrillion
-- that’s 93 million-billions -- floating

00:11:17.090 --> 00:11:20.310
point math operations per second, knows as
FLOPS.

00:11:20.310 --> 00:11:21.850
And trust me, that’s a lot of FLOPS!!

00:11:21.850 --> 00:11:25.670
No word on whether it can run Crysis at max
settings, but I suspect it might.

00:11:25.670 --> 00:11:30.070
So long story short, not only have computer
processors gotten a lot faster over the years,

00:11:30.070 --> 00:11:33.990
but also a lot more sophisticated, employing
all sorts of clever tricks to squeeze out

00:11:33.990 --> 00:11:36.460
more and more computation per clock cycle.

00:11:36.460 --> 00:11:40.580
Our job is to wield that incredible processing
power to do cool and useful things.

00:11:40.580 --> 00:11:44.600
That’s the essence of programming, which
we’ll start discussing next episode.

00:11:44.640 --> 00:11:46.040
See you next week.

