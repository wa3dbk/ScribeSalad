WEBVTT
Kind: captions
Language: en

00:00:03.080 --> 00:00:06.260
Crash Course Philosophy is brought to you
by Squarespace.

00:00:06.261 --> 00:00:08.861
Squarespace: share your passion with the world

00:00:08.861 --> 00:00:11.541
Ok, guys, real talk.
Uhh, I’m kinda worried.

00:00:11.549 --> 00:00:14.140
I think my brother John might be a robot.

00:00:14.140 --> 00:00:17.560
I know, it sounds ridiculous.
He looks like a human. Pretty much.

00:00:17.560 --> 00:00:19.800
And he acts like a human.
Most of the time.

00:00:19.800 --> 00:00:24.680
But how could I really-100-percent-for-sure
know that he is what he looks like?

00:00:24.680 --> 00:00:30.460
At least, without getting a close look at what’s inside him – in his head, his body, his inner workings?

00:00:30.460 --> 00:00:32.440
And keep in mind, I’m the younger brother.

00:00:32.440 --> 00:00:36.200
For all I know, Mom and Dad brought him home
from Radio Shack, not the hospital.

00:00:36.200 --> 00:00:41.700
So. How can I tell whether my brother John Green
is a human, or just a really intelligent machine?

00:00:41.700 --> 00:00:51.880
[Theme Music]

00:00:51.880 --> 00:00:54.739
A couple of weeks ago, we talked about what
it means to be a person.

00:00:54.740 --> 00:01:00.400
But a subject that we need to explore a little better is whether a non-living being, like a robot, could be a person, too.

00:01:00.400 --> 00:01:03.500
This isn’t just a concern for science fiction
writers.

00:01:03.500 --> 00:01:06.670
This issue matters, because technology is
getting better all the time,

00:01:06.670 --> 00:01:11.210
and we need to figure out how we're going
to treat potential new persons,

00:01:11.210 --> 00:01:14.820
if we end up creating beings that we decide
meet the threshold of personhood.

00:01:14.820 --> 00:01:18.810
I'm talking about – robots, androids, replicants,
cylons whatever you call ‘em.

00:01:18.810 --> 00:01:21.030
If you read and watch the right stuff, you
know who I’m talking about.

00:01:21.030 --> 00:01:24.790
Now, you might be thinking: Don't we have artificial intelligence already? Like, on my phone?

00:01:24.790 --> 00:01:29.560
Well, yeah. But the kind of AI that we use to send our texts, proof-read our emails,

00:01:29.560 --> 00:01:32.420
and plot our commutes to work is pretty weak
in the technical sense.

00:01:32.420 --> 00:01:37.189
A machine or system that mimics some aspect
of human intelligence is known as Weak AI.

00:01:37.189 --> 00:01:41.149
Siri is a good example, but similar technology
has been around a lot longer than that.

00:01:41.149 --> 00:01:46.579
Auto-correct, spell-check, even old school calculators are capable of mimicking portions of human intelligence.

00:01:46.579 --> 00:01:51.439
Weak AI is characterized by its relatively
narrow range of thought-like abilities.

00:01:51.440 --> 00:01:56.160
Strong AI, on the other hand, is a machine
or system that actually thinks like us.

00:01:56.160 --> 00:02:01.060
Whatever it is that our brains do, strong AI is an inorganic system that does the same thing.

00:02:01.060 --> 00:02:04.540
While weak AI has been around for a long time,
and keeps getting stronger,

00:02:04.549 --> 00:02:06.860
we have yet to design a system with strong
AI.

00:02:06.860 --> 00:02:10.079
But what would it mean for something to have
strong AI

00:02:10.079 --> 00:02:12.200
Would we even know when it happened?

00:02:12.200 --> 00:02:16.249
Way back in 1950, British mathematician Alan
Turing was thinking about this very question.

00:02:16.249 --> 00:02:23.389
And he devised a test – called the Turing Test – that he thought would be able to demonstrate when a machine had developed the ability to think like us.

00:02:23.389 --> 00:02:28.520
Turing’s description of the test was a product of its time – a time in which there were really no computers, to speak of.

00:02:28.520 --> 00:02:31.980
But if Turing were describing it today, it
would probably go something like this:

00:02:31.980 --> 00:02:34.920
You’re having a conversation, via text,
with two individuals.

00:02:34.920 --> 00:02:38.740
One is a human, and the other is a computer
or AI of some kind.

00:02:38.749 --> 00:02:40.749
And you aren’t told which is which.

00:02:40.749 --> 00:02:43.469
You may ask both of your interlocutors anything
you would like,

00:02:43.469 --> 00:02:46.580
and they are free to answer however they like
– they can even lie.

00:02:46.580 --> 00:02:49.019
Do you think you’d be able to tell which
one was the human?

00:02:49.020 --> 00:02:51.600
How would you tell?
What sort of questions would you ask?

00:02:51.600 --> 00:02:53.640
And what kind of answers would you expect
back?

00:02:53.640 --> 00:02:58.820
A machine with complex enough programming ought to be able to fool you into believing you’re conversing with another human.

00:02:58.820 --> 00:03:04.260
And Turing said, if a machine can fool a human into thinking it's a human, then it has strong AI.

00:03:04.260 --> 00:03:09.760
So in his view, all it means for something to think like us is for it to be able to convince us that it’s thinking like us.

00:03:09.760 --> 00:03:12.519
If we can’t tell the difference, there really
is no difference.

00:03:12.519 --> 00:03:14.510
It's a strictly behavior-based test.

00:03:14.510 --> 00:03:18.339
And if you think about it, isn’t behavior
really the standard we use to judge each other?

00:03:18.340 --> 00:03:20.060
I mean, really, I could be a robot!

00:03:20.069 --> 00:03:22.730
So could these guys who are helping me shoot
this episode.

00:03:22.730 --> 00:03:28.750
The reason I don’t think I’m working with a bunch of androids is that they act the way that I’ve come to expect people to act.

00:03:28.750 --> 00:03:29.999
At least, most of the time.

00:03:29.999 --> 00:03:32.929
And when we see someone displaying behaviors
that seem a lot like ours

00:03:32.929 --> 00:03:35.769
– displaying things like intentionality
and understanding –

00:03:35.769 --> 00:03:38.589
we assume that they have intentionality and
understanding.

00:03:38.589 --> 00:03:43.280
Now, fast-forward a few decades, and meet contemporary American philosopher William Lycan.

00:03:43.280 --> 00:03:49.100
He agrees with Turing on many points, and has the benefit of living in a time when artificial intelligence has advanced like crazy.

00:03:49.100 --> 00:03:53.840
But Lycan recognizes that a lot of people still think that you can make a person-like robot,

00:03:53.840 --> 00:03:57.139
but you could never actually make a robot
that’s a person.

00:03:57.139 --> 00:04:00.790
And for those people, Lycan would offer up
this guy for consideration: Harry.

00:04:00.790 --> 00:04:04.760
Harry is a humanoid robot with lifelike skin.
He can play golf and the viola.

00:04:04.760 --> 00:04:08.520
He gets nervous. He makes love.
He has a weakness for expensive gin.

00:04:08.520 --> 00:04:12.519
Harry, like John Green, gives every impression
of being a person.

00:04:12.520 --> 00:04:16.020
He has intentions and emotions.
You consider him to be your friend.

00:04:16.020 --> 00:04:21.680
So if Harry gets a cut, and then motor oil, rather than blood, spills out, you would certainly be surprised.

00:04:21.680 --> 00:04:28.080
But, Lycan says, this revelation shouldn’t cause you to downgrade Harry’s cognitive state from “person” to “person-like.”

00:04:28.080 --> 00:04:31.040
If you would argue that Harry’s not a person,
then what’s he missing?

00:04:31.040 --> 00:04:34.100
One possible answer is that he’s not a person
because he was programmed.

00:04:34.100 --> 00:04:37.680
Lycan’s response to that is, well, weren’t
we all?

00:04:37.680 --> 00:04:42.360
Each of us came loaded with a genetic code that predisposed us to all sorts of different things –

00:04:42.360 --> 00:04:46.400
you might have a short fuse like your mom,
or a dry sense of humor like your grandfather.

00:04:46.400 --> 00:04:51.800
And in addition to the coding you had at birth, you were programmed in all sorts of other ways by your parents and teachers.

00:04:51.800 --> 00:04:56.080
You were programmed to use a toilet, silverware,
to speak English, rather than Portuguese.

00:04:56.080 --> 00:04:59.800
Unless, of course, you speak Portuguese.
But if you do, you were still programmed.

00:04:59.800 --> 00:05:03.560
And what do you think I’m doing right now?
I’m programming you!

00:05:03.560 --> 00:05:08.740
Sure, you have the ability to go beyond your programming, but so does Harry. That’s Lycan’s point.

00:05:08.740 --> 00:05:14.660
Now another distinction that you might make between persons like us and Harry is that we have souls and Harry doesn’t.

00:05:14.660 --> 00:05:18.980
Now, you’ve probably seen enough Crash Course Philosophy by now to know how problematic this argument is.

00:05:18.980 --> 00:05:23.160
But let’s suppose there is a God, and let’s
suppose that he gave each of us a soul.

00:05:23.169 --> 00:05:26.930
We of course have no idea what the process
of “ensoulment” might look like.

00:05:26.930 --> 00:05:31.800
But suffice it to say, if God can zap a soul
into a fertilized egg or a newborn baby,

00:05:31.800 --> 00:05:35.580
there’s no real reason to suppose he couldn’t
zap one into Harry as well.

00:05:35.580 --> 00:05:39.960
Harry can’t reproduce, but neither can plenty
of humans, and we don’t call them non-persons.

00:05:39.960 --> 00:05:43.700
He doesn’t have blood but, really, do you think that that’s the thing that makes you you?

00:05:43.700 --> 00:05:45.560
Lycan says Harry’s a person.

00:05:45.560 --> 00:05:50.270
His origin and his material constitution are
different than yours and mine, but who cares?

00:05:50.270 --> 00:05:55.289
After all, there have been times and places in which having a different color of skin, or different sex organs,

00:05:55.289 --> 00:06:01.260
has caused someone to be labeled a “non-person,” but we know that kind of thinking doesn't hold up to scrutiny.

00:06:01.260 --> 00:06:04.760
Back in 1950, Turing knew no machine could
pass his test.

00:06:04.760 --> 00:06:06.960
But he thought it would happen by the year
2000.

00:06:06.960 --> 00:06:12.000
It turns out, though, that because we can think outside of our programming in ways that computer programs can't,

00:06:12.000 --> 00:06:15.400
it's been really hard to design a program
that can pass the Turing Test.

00:06:15.400 --> 00:06:17.400
But what will happen when an something can?

00:06:17.400 --> 00:06:22.580
Many argue that, even if a machine does pass the Turing test, that doesn't tell us that it actually has strong AI.

00:06:22.580 --> 00:06:27.000
These objectors argue that there's more to
“thinking like us” than simply being able to fool us.

00:06:27.000 --> 00:06:29.480
Let’s head over to the Thought Bubble for
some Flash Philosophy.

00:06:29.480 --> 00:06:34.980
Contemporary American philosopher John Searle constructed a famous thought experiment called the “Chinese Room,”

00:06:34.980 --> 00:06:39.880
designed to show that passing for human isn’t
sufficient to qualify for strong AI.

00:06:39.880 --> 00:06:42.340
Imagine you're a person who speaks no Chinese.

00:06:42.349 --> 00:06:45.280
You’re locked in a room with boxes filled
with Chinese characters,

00:06:45.280 --> 00:06:50.180
and a code book in English with instructions about what characters to use in response to what input.

00:06:50.180 --> 00:06:53.900
Native Chinese speakers pass written messages,
in Chinese, into the room.

00:06:53.900 --> 00:06:57.840
Using the code book, you figure out how to
respond to the characters you receive,

00:06:57.840 --> 00:07:00.180
and you pass out the appropriate characters
in return.

00:07:00.180 --> 00:07:03.980
You have no idea what any of it means, but
you successfully follow the code.

00:07:03.980 --> 00:07:08.420
You do this so well, in fact, that the native
Chinese speakers believe you know Chinese.

00:07:08.430 --> 00:07:11.030
You’ve passed the Chinese-speaking Turing
Test.

00:07:11.030 --> 00:07:13.640
But do you know Chinese?
Of course not.

00:07:13.640 --> 00:07:17.060
You just know how to manipulate symbols – with
no understanding of what they mean –

00:07:17.069 --> 00:07:20.249
in a way that fools people into thinking you
know something you don't.

00:07:20.249 --> 00:07:26.500
Likewise, according to Searle, the fact that a machine can fool someone into thinking it’s a person doesn't mean it has strong AI.

00:07:26.500 --> 00:07:31.000
Searle argues that strong AI would require
that the machine have actual understanding,

00:07:31.009 --> 00:07:34.439
which he thinks is impossible for a computer
to ever achieve.

00:07:34.440 --> 00:07:36.580
Thanks, Thought Bubble!
One more point before we get out of here.

00:07:36.580 --> 00:07:41.400
Some people have responded to the Chinese Room thought experiment by saying, sure, you don’t know Chinese.

00:07:41.400 --> 00:07:44.880
But, no particular region of your brain knows
English, either.

00:07:44.880 --> 00:07:47.539
The whole system that is your brain knows
English.

00:07:47.539 --> 00:07:51.939
Likewise, the whole system that is the Chinese
Room – you, the code book, the symbols –

00:07:51.940 --> 00:07:57.040
together know Chinese, even though the particular
piece of the system that is you, does not.

00:07:57.040 --> 00:08:01.080
So…I’ve been thinking about it.
I’m still not convinced John isn’t a robot.

00:08:01.080 --> 00:08:06.199
In fact, Harry really drove home the point for me that we don’t actually know what’s going on inside any of us.

00:08:06.199 --> 00:08:09.319
But if it would turn out that John
– the John I’ve known my entire life –

00:08:09.319 --> 00:08:13.620
has motor oil instead of blood inside, well,
he’d still be my brother.

00:08:13.620 --> 00:08:17.650
Today we learned about artificial intelligence,
including weak AI and strong AI,

00:08:17.650 --> 00:08:20.650
and the various ways that thinkers have tried
to define strong AI.

00:08:20.650 --> 00:08:25.070
We considered the Turing Test, and John Searle’s
response to the Turing Test, the Chinese Room.

00:08:25.070 --> 00:08:30.780
We also talked about William Lycan, Harry, and my brother, the still-possibly-but-probably-not android.

00:08:30.780 --> 00:08:35.700
Next time, we’ll look into an issue that has been lurking around this discussion of artificial intelligence:

00:08:35.700 --> 00:08:37.760
do any of us have free will?

00:08:37.760 --> 00:08:40.240
This episode is brought to you by Squarespace.

00:08:40.240 --> 00:08:44.900
Squarespace helps to create websites, blogs
or online stores for you and your ideas.

00:08:44.900 --> 00:08:49.110
Websites look professionally designed regardless
of skill level, no coding required.

00:08:49.110 --> 00:08:53.010
Try Squarespace at squarespace.com/crashcourse
for a special offer.

00:08:53.010 --> 00:08:55.200
Squarespace: share your passion with the world.

00:08:55.200 --> 00:08:58.680
Crash Course Philosophy is produced in association
with PBS Digital Studios.

00:08:58.680 --> 00:09:02.780
You can head over to their channel and check out a playlist of the latest episodes from shows like

00:09:02.780 --> 00:09:06.180
PBS OffBook, The Art Assignment, and Blank
on Blank.

00:09:06.180 --> 00:09:10.140
This episode of Crash Course was filmed in
the Doctor Cheryl C. Kinney Crash Course Studio

00:09:10.140 --> 00:09:15.180
with the help of all of these awesome people and our equally fantastic graphics team is Thought Cafe.

